<!DOCTYPE html>
<html>

<head>
    <title>TEST</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"></script>
</head>

<body>
    <!--  <div class="row">
        <div class="col">
            <a class="btn btn-sm btn-outline-primary" href="/infobook">info book</a>
        </div>
    </div> -->
    <!-- <div class="row">
        <div class="col">
            <a class="btn btn-sm btn-outline-primary" href="/">chapter book</a>
        </div>
    </div> -->
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#tableOfContents" aria-expanded="false" aria-controls="collapseExample">tableOfContents</a>
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#head" aria-expanded="false" aria-controls="collapseExample">head</a>
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#introduction" aria-expanded="false" aria-controls="collapseExample">introduction</a>
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter1" aria-expanded="false" aria-controls="collapseExample">chapter1</a>
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter2" aria-expanded="false" aria-controls="collapseExample">chapter2</a>
     <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter3" aria-expanded="false" aria-controls="collapseExample">chapter3</a>
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter4" aria-expanded="false" aria-controls="collapseExample">chapter4</a>
    <a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter5" aria-expanded="false" aria-controls="collapseExample">chapter5</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter6" aria-expanded="false" aria-controls="collapseExample">chapter6</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter7" aria-expanded="false" aria-controls="collapseExample">chapter7</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter8" aria-expanded="false" aria-controls="collapseExample">chapter8</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter9" aria-expanded="false" aria-controls="collapseExample">chapter9</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter10" aria-expanded="false" aria-controls="collapseExample">chapter10</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter11" aria-expanded="false" aria-controls="collapseExample">chapter11</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter12" aria-expanded="false" aria-controls="collapseExample">chapter12</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter13" aria-expanded="false" aria-controls="collapseExample">chapter13</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter14" aria-expanded="false" aria-controls="collapseExample">chapter14</a>
 	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter15" aria-expanded="false" aria-controls="collapseExample">chapter15</a>
	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter16" aria-expanded="false" aria-controls="collapseExample">chapter16</a>
	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter17" aria-expanded="false" aria-controls="collapseExample">chapter17</a>
	<a class="btn btn-sm btn-outline-primary" data-toggle="collapse" href="#chapter18" aria-expanded="false" aria-controls="collapseExample">chapter18</a>
    <div class="row">
        <div class="col"></div>
        <div class="col-8 collapse" id="tableOfContents"></div>
        <div class="col"></div>
    </div>
    <div class="row">
        <div class="col"></div>
        <div class="col-8 collapse" id="head"></div>
        <div class="col"></div>
    </div>
    <div class="row">
        <div class="col"></div>
        <div class="col-8 collapse" id="introduction"></div>
        <div class="col"></div>
    </div>
    <div class="row" id="part1">
        <div class="col-12">
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter1"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter2"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter3"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter4"></div>
                <div class="col"></div>
            </div>
        </div>
    </div>
    <div class="row" id="part2">
        <div class="col-12">
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter5"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter6"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter7"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter8"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter9"></div>
                <div class="col"></div>
            </div>
        </div>
    </div>
    <div class="row" id="part3">
        <div class="col-12">
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter10"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter11"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter12"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter13"></div>
                <div class="col"></div>
            </div>
        </div>
    </div>
    <div class="row" id="part4">
        <div class="col-12">
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter14"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter15"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter16"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter17"></div>
                <div class="col"></div>
            </div>
            <div class="row">
                <div class="col"></div>
                <div class="col-8 collapse" id="chapter18"></div>
                <div class="col"></div>
            </div>
        </div>
    </div>
    <script>
    document.getElementById('chapter18').innerHTML = 'Chapter 18<br>' +
        'Present Imperatives and Future Outlook<br>' +
        'The endless loop.<br>' +
        'This final chapter is a mix of topics. We begin with a brief<br>' +
        'guide to growing your DW/BI system after you have<br>' +
        'completed your first Lifecycle iteration. Next, we review<br>' +
        'the overall Lifecycle process, including some of the most<br>' +
        'common problems of each phase. Finally, we conclude<br>' +
        'with some of our likes and dislikes of the Microsoft DW/<br>' +
        'BI toolset and a brief wish list of how we hope to see the<br>' +
        'Microsoft BI product strategy and toolset evolve over the<br>' +
        'next few years.<br>' +
        'Growing the DW/BI System<br>' +
        'The DW/BI system is not a single one-time project; it is an<br>' +
        'ongoing, never-ending program. Once you complete an<br>' +
        'iteration of the Lifecycle, it’s time to go back and do it<br>' +
        'again with the next top priority set of data on the bus<br>' +
        'matrix. If you’ve done the first pass right, you will have<br>' +
        'some happy users and evidence that you have provided<br>' +
        'real business value. It’s always a good idea to verify the<br>' +
        'opportunity priority list with senior management before<br>' +
        'you start in on the next row of the bus matrix. It has<br>' +
        'probably been six to nine months or more, especially if this<br>' +
        'was your first round, and priorities may have changed.<br>' +
        'Checking in again helps ensure you’re working on the<br>' +
        'most valuable data set, and it also reminds the business<br>' +
        '1087<br>' +
        'folks how focused you are on business value. They will be<br>' +
        'impressed!<br>' +
        'At the same time you are focused on building the second<br>' +
        'and subsequent iterations, you need to be outwardly<br>' +
        'focused on the connections between the data warehouse<br>' +
        'and the rest of the organization. Marketing is probably the<br>' +
        'wrong term to use for this task because marketing has a<br>' +
        'bad reputation with most technical folks; although not as<br>' +
        'bad as its evil twin, sales. (Just kidding — some of our<br>' +
        'best friends are in sales.) It may be more appealing to view<br>' +
        'the activities in this section as educational efforts. But call<br>' +
        'it what you will — in this “what have you done for me<br>' +
        'lately” world, you must actively and constantly market the<br>' +
        'BI system.<br>' +
        'From an educational perspective, your goal is to make sure<br>' +
        'everyone knows what they need to know about the BI<br>' +
        'system. Management needs to know how their investment<br>' +
        'is going. Specifically, they need to know how it is being<br>' +
        'used to generate value for the organization. It also helps<br>' +
        'them to see how it is being used in different parts of the<br>' +
        'company. Analysts and other knowledge workers need to<br>' +
        'know how they can use the DW/BI system more<br>' +
        'effectively and why it’s important to them. The IT<br>' +
        'organization needs to know what’s going on with the DW/<br>' +
        'BI system. You need close working relationships with the<br>' +
        'source system managers on the input side of the data<br>' +
        'warehouse, and with other information-driven systems on<br>' +
        'the output side of the DW/BI system.<br>' +
        'Fortunately, you have some quantitative and qualitative<br>' +
        'tools to help educate all these groups. On the quantitative<br>' +
        '1088<br>' +
        'side, you can turn to your report and query usage<br>' +
        'monitoring systems. You should be able to generate some<br>' +
        'reports from these systems that show how DW/BI system<br>' +
        'usage is growing over time, in terms of the number of<br>' +
        'users you support, the number of departments they come<br>' +
        'from, and the number of queries they generate on a daily<br>' +
        'basis.<br>' +
        'Qualitative measures are a bit harder to come by. You need<br>' +
        'to go out and talk to your users to find out what kinds of<br>' +
        'analyses they’ve done and what impact it has had on the<br>' +
        'organization. We like to describe impact in specific terms,<br>' +
        'such as a dollar increase in revenue, or dollars of expense<br>' +
        'reduction.<br>' +
        'At the risk of sounding like a scratched CD, your<br>' +
        'long-term success will largely be determined by how well<br>' +
        'you identify specific, high-value business opportunities<br>' +
        'and then deliver them.<br>' +
        'RESOURCES<br>' +
        'For additional guidance on marketing your<br>' +
        'DW/BI system, search the KimballGroup.com<br>' +
        'website for the following: “Educate<br>' +
        'management.”<br>' +
        'Lifecycle Review with Common Problems<br>' +
        'We can’t resist showing you the Kimball Lifecycle<br>' +
        'drawing one last time. This time, we’ve grouped the<br>' +
        '1089<br>' +
        'Lifecycle task boxes into phases that are slightly different<br>' +
        'from the major sections of the book. These phases are:<br>' +
        '• Requirements, realities, architecture, and design<br>' +
        '• Setting up the hardware and software, and developing the<br>' +
        'databases<br>' +
        '• Developing the BI applications and portal environment<br>' +
        '• Deploying and managing the DW/BI system<br>' +
        'These phases (see Figure 18-1) are essentially linear, with<br>' +
        'each phase building on the previous one. As we review<br>' +
        'each phase, we’ll list the most common errors made by<br>' +
        'DW/BI teams.<br>' +
        'Figure 18-1: The four phases of the Kimball Lifecycle<br>' +
        'Phase I — Requirements, Realities, Plans, and Designs<br>' +
        'Phase I involves understanding and prioritizing the<br>' +
        'business requirements, creating the system architecture,<br>' +
        '1090<br>' +
        'and designing the business process dimensional model<br>' +
        'needed to meet the top-priority requirements. The BI<br>' +
        'applications specification step is also part of this design<br>' +
        'phase, although we didn’t actually describe it until Chapter<br>' +
        '10.<br>' +
        'The biggest problem we see in the projects we get called<br>' +
        'into is that the DW/BI team essentially skipped Phase I.<br>' +
        'Other than doing some project planning around system<br>' +
        'development tasks, they dove right into developing the<br>' +
        'databases. This haste leads to unnecessary pain and<br>' +
        'suffering and is often fatal to the project. A good way to<br>' +
        'tell if you’re headed in the wrong direction is that the<br>' +
        'technology involved in Phase I should be limited to a<br>' +
        'project management tool, a word processor, a presentation<br>' +
        'tool, a modeling tool, and a spreadsheet. If you’re<br>' +
        'installing server machines or SQL Server at this point,<br>' +
        'you’re getting ahead of yourself. The only reason to do this<br>' +
        'is if you need to work through the tutorials.<br>' +
        'After skipping the requirements step, the next most<br>' +
        'common problems in Phase I are failing to secure business<br>' +
        'sponsorship, and failing to take responsibility for the full<br>' +
        'solution, including the BI applications and portal. We hope<br>' +
        'that a business sponsor has been a member of your team<br>' +
        'from the very beginning. Ideally this sponsor should be a<br>' +
        'sophisticated observer of the development process along<br>' +
        'with you and should appreciate the need to do midcourse<br>' +
        'corrections and frequent recalibrations of how well the<br>' +
        'system addresses critical business issues.<br>' +
        'Other oversights that will raise their ugly heads later,<br>' +
        'during the ETL development task, are the failure to<br>' +
        '1091<br>' +
        'identify and investigate data quality issues, and the related<br>' +
        'failure to set up a data governance program to work with<br>' +
        'the source system organizations and deal with any<br>' +
        'problems early on.<br>' +
        'Finally, a problem that arises when implementing the<br>' +
        'second and subsequent data sources in the DW/BI system<br>' +
        'is the need to provide integration, especially in the form of<br>' +
        'conformed dimensions.<br>' +
        'Phase II — Developing the Databases<br>' +
        'Phase II is the hard, systems-oriented work of designing<br>' +
        'and developing the ETL systems, the DW/BI relational<br>' +
        'databases, and the Analysis Services databases. This is the<br>' +
        'comfort zone for most DW/BI teams. It’s where you<br>' +
        'wrestle with the tough technical issues, such as how to<br>' +
        'handle changes in various attributes, or how to re-create<br>' +
        'historical facts. Every decision, every design tradeoff in<br>' +
        'Phase II, must weigh the development effort against the<br>' +
        'business requirements identified in Phase I. Without those<br>' +
        'requirements, the design decisions are based on technical<br>' +
        'expediency. Statements like “The eight-character Product<br>' +
        'description is fine — it’s always worked in the source<br>' +
        'system and it will save a lot of space,” and “We’ll save a<br>' +
        'lot of time if we include only the base numbers; the users<br>' +
        'can create any calculations they like on the fly” are<br>' +
        'warning flags that your developers are making decisions<br>' +
        'that will undermine the ultimate acceptance and success of<br>' +
        'the system. These statements are much harder to make<br>' +
        'when the primary goal of the DW/BI team is to meet a set<br>' +
        'of clearly defined user requirements.<br>' +
        '1092<br>' +
        'The most common problem we see in Phase II involves<br>' +
        'underestimating the effort required to extract, clean,<br>' +
        'transform, and load the required data. This is typically the<br>' +
        'result of not identifying a high priority business<br>' +
        'opportunity that narrowly bounds the Lifecycle iteration.<br>' +
        'Another common cause is doing a poor job of digging into<br>' +
        'the dimensional model design, and not uncovering data<br>' +
        'quality problems early on. If your requirements and design<br>' +
        'phase is light, the development phase will always be worse<br>' +
        'than it initially appeared.<br>' +
        'It’s also not unusual for a DW/BI team to think of Phase II<br>' +
        'as the complete project. These teams are doomed to fail<br>' +
        'because they don’t do the upfront planning and design<br>' +
        'work in Phase I, or provide the user access tools in Phase<br>' +
        'III. Although Phase II is where the hard technical<br>' +
        'challenges are met and overcome, these technical<br>' +
        'challenges are seldom the point of failure. Missing the<br>' +
        'underlying business requirements, and therefore not<br>' +
        'delivering real value to the organization, is the root cause<br>' +
        'of almost every DW/BI system failure.<br>' +
        'Phase III — Developing the BI Applications and Portal<br>' +
        'Environment<br>' +
        'Building the BI applications is the fun part of the Lifecycle<br>' +
        '(well, for some of us, it’s all fun). You get to play with the<br>' +
        'data, building reports and other applications that you can<br>' +
        'show to the business users and get immediate feedback on.<br>' +
        'The technology is pretty easy and straightforward —<br>' +
        'although not without its frustrations — and the<br>' +
        'development process usually goes quite swiftly if you did a<br>' +
        'good job in Phase I. Even if you need to develop complex<br>' +
        '1093<br>' +
        'analytic applications, perhaps including data mining<br>' +
        'technology, this is generally easier and more fun than<br>' +
        'slogging through the mountains of bad data that you<br>' +
        'uncover when building the ETL system.<br>' +
        'It’s also common for a team to omit the BI applications<br>' +
        'from its project. This is a bad idea. First of all, why cut out<br>' +
        'the fun and rewarding piece of the project? Second and<br>' +
        'most important, if you don’t pave the path to the door of<br>' +
        'the data warehouse, only a few hardy souls will make the<br>' +
        'trek. The other risk is to start designing the BI applications<br>' +
        'too late, or without involving the business users. Getting<br>' +
        'early user input on the BI applications will help you<br>' +
        'validate your design and allow you to make relatively<br>' +
        'minor adjustments to the DW/BI system that can really<br>' +
        'please the business users.<br>' +
        'Phase IV — Deploying and Managing the DW/BI System<br>' +
        'The efforts in Phase IV revolve around the testing,<br>' +
        'training, and support needed to reach an all-systems-are-go<br>' +
        'state. This involves making sure the system works as<br>' +
        'promised, the data is correct, the users are properly trained<br>' +
        'and prepared, the system is documented, the standard<br>' +
        'reports are working and are correct, support is available,<br>' +
        'and deployment and maintenance procedures and scripts<br>' +
        'are in place and tested.<br>' +
        'The biggest problem in Phase IV comes when the team<br>' +
        'views its primary goal as delivering technology rather than<br>' +
        'a solution. In this case, most of the user-oriented work in<br>' +
        'Phase IV is seen as “not our job” or unnecessary. The team<br>' +
        'defines success as making the database available. But if<br>' +
        '1094<br>' +
        'the goal is to meet the business requirements, all of the<br>' +
        'pieces in Phase IV are crucial links in the chain. Omit any<br>' +
        'piece and the chain will break. The team must view<br>' +
        'success as delivering real, measurable, substantial business<br>' +
        'value.<br>' +
        'Another common problem in Phase IV is associated with<br>' +
        'underestimating the effort required to fully test and<br>' +
        'maintain the DW/BI system, and to start the planning for<br>' +
        'ongoing operations too late in the development cycle. For<br>' +
        'example, your strategy for backing up each day’s extract is<br>' +
        'inextricably linked to the ETL system. If you don’t think<br>' +
        'about this issue until the system is developed and ready for<br>' +
        'deployment, your maintenance plan may be awkward or<br>' +
        'weak.<br>' +
        'One of the challenges in Phase IV is correctly estimating<br>' +
        'the effort required to build out a full solution, including the<br>' +
        'documentation, support, and delivery portal.<br>' +
        'Quality-assuring the data in the DW/BI system is often<br>' +
        'another issue, and it takes a lot of time to do right. If you<br>' +
        'haven’t already gotten the business users involved with<br>' +
        'this data governance process, you need to do it now. They<br>' +
        'need to have full confidence in the data, and what better<br>' +
        'way than to have helped with the testing? Besides,<br>' +
        'sometimes deep business knowledge is needed to<br>' +
        'determine what the business rules should be, and whether<br>' +
        'the data truly is accurate.<br>' +
        'Iteration and Growth<br>' +
        'Extending the DW/BI system is about adding new business<br>' +
        'process dimensional models to the databases, adding new<br>' +
        '1095<br>' +
        'users, and adding new BI applications. In short, it’s about<br>' +
        'going back through the Lifecycle again and again,<br>' +
        'incrementally filling in the bus matrix to build a solid,<br>' +
        'robust enterprise information infrastructure. One or more<br>' +
        'of these new business process dimensional models may<br>' +
        'require data that’s near real-time. As we discussed in<br>' +
        'Chapter 9, including real-time data in the DW/BI system<br>' +
        'presents some interesting technical and design challenges.<br>' +
        'The two main challenges in growing the DW/BI system<br>' +
        'present an interesting paradox. Often, the success of the<br>' +
        'first round leads to too much demand and the DW/BI team<br>' +
        'must carefully balance these demands and maintain an<br>' +
        'achievable scope based on prioritized business<br>' +
        'requirements. This may also involve securing additional<br>' +
        'resources and revisiting priorities with senior management.<br>' +
        'At the same time, the DW/BI team must begin an ongoing<br>' +
        'education program to make sure the organization<br>' +
        'understands and takes advantage of the incredible asset<br>' +
        'that is the DW/BI system. In the age of what have you<br>' +
        'done for me lately, the DW/BI team needs to have a<br>' +
        'detailed, compelling, ongoing answer.<br>' +
        'What We Like in the Microsoft BI Toolset<br>' +
        'The appeal of a single source technology provider, like<br>' +
        'Microsoft with SQL Server, is that it makes the process of<br>' +
        'building a DW/BI system easier in several ways. First,<br>' +
        'many elements of the architecture are predefined. The<br>' +
        'major technology issues you need to tackle involve data<br>' +
        'sizing, server configurations, and performance, rather than<br>' +
        'which products to buy and whether they work well<br>' +
        '1096<br>' +
        'together. Some organizations may need to develop or buy<br>' +
        'functionality to meet specific business requirements, such<br>' +
        'as large-scale consumer name and address matching. Many<br>' +
        'organizations will also want to add one or more<br>' +
        'third-party, user-oriented query and reporting tools to the<br>' +
        'mix.<br>' +
        'The Microsoft toolset includes credible versions of all the<br>' +
        'tools you need to build and deliver a solid, viable data<br>' +
        'warehouse and business intelligence system. Some<br>' +
        'components of the SQL Server architecture are more than<br>' +
        'credible: Analysis Services, for example, is one of the top<br>' +
        'OLAP engines available.<br>' +
        'Many of the tools are designed specifically to support<br>' +
        'dimensional data warehouses. For example, Integration<br>' +
        'Services has the Slowly Changing Dimension transform,<br>' +
        'and Analysis Services is built with dimensional constructs<br>' +
        'from the ground up. Even the relational database offers<br>' +
        'star join optimization as an Enterprise Edition feature.<br>' +
        'The tools are open and programmable. If you want to build<br>' +
        'a heterogeneous DW/BI solution, you can swap out any<br>' +
        'component. If you want to build a fully automated DW/BI<br>' +
        'management system, you can script any operation in<br>' +
        'practically any programming language you wish.<br>' +
        'The overall Microsoft BI toolset includes software beyond<br>' +
        'SQL Server. This book focuses mostly on SQL Server<br>' +
        'because it provides the core DW/BI components. But BI<br>' +
        'functionality is spreading rapidly across the Microsoft<br>' +
        'product line. At the desktop level, Excel is extremely<br>' +
        'popular for accessing, manipulating, and presenting data.<br>' +
        '1097<br>' +
        'This popularity continues to grow with each release of<br>' +
        'Office, and with additional tools such as PowerPivot for<br>' +
        'Excel. Business users love Excel, and that’s where they<br>' +
        'want their data to end up. Even Reporting Services makes<br>' +
        'it easy for a business user to save a report to Excel.<br>' +
        'Beyond Office, SharePoint has its roots in the web portal<br>' +
        'space, but is becoming the applications delivery platform<br>' +
        'for the Microsoft-based enterprise. This adds a new layer<br>' +
        'of functionality and complexity to the Microsoft BI story.<br>' +
        'Future Directions: Room for Improvement<br>' +
        'There are organizations dedicated to trying to figure out<br>' +
        'what Microsoft is going to do next. We have no interest in<br>' +
        'playing that game, so rather than trying to predict the<br>' +
        'future, we’ll highlight a few of the areas we’d like to see<br>' +
        'Microsoft improve on, starting with tools and<br>' +
        'functionality, and ending with direction and strategy.<br>' +
        'Query Tools<br>' +
        'Excel, despite being the most popular data tool on the<br>' +
        'market, is not the ideal query and reporting tool. There are<br>' +
        'several problems, the most troubling of which is that Excel<br>' +
        'is fundamentally a two-dimensional grid. It’s hard for us to<br>' +
        'imagine how the Excel team will address this issue without<br>' +
        'creating a whole new product, or breaking the existing<br>' +
        '(hugely valuable) product. Lucky for us, this isn’t our<br>' +
        'problem.<br>' +
        'The existing query interfaces for Excel are imperfect, too.<br>' +
        'Queries from Excel into the Analysis Services database are<br>' +
        '1098<br>' +
        'limited. The mechanism for specifying a relational query<br>' +
        'within Excel is archaic. We detested it in the early 1990s<br>' +
        'when we first saw it, and it hasn’t improved since then.<br>' +
        'PowerPivot’s use of the PivotTable construct with the<br>' +
        'added ability to define calculated fields is a leap forward in<br>' +
        'terms of creating reports with flexible content in the Excel<br>' +
        'environment. Unfortunately, it brings with it the need to<br>' +
        'create a PowerPivot database along with the report.<br>' +
        'Report Builder 3.0 is a big step forward in terms of ad hoc<br>' +
        'query capabilities for the relational engine. It has<br>' +
        'reasonable display functionality, including mapping, and a<br>' +
        'crude but manageable interface for defining data sets. It’s<br>' +
        'still a rudimentary query tool in terms of defining more<br>' +
        'real-world queries and reports. Many relatively simple<br>' +
        'questions require you to drop out of the GUI and write<br>' +
        'your own SQL statements; this is what we did 25 years<br>' +
        'ago. It feels like we should be able to expect a bit more at<br>' +
        'this point.<br>' +
        'We were disappointed with Microsoft’s treatment of the<br>' +
        'ProClarity query tool for Analysis Services. Some<br>' +
        'components, such as the decomposition tree, have found<br>' +
        'their way into other tools, but the desktop query tool is<br>' +
        'dying a slow, painful death. Analysis Services needs a<br>' +
        'strong desktop query and report definition tool that allows<br>' +
        'users to build ad hoc queries directly against an Analysis<br>' +
        'Services database. It needs to offer a decent user interface,<br>' +
        'good flexibility, and generate well-formed MDX. And, it<br>' +
        'needs to be part of the core SQL Server toolset. Customers<br>' +
        'should not have to go buy the equivalent of a steering<br>' +
        'wheel from a third-party company after they bought the<br>' +
        'rest of the DW/BI car from Microsoft.<br>' +
        '1099<br>' +
        'Metadata<br>' +
        'Microsoft’s SQL Server metadata management story<br>' +
        'hasn’t changed since the first edition of this book came out<br>' +
        'with SQL Server 2005. The Microsoft toolset is full of<br>' +
        'metadata, as we discussed in Chapter 15. But the metadata<br>' +
        'systems don’t talk to each other: It’s a bunch of metadata<br>' +
        'islands with a few tenuous bridges thrown across between<br>' +
        'them. At the time of this writing, it’s your job to build or<br>' +
        'buy a coherent metadata bridge.<br>' +
        'The lack of integrated metadata hasn’t prevented<br>' +
        'Microsoft’s past customers from successfully<br>' +
        'implementing a DW/BI system — or else they’d have<br>' +
        'demanded a solution in this version of the toolset. Indeed,<br>' +
        'through the years we’ve seen very few good metadata<br>' +
        'implementations on any platform. But because Microsoft<br>' +
        'owns the entire toolset, they should find it easier to provide<br>' +
        'an innovative, interesting, and valuable solution for<br>' +
        'managing and integrating metadata than is possible with a<br>' +
        'heterogeneous architecture. We hope they decide to<br>' +
        'leverage this opportunity soon. We hear the next release is<br>' +
        'going to be great.<br>' +
        'Relational Database Engine<br>' +
        'The relational database engine is primarily designed to<br>' +
        'support a transaction load. We can’t comment on its<br>' +
        'advantages and disadvantages in that role. From a DW/BI<br>' +
        'point of view, we find several things puzzling or<br>' +
        'frustrating.<br>' +
        '1100<br>' +
        'Ad hoc query optimization is inconsistent. Mostly, the<br>' +
        'query optimizer does a good job with ad hoc queries<br>' +
        'against a dimensional model, and the relational database<br>' +
        'engine performs extremely well. But its performance is<br>' +
        'variable. For some queries, the optimizer takes a path that<br>' +
        'is clearly suboptimal. And the nature of ad hoc queries<br>' +
        'makes adding optimizer hints a nonviable solution. There<br>' +
        'are times, for example, when you will get better<br>' +
        'performance by directly constraining the date field in the<br>' +
        'fact table rather than constraining the date dimension. The<br>' +
        'Analysis Services query optimizer does a much more<br>' +
        'consistent job of resolving dimensional queries, even if<br>' +
        'you strip away its advantage of pre-computed<br>' +
        'aggregations. Why can’t the relational database engine<br>' +
        'perform at the same level?<br>' +
        'We’re thrilled to have true partitioning in the relational<br>' +
        'engine, so we don’t want to seem like complainers, but<br>' +
        'managing relational partitions is a headache, especially<br>' +
        'when you have a rolling set of partitions to maintain over<br>' +
        'time. In Chapter 5 we walked through the periodic process<br>' +
        'of managing partitions. All the tools are there, but it should<br>' +
        'be an order of magnitude easier to manage partitions than<br>' +
        'it is.<br>' +
        'Analysis Services<br>' +
        'Analysis Services can be a bit overwhelming. The wizards<br>' +
        'are helpful, but they still leave you with a lot of hand work<br>' +
        'to do in an environment that’s necessarily complex. We<br>' +
        'certainly wouldn’t expect any but the most intrepid power<br>' +
        'users to succeed at developing their own cubes from<br>' +
        'scratch. Granted, Analysis Services is targeted not at this<br>' +
        '1101<br>' +
        'market but at the enterprise DW/BI system. PowerPivot is<br>' +
        'an implicit acknowledgement of this cube building<br>' +
        'complexity. PowerPivot does allow analysts to throw<br>' +
        'together a cube in order to explore a specific analytic<br>' +
        'problem, but those cubes are too simple to capture the data<br>' +
        'complexities found in most organizations. Plus, the<br>' +
        'resulting PowerPivot cubes are not standard Analysis<br>' +
        'Services cubes, and cannot be directly queried from across<br>' +
        'the DW/BI environment except in a crude way through<br>' +
        'SharePoint.<br>' +
        'The other big flaw we see in Analysis Services is its<br>' +
        'inability to respond to SQL queries. It can handle only the<br>' +
        'very simplest SQL — syntax too simple to be useful.<br>' +
        'Although MDX is superior to SQL for analytics, people<br>' +
        'with expertise and investment in SQL and SQL-based<br>' +
        'tools are reluctant to move to Analysis Services. It seems<br>' +
        'unrealistic to expect the world to move to MDX-based<br>' +
        'tools in order to take advantage of even the most basic<br>' +
        'Analysis Services functionality.<br>' +
        'Master Data Services<br>' +
        'Master Data Services, described in Chapter 6, is an<br>' +
        'interesting addition to the SQL Server toolset. Its focus<br>' +
        'and use cases are a bit fuzzy in SQL Server 2008 R2,<br>' +
        'which is its first release. Is it a grown-up master data<br>' +
        'management toolset, designed to integrate and<br>' +
        'inter-operate with your transaction systems? Or is it a tool<br>' +
        'to help you build a system to help your data stewards<br>' +
        'implement good data governance procedures? Currently,<br>' +
        'the tool appears to be trying to meet both needs, without<br>' +
        'complete success.<br>' +
        '1102<br>' +
        'The master data management scenario seems to get most<br>' +
        'of the focus of the documentation and white papers. But so<br>' +
        'much is missing from the documentation that it’s hard to<br>' +
        'imagine anyone attempting to use the tool in that way<br>' +
        'without significant participation from Microsoft.<br>' +
        'The data governance tack is a much simpler problem, and<br>' +
        'it seems to be well addressed by the structure and features<br>' +
        'of MDS. Of course, the user interface in this first version is<br>' +
        'awkward, but we can be reasonably confident that<br>' +
        'Microsoft knows how to fix that problem. The<br>' +
        'documentation is probably the greatest barrier to the<br>' +
        'success of MDS. It really is very difficult for someone to<br>' +
        'walk up to the tool and figure out what to do with it.<br>' +
        'Integration<br>' +
        'The greatest problem with the Microsoft BI toolset,<br>' +
        'underlying the criticisms we’ve already discussed, is<br>' +
        'integration — or, more accurately, the lack of integration.<br>' +
        'The various components of even the SQL Server BI tools<br>' +
        '— the relational database, Analysis Services, Integration<br>' +
        'Services, and Reporting Services — are clearly built by<br>' +
        'different groups. And other Microsoft technologies outside<br>' +
        'that core set, like Office and SharePoint, appear to be<br>' +
        'developed by different companies. This results in the same<br>' +
        'class of functionality being developed with different<br>' +
        'paradigms and interfaces to solve the same problem in<br>' +
        'multiple places. The various versions of a query interface<br>' +
        'are a good example of this. The six or more different ways<br>' +
        'you can define a data model in SQL Studio, Visual Studio,<br>' +
        'Analysis Services, Report Builder, Visio, Access, and<br>' +
        'PowerPivot are another good example.<br>' +
        '1103<br>' +
        'We understand why this situation occurs; it is essentially a<br>' +
        'political and organizational problem. However, we believe<br>' +
        'it is past time for a more holistic view of business<br>' +
        'intelligence at Microsoft. We’re not holding our breath on<br>' +
        'its resolution.<br>' +
        'Customer Focus<br>' +
        'Microsoft has brought DW/BI to the small to medium<br>' +
        'sized businesses that have long been its primary customer<br>' +
        'base. In order to grow, Microsoft has worked to gain<br>' +
        'credibility with larger organizations by focusing on key<br>' +
        'features that are important to scale up the system and play<br>' +
        'with the big players. Microsoft is caught between a rock<br>' +
        'and a hard place on this issue. This fundamental shift in<br>' +
        'the nature of the product is making it more complex, and<br>' +
        'more expensive. This may be acceptable to the major<br>' +
        'corporations, but it’s tough for the small to medium sized<br>' +
        'organizations out there. Deciding how to allocate features<br>' +
        'and limitations to editions, and how to price those editions,<br>' +
        'must be a difficult task. We encourage Microsoft to make<br>' +
        'sure it does not squeeze out the smaller organizations that<br>' +
        'may need these capabilities, but cannot afford enterprise<br>' +
        'edition prices. After all, it’s these organizations that helped<br>' +
        'make Microsoft successful.<br>' +
        'Conclusion<br>' +
        'There you have it. We’ve done our best to teach you how<br>' +
        'to build a successful business intelligence system and its<br>' +
        'underlying data warehouse using the Microsoft SQL<br>' +
        'Server product set. Now it’s up to you to get out there and<br>' +
        'do the work of actually making it happen.<br>' +
        '1104<br>' +
        'As fun as it is to criticize Microsoft, its SQL Server DW/<br>' +
        'BI toolset contains the features necessary to build a<br>' +
        'complete DW/BI system. The tools are relatively easy to<br>' +
        'use, and will scale from small operations like the<br>' +
        'hypothetical Adventure Works Cycles to large enterprises<br>' +
        'with significant data volumes. The smaller<br>' +
        'implementations can rely heavily on wizards, and not<br>' +
        'worry too much about all the technical details. Large<br>' +
        'systems may use the wizards to get started, but will need to<br>' +
        'dig far deeper into the products.<br>' +
        'Microsoft is delivering the technology you need, but you<br>' +
        'are the ones who will put that technology to use. If you can<br>' +
        'maintain your focus on the needs of the business users and<br>' +
        'on adding business value, you should be able to build a<br>' +
        'great DW/BI system.<br>' +
        'Good luck!<br>' +
        '1105<br>';
    document.getElementById('chapter17').innerHTML = 'Chapter 17<br>' +
        'Operations and Maintenance<br>' +
        '“To administer is to govern: to govern is to reign.”<br>' +
        '— Honor Gabriel Riquet<br>' +
        'We’ve seen too many DW/BI teams postpone thinking<br>' +
        'about how to operate their new system until it’s nearly in<br>' +
        'production. When deadlines are looming and users are<br>' +
        'clamoring for data and reports, it’s too late to start<br>' +
        'designing your operating procedures. You’ll be making<br>' +
        'stuff up as you go along, and you’ll make mistakes.<br>' +
        'There are two major sets of issues to think about with<br>' +
        'respect to the ongoing operations of your system. The first<br>' +
        'set of issues revolves around communicating with,<br>' +
        'training, and supporting the users. Of course you’ll be<br>' +
        'publishing reports to them about the business, but you also<br>' +
        'need to communicate with them about the DW/BI system<br>' +
        'itself.<br>' +
        'The second set of issues focuses on technical systems<br>' +
        'management. You need to think, long before you go into<br>' +
        'production, about a host of issues. Your decisions about<br>' +
        'these operational issues will affect your system<br>' +
        'configuration and design. These issues include monitoring<br>' +
        'performance and usage, automating operations, and<br>' +
        'managing resources for ad hoc use.<br>' +
        '1036<br>' +
        'At launch your system’s performance might be great, but<br>' +
        'with increased data volumes and user load, performance<br>' +
        'might degrade. A solid monitoring plan, implemented from<br>' +
        'the start, is your best weapon for identifying and solving<br>' +
        'bottlenecks. With the right information you can<br>' +
        'continuously tune the system so bottlenecks are solved<br>' +
        'before users even notice them.<br>' +
        'Finally — but very important — you need to plan for,<br>' +
        'implement, and test your backup and recovery strategy.<br>' +
        'The now-familiar Business Dimensional Lifecycle diagram<br>' +
        '(see Figure 17-1) places operational issues at the end of the<br>' +
        'Lifecycle where you loop back around on the next<br>' +
        'iteration. Operationally that’s accurate, but as we discuss<br>' +
        'throughout this chapter, you need to be planning for safe<br>' +
        'operations from the outset.<br>' +
        'Figure 17-1: The Business Dimensional Lifecycle<br>' +
        'In this chapter, you’ll find answers to the following<br>' +
        'questions:<br>' +
        '1037<br>' +
        '• What do you need to worry about with respect to<br>' +
        'maintaining and extending the BI portal and BI applications?<br>' +
        '• How do you monitor the system? What kinds of counters<br>' +
        'and events should you track? How can you see what users<br>' +
        'are doing right now, and kill bad queries?<br>' +
        '• How do you conduct performance tuning of a DW/BI<br>' +
        'system?<br>' +
        '• What data do you need to back up and how often? How<br>' +
        'should you perform backup and recovery?<br>' +
        '• How do you execute Integration Services packages in<br>' +
        'production?<br>' +
        'Providing User Support<br>' +
        'The design and development of your DW/BI system has<br>' +
        'focused on building a system that’s easy to use, intuitive,<br>' +
        'and performs well. You’ve built BI applications, be they<br>' +
        'canned reports, a dashboard portal, a closed loop data<br>' +
        'mining application, or a combination of all. You’ve trained<br>' +
        'the users on the data and applications. What’s left to do? A<br>' +
        'lot.<br>' +
        'The business user community will grow and change. Some<br>' +
        'users will learn the new tools immediately. Others will<br>' +
        'require some hand-holding and retraining. Even if your<br>' +
        'system is perfect and the business users catch on<br>' +
        'immediately, you’ll need to train new employees. On an<br>' +
        'ongoing basis, it’s common to have the same number of<br>' +
        'people involved with supporting the business users as<br>' +
        'initially developed the system.<br>' +
        'The user-facing part of the DW/BI team engages in the<br>' +
        'following activities:<br>' +
        '• BI portal maintenance and enhancement<br>' +
        '1038<br>' +
        '• BI application specification and development<br>' +
        '• BI Help Desk support and training, discussed in Chapter 16<br>' +
        'We’ve seen the user-facing side of the DW/BI team range<br>' +
        'in size from dozens of people for large companies with a<br>' +
        'centralized DW/BI system, to a single person. The smallest<br>' +
        'organizations often have a single-person shop, but it’s<br>' +
        'really hard for that person to handle the back-room<br>' +
        'maintenance while communicating effectively with the<br>' +
        'business users. Ongoing, it’s hard to see how you can get<br>' +
        'by with fewer than two people: one for the back room, and<br>' +
        'one for the front room.<br>' +
        'NOTE<br>' +
        'Many CIOs and IT managers imagine that<br>' +
        'the DW is like a normal project: heavily<br>' +
        'staffed during development, but requiring<br>' +
        'only 1–2 people once it’s in operation. Not<br>' +
        'so! The front room team, often called the<br>' +
        'BI team, requires a lot of resources. So<br>' +
        'often we see technically sound DW/BI<br>' +
        'systems that are underutilized because the<br>' +
        'ongoing BI team is too small to be<br>' +
        'effective. A good rule of thumb is to expect<br>' +
        'the overall DW/BI team to be as large in<br>' +
        'production as in development, but<br>' +
        'resources shift from back room<br>' +
        'development (DW) to front room user<br>' +
        'support (BI).<br>' +
        '1039<br>' +
        'Although we refer to an integrated DW/BI<br>' +
        'team throughout this book, the BI team<br>' +
        'sometimes reports in to the business. We<br>' +
        'generally prefer a single team, because the<br>' +
        'simpler organizational structure ensures<br>' +
        'better communication between the front<br>' +
        'room and the back room. There are,<br>' +
        'however, advantages to having the BI team<br>' +
        'report to the business. Their presence<br>' +
        'embedded in the user community increases<br>' +
        'user buy-in and improves DW credibility,<br>' +
        'as long as the two teams communicate as if<br>' +
        'they were a single virtual team.<br>' +
        'Maintaining the BI Portal<br>' +
        'Even the smallest DW/BI team should maintain an intranet<br>' +
        'site where users get information about the system. In<br>' +
        'Chapter 12 we discussed how to create a portal to host the<br>' +
        'reports from Chapter 10; in Chapter 15 we described the<br>' +
        'metadata that should be published on the portal; and in<br>' +
        'Chapter 16 we talked about documentation and training<br>' +
        'materials that should be hosted on the site.<br>' +
        'Here we briefly present an additional set of information<br>' +
        'that should go on the portal, having to do with operations<br>' +
        'and maintenance:<br>' +
        '• System status — what is the most recent day of data in each<br>' +
        'business process dimensional model<br>' +
        '• Schedules of planned outages<br>' +
        '1040<br>' +
        '• Honest and complete status of any unplanned outages or data<br>' +
        'quality problems<br>' +
        '• The system’s current operational status, including:<br>' +
        '• How many queries have been answered in the last<br>' +
        'hour or day<br>' +
        '• How many reports were generated<br>' +
        '• Current number of active users<br>' +
        '• On-demand report of active queries, how long<br>' +
        'they’ve been running, and who’s running them<br>' +
        '• Proof points about the system’s capacity, to foster user<br>' +
        'confidence in the system:<br>' +
        '• How long the last data load took<br>' +
        '• How big the largest data load was, and how long it<br>' +
        'took<br>' +
        '• Maximum count of simultaneous users<br>' +
        'Every 12 to 18 months, you should review the entire DW/<br>' +
        'BI system. Evaluate what’s working well for the users, and<br>' +
        'what should change. Remember, change is inevitable, and<br>' +
        'is a sign of a healthy system. As part of this periodic<br>' +
        'evaluation, consider refreshing the look, layout, and<br>' +
        'content of the BI portal.<br>' +
        'Extending the BI Applications<br>' +
        'The initial reports and BI applications for a new business<br>' +
        'process dimensional model will soon be modified and<br>' +
        'augmented. Users don’t always know what reports and<br>' +
        'analyses they want until you show them something fairly<br>' +
        'close. Then they’ll tell you what they don’t want — the<br>' +
        'report you just created — and, let’s hope, give you clearer<br>' +
        'information about what they now think they need.<br>' +
        'The cycle repeats as business imperatives change.<br>' +
        '1041<br>' +
        'Data mining applications, and other kinds of closed loop<br>' +
        'systems, are seldom implemented in the first phase of a<br>' +
        'DW/BI system. First, the basic data is brought online.<br>' +
        'Then, business users and DW/BI team members build ad<br>' +
        'hoc models and analyses. This analytic work has<br>' +
        'tremendous value to the business, for example by<br>' +
        'improving their understanding of customers, or providing<br>' +
        'mechanisms for reducing costs. The next step, beyond<br>' +
        'improving understanding, is to systematize the knowledge<br>' +
        'gained from ad hoc analysis by building a closed loop<br>' +
        'system. In our experience, ad hoc analyses are usually<br>' +
        'valuable enough to provide a positive ROI on the DW/BI<br>' +
        'investment, and many implementations stop there. Those<br>' +
        'DW/BI teams that go on to build data mining applications<br>' +
        'and other kinds of closed loop systems usually reap greatly<br>' +
        'increased ROI.<br>' +
        'The process of developing a closed loop BI system<br>' +
        'requires close partnership between the business people,<br>' +
        'who can effectively develop the business rules and analytic<br>' +
        'models, and the DW/BI team, who will write the system<br>' +
        'specifications and formalize the models. The majority of<br>' +
        'the application development effort requires a fairly<br>' +
        'standard development skill set, which is often met by the<br>' +
        'same developers who work on the operational systems.<br>' +
        'The developer needs a relatively small amount of<br>' +
        'specialized knowledge — for example, of the Analysis<br>' +
        'Services object models — in order to implement the calls<br>' +
        'into the databases or data mining model.<br>' +
        'System Management<br>' +
        '1042<br>' +
        'Most of this chapter focuses on the back-room<br>' +
        'requirements for managing your DW/BI system in<br>' +
        'production. Although we’ve separated these operational<br>' +
        'issues into this chapter, you need to think ahead during<br>' +
        'design and development to ensure you build a<br>' +
        'maintainable system.<br>' +
        'There are several components of the back-room system<br>' +
        'management:<br>' +
        '• Monitoring resources and usage<br>' +
        '• Managing data growth and disk space<br>' +
        '• Performance tuning<br>' +
        '• Managing partitioning<br>' +
        '• Data quality monitoring<br>' +
        '• Backup and recovery<br>' +
        '• Generating statistics for the BI portal<br>' +
        '• Executing and monitoring the ETL system<br>' +
        'The more automated you can make your systems<br>' +
        'management, the better. At the very least, automate<br>' +
        'backups and launching the ETL packages. SQL Server<br>' +
        'provides enough tools that the basics are easy, and there’s<br>' +
        'no excuse for not implementing some system automation.<br>' +
        'Unlike many issues where we’ve talked about how small<br>' +
        'teams might cut corners, organizations of any size benefit<br>' +
        'from system automation. Indeed, the smallest<br>' +
        'organizations are perhaps least equipped to apply human<br>' +
        'resources to a problem that can be automated. It’s hard to<br>' +
        'imagine how a DW/BI team of one to three people could<br>' +
        'possibly operate without significant automation.<br>' +
        '1043<br>' +
        'The ideal management system requires no human<br>' +
        'intervention except for the occasional troubleshooting.<br>' +
        'Such a system automatically adds and drops partitions,<br>' +
        'checks for disk space, reports on performance problems or<br>' +
        'unusual usage, and corrects the vast majority of data<br>' +
        'oddities during the ETL process.<br>' +
        'No matter how automated your operations are, you must<br>' +
        'have a plan. Like all plans, your operations plan should be<br>' +
        'written down.<br>' +
        'Governing the DW/BI System<br>' +
        'How does the DW/BI system administrator know what’s<br>' +
        'going on in the DW/BI system right now? What tools are<br>' +
        'available to manage and maintain the DW/BI system on a<br>' +
        'day-to-day basis?<br>' +
        'As you might expect, Microsoft and SQL Server provide a<br>' +
        'wide range of tools, solutions, and approaches. Some of<br>' +
        'the technologies, notably the relational engine, are richly<br>' +
        'instrumented and easy to manage. Others are not as well<br>' +
        'instrumented, but can be fairly well managed by using a<br>' +
        'combination of third-party software, free downloads, and<br>' +
        'custom scripting.<br>' +
        'Identifying and Terminating User Sessions<br>' +
        'The most basic question for ongoing administration of a<br>' +
        'DW/BI system is who’s logged in and issuing queries right<br>' +
        'now. The immediate second question is how to kill a<br>' +
        'query. No matter how well we design the DW/BI system to<br>' +
        'support ad hoc use, and train our user community, there’s<br>' +
        '1044<br>' +
        'inevitably a need to kill the occasional query that’s using<br>' +
        'too many system resources.<br>' +
        'Relational Database<br>' +
        'The relational engine has very nice tools for real-time<br>' +
        'analysis and management of server activity. As you might<br>' +
        'expect, there are several alternative methods for<br>' +
        'identifying and terminating user sessions. The main two<br>' +
        'are T-SQL and Activity Monitor.<br>' +
        'For decades, DBAs have used T-SQL commands such as<br>' +
        'sp_who to identify user sessions, and the kill command<br>' +
        'to terminate those sessions. Activity Monitor is a tool<br>' +
        'hosted within SQL Server Management Studio (SSMS) to<br>' +
        'provide information about user sessions, queries, and other<br>' +
        'processes. As you can see in Figure 17-2, Activity Monitor<br>' +
        'presents a summary display and detailed information about<br>' +
        'active processes, resource usage, I/O, and recent queries.<br>' +
        'The Processes section is used most frequently, and you can<br>' +
        'right-click a process to terminate it. Activity Monitor runs<br>' +
        'for the entire instance of SQL Server, although you can<br>' +
        'filter down the view to focus on a single database.<br>' +
        'Figure 17-2: Relational database Activity Monitor<br>' +
        '1045<br>' +
        'Alternatively you could use SQL Server Profiler to see all<br>' +
        'the activity on the server. Profiler is launched from the<br>' +
        'Tools menu in SSMS, and captures a broad and<br>' +
        'configurable set of information about user processes. But<br>' +
        'since Profiler is read only, you’d need to switch over to<br>' +
        'Activity Monitor or a query window to actually kill a<br>' +
        'query.<br>' +
        'Analysis Services Database<br>' +
        'There are similar tools for Analysis Services to identify<br>' +
        'active sessions and queries, but these tools are not as nice<br>' +
        'as for the relational engine. The “old fashioned” way to<br>' +
        'identify users is to execute a DMX statement with familiar<br>' +
        'SQL syntax, such as:<br>' +
        '1046<br>' +
        'select * from $system.discover_connections<br>' +
        'We call this old fashioned because it’s similar to the<br>' +
        'old-fashioned way to view activity in the relational<br>' +
        'database. But this functionality, called Dynamic<br>' +
        'Management Views, was new in SSAS 2008.<br>' +
        'To list currently executing queries, you can type:<br>' +
        'select * from $system.discover_commands<br>' +
        'Within SSMS, you do not execute these statements in a<br>' +
        'normal SQL query window. Instead, you open a DMX or<br>' +
        'MDX window, as illustrated in Figure 17-3.<br>' +
        'Figure 17-3: Executing a command to see active SSAS<br>' +
        'queries<br>' +
        '1047<br>' +
        'NOTE<br>' +
        'The DMX window was originally designed<br>' +
        'for executing data mining commands.<br>' +
        'When you open a DMX window, a browser<br>' +
        'pops up that lets you choose the data<br>' +
        'mining model you want. This is perplexing<br>' +
        'the first time you try to execute a Dynamic<br>' +
        'Management View command for Analysis<br>' +
        'Services. To be honest, it continues to be<br>' +
        'perplexing, but you’ll ignore it after a<br>' +
        'while.<br>' +
        'Alternatively, you can use the MDX<br>' +
        'window to execute Dynamic Management<br>' +
        'View commands. This doesn’t make sense<br>' +
        'either (since the DMX language is not the<br>' +
        'same as MDX, despite containing the same<br>' +
        'letters in its name).<br>' +
        'Unfortunately, you can use the Dynamic Management<br>' +
        'Views only to identify problem queries. There’s no kill<br>' +
        'command that you can execute from the same DMX<br>' +
        'window. In order to kill a query from within SSMS, you<br>' +
        'must execute an XMLA script.<br>' +
        'The best solution is to download the Analysis Services<br>' +
        'Activity Viewer tool from CodePlex. This tool, illustrated<br>' +
        'in Figure 17-4, shows active sessions and queries. You can<br>' +
        'use it to kill a specific query by clicking the button in the<br>' +
        '1048<br>' +
        'lower right. Most systems use the Activity Viewer rather<br>' +
        'than DMX and XMLA.<br>' +
        'Figure 17-4: Using the SSAS Activity Viewer to kill a<br>' +
        'query<br>' +
        'Reporting Services<br>' +
        'You may occasionally want to cancel a specific report<br>' +
        'execution in Reporting Services. In SSMS, you can view<br>' +
        'currently executing reports in the Jobs folder of a<br>' +
        'Reporting Services instance. Right-click the offending<br>' +
        'report to find the option to cancel it.<br>' +
        '1049<br>' +
        'Canceling a Reporting Services job cancels only the<br>' +
        'Reporting Services activity, notably the work involved in<br>' +
        'rendering output into the display format. If the problem<br>' +
        'report is spending all its time in the database query, you’ll<br>' +
        'need to go to the underlying database (relational or<br>' +
        'Analysis Services) to cancel the query as a separate step.<br>' +
        'Resource Governance<br>' +
        'The ability to find and kill a specific user query is useful,<br>' +
        'but no one wants to sit at a monitor all day, zapping the<br>' +
        'occasional query. Instead, the DBA would like to automate<br>' +
        'the ongoing allocation of resources to users and other<br>' +
        'processes. He may even want to set up a process to<br>' +
        'automatically kill extremely long-running queries.<br>' +
        'Relational Database<br>' +
        'The SQL Server relational engine has a nice feature called<br>' +
        'Resource Governor. As you might expect from the name,<br>' +
        'Resource Governor is the right tool for the job of ongoing<br>' +
        'management of CPU and memory resources. Resource<br>' +
        'Governor doesn’t help you manage I/O.<br>' +
        'First, you can define multiple workload groups. You may<br>' +
        'want to create separate groups for:<br>' +
        '• Integration Services jobs<br>' +
        '• Reporting Services report execution<br>' +
        '• DW/BI team members<br>' +
        '• Most ad hoc users<br>' +
        '• Vice presidents<br>' +
        '• That guy in accounting who keeps trying to download half<br>' +
        'the data warehouse into Excel<br>' +
        '1050<br>' +
        'You can define resource pools and policies for each<br>' +
        'workgroup, as illustrated in Figure 17-5.<br>' +
        'Figure 17-5: Defining pools and groups in Resource<br>' +
        'Governor<br>' +
        'You must write a T-SQL function to classify requests into<br>' +
        'groups. The classification is typically based on usernames<br>' +
        'and the name of the application (such as Excel) that’s<br>' +
        'submitting the request. Any request that’s not explicitly<br>' +
        'classified into a workload group will fall into the default<br>' +
        'group.<br>' +
        'Resource Governor will limit the resources for a query<br>' +
        'only if there’s resource contention. In other words, if a<br>' +
        'user with low priority kicks off a query in the middle of<br>' +
        'the night, she may use all the server’s resources if there’s<br>' +
        'no other activity on the server at that time.<br>' +
        '1051<br>' +
        'REFERENCE<br>' +
        'Books Online has adequate documentation<br>' +
        'for the Resource Governor, starting with<br>' +
        'the topic “Managing SQL Server<br>' +
        'Workloads with Resource Governor.” The<br>' +
        'topic “Considerations for Writing a<br>' +
        'Classifier Function” is also useful.<br>' +
        'If a query exceeds the maximum CPU defined for its<br>' +
        'workload group, Resource Governor will throw a CPU<br>' +
        'Threshold Exceeded event. You can use SQL Server<br>' +
        'Profiler to monitor for this event. Usually it’s sufficient for<br>' +
        'the DBA simply to view the logs generated by Profiler, in<br>' +
        'order to determine if the maximum CPU level is set<br>' +
        'appropriately.<br>' +
        'Whether or not you’re using Resource Governor, you can<br>' +
        'set up an alert in SQL Server Agent to notify the DBA of<br>' +
        'long running queries. Although it’s possible to set up a<br>' +
        'response in SQL Server Agent to automatically kill<br>' +
        'queries, we recommend keeping a person in the loop, at<br>' +
        'least during the work day.<br>' +
        'Analysis Services Database<br>' +
        'The SSAS Activity Viewer can define alerts and actions.<br>' +
        'You can easily set up Activity Viewer to alert you when a<br>' +
        'query has been running for more than 30 minutes, or even<br>' +
        'to kill that query. You can even eliminate special people<br>' +
        '(such as yourself!) from the rule.<br>' +
        '1052<br>' +
        'Performance Monitoring<br>' +
        'You need to implement and understand two types of<br>' +
        'monitoring. First, what’s going on in your system? What<br>' +
        'system resources like memory are being used, when, and<br>' +
        'by what processes? If you can’t monitor system events and<br>' +
        'resource usage, you’ll never be able to troubleshoot and<br>' +
        'tune your system. The other kind of monitoring focuses on<br>' +
        'the users: Who’s accessing the system, and when? What<br>' +
        'queries are they running and how much time are those<br>' +
        'queries taking? If you don’t know what your users are<br>' +
        'doing, you’ll never be able to satisfy them.<br>' +
        'The good news: this is all easy to set up. There are a<br>' +
        'handful of underlying tools, including:<br>' +
        '• System Monitor is an operating system tool to collect and<br>' +
        'view real-time performance data in the form of counters, for<br>' +
        'server resources such as processor and memory use. The<br>' +
        'usage of many SQL Server resources is exposed as System<br>' +
        'Monitor counters.<br>' +
        '• SQL Server Profiler tracks an instance of the relational<br>' +
        'engine or Analysis Services. Profiler is a tool with a simple<br>' +
        'user interface for defining a trace to track activities of<br>' +
        'interest, such as user logins or query executions. Profiler can<br>' +
        'be set up to capture query text for both Analysis Services<br>' +
        'and the relational engine.<br>' +
        '• Event Viewer is an operating system tool that collects<br>' +
        'information about system events, such as Analysis Services<br>' +
        'shutting down unexpectedly.<br>' +
        'It’s useful to know about these underlying tools, and you<br>' +
        'certainly could use them directly to monitor your DW/BI<br>' +
        'system. However, most organizations can rely on a<br>' +
        'packaged monitoring configuration supplied by Microsoft,<br>' +
        '1053<br>' +
        'known as Management Data Warehouse. As we discuss<br>' +
        'next, Management Data Warehouse is useful out of the<br>' +
        'box for monitoring the relational database. And you can<br>' +
        'extend it to capture information about Analysis Services as<br>' +
        'well.<br>' +
        'Relational Database<br>' +
        'The easiest way to set up comprehensive monitoring for<br>' +
        'the relational data warehouse is to install Management<br>' +
        'Data Warehouse (MDW). You can do this by<br>' +
        'right-clicking on Data Collection in the Management<br>' +
        'folder in SSMS. MDW sets up monitoring on a wide range<br>' +
        'of counters and SQL Server events, including:<br>' +
        '• CPU usage<br>' +
        '• Memory usage<br>' +
        '• Disk I/O usage<br>' +
        '• Logins, logouts, and connections<br>' +
        '• Queries, including query SQL<br>' +
        'In addition, the MDW configuration performs the<br>' +
        'following tasks:<br>' +
        '• Sets up the SQL Profiler traces as needed (for example, to<br>' +
        'capture query SQL)<br>' +
        '• Sets up the System Monitor logging to capture information<br>' +
        'from the operating system (for example, CPU usage)<br>' +
        '• Creates a database to store the collected information<br>' +
        '• Sets up SQL Agent jobs to collect and upload logged<br>' +
        'information, including a few SSIS packages<br>' +
        '• Installs and schedules stored procedures to periodically<br>' +
        'purge aged logs<br>' +
        '• Installs a starter set of reports on the collected data<br>' +
        '1054<br>' +
        'Each set of information has a schedule for collection and<br>' +
        'upload into the MDW. For example, server usage statistics<br>' +
        'are collected every minute, and uploaded every 15<br>' +
        'minutes. The information about queries is logged first to a<br>' +
        'file, then scheduled to be moved into the MDW daily.<br>' +
        'These schedules are easy to modify.<br>' +
        'NOTE<br>' +
        'Although you can install Management Data<br>' +
        'Warehouse on the same server that it’s<br>' +
        'monitoring, it’s generally recommended to<br>' +
        'host it on a separate server. You can<br>' +
        'monitor multiple servers from a central<br>' +
        'MDW location.<br>' +
        'Figure 17-6 illustrates one of the sample MDW reports, on<br>' +
        'server activity. You can execute the report from SQL<br>' +
        'Server Management Studio. The reports are associated<br>' +
        'with the MDW database and also in the Management<br>' +
        'section of SSMS, under the Data Collection heading. As<br>' +
        'you can see, it’s a dashboard-style report that presents a<br>' +
        'rich set of information on the operations of your database<br>' +
        'server. The underlying tables and views are available for<br>' +
        'you to query, should you want to develop a custom report.<br>' +
        'Figure 17-6: Sample Management Data Warehouse report<br>' +
        '1055<br>' +
        'The MDW logic does not store the SQL for every query<br>' +
        'executed against the system. Instead, during each snapshot<br>' +
        '(default is 15 minutes), the system chooses the top three<br>' +
        'most expensive queries by each of six metrics (elapsed<br>' +
        'time, execution count, and so on). After eliminating<br>' +
        'parameters and literals, the logic identifies unique queries<br>' +
        'within that set, and then adds them to the list of “notable”<br>' +
        'queries in the MDW. It’s a good practice to minimize the<br>' +
        'number of queries for which you’re capturing and storing<br>' +
        'the SQL, as that one piece of information alone is larger<br>' +
        'than the rest of the logging combined. It’s possible, but not<br>' +
        'advised, to modify that logic.<br>' +
        'The collection, upload, and purge schedules are easy to<br>' +
        'modify. By default, SQL for notable queries is retained for<br>' +
        '1056<br>' +
        '14 days, but we recommend extending that to 30 or 60<br>' +
        'days. Note that by so doing, your MDW database will<br>' +
        'grow substantially larger! But a data warehouse that<br>' +
        'supports ad hoc queries should have a long time series of<br>' +
        'query text available for performance testing purposes.<br>' +
        'Figure 17-7 illustrates how to change the schedule for the<br>' +
        'query statistics information. You manage this information<br>' +
        'in SSMS, under Management ⇒ Data Collection.<br>' +
        'Figure 17-7: Changing the retention policy for query<br>' +
        'statistics and the text of SQL queries<br>' +
        'Analysis Services Database<br>' +
        '1057<br>' +
        'The kinds of things that we want to monitor for the core<br>' +
        'Analysis Services database are the same as for the<br>' +
        'relational database. We want to know what resources are<br>' +
        'being used (CPU, disk, memory); we want to know who’s<br>' +
        'logging in and what queries they’re issuing. The<br>' +
        'underlying tools — System Monitor counters and Profiler<br>' +
        'traces — work for Analysis Services, and can be<br>' +
        'configured to monitor similar information as for the<br>' +
        'relational database.<br>' +
        'In theory, Analysis Services monitoring should be as<br>' +
        'simple to set up as for the relational database. That’s not<br>' +
        'entirely true in practice: although similar components<br>' +
        'exist, the installation procedure is not as smooth as for the<br>' +
        'relational engine.<br>' +
        'The Analysis Services Management Data Warehouse<br>' +
        'components are not included with the installation media.<br>' +
        'Instead, you can find them on CodePlex. To be honest, it<br>' +
        'looks like these components were supposed to be a feature<br>' +
        'of SQL Server 2008 or 2008 R2 but just didn’t get finished<br>' +
        'in time. Although they’re not as polished as the relational<br>' +
        'system components, they get the job done.<br>' +
        'DOWNLOADS<br>' +
        'You can find the CodePlex project for<br>' +
        'Analysis Services monitoring at<br>' +
        'http://sqlsrvanalysissrvcs.codeplex.com/. It’s<br>' +
        'called “A Solution for Collecting Analysis<br>' +
        '1058<br>' +
        'Services Performance Data for<br>' +
        'Performance Analysis.” In addition to the<br>' +
        'scripts to set up MDW to collect<br>' +
        'information from Analysis Services, the<br>' +
        'CodePlex project includes two white<br>' +
        'papers, one eponymously titled “A Solution<br>' +
        'for Collecting Analysis Services<br>' +
        'Performance Data for Performance<br>' +
        'Analysis,” and the other is the “Analysis<br>' +
        'Services Performance Guide.” Both<br>' +
        'documents are highly recommended.<br>' +
        'Once you’ve successfully completed the installation as<br>' +
        'described in the “Solution” document, you’ll have<br>' +
        'extended MDW to collect performance information from<br>' +
        'Analysis Services as well as the relational database.<br>' +
        'However, the Analysis Services solution does not<br>' +
        'automatically purge old performance data, so you’ll have<br>' +
        'to manage that process yourself.<br>' +
        'On the plus side, the solution does come with a dozen<br>' +
        'predefined reports for examining the performance of your<br>' +
        'core Analysis Services database.<br>' +
        'The MDW solution collects the MDX text of queries,<br>' +
        'which is tremendously valuable for the performance tuning<br>' +
        'process. The query text will become the script for testing<br>' +
        'any proposed changes in the Analysis Services physical<br>' +
        'design. There is a second related reason to collect<br>' +
        'information about the queries: to feed the process that<br>' +
        'designs usage-based performance aggregations. Usage<br>' +
        '1059<br>' +
        'based aggregations are one of your most effective weapons<br>' +
        'for improving query performance of your Analysis<br>' +
        'Services database.<br>' +
        'If you do not use the MDW monitoring solution, you<br>' +
        'should nonetheless capture the information needed to feed<br>' +
        'the usage-based aggregation design wizard. Turn on the<br>' +
        'query log for usage-based aggregations within SSMS, by<br>' +
        'changing the Query Log property of the server.<br>' +
        'If you do use the MDW monitoring solution, you should<br>' +
        'turn off the query log for usage-based optimization. That’s<br>' +
        'because the MDW solution includes a process for<br>' +
        'converting the normal query logs, which are stored in<br>' +
        'human-readable MDX, into the proprietary format of the<br>' +
        'usage-based optimization logs. This way, you only need to<br>' +
        'log usage one time, then periodically convert the<br>' +
        'information to the other format.<br>' +
        'Analysis Services aggregation design and the usage-based<br>' +
        'optimization wizard were discussed in Chapter 8.<br>' +
        'Reporting Services<br>' +
        'During the course of its operation, Reporting Services<br>' +
        'stores most of the information useful for performance and<br>' +
        'operations monitoring in its ReportServer database.<br>' +
        'However, you shouldn’t report directly from ReportServer.<br>' +
        'These tables are part of the Reporting Services operational<br>' +
        'system, and reporting directly from them is analogous to<br>' +
        'reporting directly from any other transaction system.<br>' +
        '1060<br>' +
        'The ReportServer catalog is just like any other transaction<br>' +
        'system. It will lose history when its transaction logs are<br>' +
        'truncated. It will lose referential integrity when reports are<br>' +
        'deleted that have been run in the past, or when users are<br>' +
        'removed who have used the system in the past. Definitions<br>' +
        'of the tables and columns may change over time, breaking<br>' +
        'the existing library of usage reports. It is not easy to query<br>' +
        'directly because it uses codes instead of descriptions for<br>' +
        'most of the attributes. Most important, even a simple<br>' +
        'interactive query could place read locks on the running<br>' +
        'system, dramatically reducing system performance. For all<br>' +
        'these reasons, it makes sense to build a simple database<br>' +
        'that tracks the process of generating reports. As you can<br>' +
        'probably guess, the solution to these problems is in a<br>' +
        'CodePlex project.<br>' +
        'REFERENCE<br>' +
        'The relevant files are part of the SQL<br>' +
        'Server product code samples found at<br>' +
        'sqlserversamples.codeplex.com. Once you<br>' +
        'install the samples, look for the Execution<br>' +
        'Log Sample Reports directory. It contains a<br>' +
        'database definition for a simple star schema<br>' +
        'built from the ReportServer catalog, plus<br>' +
        'an Integration Services package for moving<br>' +
        'data from one database to the other. Of<br>' +
        'course, sample reports are also provided.<br>' +
        'Integration Services<br>' +
        '1061<br>' +
        'The primary goal for monitoring Integration Services<br>' +
        'package execution on the production system is to help you<br>' +
        'evaluate whether and how to improve processing<br>' +
        'performance. You also want to be able to tie the audit<br>' +
        'dimension, discussed in Chapter 7, to information about<br>' +
        'the package’s execution.<br>' +
        'The most important tools for monitoring Integration<br>' +
        'Services are System Monitor and Integration Services<br>' +
        'Logging. The SSIS System Monitor counters are not as<br>' +
        'useful as you’d hope because they track information at a<br>' +
        'high level. What you really want to see is how much<br>' +
        'memory each step of a data flow is using; instead, you can<br>' +
        'see how much memory Integration Services is using.<br>' +
        'Nonetheless, the following SQL Server: SSIS Pipeline<br>' +
        'counters are somewhat useful:<br>' +
        '• Buffer Memory: How much memory is Integration Services<br>' +
        'using? If this number is larger than the physical memory<br>' +
        'available to Integration Services, some data is being spooled<br>' +
        'to disk during processing.<br>' +
        '• Rows Read: The total number of rows read from source<br>' +
        'adapters.<br>' +
        '• Rows Written: The total number of rows written to<br>' +
        'destination adapters.<br>' +
        'The logging that’s generated by Integration Services<br>' +
        'packages is akin to the SQL Server tracing and profiling<br>' +
        'functionality that we discussed previously. Like Profiler, it<br>' +
        'tracks events. The kind of events that can be tracked and<br>' +
        'logged will be familiar to anyone who’s run an Integration<br>' +
        'Services package: It’s exactly the same kind of<br>' +
        '1062<br>' +
        'information that you can see in the Execution Results tab<br>' +
        'every time you execute a package in BIDS.<br>' +
        'You seldom want to store all those package execution<br>' +
        'results permanently. Define package logging so that you<br>' +
        'store information about interesting events only. Most<br>' +
        'often, you’d set up the packages on your production<br>' +
        'system so they log to a SQL Server database, though you<br>' +
        'have several choices.<br>' +
        'For every task, and for the overall package, you can track<br>' +
        'many events, of which the most useful are:<br>' +
        '• OnPreExecute: Logs a row when execution begins<br>' +
        '• OnPostExecute: Logs a row when execution ends<br>' +
        '• OnWarning: Logs a row when the task issues a warning<br>' +
        '• OnError: Logs a row when the task issues an error<br>' +
        'Unfortunately, the data flow task is like a black box to the<br>' +
        'logging system. There are multiple steps and<br>' +
        'transformations within a data flow task, and ideally you’d<br>' +
        'like to know how long each step takes. This is simply<br>' +
        'impossible with the logging framework as it exists. In<br>' +
        'order to get this kind of information, you need to hand<br>' +
        'craft logging into your packages, by writing row counts<br>' +
        'and timestamps at various points within the data flow task<br>' +
        'of the package.<br>' +
        'Set up logging by editing the package in BIDS. Choose<br>' +
        'SSIS ? Logging, and specify where the logs will be stored.<br>' +
        'Note that SQL Profiler is one of the options. Logging to<br>' +
        'Profiler makes it easier to interleave package events with<br>' +
        'database events, and is especially useful during testing.<br>' +
        'However, most people log to a SQL Server table in<br>' +
        '1063<br>' +
        'production. You can direct SSIS logs to the MDW<br>' +
        'database, but you should create a separate schema for your<br>' +
        'own logged information. Integration Services will<br>' +
        'automatically create its logging table.<br>' +
        'As we described in Chapter 7, you should build your<br>' +
        'packages to collect information about package execution<br>' +
        'times, rows inserted into target tables, and possibly other<br>' +
        'information like the sum of amounts. This metadata is<br>' +
        'stored in the audit dimension and, at your discretion, is<br>' +
        'available to business users for querying. You may choose<br>' +
        'to pick up additional information from logging, to include<br>' +
        'with the audit system.<br>' +
        'Finally, you may want to set up package logging to log<br>' +
        'errors to the application event log, accessible from the<br>' +
        'Event Viewer. This application event may still be written<br>' +
        'even if Something Really Bad happens on the database<br>' +
        'server, which would prevent the standard logging from<br>' +
        'working properly.<br>' +
        'PowerPivot<br>' +
        'A PowerPivot workbook by itself presents no monitoring<br>' +
        'challenges, because it’s unmanaged and there’s really<br>' +
        'nothing we can monitor. However, if your organization has<br>' +
        'implemented PowerPivot for SharePoint, your power users<br>' +
        'will be uploading their PowerPivot workbooks to<br>' +
        'SharePoint. The rest of the user community will access<br>' +
        'these analytic workbooks via SharePoint.<br>' +
        'Once you’ve set up PowerPivot to work with SharePoint,<br>' +
        'the two products work together to collect useful<br>' +
        '1064<br>' +
        'information. The basic performance monitoring<br>' +
        'information is collected for you, and is available within the<br>' +
        'PowerPivot Management Dashboard as we saw in Chapter<br>' +
        '11. You can access the dashboard from the SharePoint<br>' +
        'Central Administration site. Performance information<br>' +
        'includes CPU and memory usage on the PowerPivot<br>' +
        'SharePoint server, as well as counts and timings of queries.<br>' +
        'The dashboard contains some useful reports, and you can<br>' +
        'always write your own. Logged information is stored in<br>' +
        'SQL Server, in the PowerPivot database with a name that<br>' +
        'begins DefaultPowerPivotServiceApplication. There are<br>' +
        'about a dozen tables in the Usage schema, which hold the<br>' +
        'logged information. For performance monitoring, the most<br>' +
        'useful table is Usage.Health. By default, the usage logging<br>' +
        'information is moved into the PowerPivot database every<br>' +
        'night. The management dashboard is a web part page, so<br>' +
        'you can customize it with additional web parts showing the<br>' +
        'reports that you have created.<br>' +
        'Usage Monitoring<br>' +
        'The usage of your DW/BI system has a huge impact on its<br>' +
        'performance. Our discussion of performance monitoring<br>' +
        'suggested that you collect information about usage,<br>' +
        'including the text of notable queries. You should always<br>' +
        'be collecting counts of queries run and rows returned, by<br>' +
        'type of application.<br>' +
        'You should also log information about user logins and<br>' +
        'attempted connections. For the relational database, set up a<br>' +
        'trace in Profiler, capturing the following events:<br>' +
        '1065<br>' +
        '• Audit Security: Audit Login<br>' +
        '• Audit Security: Audit Logout<br>' +
        'Take a look through the many additional Audit Security<br>' +
        'events to see if any strikes your fancy.<br>' +
        'Similarly, set up an Analysis Services trace for logins and<br>' +
        'logouts.<br>' +
        'You can send the results of both traces either to a file or<br>' +
        'directly to a table. For a DW/BI system, we usually have a<br>' +
        'relatively modest number of logins so you can log them<br>' +
        'directly to a database table. It’s appealing to use the MDW<br>' +
        'database for this purpose, but you should separate out your<br>' +
        'trace tables into their own schema.<br>' +
        'Reporting System usage is captured in the ReportServer<br>' +
        'database. If you’ve implemented the reporting<br>' +
        'management solution described in the previous section,<br>' +
        'that information will be moved nightly to a separate<br>' +
        'database appropriate for reporting. Every report execution<br>' +
        'is logged, along with the chosen parameters.<br>' +
        'PowerPivot for SharePoint usage is captured by SharePoint<br>' +
        'and moved nightly to the PowerPivot SQL database, as<br>' +
        'described in the previous section. Usage logging is an<br>' +
        'extremely valuable feature of PowerPivot for SharePoint.<br>' +
        'The collected information and predefined reports make it<br>' +
        'easy to see who’s using the system, which PowerPivot<br>' +
        'workbooks they’re accessing, and which PowerPivot<br>' +
        'workbooks are most popular overall. By monitoring the<br>' +
        'changing popularity of PowerPivot workbooks, you can<br>' +
        'proactively manage your system.<br>' +
        '1066<br>' +
        'An example of the PowerPivot Management Dashboard<br>' +
        'activity report is illustrated in Figure 17-8.<br>' +
        'Figure 17-8: PowerPivot activity report<br>' +
        'Your BI portal website should devote a small amount of<br>' +
        'screen real estate to reporting on system usage. VPs and<br>' +
        'Directors are often very interested in how much their staff<br>' +
        'members are using your system. They tend to be<br>' +
        'competitive people, and simply seeing another department<br>' +
        'using the DW/BI system has been known to spur a VP to<br>' +
        'encourage his or her staff to use the system more. A time<br>' +
        'series of reporting and ad hoc use by department is a really<br>' +
        'good report to publish.<br>' +
        'Managing Disk Space<br>' +
        'One of the most common reasons for ETL job failure is<br>' +
        'one of the easiest to prevent: running out of disk space.<br>' +
        '1067<br>' +
        'At the very minimum, set up a System Monitor counter<br>' +
        'and alert to warn when free space on each disk falls below<br>' +
        'a certain threshold. The relevant counter is Logical Disk:<br>' +
        'Free Megabytes. Set up two alerts: one to warn when<br>' +
        'you’re within a month of running out of disk space, and<br>' +
        'one to blare stridently when you’re about a week away.<br>' +
        'Anyone — even someone who’s never seen System<br>' +
        'Monitor before — should be able to set up a simple alert<br>' +
        'on disk space. An alternative, equally effective approach is<br>' +
        'to set up an alert in SQL Agent.<br>' +
        'The Management Data Warehouse for the relational<br>' +
        'database includes a very nice sparkline report that shows<br>' +
        'how your database’s disk usage has been changing. This<br>' +
        'disk usage report is illustrated in Figure 17-9.<br>' +
        'The incremental disk space you’ll be using each month<br>' +
        'goes to fact data storage, whether in the file system,<br>' +
        'relational database, or Analysis Services database. Most<br>' +
        'dimensions are relatively small and static in size, at least<br>' +
        'compared to fact tables.<br>' +
        'Figure 17-9: Disk Usage sparkline report<br>' +
        '1068<br>' +
        'When you set up a relational database, you can specify an<br>' +
        'initial size, an automatic growth factor like 10 percent, and<br>' +
        'a maximum file size. We recommend that you enable<br>' +
        'automatic growth as a failsafe, but monitor free space so<br>' +
        'that you extend database space on your own schedule. This<br>' +
        'is classic relational database management. There are tons<br>' +
        'of books available on the subject, and there’s nothing<br>' +
        'particularly unusual for the data warehouse environment<br>' +
        '1069<br>' +
        '— except the overall size of the system and the (usually)<br>' +
        'batch nature of inserts and updates.<br>' +
        'You could get a lot fancier than the simple mechanism<br>' +
        'we’ve described here. If your data volumes are<br>' +
        'accelerating, you should write a little program to look at<br>' +
        'the sizes of your staging files and Analysis Services<br>' +
        'databases. Forecast next month’s disk requirements based<br>' +
        'on the recent trend. You could write this program as a<br>' +
        'Script Task from within Integration Services, and schedule<br>' +
        'it using SQL Agent. You could go so far as to<br>' +
        'automatically allocate more disk space for the RDBMS,<br>' +
        'staging area, or Analysis Services. But do the basics at<br>' +
        'least: You should be embarrassed if you actually run out of<br>' +
        'disk space on your production system.<br>' +
        'Service and Availability Management<br>' +
        'The DW/BI team is responsible for ensuring that the<br>' +
        'system, including data, reports, and other applications, is<br>' +
        'available to end users. The level of availability required,<br>' +
        'measured as the amount of acceptable down time, depends<br>' +
        'on the business impact of the system’s unavailability. You<br>' +
        'should work with the business users to develop a service<br>' +
        'level agreement (SLA), and build a plan for meeting that<br>' +
        'specified service level.<br>' +
        'When you develop your availability plan, consider the<br>' +
        'components of the DW/BI system within the context of<br>' +
        'your entire IT infrastructure. Let’s hope your IT team<br>' +
        'already has plans and procedures for managing hardware,<br>' +
        'software, software installation media and product keys,<br>' +
        'and usernames and passwords — all necessary for<br>' +
        '1070<br>' +
        'rebuilding or restoring a system as quickly as possible.<br>' +
        'Assess the following issues:<br>' +
        '• Do any parts of the DW/BI system need continuous,<br>' +
        '24-hour-a-day query access? Analysis Services? The<br>' +
        'relational data warehouse? If so, how do you process new<br>' +
        'data without compromising availability?<br>' +
        '• If continuous query access is not required, how do you<br>' +
        'ensure that processing fits within the nightly processing<br>' +
        'window?<br>' +
        '• How do you handle situations that require an entire Analysis<br>' +
        'Services cube to be reprocessed? As discussed in Chapter 8,<br>' +
        'this can happen if an attribute declared to be unchanging<br>' +
        'does in fact change, among other reasons.<br>' +
        '• How do you recover from a system failure during Integration<br>' +
        'Services processing? How do you restart a process<br>' +
        'mid-stream?<br>' +
        '• How is the DW/BI system protected against failure of one or<br>' +
        'more components on the server(s)?<br>' +
        '• How is the DW/BI system and data protected against failure<br>' +
        'within the enterprise, like source system outages, or serious<br>' +
        'problems with the Active Directory servers?<br>' +
        'Develop a plan for addressing these issues to achieve the<br>' +
        'necessary availability. Test each element of the plan. A<br>' +
        'well-trained staff that is prepared to handle any<br>' +
        'contingency is an essential part of any disaster recovery<br>' +
        'plan.<br>' +
        'Your availability plan must explicitly state how you’ll<br>' +
        'detect a problem with a service. Maybe you want to wait<br>' +
        'for the VP of Marketing to call, but you probably want to<br>' +
        'be a bit more proactive.<br>' +
        'Your Windows system administrators should already have<br>' +
        'procedures in place for identifying service outages. These<br>' +
        '1071<br>' +
        'procedures probably use System Monitor, Windows<br>' +
        'Events, or both. Use those same techniques to identify<br>' +
        'whether the components of your DW/BI system are<br>' +
        'currently available. If your organization doesn’t already<br>' +
        'use Service Center Operations Manager, you should<br>' +
        'consider purchasing it or a third-party operations manager<br>' +
        'like Tivoli. Any operations management product will be<br>' +
        'looking at standard Windows logs like System Monitor<br>' +
        'and Windows Events, and SQL Server Profiler.<br>' +
        'The SLA should be explicit about the availability that’s<br>' +
        'required for each component of the DW/BI system. For<br>' +
        'Analysis Services, it’s particularly important to distinguish<br>' +
        'between the SLA for unavailability and the SLA for query<br>' +
        'performance. In many cases, Analysis Services<br>' +
        'incremental processing can occur in the background, with<br>' +
        'user access to the cubes undisturbed. During this period,<br>' +
        'however, query performance may degrade significantly.<br>' +
        'You need to help your business users understand these<br>' +
        'tradeoffs, so they can help make the case for a different<br>' +
        'architecture if needed to meet their availability and<br>' +
        'performance requirements.<br>' +
        'The same issues of availability versus query performance<br>' +
        'are relevant for the relational data warehouse. Many<br>' +
        'organizations simply close access to the DW/BI system<br>' +
        'during ETL processing. Others, with higher availability<br>' +
        'requirements, will perform the bulk of processing in the<br>' +
        'background or on a remote server. User query performance<br>' +
        'on the relational data warehouse usually suffers during<br>' +
        'ETL processing on the same server, perhaps intolerably so.<br>' +
        'A careful consideration of these issues, and good<br>' +
        '1072<br>' +
        'communication with the users, will help you design a<br>' +
        'cost-effective architecture that meets your requirements.<br>' +
        'Performance Tuning the DW/BI System<br>' +
        'The system operations plan should include strategies for<br>' +
        'periodic performance tuning of all system components.<br>' +
        'The performance monitoring that we described earlier in<br>' +
        'the chapter provides the base information you’ll need to<br>' +
        'determine how to improve system performance. If<br>' +
        'performance is degrading, you need to identify which<br>' +
        'operations are causing the problem, and whether the<br>' +
        'primary bottleneck is in memory, disk, or processing<br>' +
        'power. All the BI components love memory, so it’s a good<br>' +
        'bet to check memory usage first.<br>' +
        'The best way to solve resource contention may be to<br>' +
        'distribute your DW/BI system across multiple servers. A<br>' +
        'lot of DW/BI teams will try to build an all-in-one system,<br>' +
        'with the four major BI components (relational database,<br>' +
        'Integration Services, Analysis Services, and Reporting<br>' +
        'Services) on a single server. As we discussed in Chapter 4,<br>' +
        'it’s often sensible to distribute across multiple servers.<br>' +
        'There’s no hard and fast rule about which components to<br>' +
        'group together, although most often we see the relational<br>' +
        'database and Integration Services on the same box.<br>' +
        'Look to see if your system is memory-bound. Although<br>' +
        'there are circumstances where you can redesign your DW/<br>' +
        'BI system components to use less memory, it’s usually far<br>' +
        'cheaper simply to buy more memory. It feels intellectually<br>' +
        'lazy to recommend a hardware upgrade to solve<br>' +
        'performance problems, but if you’ve done a good job on<br>' +
        '1073<br>' +
        'your system design it’s often the easiest — if not the only<br>' +
        '— degree of freedom.<br>' +
        'The query performance of your relational data warehouse<br>' +
        'database may be improved by adding or changing:<br>' +
        '• Fact table partitioning<br>' +
        '• Indexes and indexed views<br>' +
        '• Filtered indexes, especially for the common scenario where<br>' +
        'many queries are limited to the most recent data (filter on<br>' +
        'date)<br>' +
        '• Statistics, including filtered statistics. Everyone knows that<br>' +
        'statistics are associated with each index you build. But you<br>' +
        'can also create standalone statistics, which at times can<br>' +
        'greatly assist the query optimizer in determining the correct<br>' +
        'query plan. Statistics aren’t free to build, but they are much<br>' +
        'cheaper to build and store than the corresponding index.<br>' +
        'For Analysis Services, your two big weapons for<br>' +
        'improving query performance are:<br>' +
        '• Adding or increasing partitioning. Especially helpful is to<br>' +
        'partition by multiple dimensions.<br>' +
        '• Adding or changing your aggregation design. Use the usage<br>' +
        'based aggregation wizard.<br>' +
        'REFERENCE<br>' +
        'There is a very nice performance guide<br>' +
        'available for download from<br>' +
        'http://download.microsoft.com, called<br>' +
        '“SQL Server 2008 White Paper: Analysis<br>' +
        'Services Performance Guide.” Though<br>' +
        '1074<br>' +
        'written for Analysis Services 2008, it<br>' +
        'remains relevant for 2008 R2.<br>' +
        'No matter what you’re doing to improve performance, it’s<br>' +
        'really important to follow good change management and<br>' +
        'tuning techniques:<br>' +
        '• Work on the test system, and script any changes to<br>' +
        'production.<br>' +
        '• Document baseline performance.<br>' +
        '• Change one thing at a time.<br>' +
        '• Document changes to performance.<br>' +
        '• Test all the changes together before moving to production.<br>' +
        'Backup and Recovery<br>' +
        'No matter what the availability requirements are on your<br>' +
        'system, you need a backup and recovery plan. This seems<br>' +
        'like an intuitively obvious statement, but we’ve seen any<br>' +
        'number of DW/BI systems that purported to be in<br>' +
        'production, but which had no backup plan.<br>' +
        'It’s as important to have a recovery plan as it is to have a<br>' +
        'backup plan. And it’s equally important to test these<br>' +
        'procedures. When the inevitable emergency happens, you<br>' +
        'want to be ready, practiced, and calm. Your test system is<br>' +
        'an ideal platform for testing these procedures. If you<br>' +
        'haven’t fully tested your recovery procedures, you’re lying<br>' +
        'to yourself and your management that you have a real<br>' +
        'backup and recovery plan.<br>' +
        '1075<br>' +
        'In the DW/BI world, you can experience the same kinds of<br>' +
        'emergencies as transaction systems, from server outages<br>' +
        'and disk failures to earthquakes and floods. Plan for your<br>' +
        'daily or monthly load cycle to break down occasionally.<br>' +
        'Develop your ETL system so that failure is fairly unlikely.<br>' +
        'But let’s face it: The DW/BI system is at the end of a long<br>' +
        'train of data flows over which you have no control. Only a<br>' +
        'foolish manager would neglect to plan for backing out a<br>' +
        'bad load. The auditing system described in Chapter 7 lays<br>' +
        'the foundation for identifying the rows that were changed<br>' +
        'during a specific load process.<br>' +
        'Relational Databases<br>' +
        'The relational databases are usually the most vital sets of<br>' +
        'information to back up regularly. Ideally, back up the<br>' +
        'following databases after each load:<br>' +
        '• Relational data warehouse databases<br>' +
        '• Staging databases and files<br>' +
        '• Metadata databases<br>' +
        '• ReportServer<br>' +
        'Other databases should be backed up on a regular<br>' +
        'schedule, though perhaps not daily. These include:<br>' +
        '• Msdb, which contains SQL Agent job definitions and any<br>' +
        'SSIS packages that are stored in SQL Server.<br>' +
        '• Logging databases, including MDW and the Reporting<br>' +
        'Services logging database.<br>' +
        'Your backup and recovery strategies are intertwined with<br>' +
        'each database’s recovery model. The Simple recovery<br>' +
        'model lets you restore only to the point of a backup. The<br>' +
        'transaction log is not backed up. This works fine for many<br>' +
        '1076<br>' +
        'relational data warehouses, where data flows in nightly,<br>' +
        'weekly, or monthly. The Simple recovery model is<br>' +
        'appropriately named; it’s faster and simpler to manage<br>' +
        'than the Full recovery model. Nonetheless, as your DW/BI<br>' +
        'system moves closer to real time, the Full recovery model<br>' +
        'becomes increasingly appropriate.<br>' +
        'REFERENCE<br>' +
        'See the Books Online topics “Overview of<br>' +
        'the Recovery Models,” “Restore and<br>' +
        'Recovery Overview,” and “Selecting a<br>' +
        'Recovery Model” for more information.<br>' +
        'Most systems use the standard SQL Server backup<br>' +
        'facilities for relational backup and recovery. The relational<br>' +
        'data warehouse database is usually quite large, and so it’s<br>' +
        'often challenging to run a backup at the end of each<br>' +
        '(nightly) load cycle. There are several alternatives:<br>' +
        '• Store the database on a Storage Area Network (SAN), and<br>' +
        'use the SAN software to perform the backup. The SAN<br>' +
        'backup techniques are high performance, and this approach<br>' +
        'has been a common practice for very large databases with<br>' +
        'SQL Server 2000.<br>' +
        '• Partition the large fact tables, and set aged partitions to be<br>' +
        'read-only. Perform occasional full backups, but rely<br>' +
        'primarily on a strategy of filegroup and partial differential<br>' +
        'backups. Under the Simple recovery model, partial backups<br>' +
        'back up the primary filegroup and all the read-write<br>' +
        'filegroups. Read-only partitions are backed up when they’re<br>' +
        'filled and converted to read-only status. The innovation of<br>' +
        'read-only partitions greatly improves your ability to quickly<br>' +
        '1077<br>' +
        'back up the changed portions of the relational data<br>' +
        'warehouse. However, if you have late-arriving fact data,<br>' +
        'you’ll need to partition by load date in order to take greatest<br>' +
        'advantage of fast partition table backups. If instead you are<br>' +
        'partitioning a very large fact table by transaction date and<br>' +
        'have late-arriving fact data, you’ll need to plan for SAN<br>' +
        'backups.<br>' +
        'REFERENCE<br>' +
        'See the Books Online topics “Partial<br>' +
        'Backups” and “Differential Partial<br>' +
        'Backups” for more details.<br>' +
        'The logging database is written to constantly. Some DW/<br>' +
        'BI teams think the logging data is vitally important, and<br>' +
        'implement a very strong backup strategy. Other teams are<br>' +
        'sanguine about the notion of losing a week’s worth of<br>' +
        'logging data, and manage the database far more loosely.<br>' +
        'Obviously, if your logging data contains usage data<br>' +
        'necessary for regulatory compliance, you need to develop<br>' +
        'a serious backup and recovery strategy. Use Full recovery<br>' +
        'mode and a backup strategy appropriate for a transaction<br>' +
        'database. Books Online, and any number of SQL Server<br>' +
        'books, are filled with information about backup strategies<br>' +
        'for transaction databases.<br>' +
        'Approaches differ on backup and recovery strategies for<br>' +
        'the staging databases. Many DW/BI teams think of the<br>' +
        'data in the staging tables as ephemeral, and back up only<br>' +
        'the table CREATE scripts. On the other hand, most staging<br>' +
        'databases contain only data for the most recent loads —<br>' +
        '1078<br>' +
        'for example, the last seven days — so a full database<br>' +
        'backup is really fast.<br>' +
        'You may have built a simple application for business users<br>' +
        'to manipulate custom hierarchies or other attributes of a<br>' +
        'dimension. Such an application is a transaction system,<br>' +
        'however small scale. Typically you want the application to<br>' +
        'write directly to a different database than the data<br>' +
        'warehouse, one with Full recovery mode and log backups.<br>' +
        'Similarly, the metadata database should also be treated<br>' +
        'more like a transactional database than the large data<br>' +
        'warehouse database.<br>' +
        'The msdb system database may include your Integration<br>' +
        'Services packages. It will certainly include any SQL Agent<br>' +
        'job definitions and schedules, and other information used<br>' +
        'by Management Studio, including information about which<br>' +
        'databases were backed up. For that reason, the msdb<br>' +
        'database should always be backed up immediately after<br>' +
        'any other backup operation. Use Full recovery mode for<br>' +
        'msdb.<br>' +
        'Integration Services<br>' +
        'The most important information to back up for Integration<br>' +
        'Services is the package definitions themselves. Packages<br>' +
        'can be stored in SQL Server, in the file system, or in a<br>' +
        'managed mode in the file system called the Package Store.<br>' +
        'If the package definitions are stored in SQL Server, they’re<br>' +
        'located in the msdb system database, which as we’ve<br>' +
        'already discussed should be backed up religiously.<br>' +
        '1079<br>' +
        'If the packages are stored in the file system or the Package<br>' +
        'Store, simply use a file system backup utility like<br>' +
        'Windows Backup to back up the package definitions,<br>' +
        'configuration files, and associated information. Of course,<br>' +
        'package definitions should be under source control, and<br>' +
        'that source control database should be backed up too.<br>' +
        'As we discussed in Chapter 7, you may be staging or<br>' +
        'storing data in the file system. Use Windows Backup or<br>' +
        'another copy utility to back up staged data. This is<br>' +
        'especially vital if you’re relying on re-running staged<br>' +
        'extracts to bring your data warehouse database up-to-date.<br>' +
        'Analysis Services<br>' +
        'Throughout this book we’ve encouraged you to think of<br>' +
        'the Analysis Services database as ephemeral — a database<br>' +
        'that may need to be fully reprocessed at some point. That’s<br>' +
        'necessary because Analysis Services doesn’t support the<br>' +
        'full level of data manageability, notably updates and<br>' +
        'deletes, as the relational database. The great benefits<br>' +
        'provided by Analysis Services in query performance,<br>' +
        'complex security, a calculation engine, and easy user<br>' +
        'navigation come at a cost. You need a plan for being able<br>' +
        'to fully reprocess the dimensional database; never throw<br>' +
        'away the relational data.<br>' +
        'You absolutely must back up the definition of the Analysis<br>' +
        'Services database: the information that enables you to fully<br>' +
        'process the database. You might think that, because you<br>' +
        'have the database definition on your development server<br>' +
        'and checked into source control, you’re safe. You could<br>' +
        'always re-deploy and re-process the Analysis Services<br>' +
        '1080<br>' +
        'database. That’s largely true, but you’ve probably<br>' +
        'modified aggregation design and partition strategy on the<br>' +
        'production database; these changes are not reflected in the<br>' +
        'version on the development server.<br>' +
        'You won’t find a formal command or utility for backing<br>' +
        'up the database’s definition. The most straightforward<br>' +
        'approach is to generate a complete CREATE script for the<br>' +
        'database, and back up that script.<br>' +
        'The recommended method for backing up the Analysis<br>' +
        'Services database is to use the Analysis Services backup<br>' +
        'and restore facility in Management Studio. The greatest<br>' +
        'drawback of Analysis Services backup is that it works only<br>' +
        'at the database level. On the plus side, you can launch the<br>' +
        'Backup and Restore wizards from Management Studio.<br>' +
        'From within the wizard, you can script the commands for<br>' +
        'automated operations. Schedule the backup from SQL<br>' +
        'Agent or launch it from an Integration Services package.<br>' +
        'The Analysis Services Backup facility backs up all<br>' +
        'metadata, but only data that’s stored in MOLAP format.<br>' +
        'This includes all data and aggregations for MOLAP<br>' +
        'partitions, and aggregations only for HOLAP partitions.<br>' +
        'Data stored in the relational data warehouse should be<br>' +
        'backed up using relational backup techniques. Plan for<br>' +
        'your Analysis Services backups to take about as much<br>' +
        'space as the database itself. Analysis Services databases<br>' +
        'are stored so efficiently that we see very little additional<br>' +
        'compression upon backup.<br>' +
        'If your Analysis Services database is small, in the tens of<br>' +
        'gigabytes, the simplest approach is to perform a full<br>' +
        '1081<br>' +
        'backup every load cycle, or whenever you make a<br>' +
        'metadata change. No matter how efficient the backup<br>' +
        'utility might be, if your Analysis Services database is<br>' +
        'multiple terabytes, it might not be practical to perform<br>' +
        'daily full backups.<br>' +
        'NOTE<br>' +
        'If you have a Storage Area Network<br>' +
        '(SAN), frequent full backups are more<br>' +
        'practical, with very limited downtime and<br>' +
        'minimal pressure on server resources. You<br>' +
        'can circumvent the backup utility and<br>' +
        'create a static copy of the files directly:<br>' +
        '• Create a mirror set and wait for it to fully<br>' +
        'synchronize.<br>' +
        '• Stop Analysis Services; break the mirror;<br>' +
        'restart Analysis Services.<br>' +
        '• Mount the mirrored image as a separate<br>' +
        'drive and perform a file level backup of the<br>' +
        'entire data folder (Program<br>' +
        'Files\\Microsoft SQL<br>' +
        'Server\\MSSQL\\OLAP\\Data).<br>' +
        'Our recommended practice is to back up the database<br>' +
        'whenever the metadata changes. Metadata changes include<br>' +
        'redesigning aggregations, adding a partition, or changing<br>' +
        'security groups and permissions. If you just can’t do a full<br>' +
        'backup every time the metadata changes, you must capture<br>' +
        'the complete database definition scripts.<br>' +
        'Reporting Services<br>' +
        '1082<br>' +
        'All of your Reporting Services report definitions and<br>' +
        'schedules are in the ReportServer database. This database<br>' +
        'should use Full recovery mode, and be backed up like any<br>' +
        'transactional database.<br>' +
        'Recovery<br>' +
        'It is as important to document and test your recovery plan<br>' +
        'as it is to perform backups. During an emergency is not the<br>' +
        'time to test out your recovery procedures. We could regale<br>' +
        'you with sad tales of daily backups to corrupt media that<br>' +
        'were never tested until too late. Despite the fact that this is<br>' +
        'kindergarten-level system administration, we are past<br>' +
        'being astonished at finding people who don’t know if their<br>' +
        'recovery procedures will work.<br>' +
        'NOTE<br>' +
        'We’ve said it several times already, but<br>' +
        'once more: Backup without verification is<br>' +
        'meaningless and a waste of time. You’re<br>' +
        'better off not even doing the backup, and<br>' +
        'not kidding yourself that you’re protected.<br>' +
        'Good intentions don’t count.<br>' +
        'Verification doesn’t mean checking the<br>' +
        'checkbox in the utility, which verifies the<br>' +
        'physical media. That’s a good thing to do;<br>' +
        'it’s just not what we’re talking about.<br>' +
        'We’re talking about testing the full<br>' +
        '1083<br>' +
        'recovery process, including the BI<br>' +
        'applications, to make sure everything really<br>' +
        'works. And it’s not just that the scripts<br>' +
        'work. You need to confirm, by testing, that<br>' +
        'your staff knows what steps to take to<br>' +
        'successfully restore the system.<br>' +
        'Executing the ETL Packages<br>' +
        'During development, you design and execute Integration<br>' +
        'Services packages within BI Development Studio (BIDS).<br>' +
        'In the development environment, you can set breakpoints,<br>' +
        'examine variables’ values and status, watch the movement<br>' +
        'of data through a data flow task, and explore the data with<br>' +
        'data viewers. All of these tools are valuable during<br>' +
        'development, but are of no interest in production. You<br>' +
        'want the ETL system to execute, upon a schedule, with no<br>' +
        'human intervention.<br>' +
        'Integration Services packages are easy to schedule. SQL<br>' +
        'Server ships the tools you need: dtexec and SQL Server<br>' +
        'Agent. You should already be familiar with dtexec and its<br>' +
        'friend dtexecui, from the process of testing the<br>' +
        'Integration Services packages. Dtexec and dtexecui<br>' +
        'execute a package from the command line. They are<br>' +
        'basically the same, except that dtexecui brings up a user<br>' +
        'interface that helps you construct the command by<br>' +
        'choosing which package to run and picking various<br>' +
        'options like logging levels and connection information.<br>' +
        '1084<br>' +
        'Once you’ve set up the package execution options, you can<br>' +
        'create a SQL Agent job to run the package, and then<br>' +
        'schedule that job. SQL Agent is a standard feature of the<br>' +
        'SQL Server database.<br>' +
        'REFERENCE<br>' +
        'The dtexec utility is well documented in<br>' +
        'the Books Online topics “dtexec Utility”<br>' +
        'and “How to: Run a Package Using the<br>' +
        'DTExec Utility.”<br>' +
        'Once you’ve set up your SQL Agent job step, schedule it<br>' +
        'to run on the appropriate schedule. In most cases, you’ll<br>' +
        'define one or a small number of SQL Agent jobs for your<br>' +
        'ETL system. Each job calls a master Integration Services<br>' +
        'package, which in turn calls subpackages in the correct<br>' +
        'order. Build logic flow and dependencies into the master<br>' +
        'package, and use SQL Agent only to kick off the main job.<br>' +
        'Some organizations have standardized on enterprise-wide<br>' +
        'job management software like Autosys. SQL Agent is<br>' +
        'nothing special; if your organization uses such software,<br>' +
        'by all means conform to the corporate standard. The job<br>' +
        'management software simply calls dtexec on the<br>' +
        'computer running Integration Services.<br>' +
        'Summary<br>' +
        'Most of us find it more fun to think about designing and<br>' +
        'developing, than operating and maintaining. But there’s no<br>' +
        '1085<br>' +
        'point in undertaking the design activities if you’re not<br>' +
        'confident your system can operate smoothly, efficiently,<br>' +
        'and with good performance. And it’s important to think<br>' +
        'about these issues early, during the design and<br>' +
        'development phases of the system. Good operating<br>' +
        'procedures are cooked into the system, not tacked on at the<br>' +
        'end.<br>' +
        'This chapter talked about two kinds of operational<br>' +
        'procedures: front-room operations and back-room<br>' +
        'operations. Front-room operations, from maintaining the<br>' +
        'BI portal to extending BI applications and educating users,<br>' +
        'requires a continuing commitment to meeting the needs of<br>' +
        'the business. It requires a significant number of ongoing<br>' +
        'staff, usually as many, if not more, as were involved with<br>' +
        'the initial development of the DW/BI system.<br>' +
        'Most of the chapter was devoted to a discussion of<br>' +
        'back-room operations. We described the tools available to<br>' +
        'easily monitor your system and usage. We introduced<br>' +
        'some of the issues you will need to consider in order to<br>' +
        'meet availability and performance SLAs. And we<br>' +
        'discussed the most important factors to consider when<br>' +
        'tuning your DW/BI system for excellent performance.<br>' +
        'The last section of this chapter discussed issues around<br>' +
        'backing up and restoring your databases and other<br>' +
        'components of your system. We’ll take one last<br>' +
        'opportunity to remind you to take backup and recovery<br>' +
        'seriously, or don’t do it at all.<br>' +
        '1086<br>';
    document.getElementById('chapter16').innerHTML = 'Chapter 16<br>' +
        'Deployment<br>' +
        '“Great occasions do not make heroes or cowards; they<br>' +
        'simply unveil them to the eyes of men.”<br>' +
        '— Brooke Foss Wescott<br>' +
        'As you see in Figure 16-1, the deployment step in the<br>' +
        'Kimball Lifecycle is where the three parallel development<br>' +
        'tracks come back together. This is the great unveiling of<br>' +
        'the DW/BI system to the business community. The quality<br>' +
        'of this first impression will strongly influence the<br>' +
        'acceptance of the system — and you get only one shot at<br>' +
        'it. Like any big event, a lot of details must fall into place in<br>' +
        'order for the show to be successful.<br>' +
        'Figure 16-1: The deployment step in the Kimball<br>' +
        'Lifecycle<br>' +
        '970<br>' +
        'Most systems professionals think of deployment as moving<br>' +
        'code from development to test to production. As far as it<br>' +
        'goes, they’re correct: Deploying code and data to the<br>' +
        'production servers is a fundamental part of providing end<br>' +
        'user access. But the system deployment is only part of the<br>' +
        'deployment story. The deployment step includes<br>' +
        'pre-deployment testing, as well as all the pieces needed to<br>' +
        'give the business users access to the information. To be<br>' +
        'effective, the deployment effort should begin early in the<br>' +
        'Lifecycle. While you’re creating the architecture and<br>' +
        'building the data warehouse and the BI applications, you<br>' +
        'must also be defining and running tests, creating<br>' +
        'documentation, preparing training, and organizing the user<br>' +
        'support processes. All these services need to be in place<br>' +
        'before the curtain goes up for the first time.<br>' +
        'This chapter is split into two parts: The first concentrates<br>' +
        'on the system testing and deployment process and the<br>' +
        'second spotlights all the other critical but less technical<br>' +
        'activities needed to ensure a successful deployment.<br>' +
        'In this chapter, you will learn:<br>' +
        '• What kinds of testing need to be performed for a new DW/<br>' +
        'BI system or changes to an existing system.<br>' +
        '• How to effectively set up test environments to test<br>' +
        'continuously.<br>' +
        '• How to deploy the various components of the SQL Server<br>' +
        'DW/BI system into production.<br>' +
        '• Various strategies for deploying changes to a system already<br>' +
        'in production.<br>' +
        '• What steps you need to take to ensure your user community<br>' +
        'is ready for the roll out of the new DW/BI system.<br>' +
        '971<br>' +
        'Setting Up the Environments<br>' +
        'Although deployment is one of the final steps in a project’s<br>' +
        'lifecycle, you must plan for deployment from the outset.<br>' +
        'Many deployment tasks are painful if you wait until the<br>' +
        'last minute. But as with most things, a bit of planning will<br>' +
        'minimize that pain.<br>' +
        'For your initial project deployment of a brand new DW/BI<br>' +
        'system, you may be able to get by with two environments:<br>' +
        'development and test. You can have one very easy<br>' +
        'technical deployment by building out and testing in the test<br>' +
        'environment, and then simply declaring it to be<br>' +
        'production. But that technique only works once. To<br>' +
        'manage changes to a system already in production, you<br>' +
        'need multiple environments:<br>' +
        '• Development: The development environment includes<br>' +
        'development server(s) and developer workstations. The<br>' +
        'development server environment should mimic production<br>' +
        'with respect to:<br>' +
        '• CPU architecture. Do yourself a favor: don’t use<br>' +
        'an old 32-bit machine as your development<br>' +
        'database server. Drivers and connectivity are<br>' +
        'different in the 64-bit environment, and finding the<br>' +
        'best new drivers can be tedious. This is<br>' +
        'particularly true if you will be connecting to<br>' +
        'databases other than SQL Server.<br>' +
        '• Disk layout. Ideally, development will have the<br>' +
        'same number of drives and addresses as<br>' +
        'production, though of course much smaller.<br>' +
        '• Server topology. Ideally, have as many<br>' +
        'development servers as you plan in production.<br>' +
        'Place the various server components (relational,<br>' +
        'Integration Services, Analysis Services, Reporting<br>' +
        'Services) as you plan to do on production.<br>' +
        '972<br>' +
        '• Virtual machines may be helpful.<br>' +
        '• Security. At the very least, use service accounts to<br>' +
        'administer the server components. Ideally, use the<br>' +
        'same service accounts that you’ll use in<br>' +
        'production.<br>' +
        '• Operating system and other software versions<br>' +
        'should be identical, although it’s fine to use SQL<br>' +
        'Server Developer Edition.<br>' +
        '• Primary Test: The primary test environment is used to test<br>' +
        'the DW/BI system changes that are soon to go into<br>' +
        'production. The primary test system must mimic production<br>' +
        'in all particulars. We recommend that the primary test<br>' +
        'servers and environment be identical to production. This is<br>' +
        'particularly true if you will conduct performance tests. Of<br>' +
        'course, this is an ideal configuration, and many installations<br>' +
        'fall short of purchasing a second high powered server just<br>' +
        'for testing. However, the primary test environment must<br>' +
        'contain a complete copy of the data warehouse, or at least a<br>' +
        'complete copy of the portion of the data warehouse that’s<br>' +
        'under development or change. How can you possibly test<br>' +
        'data quality unless you have all the data? Pay particular<br>' +
        'attention to:<br>' +
        '• Disk layout and server topology.<br>' +
        '• Software versions.<br>' +
        '• Security: the service accounts must be the same as<br>' +
        'in production, or you are guaranteed a rocky<br>' +
        'deployment.<br>' +
        '• User Acceptance Test (UAT): If you are outsourcing the<br>' +
        'development of your DW/BI system, you should require a<br>' +
        'complete UAT system in addition to the vendor’s internal<br>' +
        'test system.<br>' +
        '• Deployment Test: As we will discuss, one of your<br>' +
        'pre-deployment tests is to confirm that the deployment<br>' +
        'scripts and processes will work. You need a copy of the<br>' +
        'production system, usually with significantly scaled down<br>' +
        'data volumes, on which you can test the deployment process.<br>' +
        'Often, the deployment test server can be a virtual machine.<br>' +
        '973<br>' +
        '• Other test environments: Depending on the complexity of<br>' +
        'your environment, you may need additional test systems.<br>' +
        'Multiple projects may be working on the DW/BI system, and<br>' +
        'each may need slightly different flavors of an early test<br>' +
        'environment. For example, you may be working on a project<br>' +
        'to move to a different version of SQL Server at the same<br>' +
        'time a new subject area is being added to the DW. Often,<br>' +
        'these other test environments can use a scaled down<br>' +
        'database. It may be possible to use virtual machines.<br>' +
        'There are several additional considerations for the<br>' +
        'development environment.<br>' +
        '• Install SQL Server Developer Edition on the development<br>' +
        'database servers.<br>' +
        '• Install SQL Server Business Intelligence Development<br>' +
        'Studio (BIDS) on the ETL development server, which is<br>' +
        'usually the same server as the relational data warehouse<br>' +
        'database. ETL developers typically use remote desktop to<br>' +
        'access this server while working on SSIS packages.<br>' +
        'Otherwise, SSIS will run on the developers’ desktops as they<br>' +
        'develop and debug packages.<br>' +
        '• Install BIDSHelper on the ETL development server.<br>' +
        '• Install BIDSHelper on the workstation of each Analysis<br>' +
        'Services developer.<br>' +
        'NOTE<br>' +
        'BIDSHelper is an add-on to BI<br>' +
        'Development Studio (BIDS). It’s available<br>' +
        'from CodePlex, and has been developed<br>' +
        'and maintained since the SQL Server 2005<br>' +
        'era. Although we hate to recommend that<br>' +
        'development teams depend on third-party<br>' +
        'software, we do so in this case.<br>' +
        '974<br>' +
        '• BIDSHelper affects the development<br>' +
        'experience, not the final database structure.<br>' +
        '• It is an open source Visual Studio.Net<br>' +
        'add-in, with source code available if<br>' +
        'necessary.<br>' +
        '• Among its many valuable features is<br>' +
        'SmartDiff, which compares two SSIS<br>' +
        'packages or Analysis Services (SSAS)<br>' +
        'databases, to identify changes. We know of<br>' +
        'no other usable tool for performing this<br>' +
        'vital action.<br>' +
        'Your organization should already have change<br>' +
        'management processes and applications in place to help<br>' +
        'manage any development project. These include:<br>' +
        '• Version control, also called source control<br>' +
        '• Work item tracking, also called bug tracking<br>' +
        '• Test case creation, scripting, and test execution tracking<br>' +
        'Visual Studio 2010 and its companion server Team<br>' +
        'Foundation Server 2010 are the current Microsoft products<br>' +
        'to provide this functionality. They do a really nice job, and<br>' +
        'the server is far easier to install than it used to be. Older<br>' +
        'versions of Visual Studio, or other Microsoft or third-party<br>' +
        'software, are perfectly acceptable as well.<br>' +
        'Figure 16-2 illustrates source control integration with<br>' +
        'BIDS. In Figure 16-2, an entire SSIS project has been<br>' +
        'placed under source control. In the Solution Explorer on<br>' +
        'the right, you can see the locked sign by most of the<br>' +
        'packages. You can check in a package directly from the<br>' +
        'Solution Explorer. Note that the dropdown menu has new<br>' +
        'options for checking in and checking out. There’s also a<br>' +
        '975<br>' +
        'new Pending Checkins window, displayed in the bottom<br>' +
        'left.<br>' +
        'Figure 16-2: Source control integration with BIDS<br>' +
        'NOTE<br>' +
        'Visual Studio 2010 was released at almost<br>' +
        'the same time as SQL Server 2008 R2, and<br>' +
        'as a result the two products are not fully<br>' +
        'integrated. SQL Server BIDS is based on<br>' +
        'Visual Studio 2008. It may seem that with<br>' +
        'a simultaneous release, there’s a better<br>' +
        'opportunity to fully integrate the two<br>' +
        '976<br>' +
        'products, but that’s true only if they were<br>' +
        'originally planned to release together.<br>' +
        'Testing<br>' +
        'Before you start deploying your system, you should<br>' +
        'perform extensive testing. Throughout the development<br>' +
        'process, you should have been conducting unit tests to<br>' +
        'confirm that components have been written properly. As<br>' +
        'we discuss in the next section, you need to perform<br>' +
        'extensive end-to-end testing, too.<br>' +
        'The less time you spend on testing, the harder the<br>' +
        'deployment process will be. If you don’t perform<br>' +
        'end-to-end testing before you deploy to production, you’re<br>' +
        'guaranteed to find problems on production. If you don’t<br>' +
        'rigorously check data quality, your business users will do<br>' +
        'it for you, and lose confidence in the DW/BI system at the<br>' +
        'same time. If you don’t check performance and tune the<br>' +
        'system in advance, you’ll have to do it while trying to<br>' +
        'avoid disrupting business users.<br>' +
        'If you test, tune, and adjust the system before you begin<br>' +
        'deployment, the move to production will be fairly<br>' +
        'straightforward. This is especially true if this is a new<br>' +
        'system. Moving incremental changes into production while<br>' +
        'minimizing end-user impact can be a delicate dance.<br>' +
        'To successfully deploy a new DW/BI system, or changes<br>' +
        'to an existing system, plan ahead and test repeatedly until<br>' +
        'you’ve completed:<br>' +
        '977<br>' +
        '• Development testing: The developers test as they develop the<br>' +
        'ETL system. This is also called unit testing.<br>' +
        '• System testing: The databases load and process correctly,<br>' +
        'from start to finish. Cubes are processed, automated reports<br>' +
        'are run, and downstream BI application processes are<br>' +
        'launched.<br>' +
        '• Data quality assurance testing: The data is accurate and<br>' +
        'complete, both for the historical load and for ongoing<br>' +
        'incremental loads.<br>' +
        '• Performance testing: The system performs well both for<br>' +
        'loads and for queries and reports, with live data rather than a<br>' +
        'smaller development data set.<br>' +
        '• Usability testing: Business users can find what they need<br>' +
        'and accomplish necessary tasks.<br>' +
        '• Deployment testing: Deployment scripts are solid and have<br>' +
        'been rehearsed.<br>' +
        'The following section covers each of these testing areas in<br>' +
        'greater detail.<br>' +
        'Development Testing<br>' +
        'The first phase of testing is performed in concert with all<br>' +
        'development activities, especially the development of the<br>' +
        'SSIS packages for the ETL system. Development testing is<br>' +
        'almost always performed by the developer, and is often<br>' +
        'called unit testing.<br>' +
        'Testing for a DW/BI system is no different in theory than<br>' +
        'testing for any other kind of system development.<br>' +
        'Developers need to create and run tests throughout the<br>' +
        'development process. Tests should be additive: run all old<br>' +
        'tests every night, rather than testing only new<br>' +
        'functionality. Test results should be logged and reported<br>' +
        'on.<br>' +
        '978<br>' +
        'In practice, DW/BI testing is difficult. There are plenty of<br>' +
        'methodologies and tools available to help build test cases<br>' +
        'into code, but the SSIS developer doesn’t write much code.<br>' +
        'Instead, SSIS developers manipulate the user interface in<br>' +
        'BIDS to create a package. Of course, a package is a unit of<br>' +
        'code, but the developer doesn’t have the same level of<br>' +
        'control over it as, say, a C# module or a stored procedure.<br>' +
        'As a developer creates functionality in the package, he<br>' +
        'should create a test for that functionality. Developers<br>' +
        'create these unit tests as a natural part of the development<br>' +
        'process, and the tests should be documented and<br>' +
        'formalized. Expect developers to create a lot of tests; even<br>' +
        'a simple package will have dozens, and a complex package<br>' +
        'can easily have several hundred.<br>' +
        'NOTE<br>' +
        'Your organization should already have test<br>' +
        'management software and processes in<br>' +
        'place for documenting tests. The key<br>' +
        'metadata about a test is:<br>' +
        '• Who created it, and when.<br>' +
        '• Description of what condition is being<br>' +
        'tested, for example, “Surrogate key lookup<br>' +
        'failure between orders fact and currency<br>' +
        'dimension.”<br>' +
        '• Part of the system the test applies to, for<br>' +
        'example, “Data flow task of the<br>' +
        'orders_xform.dtsx package.”<br>' +
        '• How to run the test.<br>' +
        '979<br>' +
        'Testing begins from a known, unchanging test source<br>' +
        'database: a subset of the real transaction database. A<br>' +
        'realistic test source database will include all sorts of data<br>' +
        'problems, so many unit tests can work directly on that<br>' +
        'source. Other tests — the most obvious kind being a test of<br>' +
        'the incremental extract process — require that the test<br>' +
        'source database be modified. Each developer should<br>' +
        'maintain a script for modifying the test source database to<br>' +
        '“cook in” test cases.<br>' +
        'Creating a Source Test Database<br>' +
        'In order to develop consistent unit tests, you must<br>' +
        'start from a known, unchanging source. We<br>' +
        'recommend that you take the time to develop a<br>' +
        'source database that replicates the schema from the<br>' +
        'source transaction system. This test database will<br>' +
        'be used to test the functionality of the system,<br>' +
        'primarily the ETL system. It’s not used to check<br>' +
        'data quality or performance.<br>' +
        'The source test database should have the following<br>' +
        'characteristics:<br>' +
        '• Static: Once it’s developed and in use, it shouldn’t be<br>' +
        'changed. Changes can invalidate existing tests, so any<br>' +
        'change should be managed and thoroughly<br>' +
        'communicated to the entire team.<br>' +
        '• Small: In most cases, you want a small but not<br>' +
        'tiny set of data, on the order of 50,000 to 100,000<br>' +
        'rows.<br>' +
        '• Consistent: More accurately, as consistent as the<br>' +
        'source system allows. If the source system has<br>' +
        '980<br>' +
        'referential integrity in place between two tables,<br>' +
        'maintained via foreign key relationships, so<br>' +
        'should the test database.<br>' +
        '• Representative: Grab data from the entire history<br>' +
        'that will be loaded into the data warehouse, not<br>' +
        'just the most recent month.<br>' +
        'Copy small tables in their entirety. There’s usually<br>' +
        'an obvious driver table — often Customer — that<br>' +
        'you can use to subset the transactional data. Pull a<br>' +
        'random subset of perhaps 5,000 customers, and all<br>' +
        'the transactions associated with them.<br>' +
        'If it’s important that developers not have access to<br>' +
        'real data, you can transform the data in the test<br>' +
        'database. There are programs that will generate test<br>' +
        'data for you, but they’re not very useful for testing<br>' +
        'ETL, because real data is never so well behaved.<br>' +
        'If your development project spans multiple source<br>' +
        'systems, you should develop consistent test<br>' +
        'databases for each.<br>' +
        'Each developer should also maintain a script for<br>' +
        'performing the tests. Most often, these scripts involve<br>' +
        'counting rows in the test source and the test target (the data<br>' +
        'warehouse or staging database). Sums and hashes are also<br>' +
        'used to confirm that data was not corrupted or modified.<br>' +
        'Of course, each test should log its results.<br>' +
        'We have found it effective to execute many unit tests<br>' +
        'together in a single test group. Begin with a script to<br>' +
        'modify the test source database; run the package or<br>' +
        '981<br>' +
        'packages; and then run a script to test the results. You may<br>' +
        'be able to group hundreds of unit tests together; remember<br>' +
        'that many test cases will be covered by the data in the<br>' +
        'static test database, without need for modification.<br>' +
        'NOTE<br>' +
        'The recommendation of grouping unit tests<br>' +
        'runs counter to most testing methodology<br>' +
        'best practices, which would have you make<br>' +
        'a small change, run the test, clean up, and<br>' +
        'run the next test. We prefer to group tests<br>' +
        'to avoid running the packages hundreds of<br>' +
        'times.<br>' +
        'An alternative, and arguably better,<br>' +
        'approach would be to bind the source<br>' +
        'database changes and the SSIS package<br>' +
        'execution into a single transaction. The<br>' +
        'script would make (but not commit)<br>' +
        'modifications in the test source database,<br>' +
        'run the package and record the results, then<br>' +
        'rollback the transaction.<br>' +
        'You will need to invest in a little infrastructure to make<br>' +
        'consistent, ongoing unit testing be effective. It has to be<br>' +
        'easy for the developers, or they’ll constantly put off<br>' +
        'writing formal tests.<br>' +
        'Today, many ETL developers keep their own data sets,<br>' +
        'containing the malformed data used in unit tests. Instead,<br>' +
        '982<br>' +
        'have developers keep the script to make those<br>' +
        'modifications to the test source database, rather than keep<br>' +
        'the data itself.<br>' +
        'The execution of a group of tests consists of the following<br>' +
        'steps:<br>' +
        '• Test initialization: Set up the test environment by restoring a<br>' +
        'copy of the static test source database and target relational<br>' +
        'DW. The restored database is uniquely named (auto-name it<br>' +
        'by adding a timestamp to the name, or tag it with the<br>' +
        'developer’s name). Alternatively, clone a virtual machine<br>' +
        'with the test environment.<br>' +
        '• Test setup: Run a script or SSIS package to modify the test<br>' +
        'environment, for example to apply inserts, updates, and<br>' +
        'deletes to data in the test source systems. This step can<br>' +
        'include multiple data modifications to cover multiple unit<br>' +
        'tests.<br>' +
        '• Test execution: Run the SSIS package(s).<br>' +
        '• Test result verification: Evaluate whether the test passed or<br>' +
        'failed. Many tests are verified simply, by counting rows in<br>' +
        'the source and target. Other tests look for the existence of an<br>' +
        'error file. Results should be logged to a tracking table, which<br>' +
        'is probably the same test results tracking table you use<br>' +
        'elsewhere in your test environment.<br>' +
        '• Test cleanup: Drop the test databases and clean up any<br>' +
        'operating system artifacts. If you’re using a virtual machine,<br>' +
        'simply delete it.<br>' +
        'Every time a developer checks in a package, he should<br>' +
        'annotate the checkin with a reference to the unit test run<br>' +
        'results. Alternatively, the check-in process can<br>' +
        'automatically launch the test runs. All unit tests should run<br>' +
        'every night on all checked in code. If a new failure pops<br>' +
        'up, it will be much easier for the developers to diagnose<br>' +
        'and fix it.<br>' +
        '983<br>' +
        'Using Integration Services Configurations<br>' +
        'In order to test SSIS packages, you’ll need to<br>' +
        'modify some characteristics of the package,<br>' +
        'including:<br>' +
        '• Connection information for both source and target<br>' +
        'systems.<br>' +
        '• Extract date range.<br>' +
        '• Data quality limits built into the package, such as a<br>' +
        'minimum number of rows to extract, below which<br>' +
        'threshold processing is halted. Generally you’d want<br>' +
        'to lower this threshold if you are testing with smaller<br>' +
        'data sets.<br>' +
        'These characteristics can easily be modified at<br>' +
        'runtime by using SSIS package configurations.<br>' +
        'Integration Services configurations are a powerful<br>' +
        'tool for modifying the way a package behaves at<br>' +
        'runtime, without editing the package. This<br>' +
        'capability is very important for controlling package<br>' +
        'contents and quality: Once a package’s quality has<br>' +
        'been assured, it should be locked down and not<br>' +
        'edited again. But some things about a package may<br>' +
        'have to change. This is particularly true of<br>' +
        'connectivity information, which almost always<br>' +
        'changes as you move a package from development<br>' +
        'to test to production.<br>' +
        'You can pull out almost any characteristic of a task<br>' +
        'or connection into a configuration file. At the time<br>' +
        '984<br>' +
        'the package is executed, SSIS reads the<br>' +
        'configuration files, overwriting the default value of<br>' +
        'a parameter or variable with its value from the<br>' +
        'configuration file. You can have many<br>' +
        'configuration files, and even overwrite the same<br>' +
        'parameter multiple times. The last configuration<br>' +
        'file in order will “win.”<br>' +
        'Some applications, especially those built as<br>' +
        'shrink-wrapped systems, will make extensive use<br>' +
        'of configurations. Even the simplest application<br>' +
        'will probably use them at least twice:<br>' +
        '• To pass parameter values from a master package to<br>' +
        'its child packages<br>' +
        '• To modify package connection information as<br>' +
        'packages move from development to test and<br>' +
        'production<br>' +
        'When you create a package configuration in BIDS,<br>' +
        'the product will create a template XML file<br>' +
        'structure for you containing the elements you’ve<br>' +
        'chosen to configure. You should script the<br>' +
        'modification of the configuration files. For<br>' +
        'example, the nightly load process would<br>' +
        'automatically set the extract date range variables to<br>' +
        'yesterday’s date.<br>' +
        'There is nothing magic about SSIS configurations.<br>' +
        'If you’re more comfortable keeping parameter<br>' +
        'values and connection information in a database<br>' +
        '985<br>' +
        'table, and reading them into your packages via an<br>' +
        'Execute SQL task, that’s perfectly fine.<br>' +
        'System Testing<br>' +
        'System testing ensures the system is complete, and the<br>' +
        'requirements (as documented) have been met. These are<br>' +
        'not only functional requirements, but also quality<br>' +
        'requirements like performance and security. Data quality<br>' +
        'can be considered part of system testing, but it’s so central<br>' +
        'to the data warehouse that we discuss it separately.<br>' +
        'The system tester must not be a developer. Even if you’re<br>' +
        'a tiny organization with a one-person development “team,”<br>' +
        'you should recruit someone else to evaluate the operation<br>' +
        'of the system. A developer is simply too tied in to the way<br>' +
        'things are. You need a second set of eyes.<br>' +
        'The system tester should design her tests from the system<br>' +
        'specifications. In the all too common absence of a formal<br>' +
        'specification document, work with the DW/BI team lead to<br>' +
        'understand detailed requirements.<br>' +
        'The goal of system testing is to ensure the periodic<br>' +
        '(usually nightly) load process works smoothly and<br>' +
        'completely. It takes place on the test server, beginning<br>' +
        'with a copy of the source and target databases. The last<br>' +
        'system tests, just before the system goes into production,<br>' +
        'should use live feeds if possible. Typically, the one-time<br>' +
        'historical load is not subject to rigorous system tests. The<br>' +
        '986<br>' +
        'one-time historical load is subject to extensive data quality<br>' +
        'tests, as we discuss in the next section.<br>' +
        'The system testing will test the following major categories:<br>' +
        '• Job environment setup<br>' +
        '• Setting up variables and connection strings.<br>' +
        '• Creating a clean copy of the source environments<br>' +
        'if necessary.<br>' +
        '• Job startup<br>' +
        '• Waiting for any startup conditions, such as files<br>' +
        'showing up in a directory or another signal that a<br>' +
        'source process has finished.<br>' +
        '• Launching the master package with the correct<br>' +
        'configuration file.<br>' +
        '• Handling correctly any startup problems (it’s 4<br>' +
        'a.m. and the source system process still hasn’t<br>' +
        'completed).<br>' +
        '• ETL job execution<br>' +
        '• With correctly formed input data, the ETL job runs<br>' +
        'to completion.<br>' +
        '• Error rows are handled appropriately.<br>' +
        '• If the input data violates any conditions built into<br>' +
        'the ETL job stream (such as a minimum number of<br>' +
        'extracted rows), the ETL job terminates smoothly.<br>' +
        '• Automated data quality checking<br>' +
        '• Rowcounts, sums, and hashes that are built into the<br>' +
        'ETL process are checked and errors reported<br>' +
        'appropriately.<br>' +
        '• Analysis Services cube processing<br>' +
        '• Completed without error.<br>' +
        '• Provides accurate data (run the same query in<br>' +
        'SSAS and the relational database).<br>' +
        '• Automatic report execution and distribution.<br>' +
        '• Data mining and other BI application execution.<br>' +
        '987<br>' +
        '• Job environment clean-up, including copying, deleting, or<br>' +
        'renaming files or other objects as needed.<br>' +
        'If you’ve followed our advice in the preceding section, to<br>' +
        'set up development unit tests to run automatically,<br>' +
        'continuously, even obsessively, then you’ll find most of<br>' +
        'system testing to be straightforward. Setting up the system<br>' +
        'tests requires little new testing infrastructure beyond what<br>' +
        'you set up for the development tests.<br>' +
        'Ideally, system testing begins as soon as there is even a<br>' +
        'single SSIS package checked in. More precisely: two<br>' +
        'packages, a master package and a child package.<br>' +
        'Implement tests for all components of the final system,<br>' +
        'even if you know those tests will fail now. For example,<br>' +
        'you can add a test for the accurate incremental processing<br>' +
        'of the SSAS database even before that database is created.<br>' +
        'Start running system tests as soon as possible. The test<br>' +
        'process will mature in tandem with development. Long<br>' +
        'before you’re really interested in the results of the system<br>' +
        'tests, the details of how to test should have been worked<br>' +
        'out. The system tester should start testing against the test<br>' +
        'data, rather than waiting for the live data to come online.<br>' +
        'This will mean that the system tester will need to generate<br>' +
        'some transaction data. They can talk to the developers and<br>' +
        'look at their unit tests, but should create their own scripts<br>' +
        'for inserting, updating, and deleting data in the test source<br>' +
        'system. One of the many advantages of starting system<br>' +
        'testing against test data is that you can cook in data<br>' +
        'problems, such as a referential integrity violation, that<br>' +
        'might not surface in the real data during the period on<br>' +
        'which you’re performing live testing.<br>' +
        '988<br>' +
        'While there are potentially dozens of infrastructure-related<br>' +
        'problems that surface during system testing, you’ll almost<br>' +
        'inevitably stumble over security issues. As we described in<br>' +
        'Chapter 14, system operations should use system accounts<br>' +
        'that are independent of any individual user ID. System<br>' +
        'testing must use the same security roles and accounts as<br>' +
        'production.<br>' +
        'System Testing for SQL Server Standard<br>' +
        'Edition<br>' +
        'Most organizations use Developer Edition for their<br>' +
        'development and testing environments. The best<br>' +
        'practice is to use the same edition as production for<br>' +
        'your final system testing on live data. This is<br>' +
        'especially true if you’re deploying to Standard<br>' +
        'Edition, because Developer Edition contains all the<br>' +
        'functionality in Enterprise Edition.<br>' +
        'Data Quality Assurance Testing<br>' +
        'We can’t overemphasize the importance of testing the<br>' +
        'quality of the data in your DW/BI system. The data needs<br>' +
        'to be checked very carefully before the system goes live.<br>' +
        'Develop subsystems and procedures for continuing to<br>' +
        'check data accuracy after deployment. Delivering bad data<br>' +
        'is much worse than delivering no data at all.<br>' +
        'Data quality testing, at its heart, consists of running a<br>' +
        'query or report from the source system or systems, running<br>' +
        'the corresponding query or report from the DW/BI system,<br>' +
        '989<br>' +
        'and comparing results. The magic comes in knowing the<br>' +
        'corresponding query or report. You may need significant<br>' +
        'business knowledge to match up the multiple systems,<br>' +
        'applied transformations, and business rules. For this reason<br>' +
        'and for buy-in from the business community, you must<br>' +
        'include the business users in the data quality assurance<br>' +
        'process. Hopefully, you already have some data stewards<br>' +
        'from the user community identified as subject area data<br>' +
        'quality experts.<br>' +
        'NOTE<br>' +
        'Even in the unusual case in which you’re<br>' +
        'completely outsourcing your DW/BI<br>' +
        'system design development, your<br>' +
        'organization — and your business user<br>' +
        'community — must take ownership of data<br>' +
        'quality test definition. Your vendor should<br>' +
        'provide a framework for running those tests<br>' +
        'and folding them into the ETL process, but<br>' +
        'only those with deep business knowledge<br>' +
        'can confirm that the data is accurate.<br>' +
        'The vendor user acceptance testing (UAT)<br>' +
        'process is far too late for you to start<br>' +
        'testing data quality. By the time you’re in<br>' +
        'UAT, there’s going to be huge political<br>' +
        'pressure, from both your management and<br>' +
        'the vendor, to accept the system and move<br>' +
        'into production. This is especially true if<br>' +
        '990<br>' +
        'the DW/BI system is an add-on to a new<br>' +
        'source system that’s also part of the UAT.<br>' +
        'Any significant data quality problem can<br>' +
        'easily require large re-work, leading to<br>' +
        'delays of weeks or months.<br>' +
        'Make sure you can reproduce existing reports to the penny.<br>' +
        'Sometimes we find during DW/BI system development<br>' +
        'that existing reports have long been in error. This makes it<br>' +
        'harder to verify the new reports, but you absolutely must<br>' +
        'audit them and document any discrepancies. If you have an<br>' +
        'Internal Audit group, enlist their assistance in this process.<br>' +
        'We’ve often been asked to recommend tools to help test<br>' +
        'data quality. We don’t know of any tools that help with the<br>' +
        'hard part of testing: determining what data needs to be<br>' +
        'tested and how to do so. At the very least, the data quality<br>' +
        'testing reports should include row counts, grand totals, and<br>' +
        'subtotals along major dimensions, hierarchies, and by<br>' +
        'time.<br>' +
        'Your Analysis Services database may include some<br>' +
        'complex calculations and KPIs. You should have someone<br>' +
        'test the calculations externally — usually in Excel — to<br>' +
        'confirm that the MDX expressions are correct.<br>' +
        'Report definitions sometimes include calculations as well.<br>' +
        'Check everything: in a budget variance report that displays<br>' +
        'budgets, actuals, and variance, confirm that the variance is<br>' +
        'truly the difference between the other columns. Even in<br>' +
        'this trivial case you may see a penny difference due to<br>' +
        '991<br>' +
        'rounding. Discuss that rounding with your business users<br>' +
        'and get a decision from them on how to handle it.<br>' +
        'As with other kinds of tests, automate data quality tests as<br>' +
        'much as possible. During testing and deployment, you’ll<br>' +
        'typically run the data quality tests at least three times:<br>' +
        '• Test the outcome of running the primary test data set: This<br>' +
        'static data set is small, and is easy to check thoroughly.<br>' +
        '• Test the historical load: Perform extensive data quality<br>' +
        'checks on the one-time historical load. Get a handful of<br>' +
        'business users involved. Not only are they the ones who<br>' +
        'know the data, but you can leverage their reputation among<br>' +
        'the user community.<br>' +
        '• Test the live data: Once you start running live data through<br>' +
        'your test system, continue testing the validity of that data.<br>' +
        'Often the data stewards are granted permissions into the<br>' +
        'system while it’s still in test.<br>' +
        'Any automated data quality tests that you developed for<br>' +
        'the deployment phase should eventually be folded into the<br>' +
        'ETL process. Develop processes for logging the results of<br>' +
        'the ongoing data quality tests, and publish data quality<br>' +
        'reports to the business community.<br>' +
        'RESOURCES<br>' +
        'The management of data quality is a whole<br>' +
        'topic unto itself and beyond the scope of<br>' +
        'this book. Beginning with a data profiling<br>' +
        'tool, data anomalies need to be identified,<br>' +
        'and checks or filters need to be built to<br>' +
        'catch those data problems that cannot be<br>' +
        '992<br>' +
        'fixed in the source transaction-processing<br>' +
        'system. The Kimball Group’s<br>' +
        'recommended architecture includes many<br>' +
        'data quality screens inserted into the data<br>' +
        'flows coming from the source systems and<br>' +
        'leading to the final presentation schemas<br>' +
        'used by the BI tools. Each time one of<br>' +
        'these screens detects a data quality<br>' +
        'problem, a record is written to an error<br>' +
        'event schema. This back room dimensional<br>' +
        'structure is the primary source for<br>' +
        'managing data quality issues. This<br>' +
        'architecture is discussed in detail in a<br>' +
        'Kimball Group white paper, “An<br>' +
        'Architecture for Data Quality,” published<br>' +
        'in 2007, which can be found in the Kimball<br>' +
        'Group article archive at<br>' +
        'http://www.kimballgroup.com/html/articles.html.<br>' +
        'Performance Testing<br>' +
        'The larger and more complex your system is, and the more<br>' +
        'users — especially ad hoc users — you have, the more<br>' +
        'important it is for you to conduct rigorous performance<br>' +
        'testing before you go into production. You want to launch<br>' +
        'your system with the best performance possible, and you<br>' +
        'certainly want to be confident that you can perform all<br>' +
        'processing within the necessary load windows.<br>' +
        'You may have several goals for conducting performance<br>' +
        'tests. The most common are:<br>' +
        '993<br>' +
        '• System tuning: How can you tweak the system to deliver the<br>' +
        'best possible performance?<br>' +
        '• Confirmation of service levels: Will you meet your uptime<br>' +
        'and query performance requirements?<br>' +
        '• Headroom analysis: How long will today’s system and<br>' +
        'hardware meet your requirements, as the DW/BI system<br>' +
        'continues to grow?<br>' +
        'As with everything else associated with systems, the<br>' +
        'performance testing process is best begun with some<br>' +
        'planning. Specify the goals from your testing process, and<br>' +
        'develop tests to address those goals.<br>' +
        'Rather than thinking about performance testing as<br>' +
        'associated with each of the components of SQL Server<br>' +
        '(RDBMS, Analysis Services, and so on), we prefer a more<br>' +
        'integrated approach. Test processing performance as a<br>' +
        'whole: ETL to cube processing to standard report<br>' +
        'generation. Test query performance as a whole, including<br>' +
        'ad hoc queries being executed at the same time as standard<br>' +
        'reports are being run.<br>' +
        'Service Level Confirmation<br>' +
        'Increasingly, DW/BI teams are entering into Service Level<br>' +
        'Agreements with the user community. These agreements<br>' +
        'cover data latency, and system availability often user query<br>' +
        'performance.<br>' +
        'If you have such an agreement in place, then you surely<br>' +
        'must test that your system is likely to conform to the<br>' +
        'agreement. This is often a first step that leads to more<br>' +
        'extensive performance testing for tuning work, or even<br>' +
        'alternative system sizing and configuration efforts. But if<br>' +
        '994<br>' +
        'you’ve cleverly made an agreement with pretty low<br>' +
        'minimum standards, you may simply need to confirm that<br>' +
        'you’re above those standards.<br>' +
        'NOTE<br>' +
        'Service Level Agreements (SLAs) are a<br>' +
        'valuable tool for focusing management<br>' +
        'attention on important issues. But don’t let<br>' +
        'your SLA drive you to deliver mediocrity<br>' +
        'by striving only to meet the stated<br>' +
        'requirements. Under-promise and<br>' +
        'over-deliver. Never promise more than<br>' +
        'your clients are requesting, but always try<br>' +
        'to deliver more than they’ve imagined<br>' +
        'possible.<br>' +
        'Be very careful in negotiating Service<br>' +
        'Level Agreements that include metrics for<br>' +
        'ad hoc query performance. Don’t let<br>' +
        'yourself agree to an absolute ceiling for ad<br>' +
        'hoc query times, like all queries complete<br>' +
        'in 10 seconds. You’d be much better off<br>' +
        'agreeing that 90 percent of queries would<br>' +
        'complete in 5 seconds. In a system of any<br>' +
        'size and complexity, it’s always possible to<br>' +
        'write an ad hoc query that exceeds any<br>' +
        'reasonable maximum.<br>' +
        '995<br>' +
        'Clearly specify in the SLA what you mean<br>' +
        'by important terms, like query completion.<br>' +
        'Does this mean on the server side, or does<br>' +
        'it also include the transport (over a<br>' +
        'potentially low bandwidth WAN) to the<br>' +
        'client? The SLA is, basically, a contract<br>' +
        'between you and your users. You probably<br>' +
        'don’t need to include Legal on this<br>' +
        'contract, but you should take it seriously.<br>' +
        'Your management will take it seriously if<br>' +
        'you don’t maintain service levels.<br>' +
        'Processing Performance: Getting Data In<br>' +
        'Performance testing for the data processing side of the<br>' +
        'problem is fairly straightforward. The live testing that we<br>' +
        'described earlier in this chapter is the basis for the<br>' +
        'processing performance tests.<br>' +
        'The simplest approach to building a processing system is<br>' +
        'to serialize the major components. All ETL work to the<br>' +
        'RDBMS finishes before you begin cube processing, and<br>' +
        'that completes before you start generating reports. Such a<br>' +
        'serialized system is easy to performance test and diagnose,<br>' +
        'because the units of work are isolated. You can test and<br>' +
        'tune each unit separately. Unless your load window is very<br>' +
        'small or your latency requirements approach real time,<br>' +
        'you’ll probably start off with serialized processing.<br>' +
        'You may be able to design your processing system so that<br>' +
        'work is parallelized. You need to process shared<br>' +
        '996<br>' +
        'dimensions first, but you should be able to start the work<br>' +
        'on one fact table while a second fact table is still loading.<br>' +
        'You can make significant improvements in the overall<br>' +
        'loading time by parallelizing some activities, but this is a<br>' +
        'much harder system to design and tune. Your performance<br>' +
        'tests must run on the integrated processing system. All<br>' +
        'parts of the DW/BI system compete for resources. You<br>' +
        'can’t test each component separately and sum their<br>' +
        'processing times. This is true even if the different<br>' +
        'components are distributed across multiple servers because<br>' +
        'there’s always some burden placed on the upstream<br>' +
        'servers.<br>' +
        'Another issue to consider is confirming that changes made<br>' +
        'to improve the performance of one part of the system don’t<br>' +
        'negatively impact another part of the system. The classic<br>' +
        'problem is index and aggregation design. You may want<br>' +
        'lots of indexes and aggregations for queries to run quickly.<br>' +
        'But these structures must be maintained, which can place<br>' +
        'an intolerable burden on the processing performance.<br>' +
        'Every time a change is considered, evaluate the effects on<br>' +
        'a complete test system before deploying to production.<br>' +
        'Query Performance: Getting Data Out<br>' +
        'Testing query performance, especially ad hoc query<br>' +
        'performance, is much harder than testing processing<br>' +
        'performance. The fundamental problem is that you don’t<br>' +
        'know what your users are going to want to do. You can ask<br>' +
        'them and get some ideas, but those ideas are going to bear,<br>' +
        'at best, only a resemblance to reality.<br>' +
        '997<br>' +
        'Standard reports are either pre-run and cached, or run on<br>' +
        'demand. Pre-run reports are executed at the end of the ETL<br>' +
        'processing. You can set up Reporting Services to email the<br>' +
        'results of a pre-run report to the users; doing so shifts the<br>' +
        'entire burden of report generation to a scheduled time.<br>' +
        'Alternatively, users might access the pre-run report from<br>' +
        'the BI portal, in which case there’s a modest on-demand<br>' +
        'element associated with displaying the report. A solid<br>' +
        'performance test of pre-run standard reports uses estimated<br>' +
        'usage patterns for accessing the pre-run reports. For<br>' +
        'example, 500 people will access the report at random times<br>' +
        'between 8 a.m. and 9:30 a.m. The relational database<br>' +
        'where the Reporting Services catalog is stored, the<br>' +
        'Reporting Services engine, and the web servers are all<br>' +
        'involved with serving pre-stored reports.<br>' +
        'A standard report that’s executed on demand involves<br>' +
        'more work. The first demand is on the database upon<br>' +
        'which the report is defined, to serve up the basic data for<br>' +
        'the report. Then, Reporting Services works on that result<br>' +
        'set to render the report. Finally, the report is distributed,<br>' +
        'usually across the web to the user’s browser window.<br>' +
        'On-demand reports are often used for parameterized<br>' +
        'reports, for infrequently accessed reports that don’t<br>' +
        'warrant pre-executing and storing the results, and for<br>' +
        'reports with low latency. A good performance test for<br>' +
        'on-demand reports includes a realistic estimate of who is<br>' +
        'running the reports, when, and with what parameters.<br>' +
        'Reports can be cached in memory, which is great for<br>' +
        'performance. In the absence of real-world data about how<br>' +
        'users are running reports, it’s very difficult to accurately<br>' +
        'estimate the use of the report cache.<br>' +
        '998<br>' +
        'Finally, laboratory testing of the performance of ad hoc<br>' +
        'queries is fiendishly difficult. The first problem is to know<br>' +
        'what users are going to want to do. You know your<br>' +
        'predefined reports and other BI applications, but ad hoc is,<br>' +
        'well, ad hoc. You have to return to your business<br>' +
        'requirements document to extract information about<br>' +
        'analyses. Watch (by collecting query text) what the early<br>' +
        'business users and testers are doing with the system. Of<br>' +
        'course, if you’re testing a system that’s already in<br>' +
        'production, you should collect a broad range of queries<br>' +
        'from the system logs that we discuss in Chapter 17.<br>' +
        'Usability Testing<br>' +
        'Unless you’ve developed custom user-oriented software as<br>' +
        'part of your DW/BI solution, usability testing will not be a<br>' +
        'huge burden. In large part this is because, with<br>' +
        'shrink-wrapped front-end tools, there are relatively few<br>' +
        'things you can change. You can typically change the<br>' +
        'names of things (columns, tables, and reports) and the way<br>' +
        'they are organized.<br>' +
        'Nonetheless, perform some usability testing with actual<br>' +
        'business users. As with all usability tests, you need to find<br>' +
        'fresh minds: people who have not been intimately<br>' +
        'associated with the project. Walk a few people through the<br>' +
        'BI portal and the reports, and see what trips them up.<br>' +
        'Earlier in the system development process, when you start<br>' +
        'working on the Analysis Services database and defining<br>' +
        'reports, you should show a draft of the object names to<br>' +
        'business users. Rely on them to tell you what objects<br>' +
        'should be called in the interfaces they see. You tried to get<br>' +
        '999<br>' +
        'object names correct when you were designing the<br>' +
        'dimensional model. But often business users change their<br>' +
        'minds about names when the system gets closer to reality.<br>' +
        'We let business user names diverge from the physical<br>' +
        'names, if it helps the business users understand the system.<br>' +
        'Because there’s always at least one layer between the<br>' +
        'physical relational database and the business user — be it<br>' +
        'relational views, Analysis Services cubes, Reporting<br>' +
        'Services Report Models, or all of these layers — you can<br>' +
        'change names that business users see without messing up<br>' +
        'your ETL system development. But you do need to get the<br>' +
        'names right for Analysis Services and the reporting<br>' +
        'metadata layers.<br>' +
        'NOTE<br>' +
        'Another issue to think about early on is the<br>' +
        'hierarchies within dimensions. Dimension<br>' +
        'hierarchies become dropdown lists for ad<br>' +
        'hoc queries and parameterized reports.<br>' +
        'During the database design sessions you<br>' +
        'should think about the user experience<br>' +
        'associated with large, flat hierarchies. What<br>' +
        'will the user experience be if they are<br>' +
        'trying to navigate a dropdown list and you<br>' +
        'populate a list with 100,000 items? This is<br>' +
        'a common problem, and not one that’s<br>' +
        'easily fixed at the last minute, just before<br>' +
        'rollout.<br>' +
        '1000<br>' +
        'Before your new system goes live, you must have<br>' +
        'implemented and tested security at the user, role, report,<br>' +
        'and database levels, as we described in Chapter 14. If users<br>' +
        'don’t have the correct security permissions, the system is<br>' +
        '— from their point of view — completely unusable.<br>' +
        'Testing Summary<br>' +
        'Our main advice with respect to testing your DW/BI<br>' +
        'system is to test early, often, and thoroughly. Visual Studio<br>' +
        'or other testing tools can help by providing a central<br>' +
        'environment in which to define tests and log their results.<br>' +
        'Write reports on test logs so that they highlight changes,<br>' +
        'especially tests that worked yesterday but not today.<br>' +
        'NOTE<br>' +
        'Visual Studio Team Foundation Server<br>' +
        'comes with a project data warehouse,<br>' +
        'which includes an Analysis Services cube<br>' +
        'and Reporting Services reports on work<br>' +
        'items, bugs, and so on. It is really fun to<br>' +
        'use.<br>' +
        'The biggest challenge of DW/BI testing is that the project<br>' +
        'is, by its nature, about integration. So standard test<br>' +
        'methodologies and tools aren’t a perfect fit for our world.<br>' +
        'Begin by investing in some infrastructure:<br>' +
        '• A static test source database<br>' +
        '1001<br>' +
        '• A development testing environment that begins from that<br>' +
        'known source, and makes it trivially easy for developers to<br>' +
        'run automated tests every night<br>' +
        'Throughout the development cycle, developers must<br>' +
        'maintain a script to modify the test source databases, and a<br>' +
        'second script to execute the tests. This isn’t fundamentally<br>' +
        'different from what most developers do anyway, you’re<br>' +
        'just asking them to manage the process, and commit to<br>' +
        'running tests repeatedly.<br>' +
        'Fairly early in the development cycle, after the kinks have<br>' +
        'been ironed out of the daily development testing process,<br>' +
        'begin system testing. The system tester, who should not be<br>' +
        'a developer, defines tests that cover the entire DW/BI<br>' +
        'process, from data extract, transformation, and loading to<br>' +
        'cube processing, report delivery, and downstream BI<br>' +
        'applications. Begin system testing on the test databases.<br>' +
        'There are several reasons for this recommendation:<br>' +
        '• Iron out problems with the system testing process early<br>' +
        'before you’re pressured to go live in two weeks.<br>' +
        '• Some system tests of unusual data conditions might not be<br>' +
        'encountered during normal live testing.<br>' +
        '• In the absence of perfect specifications, the system tester(s)<br>' +
        'will have more time to identify and develop tests.<br>' +
        '• The developers will have more time to respond to faults<br>' +
        'found in the system testing process.<br>' +
        'Once the testing infrastructure is in place, the system tester<br>' +
        'will need to devote a significant amount of time<br>' +
        'developing tests. The time required ranges from a few<br>' +
        'weeks of full-time effort to significantly more than that,<br>' +
        'depending on the quality of the system specifications.<br>' +
        'After the tests are defined, the system tester can spend<br>' +
        '1002<br>' +
        'perhaps a day a week on system testing, until it’s time to<br>' +
        'move to live testing and the run up to deployment.<br>' +
        'Data quality testing primarily consists of tests of<br>' +
        'rowcounts, sums, and hashes. A key component of data<br>' +
        'quality testing is to match reports from the source system.<br>' +
        'Some of this work can begin early in the development<br>' +
        'cycle as well, but the first big milestone for data quality<br>' +
        'testing is the one-time load of historical data. This data set<br>' +
        'should be extensively tested. It some cases it’s possible to<br>' +
        'match old reports exactly. If not, write an audit report that<br>' +
        'describes exactly why the new reports are different. A<br>' +
        'small number of highly regarded business users must<br>' +
        'participate in data quality testing. These business users<br>' +
        'should be the subject area data stewards, who are part of<br>' +
        'the overall data governance program.<br>' +
        'Finally, once development is largely complete, you’ll hook<br>' +
        'up the ETL process to live data. At this point, the real<br>' +
        'system testing is under way. You can also perform<br>' +
        'performance tests, both of the processing steps, and the<br>' +
        'queries. If you’ve been running all tests nightly, you<br>' +
        'should be in great shape for getting through the testing<br>' +
        'period with minimal pain. The next big step: deployment!<br>' +
        'Deploying to Production<br>' +
        'Once your system is fully tested and accepted by the data<br>' +
        'stewards, it’s time actually to deploy. Whether you’re a<br>' +
        'tiny team or a large organization with deployment<br>' +
        'specialists on staff, you need to think through all of the<br>' +
        'issues before the actual day arrives.<br>' +
        '1003<br>' +
        'If you’re implementing a new system on new hardware,<br>' +
        'your deployment procedures can be somewhat casual. It’s<br>' +
        'common to use the production system for testing and even<br>' +
        'user training, before the new system goes live. You can<br>' +
        'think of the deployment process in this simple case as via<br>' +
        'email — send a message when the system has passed all<br>' +
        'tests and is ready to go live.<br>' +
        'After that first free deployment, it gets a lot harder. Any<br>' +
        'modifications to the system — and there are always<br>' +
        'modifications — should be accomplished with minimal<br>' +
        'disruption to the business user community. The only way<br>' +
        'to do this is to:<br>' +
        '• Perform testing on a test system that’s as identical to the<br>' +
        'production system as possible.<br>' +
        '• Use scripts rather than clicking through a user interface. Any<br>' +
        'time you need to open a tool and especially to click through<br>' +
        'a wizard, you open the possibility of doing the wrong thing.<br>' +
        '• Develop a deployment process playbook that describes<br>' +
        'exactly what to do and in what order. This is especially vital<br>' +
        'if you can’t run a script but instead must do something<br>' +
        'within a tool. It’s tempting during development to think you<br>' +
        'can remember all the little steps of deployment, but after you<br>' +
        'have been away from the system for a few weeks or months,<br>' +
        'you will be very glad you made a detailed playbook. If you<br>' +
        'create a really awesome playbook, you will be surprised that<br>' +
        'it contains a hundred steps or more.<br>' +
        '• Test the playbook on a test system before trying it on<br>' +
        'production.<br>' +
        'Relational Database Deployment<br>' +
        'Perhaps the simplest way to deploy changes in the<br>' +
        'relational database is to back up and restore the test<br>' +
        'database to the production environment. The standard data<br>' +
        '1004<br>' +
        'warehouse environment, with nightly loads, enables this<br>' +
        'technique on a moderately large data warehouse. You just<br>' +
        'need to bring the DW/BI system down for several hours or<br>' +
        'a day, back up the database(s) from the test system, and<br>' +
        'restore them on production.<br>' +
        'Backup and restore is a relatively uncommon approach to<br>' +
        'deploying changes. In many projects, the database is either<br>' +
        'too large for this approach to be comfortable, or the test<br>' +
        'system includes only the portion of the DW/BI system<br>' +
        'currently under modification. Instead, most deployment<br>' +
        'projects script the changes to conform the test and<br>' +
        'production databases.<br>' +
        'In the old days, it was the job of the development DBA to<br>' +
        'keep track of schema and data changes, and maintain<br>' +
        'scripts for modifying the production database. That’s still<br>' +
        'true, but the job is much easier now that there are tools<br>' +
        'available to automate the comparison of two schemas.<br>' +
        'Visual Studio 2010 includes this schema comparison<br>' +
        'functionality in both its Premium and Ultimate versions.<br>' +
        'In Figure 16-3 we illustrate the results of running a<br>' +
        'SchemaCompare on two versions of a database. The<br>' +
        'schema on the left is the new database. We’ve added a new<br>' +
        'column (NewColumn) to the StageSpecialOffer table.<br>' +
        'In the top panel of the window is the list of tables in the<br>' +
        'schema. The schema compare process has correctly<br>' +
        'identified the table and column that are different. In the<br>' +
        'bottom panel is a small snippet of the deployment script<br>' +
        'that Visual Studio has generated.<br>' +
        '1005<br>' +
        'When you run the schema compare process, you have fine<br>' +
        'control over the details to compare, including security,<br>' +
        'extended properties, filespaces, and so on.<br>' +
        'NOTE<br>' +
        'The schema compare feature is part of<br>' +
        'Visual Studio 2010. It’s also available in<br>' +
        'several earlier versions of Visual Studio.<br>' +
        'It’s not a feature of BIDS.<br>' +
        'There are third-party tools that perform the<br>' +
        'same function.<br>' +
        'Figure 16-3: Creating a database deployment script<br>' +
        '1006<br>' +
        'Visual Studio contains a second, related feature to compare<br>' +
        'data between two identically structured tables. Visual<br>' +
        'Studio will generate a script to insert, update, or delete<br>' +
        'rows in the target. This functionality is very useful for<br>' +
        'most dimension tables, and other reasonably sized tables in<br>' +
        'the environment. Fact tables and extremely large<br>' +
        'dimension tables should be excluded, as you’ll want to<br>' +
        'update their data in bulk. For large tables, build a simple<br>' +
        'Integration Services package to copy the data from test to<br>' +
        'production.<br>' +
        'All deployment scripts should be tested before being<br>' +
        'applied to the real production system.<br>' +
        '1007<br>' +
        'NOTE<br>' +
        'The Deployment Playbook for deploying a<br>' +
        'new relational database or modifications to<br>' +
        'an existing database should include the<br>' +
        'following:<br>' +
        '• If your test database structure doesn’t<br>' +
        'mirror production, include any edits to<br>' +
        'make to SQL scripts (like editing file<br>' +
        'locations or database names). It’s far safer<br>' +
        'to parameterize these changes, but writing<br>' +
        'the scripts is a lot more complicated if you<br>' +
        'do so.<br>' +
        '• A mechanism for verifying that any<br>' +
        'necessary edits were done correctly. At the<br>' +
        'very least, advise the operator to search for<br>' +
        'specific phrases that should have been<br>' +
        'changed during the editing process.<br>' +
        '• DML scripts to insert and update data in<br>' +
        'the new database. This data can include<br>' +
        'small dimension tables, configuration data,<br>' +
        'metadata, SSIS configurations, or global<br>' +
        'variables. And don’t forget to populate any<br>' +
        'new static dimensions.<br>' +
        '• Integration Services packages to run that<br>' +
        'will load data into the fact tables.<br>' +
        '• The run command for any scripts, like SQL<br>' +
        'scripts, including any parameters used in<br>' +
        'the script execution.<br>' +
        '• A script or instructions to verify that all<br>' +
        'completed correctly.<br>' +
        'Integration Services Package Deployment<br>' +
        'The process of deploying a package from development to<br>' +
        'test to production is straightforward. Fundamentally, you<br>' +
        '1008<br>' +
        'copy the packages into the production environment.<br>' +
        'Production packages should be locked down in source<br>' +
        'control.<br>' +
        'There are tools to help with package deployment. The first<br>' +
        'tool is Integration Services configurations, which we<br>' +
        'discussed earlier in this chapter. Configurations let you<br>' +
        'change at runtime the characteristics of an Integration<br>' +
        'Services package, like the connection string to sources, the<br>' +
        'location of file folders, a parameter, or a variable value.<br>' +
        'Changing values at runtime is valuable because it lets you<br>' +
        'modify the way a package executes without opening and<br>' +
        'editing the package.<br>' +
        'The second feature of Integration Services that simplifies<br>' +
        'deployment is the aptly named deployment utility. Launch<br>' +
        'the deployment utility by choosing Deploy from the<br>' +
        'Solution Explorer in BIDS. The deployment utility bundles<br>' +
        'into a deployment folder all the components associated<br>' +
        'with a set of packages, including any configuration files,<br>' +
        'code libraries, or other files that you included in your<br>' +
        'project. You can copy the deployment folder from one<br>' +
        'server to another, and then open the deployment folder to<br>' +
        'launch a simple wizard to install the packages.<br>' +
        'NOTE<br>' +
        'During the development process, you<br>' +
        'probably discovered that you don’t need<br>' +
        'the Deployment Wizard and Installation<br>' +
        '1009<br>' +
        'Wizard. You can easily copy SSIS package<br>' +
        'and configuration files, and they work just<br>' +
        'fine on the new server. The deployment<br>' +
        'and Package Installation Wizards are a<br>' +
        'convenience.<br>' +
        'Some organizations are deeply opposed to using a wizard<br>' +
        'for deploying to production. Scripts can be fully tested and<br>' +
        'automated in advance. If you use a wizard with a user<br>' +
        'interface, like the Package Installation Wizard, you run the<br>' +
        'risk of someone clicking the wrong box or making a typo<br>' +
        'during the deployment process. If your organization is<br>' +
        'adamant about not using wizards, you can write a batch<br>' +
        'script that calls dtexec. Dtexec is a command-line utility<br>' +
        'that is used mainly to execute packages in test and<br>' +
        'production. But with different parameters, dtexec will<br>' +
        'copy, delete, encrypt, or deploy a package into SQL Server<br>' +
        'or a file folder. If your deployment folder includes<br>' +
        'multiple packages, you’d need to call dtexec multiple<br>' +
        'times to install them one by one. In addition, if your<br>' +
        'deployment folder contains configuration or other files,<br>' +
        'your script would also need to move them to the<br>' +
        'appropriate places.<br>' +
        'As part of the pre-deployment testing and code review<br>' +
        'process, it’s extremely helpful to be able to identify the<br>' +
        'changes between two versions of an SSIS package. SSIS<br>' +
        'packages are XML documents, but simply comparing the<br>' +
        'two versions isn’t very effective. A minor change in the<br>' +
        'physical layout of objects in the package can have a large<br>' +
        'effect on the XML document. However, the recommended<br>' +
        '1010<br>' +
        'add-on BIDSHelper has a SmartDiff SSIS utility, which<br>' +
        'you can execute from BIDS. It strips out formatting<br>' +
        'information and regularizes the layout of the SSIS<br>' +
        'package, to highlight real changes. It’s a valuable addition<br>' +
        'to your change management procedures.<br>' +
        'NOTE<br>' +
        'The Deployment Playbook for deploying a<br>' +
        'package should include the following:<br>' +
        '• The location of the deployment folder to be<br>' +
        'copied to the production server.<br>' +
        '• Where to copy the deployment folder to.<br>' +
        '• Instructions for running the Package<br>' +
        'Installation Wizard, including all choices to<br>' +
        'make in the dialog boxes. Alternatively, the<br>' +
        'command script to run that calls dtexec to<br>' +
        'install the packages, and which copies<br>' +
        'other necessary files to the appropriate<br>' +
        'destinations. Don’t forget the configuration<br>' +
        'files and code libraries.<br>' +
        '• Instructions for creating any Windows<br>' +
        'global variables that the package uses.<br>' +
        '• A script or instructions to verify that all<br>' +
        'completed correctly.<br>' +
        'Analysis Services Database Deployment<br>' +
        'As with the other software components of the Microsoft<br>' +
        'DW/BI solution, there are several ways to deploy a new<br>' +
        'Analysis Services database or to make modifications to an<br>' +
        'existing database. These are included in Table 16-1.<br>' +
        '1011<br>' +
        'Table 16-1: Methods for deploying changes to an SSAS<br>' +
        'database<br>' +
        'Method Comments Good<br>' +
        'choice<br>' +
        'for…<br>' +
        'Backup/<br>' +
        'restore<br>' +
        'If the full SSAS database exists on your test<br>' +
        'system, fully processed with all data, then you<br>' +
        'can back it up and restore it on the production<br>' +
        'server. SSAS backup and restore work at the<br>' +
        'database level only, and can be fully scripted.<br>' +
        'Initial<br>' +
        'deployment<br>' +
        'or a<br>' +
        'significant<br>' +
        'structural<br>' +
        'change.<br>' +
        'Scenarios<br>' +
        'where you<br>' +
        'don’t need<br>' +
        'to minimize<br>' +
        'downtime.<br>' +
        'Deployment<br>' +
        'Wizard<br>' +
        'The Deployment Wizard is a standalone utility.<br>' +
        'It creates a script to deploy the metadata of the<br>' +
        'entire SSAS database. Then, either now or at a<br>' +
        'future time, you kick off processing of the entire<br>' +
        'SSAS database on the production server.<br>' +
        'Moving<br>' +
        'from<br>' +
        'development<br>' +
        'to test.<br>' +
        'Synchronize<br>' +
        'Database<br>' +
        'Synchronize Database also works at the database<br>' +
        'level. It copies the metadata of the entire SSAS<br>' +
        'database. It also copies the data files from source<br>' +
        'to target. Synchronization’s data copy is a much<br>' +
        'less expensive operation than full processing of<br>' +
        'the database. Synchronization creates a shadow<br>' +
        'copy of the existing database on the target<br>' +
        'server, so users can continue to access the<br>' +
        'database during the synchronization. Users are<br>' +
        'automatically shifted to the new version when<br>' +
        'the synchronization is complete. Note that this<br>' +
        'means you’ll need twice the disk space on your<br>' +
        'production server.<br>' +
        'Synchronization is launched from Management<br>' +
        'Studio and can be scripted.<br>' +
        'Moving<br>' +
        'from test<br>' +
        '(staging) to<br>' +
        'production.<br>' +
        'Scenarios<br>' +
        'where you<br>' +
        'must<br>' +
        'minimize<br>' +
        'downtime.<br>' +
        'XMLA<br>' +
        'script<br>' +
        'An XMLA (XML for Analysis) script is the<br>' +
        'closest thing to DDL constructs familiar from<br>' +
        'the relational database. But they are in the form<br>' +
        'Changing<br>' +
        'calculations.<br>' +
        'Making<br>' +
        '1012<br>' +
        'Method Comments Good<br>' +
        'choice<br>' +
        'for…<br>' +
        'of XML syntax rather than something like<br>' +
        'TSQL. You can write or generate an XMLA<br>' +
        'script to create the new structure of an SSAS<br>' +
        'database. Most often, it’s used to make relatively<br>' +
        'minor changes to an existing structure.<br>' +
        'You can write and execute XMLA scripts in<br>' +
        'Management Studio.<br>' +
        'minor<br>' +
        'changes to<br>' +
        'an existing<br>' +
        'system.<br>' +
        'AMO code<br>' +
        'You can write to the AMO (Analysis<br>' +
        'Management Objects) object model to create or<br>' +
        'modify SSAS databases.<br>' +
        'Packaged BI<br>' +
        'solution<br>' +
        'vendor.<br>' +
        'The biggest problem with SSAS system deployments is<br>' +
        'that the easiest methods work at the database level:<br>' +
        'backup/restore, deployment, and synchronization. This is<br>' +
        'fine if your SSAS database is of modest size, but can be<br>' +
        'quite problematic at the terabyte scale.<br>' +
        'The only way to deploy incremental changes is to write<br>' +
        'some code: XMLA code, which is analogous to T-SQL in<br>' +
        'the relational database, or AMO code. Most organizations<br>' +
        'use XMLA rather than AMO.<br>' +
        'Incremental deployment of changes is a good choice for<br>' +
        'modifications that don’t affect data storage. These include:<br>' +
        '• The MDX script for all cube calculations such as calculated<br>' +
        'measures<br>' +
        '• Definitions of Actions and KPIs<br>' +
        '• Security definitions<br>' +
        'The XMLA specification is complete, and you can write<br>' +
        'an XMLA script to implement any modification in the<br>' +
        'production database. However, you must understand that<br>' +
        '1013<br>' +
        'many modifications will result in significant database<br>' +
        'reprocessing. Some changes to a core conformed<br>' +
        'dimension that’s used throughout the database will<br>' +
        'effectively result in the need to reprocess much of that<br>' +
        'database — all the fact tables that subscribe to the<br>' +
        'dimension. For this reason, many organizations use XMLA<br>' +
        'scripts only for the calculated information described<br>' +
        'earlier. Any more significant change is made by one of the<br>' +
        'other methods, usually database synchronization.<br>' +
        'If your SSAS database is extremely large, you may want to<br>' +
        'invest in the XMLA skills to perform more incremental<br>' +
        'modifications.<br>' +
        'NOTE<br>' +
        'The easiest way to generate the XMLA<br>' +
        'script for changes in calculations is to use<br>' +
        'the Deploy MDX Script feature of<br>' +
        'BIDSHelper. You should never deploy an<br>' +
        'MDX script from BIDS directly into<br>' +
        'production, but you can do so into a test<br>' +
        'server. Use SQL Server Profiler to monitor<br>' +
        'the test server; it will pick up the text of the<br>' +
        'XMLA script. You can copy that script and<br>' +
        'move it into your deployment script library.<br>' +
        'Chapter 17 describes Profiler in more<br>' +
        'detail.<br>' +
        'As part of your change management and deployment<br>' +
        'procedures, you should thoroughly document the changes<br>' +
        '1014<br>' +
        'to a system in production. There’s no built-in tool to help<br>' +
        'with that, but BIDSHelper provides the same kind of<br>' +
        'assistance described previously. The BIDSHelper feature<br>' +
        'SmartDiff for Analysis Services lets you compare the full<br>' +
        'database definitions for two SSAS databases, highlighting<br>' +
        'the differences in a reasonably useful way.<br>' +
        'NOTE<br>' +
        'We certainly look forward to the day when<br>' +
        'Microsoft provides the same kinds of<br>' +
        'management and tools for Analysis<br>' +
        'Services as exist for the relational database.<br>' +
        'Reporting Services Report Deployment<br>' +
        'Deploying a new report is generally a lot easier and less<br>' +
        'exciting than deploying or changing a database or package.<br>' +
        'When you launch a new DW/BI system or add a business<br>' +
        'process dimensional model, you will probably begin the<br>' +
        'development of the initial suite of reports on a test server,<br>' +
        'ideally against a complete set of data. As soon as the<br>' +
        'production server is populated, migrate any existing<br>' +
        'reports, and continue report development on the<br>' +
        'production server. If you use shared data sources for<br>' +
        'Reporting Services reports, it’s a simple task to point the<br>' +
        'reports to the production databases.<br>' +
        'You will modify and create reports far more often than<br>' +
        'you’ll modify the underlying databases. All reports should<br>' +
        'be tested before they’re released to the user community.<br>' +
        '1015<br>' +
        'The most important tests are to ensure the report<br>' +
        'definitions are accurate. Complex reports that access a lot<br>' +
        'of data, especially if the reports are run by a lot of people,<br>' +
        'should be tested for performance. You may find that you<br>' +
        'need to write a stored procedure to generate the report’s<br>' +
        'data set as efficiently as possible.<br>' +
        'Report Deployment Process<br>' +
        'Complete the following steps to safely deploy a<br>' +
        'new or changed report on the production system:<br>' +
        '• Identify the business users who will test and verify<br>' +
        'the new or changed report.<br>' +
        '• Create a Reporting Services role named<br>' +
        'ReportTest that includes the DW/BI team and the<br>' +
        'business users who’ll test the report. You may<br>' +
        'need several report testing roles, if there are a lot<br>' +
        'of report development projects going on at once.<br>' +
        '• Set up a TestFolder folder structure in the BI<br>' +
        'portal that’s accessible only to the ReportTest<br>' +
        'role.<br>' +
        '• Develop the new report in BI Studio or Report<br>' +
        'Builder and deploy it to TestFolder.<br>' +
        '• Notify the testers that the report’s available, and<br>' +
        'when you expect to hear back from them about it.<br>' +
        'Your organization may have formal user<br>' +
        'acceptance procedures for you to rely on here. If<br>' +
        'you are using SharePoint, you can set up the<br>' +
        'TestFolder to automatically generate an approval<br>' +
        'workflow.<br>' +
        '• When the relevant people have signed off on the<br>' +
        'report, redeploy it to its appropriate place in the<br>' +
        'BI portal with the appropriate security.<br>' +
        '1016<br>' +
        'Most companies will develop and test new reports in a<br>' +
        'private area of the production report server, rather than set<br>' +
        'up a completely separate test instance of Reporting<br>' +
        'Services. Standard reports don’t change data, so you don’t<br>' +
        'need to worry about damaging the databases. All you need<br>' +
        'is to insulate most users from the test area, which is easy to<br>' +
        'do with the Reporting Services security settings discussed<br>' +
        'in Chapter 14.<br>' +
        'As you may expect, the hardest part of deploying reports<br>' +
        'isn’t technical but political. The greatest challenge is to<br>' +
        'create policies and procedures that enable your business<br>' +
        'community to contribute new reports and analyses, while<br>' +
        'maintaining the appropriate level of control over published<br>' +
        'reports. You should develop a quality assurance process,<br>' +
        'and procedures for publishing reports to a broad audience.<br>' +
        'This is particularly important for highly regulated<br>' +
        'companies.<br>' +
        'NOTE<br>' +
        'Sometimes, changes to the underlying<br>' +
        'databases require existing reports to be<br>' +
        'modified. Earlier in this chapter, we<br>' +
        'stressed the importance of end-to-end<br>' +
        'testing for any significant modifications to<br>' +
        'the DW/BI system. The standard report<br>' +
        'suite must be tested before database<br>' +
        'changes are moved into production. It’s<br>' +
        'usually easy to fix reports in response to a<br>' +
        '1017<br>' +
        'schema change, but if you forget this step,<br>' +
        'the user experience is the same as if you<br>' +
        'messed up the underlying data. From their<br>' +
        'point of view, the DW/BI system is broken.<br>' +
        'Also, any change that breaks a standard<br>' +
        'report will likely break user reports, too. If<br>' +
        'you’re implementing these kinds of<br>' +
        'changes, notify your users early on and<br>' +
        'discuss what they’ll need to do to deal with<br>' +
        'the changes. You may want to set up a user<br>' +
        'report migration project to help rewrite<br>' +
        'some of the key user reports.<br>' +
        'Master Data Services Deployment<br>' +
        'Master Data Services applications are easy to deploy. The<br>' +
        'main characteristic that simplifies their deployment is that<br>' +
        'they typically include small volumes of data — dimension<br>' +
        'data — rather than the large fact tables in the full DW/BI<br>' +
        'system.<br>' +
        'MDS has two features that you can use for deployment.<br>' +
        'Within any one MDS implementation, an MDS model (for<br>' +
        'example, for the customer dimension) can have multiple<br>' +
        'versions. You can theoretically have a single server, and a<br>' +
        'single implementation of MDS, supporting a production<br>' +
        'version of the model at the same time as a development<br>' +
        'and/or test version. A version cannot be validated and<br>' +
        'committed unless all the data conforms to the structure and<br>' +
        'business rules that have been defined.<br>' +
        '1018<br>' +
        'We don’t want to suggest that maintaining development<br>' +
        'and test versions on the same server as production is a best<br>' +
        'practice; we always want to isolate production systems.<br>' +
        'However, it’s a matter of just a few mouse clicks in the<br>' +
        'management console to package up a model, including the<br>' +
        'structure, the business rules, and all existing data. Copy the<br>' +
        'deployment package to the production environment,<br>' +
        'launch the management console, and deploy.<br>' +
        'Data Warehouse and BI Documentation<br>' +
        'We all seem to skimp on documentation in the run up to<br>' +
        'system deployment. It seems as though the business should<br>' +
        'be able to use the system without a ton of documentation.<br>' +
        'After all, we spent a lot of time and trouble organizing and<br>' +
        'naming things in a sensible way. The bad news here is that<br>' +
        'the team needs to do a lot of documentation of the system<br>' +
        'in order to offer a complete solution to the users. The good<br>' +
        'news is most of the documentation is really metadata<br>' +
        'dressed up in presentable clothes. If you’ve been capturing<br>' +
        'metadata all along, much of the job now is to create a nice<br>' +
        'front end for users to access that metadata. If you’ve been<br>' +
        'ignoring the metadata issue, you’ve got a lot of work ahead<br>' +
        'of you.<br>' +
        'As we detail in Chapter 12, the BI portal is the<br>' +
        'organization’s single source for reporting and analysis and<br>' +
        'associated information. The main content of the BI portal<br>' +
        'will be the navigation hierarchy and the standard reports<br>' +
        'contained therein. Around the edges of the main BI portal<br>' +
        'page, users should find links to all the documentation and<br>' +
        'tools described here.<br>' +
        '1019<br>' +
        'Core Descriptions<br>' +
        'The first things to document are the data: the business<br>' +
        'process subject areas including facts and dimensions, and<br>' +
        'the tables, columns, calculations, and other rules that make<br>' +
        'up those subject areas. Standard reports and other BI<br>' +
        'applications should also be documented, though their<br>' +
        'documentation is often integrated with the reports<br>' +
        'themselves.<br>' +
        'Business Process Dimensional Model Descriptions<br>' +
        'The BI documentation begins with the dimensional model.<br>' +
        'The DW/BI team must write a clear, succinct description<br>' +
        'of each dimensional model in the warehouse. This<br>' +
        'document will be the starting point for anyone who wants<br>' +
        'to understand what’s in the DW/BI system. If orders was<br>' +
        'the initial row selected on the bus matrix, write a document<br>' +
        'that describes the orders dimensional model. Recall that a<br>' +
        'business process dimensional model usually consists of a<br>' +
        'small number of related fact tables (1–3), and their<br>' +
        'associated dimension tables. The document answers such<br>' +
        'questions as:<br>' +
        '• What’s the nature of the business process captured in this<br>' +
        'data?<br>' +
        '• What are the salient business rules?<br>' +
        '• What’s the grain of each fact table?<br>' +
        '• What date range is included in each fact table?<br>' +
        '• What data has been left out (and why)?<br>' +
        '• What dimensions participate in this business process? Many<br>' +
        'of the dimensions will need their own descriptive documents<br>' +
        'that this document can link to.<br>' +
        '1020<br>' +
        'This document should have a few screen captures that<br>' +
        'show the target dimensional model in a graphical form,<br>' +
        'some example values, and a few reports to demonstrate the<br>' +
        'kinds of business questions it can address. The graphic of<br>' +
        'the dimensional model can be derived directly from a data<br>' +
        'model, or it can be a simple drawing as illustrated in<br>' +
        'Figure 16-4. Remember that the purpose of the picture is to<br>' +
        'communicate to the user community. It helps to start at a<br>' +
        'high level as illustrated here, and then to drill down to<br>' +
        'greater levels of detail. Don’t overwhelm users with the<br>' +
        'details of a 200-table data model without providing<br>' +
        'context.<br>' +
        'Figure 16-4: High level graphic of a dimensional model<br>' +
        'Table and Column Descriptions<br>' +
        'Once people have a general understanding of a particular<br>' +
        'schema, they need to be able to drill down into the details<br>' +
        'table by table and column by column. This is where the<br>' +
        'descriptive metadata you captured when you were building<br>' +
        '1021<br>' +
        'the initial target model comes back into service. Refer<br>' +
        'back to Chapter 15 for the details of this metadata.<br>' +
        'DOWNLOADS<br>' +
        'As you may recall from Chapter 15, we<br>' +
        'have provided tools to populate a metadata<br>' +
        'database with much of the descriptive<br>' +
        'information that you captured when you<br>' +
        'created the data model. On the book’s<br>' +
        'website you will find the metadata schema<br>' +
        'scripts:<br>' +
        '• The script to create a business metadata<br>' +
        'relational database.<br>' +
        '• Scripts to populate the metadata database<br>' +
        'from the system tables and table and<br>' +
        'column extended properties.<br>' +
        '• A set of linked reports for users to browse<br>' +
        'the business metadata catalog.<br>' +
        'Report Descriptions<br>' +
        'Each report must have a base set of descriptive information<br>' +
        'as part of the standard template described in Chapter 10.<br>' +
        'Some of this information, like the report title and<br>' +
        'description, can be written out to the Reporting Services<br>' +
        'metadata structures when the reports are built or updated.<br>' +
        'Other information will need to be captured in the metadata<br>' +
        'repository described in Chapter 15. The navigation<br>' +
        'framework described in Chapter 10, and the assignment of<br>' +
        'individual reports to categories and groups, help people<br>' +
        'understand what information is available. These category<br>' +
        '1022<br>' +
        'assignments should follow the same organizing framework<br>' +
        'used to present the reports in the BI portal. In fact, this<br>' +
        'metadata can be used to dynamically create the portal<br>' +
        'interface.<br>' +
        'Additional Documentation<br>' +
        'Data and report documentation are certainly the most<br>' +
        'commonly used, but other documentation is also<br>' +
        'important. The most valuable additional documentation<br>' +
        'comes in the form of online tutorials, support guides, and a<br>' +
        'list of colleagues who use the system and may be able to<br>' +
        'help.<br>' +
        'As we discuss later in this chapter, you should develop and<br>' +
        'deliver training to the business users. This training should<br>' +
        'be mandatory for business users who’ll be creating ad hoc<br>' +
        'queries, but it’s useful for everyone. Realistically, not all<br>' +
        'users will come to a class. Even if you do have 100 percent<br>' +
        'attendance, users can benefit from online tutorials and<br>' +
        'class materials. These may be as simple as an annotated<br>' +
        'version of the classroom materials made available on the<br>' +
        'website.<br>' +
        'A support guide will help your business users know whom<br>' +
        'to call when they have a problem. You should list the<br>' +
        'escalation hierarchy with contact names, emails, and phone<br>' +
        'numbers. You may get significant leverage out of<br>' +
        'publishing a list of frequently asked questions and<br>' +
        'answers. We discuss user support issues later in this<br>' +
        'chapter.<br>' +
        '1023<br>' +
        'Cheat sheets are brief summaries of common commands,<br>' +
        'processes, terminology, constraint lists, and so on —<br>' +
        'whatever people use or do on a regular basis that might be<br>' +
        'hard to remember. A cheat sheet is a single-page<br>' +
        'document, often meant to be folded into a tri-fold format<br>' +
        'for easy access and storage. In some ways, these cheat<br>' +
        'sheets are marketing brochures for the DW/BI system.<br>' +
        'They will be prominently displayed in your users’ offices,<br>' +
        'so make them look professional. The cheat sheets should<br>' +
        'also be part of the BI portal content.<br>' +
        'Publish a current list of users on the BI portal, with an<br>' +
        'indicator showing which users are designated analytic<br>' +
        'support people and which users have at least had ad hoc<br>' +
        'tool training. Include a simple report showing query<br>' +
        'activity by user in the last month or so, sorted from most to<br>' +
        'least active.<br>' +
        'Your BI portal should incorporate additional functionality,<br>' +
        'including:<br>' +
        '• Metadata browser: The metadata browser is the reporting<br>' +
        'and navigation front end for the metadata repository.<br>' +
        'Chapter 15 describes a metadata schema that should be<br>' +
        'accessible from the BI portal.<br>' +
        '• Search function: The BI portal should include the ability to<br>' +
        'search the contents of the warehouse, and especially the<br>' +
        'report descriptions. Chapter 12 describes how to set up your<br>' +
        'SharePoint BI portal to include search (which is not as trivial<br>' +
        'as it sounds).<br>' +
        '• Warehouse activity monitors: Power users and the DW/BI<br>' +
        'team always want to know what’s going on in the DW/BI<br>' +
        'system right now. You might hear this question in slightly<br>' +
        'different forms, like “Who’s on the system?” or “Why is the<br>' +
        'report so slow?” Develop a few reports that execute against<br>' +
        '1024<br>' +
        'the SQL Server system tables or Analysis Services database.<br>' +
        'We discuss activity monitoring in greater detail in Chapter<br>' +
        '17.<br>' +
        'User Training<br>' +
        'One of the main purposes of the BI applications is to<br>' +
        'provide information for the 80 percent of the organization<br>' +
        'who’ll never learn to access the data directly.<br>' +
        'Unfortunately, the remaining 20 percent will never learn<br>' +
        'either, unless you teach them. Offer classes that will help<br>' +
        'ad hoc users climb the learning curve to master both the ad<br>' +
        'hoc tool and the underlying data.<br>' +
        'It’s hard to know when to start developing user training.<br>' +
        'You need to start after the database is stable and the<br>' +
        'front-end ad hoc tool has been selected, but long enough<br>' +
        'before the actual rollout begins to be able to create and test<br>' +
        'a solid set of course materials. Training development<br>' +
        'breaks down into two primary tasks: design and<br>' +
        'development of the course materials. Beyond these, the<br>' +
        'DW/BI educator might also need to create supporting<br>' +
        'materials and a training database.<br>' +
        'After the system has been in use for a few months, you<br>' +
        'may add an advanced techniques class for ad hoc users.<br>' +
        'You may also provide a separate, data-centric class for<br>' +
        'each new business process dimensional model added to the<br>' +
        'DW/BI system.<br>' +
        'Part of the design process includes outlining each class.<br>' +
        'The sidebar titled “Introductory One-Day Ad Hoc Query<br>' +
        'Course Outline” shows a typical outline. The outline will<br>' +
        'evolve during development, testing, and delivery of the<br>' +
        '1025<br>' +
        'class based on the reality of what it takes to teach people,<br>' +
        'and how long it takes them to learn.<br>' +
        'Introductory One-Day Ad Hoc Query Course<br>' +
        'Outline<br>' +
        'Introduction (gain attention) [30min]<br>' +
        '• DW/BI system overview (goals, data, status, and<br>' +
        'players)<br>' +
        '• Goals of the class<br>' +
        '• Student expectations for the class<br>' +
        'Tool Overview (Demo) [15]<br>' +
        '• Basic elements and user interface<br>' +
        '• The query building process<br>' +
        'Exercise 1 — Simple query [45]<br>' +
        'Break [15]<br>' +
        'Querying orders from the orders fact table (Demo)<br>' +
        '[15]<br>' +
        'Exercise 2 — Simple multi-table query [45]<br>' +
        'Review and questions [15]<br>' +
        'Lunch [60]<br>' +
        'Working with query templates (Demo) [15]<br>' +
        '1026<br>' +
        'Exercise 3 — Sales over time [60]<br>' +
        'Exercise 3 review (Demo) [15]<br>' +
        'Break [15]<br>' +
        'Saving, scheduling, and sharing reports (Demo)<br>' +
        '[15]<br>' +
        'Exercise 4 — Saving and scheduling reports [30]<br>' +
        'Overall review and next steps [15]<br>' +
        'Exercise 5 — Self-paced problem set [75]<br>' +
        'Creating the course materials for hands-on training<br>' +
        'requires a good sense for computer-based education. Many<br>' +
        'of the classic communications principles apply. Each<br>' +
        'module should be short enough to finish within the average<br>' +
        'adult attention span of 45 minutes to an hour. Each module<br>' +
        'should follow the same structure, beginning with a<br>' +
        'summary of the lesson and the key points the student<br>' +
        'should learn from the module. The body of the module<br>' +
        'should use a relevant business problem as the motivation<br>' +
        'for working through the material. Learning how to count<br>' +
        'the number of customers who responded to a new<br>' +
        'promotion would be more interesting than learning how to<br>' +
        'count the number of rows in the TABLES system table,<br>' +
        'even if the two exercises teach exactly the same concept.<br>' +
        'The exercises should be well illustrated with screen<br>' +
        '1027<br>' +
        'captures that look exactly like what the students will see<br>' +
        'on their computers.<br>' +
        'RESOURCES<br>' +
        'Much of the instructional design approach<br>' +
        'we follow is based on the work of Robert<br>' +
        'Gagné. His influential books, The<br>' +
        'Conditions of Learning and Theory of<br>' +
        'Instruction (Harcourt Brace College<br>' +
        'Publishers; 4th edition, 1985), and his more<br>' +
        'practical Principles of Instructional Design<br>' +
        '(Gagné, et.al., Wadsworth Publishing; 5th<br>' +
        'edition, June 15, 2004), take an approach<br>' +
        'based on cognitive psychology and<br>' +
        'information-processing theory. These<br>' +
        'theories posit that there are internal mental<br>' +
        'processes involved in learning that are<br>' +
        'influenced by external events.<br>' +
        'Gagné uses this relationship by viewing<br>' +
        'instruction as the arrangement of external<br>' +
        'events to activate and support the internal<br>' +
        'processes of learning. There is a lot more to<br>' +
        'creating effective training materials than<br>' +
        'just writing down a list of “click here”<br>' +
        'steps.<br>' +
        'The modules should become progressively more complex,<br>' +
        'with the early ones providing step-by-step instructions and<br>' +
        'the later ones offering higher-level guidance. Include<br>' +
        '1028<br>' +
        'bonus exercises at the end of each module to keep the<br>' +
        'quick learners occupied.<br>' +
        'Include time to test the training materials as part of the<br>' +
        'course development plan. Test each module on a few<br>' +
        'people from your team, and then test the whole package on<br>' +
        'a few of your friendly end users.<br>' +
        'The Training Database<br>' +
        'Most organizations that create ad hoc training<br>' +
        'materials also create a training database. The<br>' +
        'training database contains the same schema as the<br>' +
        'real DW/BI system, but it is scaled down and<br>' +
        'doesn’t require security. It’s typically derived from<br>' +
        'the static test database used in the development<br>' +
        'process.<br>' +
        'Although it’s nice to have real, up-to-the-minute<br>' +
        'data in training, you also need users to be able to<br>' +
        'see on their screens the exact image that’s in the<br>' +
        'training book. A static database that doesn’t require<br>' +
        'security filters is the easiest way to make that<br>' +
        'happen.<br>' +
        'Creating a good class takes a lot of work. Count on at least<br>' +
        'eight hours of work to create an hour of class materials. A<br>' +
        'one-day class will take about a week and a half to two<br>' +
        'weeks of hard work for someone with experience<br>' +
        'developing course materials. If this is your first time,<br>' +
        'double the estimate to give you more time to research<br>' +
        '1029<br>' +
        'other examples of good materials, and to test the materials<br>' +
        'you create.<br>' +
        'Keep hands-on classes for ad hoc users relatively small —<br>' +
        '10 to 20 people at a time. Have an assistant in the<br>' +
        'classroom to help answer individual questions during the<br>' +
        'exercises. Plan to have one assistant for every 10 students.<br>' +
        'User Support<br>' +
        'A well-designed and well-implemented DW/BI system is<br>' +
        'much easier to use than any alternative, but it’s still not<br>' +
        'that easy. The DW/BI team will need to provide ongoing<br>' +
        'support to its user community. We recommend a<br>' +
        'three-tiered approach to providing user support. The first<br>' +
        'tier is the website and self-service support, the second tier<br>' +
        'is your power users in business units, and the third tier is<br>' +
        'the front-end people on DW/BI team (the BI part of the<br>' +
        'group).<br>' +
        '• Tier 1, the Website: We’ve already discussed the<br>' +
        'support-related contents of the website in the documentation<br>' +
        'section. Having great content and the tools to find it<br>' +
        '(navigation, search, and metadata browser) is fundamental to<br>' +
        'providing support through the website.<br>' +
        '• Tier 2, the Expert Users: If someone needs help creating an<br>' +
        'ad hoc query, or needs a specific report that doesn’t already<br>' +
        'exist, they need to talk to someone with the skills to help. Set<br>' +
        'the expectation that this initial contact should be with<br>' +
        'someone who is based in the business, preferably in the<br>' +
        'person’s department.<br>' +
        '• Tier 3, the DW/BI Team: When the website and local experts<br>' +
        'are unable to solve the problem, the DW/BI team must offer<br>' +
        'a support resource of last resort. This front-end team actually<br>' +
        'has responsibilities across all support tiers. They own the BI<br>' +
        '1030<br>' +
        'portal site and must maintain and enhance its content<br>' +
        'including the BI applications. They own the relationships<br>' +
        'with and the training of the expert users. And, they provide<br>' +
        'direct support to the users when needed. This list of<br>' +
        'responsibilities represents a significant amount of work. Plan<br>' +
        'to have more people on the DW/BI team dedicated to these<br>' +
        'front-room tasks than to the back room — in an eight-person<br>' +
        'DW/BI team, for example, at least four people will be<br>' +
        'dedicated to front-room responsibilities.<br>' +
        'NOTE<br>' +
        'In some organizations, the BI portion of the<br>' +
        'DW/BI team gets split off to become its<br>' +
        'own entity. While there are probably good<br>' +
        'reasons to do this, we believe the DW/BI<br>' +
        'system is so closely tied to the business that<br>' +
        'splitting the two is like what happens when<br>' +
        'a cartoon character gets cut in half. The<br>' +
        'bottom half can’t see where it’s going, and<br>' +
        'the top half has lost its mobility. It’s<br>' +
        'important to dedicate people to the<br>' +
        'front-end responsibilities, but separating<br>' +
        'them into their own organization is<br>' +
        'generally not productive in the long run.<br>' +
        'The BI applications should be self-supporting, with<br>' +
        'pulldown menus, pick lists, and help screens. The DW/BI<br>' +
        'team will need to monitor their usage, maintain them as the<br>' +
        'data and data structures change, and extend and enhance<br>' +
        'them as additional data becomes available. Provide a<br>' +
        'means for users to give feedback on existing BI<br>' +
        'applications and request new ones.<br>' +
        '1031<br>' +
        'Many IT organizations began building their DW/BI<br>' +
        'systems with the goal of letting users create their own<br>' +
        'reports. The real goal was a bit more self-serving — the IT<br>' +
        'folks wanted to get off the report generation treadmill.<br>' +
        'Unfortunately, while this treadmill may slow down a bit, it<br>' +
        'never goes away. Even though accessing data is easier, the<br>' +
        'majority of knowledge workers don’t have the time or<br>' +
        'interest to learn how to meet their own information needs<br>' +
        'from scratch. Often, these people can be found at senior<br>' +
        'levels in the organization, so meeting their needs is<br>' +
        'particularly important. The DW/BI team will need to<br>' +
        'include creating custom reports in its responsibilities list,<br>' +
        'and to make sure there are resources available to meet the<br>' +
        'most important requests. The good news is that these<br>' +
        'custom reports can almost always be turned into<br>' +
        '(parameterized) standard reports and integrated into the<br>' +
        'existing BI application set.<br>' +
        'Desktop Readiness and Configuration<br>' +
        'The initial deployment must consider issues across the<br>' +
        'entire information chain, from the source systems to the<br>' +
        'user’s computer screen. Most PCs can handle the rigors of<br>' +
        'querying, reporting, and analysis. They already support<br>' +
        'large spreadsheets and Access databases. In some ways,<br>' +
        'the DW/BI system should reduce the strain on the user’s<br>' +
        'PC by moving most of the data management back to the<br>' +
        'servers. Don’t assume that everything will work fine at the<br>' +
        'user desktop. Test this assumption well before users attend<br>' +
        'training.<br>' +
        'Before you inspect the situation, decide how much<br>' +
        'capability a user’s desktop machine will need to have.<br>' +
        '1032<br>' +
        'Create a minimum configuration based on the front-end<br>' +
        'tools, the amount of data typically returned, and the<br>' +
        'complexity of the BI applications. This minimum<br>' +
        'configuration includes CPU speed, memory, disk space,<br>' +
        'and monitor size. It should also indicate the base computer<br>' +
        'type and operating system supported, and browser version<br>' +
        'requirements. We’ve been in organizations that insist on<br>' +
        'supporting multiple operating systems on users’ desktops:<br>' +
        'Windows, Apple, Linux, and UNIX. Obviously, this<br>' +
        'diversity has a big impact on the architecture and tool<br>' +
        'selection steps, long before you get to deployment. Let’s<br>' +
        'hope those who are implementing a Microsoft DW/BI<br>' +
        'system are less interested in supporting multiple types of<br>' +
        'operating systems, but there are still many flavors of<br>' +
        'Windows.<br>' +
        'When you go out into the user community, consider the<br>' +
        'following issues.<br>' +
        '• Connectivity: Connectivity is not usually an issue in today’s<br>' +
        'workplaces, but there can be a problem getting from one part of<br>' +
        'the organization to another. For example, a remote field office<br>' +
        'may not have the appropriate network configuration to get to the<br>' +
        'DW/BI server. Bandwidth to the desktop is usually not an issue<br>' +
        'either, but it’s worth verifying, especially for a mobile<br>' +
        'workforce.<br>' +
        'NOTE<br>' +
        'Windows Remote Desktop can be a good,<br>' +
        'inexpensive solution for bandwidth<br>' +
        'problems. It works very well across even<br>' +
        '1033<br>' +
        'fairly slow connections. We use it all the<br>' +
        'time over a virtual private network to<br>' +
        'shared servers. Many of our clients have<br>' +
        'reported great success in increasing user<br>' +
        'satisfaction — especially for salespeople<br>' +
        'and others who are often on the road.<br>' +
        '• Installation: Some front-end tools are desktop-based and need<br>' +
        'to have software installed on the user’s machine. PowerPivot,<br>' +
        'for example, requires Excel 2010 plus the actual PowerPivot<br>' +
        'add-in for Excel. Even browser-based tools may require a later<br>' +
        'version of Internet Explorer than your organization supports.<br>' +
        'Test the installation process from a selection of user machines<br>' +
        'and document any problems. If the installation process is<br>' +
        'anything more than clicking a button on the BI portal, make sure<br>' +
        'you document it clearly and use it to create a set of installation<br>' +
        'instructions.<br>' +
        'Summary<br>' +
        'The goal of this chapter is to highlight the most important<br>' +
        'issues to think about when deploying a DW/BI system.<br>' +
        'Deploying a system safely and successfully requires a lot<br>' +
        'of work and planning. You need the entire DW/BI team<br>' +
        'and help from business experts.<br>' +
        'The DW/BI team has to focus on developing solid<br>' +
        'operations and performance tests. Equally important, these<br>' +
        'back-room folks should concentrate on building and<br>' +
        'testing a playbook for the actual deployment process. The<br>' +
        'deployment playbook is vitally important when you’re<br>' +
        'adding new functionality to an existing system, while<br>' +
        'minimizing end user impact. The goal for the playbook<br>' +
        '1034<br>' +
        'should be to write instructions so clear and simple that<br>' +
        'anyone can follow them. System deployment is no time to<br>' +
        'be thinking!<br>' +
        'The front-room team focuses on queries, reports, and user<br>' +
        'interactions. During the deployment process, this team<br>' +
        'concentrates on running quality assurance tests on the data<br>' +
        'and reports. You have to rely heavily on the business<br>' +
        'experts for this testing work too. They’re the ones who will<br>' +
        'confirm the data’s accuracy. The front-room team also<br>' +
        'needs to develop system documentation, tools for<br>' +
        'searching and viewing that documentation, and training for<br>' +
        'the business users. Unless you’ve done documentation and<br>' +
        'training development before, you’ll be surprised at how<br>' +
        'much time it takes to do this “soft” stuff well.<br>' +
        'In our experience, problems with deploying a system<br>' +
        'almost always derive from incomplete testing. Test early<br>' +
        'and often. Test everything: your procedures, operations,<br>' +
        'and performance. Test the upgrade scripts. Check the<br>' +
        'results. Don’t approach the actual rollout date with an<br>' +
        'attitude that says, “This should work.” The right attitude is<br>' +
        '“We’ve tested this every way we can think of, and it will<br>' +
        'work.”<br>' +
        'When you do finally roll out the system, with no wrinkles<br>' +
        'at all, take a break. You’ve earned it! Then turn on your<br>' +
        'usage-monitoring tools, sit back, and watch what the<br>' +
        'business users are doing with their great new system.<br>' +
        '1035<br>';
    document.getElementById('chapter15').innerHTML = 'Chapter 15<br>' +
        'Metadata Plan<br>' +
        'The Bermuda Triangle of data warehousing.<br>' +
        'Metadata is a vast, relatively uncharted region of the DW/<br>' +
        'BI system. Some teams sail into it full speed ahead, never<br>' +
        'to be heard from again. Most teams try to avoid the<br>' +
        'problem by sailing around it. Unfortunately, the metadata<br>' +
        'region is smack in the middle of your path to the great new<br>' +
        'world of business value, and you need to figure out how to<br>' +
        'navigate it successfully.<br>' +
        'One of the first metadata challenges is figuring out what<br>' +
        'metadata is. We begin this chapter with a brief definition<br>' +
        'and description of the three major categories of metadata<br>' +
        'found in a DW/BI system: business, technical, and process<br>' +
        'metadata.<br>' +
        'With a common terminology in place, your next challenge<br>' +
        'is to figure out what metadata you have and where it<br>' +
        'comes from. We explore the various sources and uses of<br>' +
        'metadata across the SQL Server toolset. Every component<br>' +
        'of the toolset is metadata-driven. The problem is the<br>' +
        'metadata is kept in different locations and different<br>' +
        'formats, so finding and managing the metadata is<br>' +
        'challenging. Finally, we describe a basic, practical<br>' +
        'approach for dealing with the most important and broadly<br>' +
        'used metadata elements.<br>' +
        '938<br>' +
        'Metadata creation and management can be a tangled topic.<br>' +
        'What we present here is a starting point. Many of you, and<br>' +
        'certainly those of you in larger organizations, will need to<br>' +
        'expand on our recommendations and customize them to<br>' +
        'your environment.<br>' +
        'In this chapter, you will learn:<br>' +
        '• What we mean by metadata and why you need it.<br>' +
        '• What metadata features are included in SQL Server 2008<br>' +
        'R2.<br>' +
        '• How to implement an effective metadata strategy with the<br>' +
        'tools available.<br>' +
        'Metadata Basics<br>' +
        'One of the most common definitions of metadata is<br>' +
        '“Metadata is data about data.” This is vague to the point of<br>' +
        'uselessness. It doesn’t help you understand what metadata<br>' +
        'is or why you should care. We think about metadata as all<br>' +
        'the information that defines and describes the contents,<br>' +
        'structures, and operations of the DW/BI system. Metadata<br>' +
        'describes the contents of the warehouse and defines the<br>' +
        'structures that hold those contents and the processes that<br>' +
        'brought those contents into being. In this section, we talk<br>' +
        'about the purpose of metadata, describe the common types<br>' +
        'of metadata found in the DW/BI environment, and discuss<br>' +
        'the concept of the metadata catalog.<br>' +
        'The Purpose of Metadata<br>' +
        'Metadata serves two main purposes: defining and<br>' +
        'describing the objects and processes in a system.<br>' +
        '939<br>' +
        'Some metadata is used to define a process, object, or<br>' +
        'behavior. When you change the metadata, you change the<br>' +
        'process. A simple example is the start time of a SQL<br>' +
        'Server Agent job. Change the value of the start time<br>' +
        'element and you change the start time of the process. This<br>' +
        'idea of using metadata to define a process outside the code<br>' +
        'was an early, practical use of metadata. Separating a<br>' +
        'program’s code from its parameters and definitions allows<br>' +
        'the developer (and in some cases, the user) to change the<br>' +
        'parameters and definitions without having to edit and<br>' +
        'recompile the code. This concept has been around for<br>' +
        'decades in forms like table-driven programs and<br>' +
        'configuration files.<br>' +
        'Other metadata is used to describe an object or process.<br>' +
        'This kind of descriptive metadata is essentially<br>' +
        'documentation. If you change the process, but don’t<br>' +
        'change the description, the process itself still works, but<br>' +
        'your understanding of the process based on its description<br>' +
        'is now incorrect. Some of the common properties of an<br>' +
        'object, like its name or description, do not affect its<br>' +
        'appearance or behavior; they simply describe it in some<br>' +
        'way. It’s this idea of describing that leads to metadata as<br>' +
        'documentation.<br>' +
        'Metadata Categories<br>' +
        'The DW/BI industry often refers to two main categories of<br>' +
        'metadata: technical and business. We’ve added a third<br>' +
        'category called process metadata. As you’ll see in the<br>' +
        'descriptions of these categories that follow, technical<br>' +
        'metadata is primarily definitional, while business and<br>' +
        '940<br>' +
        'process metadata are primarily descriptive. There is some<br>' +
        'overlap between these categories.<br>' +
        '• Technical metadata defines the objects and processes that<br>' +
        'make up the warehouse itself from a technical perspective.<br>' +
        'This includes the system metadata that defines the data<br>' +
        'structures, like tables, columns, data types, dimensions,<br>' +
        'measures, data mining models, and partitions in the<br>' +
        'databases. In the ETL process, technical metadata defines<br>' +
        'the sources and targets for a particular task, the<br>' +
        'transformations, and so on. This description of technical<br>' +
        'metadata is cause for some confusion, because some of this<br>' +
        'technical metadata can also be used as business metadata.<br>' +
        'For example, tables and columns, security rules, and<br>' +
        'occasionally even ETL rules are of interest to some users.<br>' +
        '• Business metadata describes the contents of the data<br>' +
        'warehouse in more accessible terms. It tells us what data we<br>' +
        'have, where it comes from, what it means, and what its<br>' +
        'relationship is to other data in the warehouse. The name and<br>' +
        'description fields in Analysis Services are good examples of<br>' +
        'business metadata. Business metadata often serves as<br>' +
        'documentation for the data warehouse. As such, it may<br>' +
        'include additional layers of categorization that simplify the<br>' +
        'user’s view by subsetting tables into business-oriented<br>' +
        'groups, or omitting certain columns or tables. When users<br>' +
        'browse the metadata to see what’s in the warehouse, they are<br>' +
        'primarily viewing business metadata.<br>' +
        '• Process metadata describes the results of various operations<br>' +
        'in the warehouse. In the ETL process, each task logs key<br>' +
        'data about its execution, like start time, end time, rows<br>' +
        'processed, and so on. Similar process metadata is generated<br>' +
        'when users query the warehouse. This data is initially<br>' +
        'valuable for troubleshooting the ETL or query process. After<br>' +
        'people begin using the system, this data is a critical input to<br>' +
        'the performance monitoring and improvement process.<br>' +
        'The Metadata Repository<br>' +
        '941<br>' +
        'All of these metadata elements need a place to live.<br>' +
        'Ideally, each tool would keep its metadata in a shared<br>' +
        'repository where it can be easily reused by other tools and<br>' +
        'integrated for reporting and analysis purposes. This shared<br>' +
        'repository would follow standards for how metadata is<br>' +
        'stored so the repository can be easily accessed by any tool<br>' +
        'that needs metadata, and new tools can easily replace old<br>' +
        'tools by simply reading in their metadata.<br>' +
        'For example, suppose you had a shared, centralized<br>' +
        'repository in your DW/BI system. When you use your<br>' +
        'ETL tool to design a package to load your dimensions, the<br>' +
        'ETL tool would save that package in the repository in a set<br>' +
        'of structures that at least allow inquiry into the content and<br>' +
        'structure of the package. If you wanted to know what<br>' +
        'transforms were applied to the data in a given dimension<br>' +
        'table, you could query the repository.<br>' +
        'Unfortunately, this wonderful, integrated, shared<br>' +
        'repository is rare in the DW/BI world today, and when it<br>' +
        'does exist, it must be built and maintained with significant<br>' +
        'effort. Most DW/BI systems are like the Tower of Babel,<br>' +
        'and Microsoft SQL Server is no exception. Each<br>' +
        'component keeps its own metadata in its own structures<br>' +
        'and formats.<br>' +
        'It may provide some comfort to know that managing<br>' +
        'metadata is a challenge that is not unique to the Microsoft<br>' +
        'platform. For decades, people in the software industry<br>' +
        'have realized that managing metadata is a problem. There<br>' +
        'have been, and continue to be, major efforts within many<br>' +
        'companies to build a central metadata repository. At best,<br>' +
        'these are unstable successes. The amount of effort it takes<br>' +
        '942<br>' +
        'to build and maintain the central repository ends up being<br>' +
        'more than most companies are willing to pay. At the same<br>' +
        'time, several major software companies have tried to<br>' +
        'address the problem from a product perspective. Most of<br>' +
        'these products are large-scale, enterprise repositories that<br>' +
        'are built to handle every kind of system complexity.<br>' +
        'Implementers have a hard time navigating the product<br>' +
        'complexity, so most of the functionality remains unused.<br>' +
        'The cost and effort often bring the project stumbling to its<br>' +
        'knees. We’ve often seen initial success at implementation<br>' +
        'followed by a slow (or rapid) divergence from reality until<br>' +
        'the repository falls into complete disuse.<br>' +
        'Metadata Standards<br>' +
        'While projects to build an enterprise repository are often<br>' +
        'less than successful, the effort continues because there are<br>' +
        'at least four good reasons to have a standard, shared<br>' +
        'repository for metadata:<br>' +
        '• When tools exchange metadata, you can reuse existing<br>' +
        'metadata to help define each new step in the implementation<br>' +
        'process. Column names and descriptions captured in the data<br>' +
        'model design step can be used to build the relational tables,<br>' +
        'reused to populate the OLAP engine, and used again to<br>' +
        'populate the front-end tool’s metadata layer.<br>' +
        '• If the metadata is in a standard form, your investment in<br>' +
        'defining objects and processes is protected — you are not<br>' +
        'locked in to a particular tool. For example, Reporting<br>' +
        'Services Report Definition Language (RDL) is a language<br>' +
        'standard that describes a report. If all your reports are stored<br>' +
        'in RDL, you can potentially switch to a new front-end tool<br>' +
        'and still be able to use your existing report library. (This<br>' +
        'benefit is not particularly popular with tool vendors.)<br>' +
        '943<br>' +
        '• A central repository gives you a single, common<br>' +
        'understanding of the contents and structure of the data<br>' +
        'warehouse — it’s the best documentation you can have. To<br>' +
        'the extent that this shared repository holds the official, active<br>' +
        'metadata for each tool, you know it’s the current version and<br>' +
        'not a copy that may be out of date.<br>' +
        '• An integrated metadata repository allows you to more easily<br>' +
        'assess changes through impact and lineage analysis. The two<br>' +
        'concepts are essentially like looking down the same pipe<br>' +
        'from either end. In impact analysis, you want to know what<br>' +
        'downstream elements will be affected as a result of a<br>' +
        'potential change, like dropping a table. In lineage analysis,<br>' +
        'you want to know how a certain element came into being —<br>' +
        'what sources and transformations it went through to get to<br>' +
        'where it is.<br>' +
        'You’d like to have a standard repository for metadata in<br>' +
        'the DW/BI environment. The tools would write their<br>' +
        'metadata to the repository during the design phase and<br>' +
        'read it back in during the execution phase. In theory, any<br>' +
        'tool can write to and read from the standard model.<br>' +
        'There is a published standard framework for data<br>' +
        'warehouse metadata, called the Common Warehouse<br>' +
        'Metamodel (CWM). That standard was published in 2001,<br>' +
        'but it is not maintained and seems to be dead. At the time<br>' +
        'of this writing, there appears to be no effective, active,<br>' +
        'independent data warehouse metadata framework.<br>' +
        'SQL Server 2008 R2 Metadata<br>' +
        'The good news is that the SQL Server toolset is metadata<br>' +
        'driven. The relational engine has a slew of system tables<br>' +
        'and views that define and describe the data structures,<br>' +
        'activity monitoring, security, and other functions along<br>' +
        'with a set of stored procedures to manage them. Other<br>' +
        '944<br>' +
        'components, like Analysis Services and Integration<br>' +
        'Services, are based on similar metadata, stored in an<br>' +
        'object-oriented structure in XML files. Much, if not all, of<br>' +
        'the property based metadata in SQL Server can be<br>' +
        'accessed through the various object models.<br>' +
        'The bad news is that every major component of SQL<br>' +
        'Server keeps its metadata in independent structures, from<br>' +
        'database tables to XML files, which have their own access<br>' +
        'methods, from SQL Management Objects (SMO) and<br>' +
        'Analysis Management Objects (AMO) to stored<br>' +
        'procedures to APIs. Not only do the tools manage their<br>' +
        'own metadata, but the metadata they use is not integrated<br>' +
        'across the tools.<br>' +
        'As we describe in the next section, the first step in every<br>' +
        'metadata strategy is to assess the situation. You need to<br>' +
        'conduct a detailed inventory of what metadata structures<br>' +
        'are available, which ones are actually being used, what<br>' +
        'tools you have to view the metadata, and what tools you<br>' +
        'have to manage it. Table 15-1 provides a convenient<br>' +
        'summary of the various metadata sources and stores across<br>' +
        'the SQL Server BI platform, and identifies various tools<br>' +
        'for accessing and viewing them. The remainder of this<br>' +
        'section describes the major components in Table 15-1.<br>' +
        'Cross-Tool Components<br>' +
        'Several metadata components in SQL Server support more<br>' +
        'than one tool in the system. We decided to list those once<br>' +
        'rather than repeating them under each heading:<br>' +
        '945<br>' +
        '• SQL Server Agent is the job scheduler of the SQL Server<br>' +
        'world. It contains information about jobs, job steps, and<br>' +
        'schedules. SQL Server Agent metadata can be accessed<br>' +
        'through a set of stored procedures, system tables, SQL<br>' +
        'Management Objects (SMO), and SQL Server Management<br>' +
        'Studio.<br>' +
        '• SQL Server Profiler is SQL Server’s activity monitoring<br>' +
        'tool. You define a trace to track the occurrences of specific<br>' +
        'events, like Audit Logon, and have those occurrences written<br>' +
        'out to a table or file. SQL Server Profiler can be used<br>' +
        'interactively from the Profiler tool, or initiated<br>' +
        'programmatically using stored procedures. Chapter 17<br>' +
        'describes using SQL Server Profiler to create an audit log of<br>' +
        'DW/BI system usage.<br>' +
        '• BIDS Helper is a nice Visual Studio.NET add-in that extends<br>' +
        'BI Development Studio (BIDS). It has functionality for<br>' +
        'Analysis Services, Integration Services, and Reporting<br>' +
        'Services, including design warnings and health checks,<br>' +
        'synchronizing descriptions, and project cleanups.<br>' +
        '• SQL Server Metadata Toolkit provides a hint of what a<br>' +
        'future metadata tool might look like. The Toolkit will read<br>' +
        'your SSIS packages and Analysis Services cubes, put the<br>' +
        'information into a metadata database, and provide some<br>' +
        'tools for seeing the chain of relationships between packages<br>' +
        'and cubes.<br>' +
        'Table 15-1: Metadata sources and stores in the SQL<br>' +
        'Server 2008 DW/BI platform<br>' +
        '946<br>' +
        '947<br>' +
        'Relational Engine Metadata<br>' +
        'The SQL Server relational engine itself has system tables<br>' +
        'that can be accessed through a set of system catalog views<br>' +
        'or system stored procedures designed to report on the<br>' +
        'system table contents and, in some cases, to edit the<br>' +
        'contents. Alternatively, they can be accessed<br>' +
        'programmatically through SQL Server Management<br>' +
        'Objects (SMO).<br>' +
        'Business metadata can be stored along with the table and<br>' +
        'column definitions by using the extended properties<br>' +
        'function. This allows the developer to append any number<br>' +
        'of additional descriptive metadata elements onto the<br>' +
        'database objects. The dimensional model design<br>' +
        'spreadsheet described in Chapter 2 makes use of extended<br>' +
        'properties to store several metadata columns like<br>' +
        'description, comments, example values, and ETL rules.<br>' +
        'The names of these extended properties must be part of<br>' +
        'your naming convention so programs and queries can find<br>' +
        'them.<br>' +
        'Process metadata for the relational engine can be captured<br>' +
        'through the SQL Server Profiler component described<br>' +
        'earlier in this section. You can use the Management Data<br>' +
        'Warehouse feature of SQL Server to configure Profiler and<br>' +
        'other tools to collect and report on the key elements of<br>' +
        'ongoing database activity. Chapter 17 describes a<br>' +
        'performance monitoring log for the relational engine.<br>' +
        'Analysis Services<br>' +
        '948<br>' +
        'Analysis Services’ object model has all the same kinds of<br>' +
        'metadata that the relational engine has and more. From the<br>' +
        'definitions of the databases, cubes, dimensions, facts, and<br>' +
        'attributes to the KPIs, calculated columns, and hierarchy<br>' +
        'structures, not to mention the data mining models — all<br>' +
        'are available through the object model. The first and<br>' +
        'obvious access tools for the developer are the BI<br>' +
        'Development Studio and the SQL Server Management<br>' +
        'Studio. Beyond these, you can build custom .NET based<br>' +
        'applications that use Analysis Management Objects<br>' +
        '(AMO) to access the Analysis Services object model. In<br>' +
        'addition, there are Dynamic Management Views that use<br>' +
        'SQL syntax to access the Analysis Services object model.<br>' +
        'Your DBAs can use these DMVs to monitor activity and<br>' +
        'processes in the Analysis Services database, the same way<br>' +
        'they use system views to monitor the relational database.<br>' +
        'Analysis Services process metadata can be captured<br>' +
        'through the SQL Server Profiler component described<br>' +
        'earlier in this section. Chapter 17 describes a performance<br>' +
        'monitoring log for Analysis Services.<br>' +
        'Analysis Services PowerPivot is the Excel-based cube<br>' +
        'development and analysis functionality added in SQL<br>' +
        'Server 2008 R2. There must be metadata associated with<br>' +
        'PowerPivot, but its object model is not exposed in its<br>' +
        'initial release. We must await a future release of SQL<br>' +
        'Server to develop a coherent metadata story for a DW/BI<br>' +
        'architecture that includes PowerPivot.<br>' +
        'Integration Services<br>' +
        '949<br>' +
        'As one might expect, SQL Server Integration Services has<br>' +
        'its own object model. Integration Services is essentially a<br>' +
        'visual programming environment, as opposed to a<br>' +
        'structured database environment like the relational engine<br>' +
        'or Analysis Services. SSIS packages can be incredibly<br>' +
        'complex and do not follow a common structure. In fact,<br>' +
        'much of what might be considered Integration Services<br>' +
        'metadata will actually be the result of defining and using<br>' +
        'standard naming conventions for packages and tasks.<br>' +
        'Another challenge with Integration Services metadata is its<br>' +
        'time-dependent nature. At any point in time, it is difficult<br>' +
        'to tell which package was used to execute which load. One<br>' +
        'day, a certain ETL process can be run from a package<br>' +
        'saved in the file system. The next day, the ETL developer<br>' +
        'changes the SQL Agent task to point to another package<br>' +
        'stored in the database that uses different logic and business<br>' +
        'rules. While the packages will have distinct GUIDs, the<br>' +
        'names will be the same. Keeping track of this obviously<br>' +
        'requires a clear set of development process rules, naming<br>' +
        'standards, discipline, and monitoring.<br>' +
        'Regardless of where a package comes from, it can and<br>' +
        'should be set up to create its own process metadata. As we<br>' +
        'describe in Chapter 17, you should turn on logging at the<br>' +
        'package level, and select from a long list of events to log.<br>' +
        'Table 15-1 references some example Reporting Services<br>' +
        'reports on CodePlex that demonstrate how to access the<br>' +
        'log data and use it to track the execution and performance<br>' +
        'of your SSIS packages.<br>' +
        'In addition to this base process-level logging, the data<br>' +
        'warehouse audit system described in Chapter 7 ties the<br>' +
        'process metadata back to the actual data that was loaded in<br>' +
        '950<br>' +
        'a given ETL package. This metadata surfaces in the user<br>' +
        'interface in the form of an audit dimension and associated<br>' +
        'audit tables that allow the user to get a sense for where the<br>' +
        'data came from and how it was loaded.<br>' +
        'Reporting Services<br>' +
        'Reporting Services is entirely metadata driven. The<br>' +
        'contents, operation, usage, and security are all described in<br>' +
        'a set of metadata tables in the ReportServer database. It is<br>' +
        'possible to query these tables directly to see how<br>' +
        'Reporting Services works. However, rather than build<br>' +
        'reports on top of the production database, which can<br>' +
        'impact performance and potentially break when Microsoft<br>' +
        'changes the database, it makes sense to extract the process<br>' +
        'metadata into a separate reporting and analysis schema.<br>' +
        'Microsoft has included a couple of SQL scripts that create<br>' +
        'this schema and update it on a scheduled basis, along with<br>' +
        'a few example reports written against the schema. A<br>' +
        'similar tool, called SCRUBS, is available on CodePlex.<br>' +
        'The Reporting Services database is also surfaced through<br>' +
        'an object model that itself is accessible through a web<br>' +
        'service.<br>' +
        'Like many front-end tools, one of the Report Builder query<br>' +
        'designers uses metadata to define the objects, attributes,<br>' +
        'join paths, and calculations that it needs to formulate a<br>' +
        'query. This metadata set is called a Report Builder model<br>' +
        'and is created using the BI Development Studio. The<br>' +
        'relational version is built on top of its own Data Source<br>' +
        'View and is kept in the Reporting Services database.<br>' +
        'Report Builder models used to access Analysis Services<br>' +
        'are built directly from the Analysis Services cube.<br>' +
        '951<br>' +
        'Master Data Services<br>' +
        'Like the other components of SQL Server, Master Data<br>' +
        'Services is metadata driven, and its services and<br>' +
        'components are available through a rich object model.<br>' +
        'There are also stored procedures installed in each master<br>' +
        'data services database. You can use these stored<br>' +
        'procedures to interrogate the model, load data, and verify<br>' +
        'business rules.<br>' +
        'SharePoint<br>' +
        'SharePoint keeps its metadata in several relational<br>' +
        'databases which it sets up and manages in SQL Server.<br>' +
        'This metadata is programmatically accessible through<br>' +
        'SharePoint’s object model which includes both client and<br>' +
        'server components.<br>' +
        'SharePoint is a content management system at its heart. It<br>' +
        'supports the capture of metadata in lists and provides the<br>' +
        'ability to define shared taxonomies and terms in a<br>' +
        'managed metadata service. Most of this metadata is not<br>' +
        'relevant to the DW/BI system, but the tools can be used to<br>' +
        'keep track of your business metadata.<br>' +
        'External Metadata Sources<br>' +
        'Several useful metadata sources exist in the broader<br>' +
        'computing environment. Chief among these are the System<br>' +
        'Monitor tool and Active Directory. If you use a source<br>' +
        'control tool like Team Foundation Server, you may also<br>' +
        'consider that tool’s data as a metadata source.<br>' +
        '952<br>' +
        'System Monitor<br>' +
        'The Windows System Monitor performance tool is<br>' +
        'familiar to anyone who has done system performance<br>' +
        'troubleshooting in Windows. It works at the operating<br>' +
        'system level much like SQL Server Profiler does at the<br>' +
        'SQL Server level. You can define traces for a whole range<br>' +
        'of system activities and measures across the BI toolset and<br>' +
        'in the operating system itself. These traces can be defined<br>' +
        'to run in the background and to write to log files. If<br>' +
        'necessary, these logs can be tied back to SQL Profiler logs<br>' +
        'to help understand the possible causes of any problems.<br>' +
        'Chapter 17 describes which System Performance<br>' +
        'indicators are most important to log during regular<br>' +
        'operations.<br>' +
        'Active Directory<br>' +
        'Active Directory is Microsoft’s network based user logon<br>' +
        'management facility. It is a directory structure that can<br>' +
        'hold user, group, security, and other organizational<br>' +
        'information. Depending on how your organization is using<br>' +
        'Active Directory, it can be a source for security-related<br>' +
        'information. It can also be the system of record for some<br>' +
        'of the descriptive attributes of the employee and<br>' +
        'organization dimensions. This is another example of where<br>' +
        'the line between metadata and data can get fuzzy.<br>' +
        'Looking to the Future<br>' +
        'Clearly the SQL Server development team understands the<br>' +
        'importance of metadata from a development point of view.<br>' +
        'However, even though SQL Server has lots of metadata,<br>' +
        '953<br>' +
        'that metadata is not integrated. Careful metadata<br>' +
        'integration and active metadata management has not been<br>' +
        'a top priority for the core SQL Server product. In the past,<br>' +
        'Microsoft has been a leader on metadata issues, so we<br>' +
        'continue to hope that upcoming releases of SQL Server<br>' +
        'address the problem of metadata integration and<br>' +
        'management in a serious way.<br>' +
        'A Practical Metadata Approach<br>' +
        'This brings us to the question you’ve all been asking:<br>' +
        'What do we do about metadata to support data<br>' +
        'warehousing and business intelligence in the SQL Server<br>' +
        '2008 R2 environment? In the long term, we expect<br>' +
        'Microsoft to tackle the metadata management problem.<br>' +
        'Meanwhile, you have to figure out what you are going to<br>' +
        'do about metadata in the short to medium term. It’s easy to<br>' +
        'get trapped in the metadata morass (as the authors can<br>' +
        'certainly attest). It’s a major effort to figure out what<br>' +
        'metadata to capture, where to capture it, how to integrate<br>' +
        'it, how it should be used in the warehouse processes, and<br>' +
        'how to keep it synchronized and maintained. Vendors have<br>' +
        'been building metadata repositories and maintenance<br>' +
        'utilities for decades, and companies (some companies)<br>' +
        'have been trying to use these tools, or tools of their own<br>' +
        'creation, to tame the metadata beast for just as long. Even<br>' +
        'so, there are very few examples of large-scale, robust,<br>' +
        'successful metadata systems. It’s a really hard problem.<br>' +
        'Again you ask, “So what am I supposed to do?” First you<br>' +
        'need to appoint someone on the team to the role of<br>' +
        'metadata manager. If no one owns the problem, it will not<br>' +
        'be addressed. The metadata manager is responsible for<br>' +
        '954<br>' +
        'creating and implementing the metadata strategy. The ideal<br>' +
        'candidate has to know everything. No joke. If one person<br>' +
        'has to do the whole thing, he or she will need to have SQL<br>' +
        'and DBA skills. The metadata manager needs to know<br>' +
        'how to program in the Visual Studio environment and how<br>' +
        'to create and publish reports, and needs to understand the<br>' +
        'business, at a detailed level.<br>' +
        'Creating the Metadata Strategy<br>' +
        'We believe the following approach is a good compromise<br>' +
        'between having little or no managed metadata and building<br>' +
        'an enterprise metadata system. Our main recommendation<br>' +
        'is to concentrate on business metadata first. Make sure it is<br>' +
        'correct, complete, maintained, and accessible to the<br>' +
        'business users. Once that’s done, provide a way to view<br>' +
        'the other major metadata stores. We often see a tendency<br>' +
        'to over-engineer metadata. The key to making this strategy<br>' +
        'work is to not overdo it. Here’s the basic outline:<br>' +
        '1. Survey the landscape to identify the various locations,<br>' +
        'formats, and uses of metadata in SQL Server. The<br>' +
        'previous section and Table 15-1 give you a starting<br>' +
        'point. Use the tools described to explore the system for<br>' +
        'metadata locations. Where there aren’t any tools, you<br>' +
        'will need to create query or programmatic access to the<br>' +
        'rest of the sources so you can explore and track them.<br>' +
        'Create a list of the metadata elements you find,<br>' +
        'including where they are, where they came from, who<br>' +
        'owns them, how you view and change them, and where<br>' +
        'and how you might use them.<br>' +
        '955<br>' +
        '2. Identify or define metadata that needs to be captured<br>' +
        'and managed. These are the elements you’ll use more<br>' +
        'broadly and therefore need to keep updated and<br>' +
        'distributed throughout the system. What you need to<br>' +
        'manage depends on a lot of factors: your organizational<br>' +
        'commitment to data stewardship and predisposition to<br>' +
        'actively manage metadata, the level of support for<br>' +
        'actively managing metadata on the DW/BI team and by<br>' +
        'the user community, and the resources available to<br>' +
        'address the problem. At the very least, you must manage<br>' +
        'a basic level of business metadata. We will describe<br>' +
        'alternative approaches to this later in this section.<br>' +
        '3. While you’re at it, decide on the definitive location<br>' +
        'for each metadata element to be managed. This is the<br>' +
        'location where the element will be stored and edited. It<br>' +
        'is the source for any copies that are needed by other<br>' +
        'parts of the system. It might be in the relational database<br>' +
        'for some elements, in Analysis Services for others, and<br>' +
        'so on. For some elements, you might decide to keep it in<br>' +
        'a third-party tool. In the Adventure Works example, we<br>' +
        'are using extended properties in the relational database<br>' +
        'to capture several useful metadata fields.<br>' +
        '4. Create systems to capture any business or process<br>' +
        'metadata that does not have a home. Try to use all<br>' +
        'available pre-existing metadata structures, like<br>' +
        'description fields, before you add your own metadata<br>' +
        'tables. However, you will likely identify many fields<br>' +
        'that need a place to live; the comment field in the data<br>' +
        'model spreadsheet and the data warehouse usage history<br>' +
        'table are good examples of this. If the users are the<br>' +
        'owners of these elements, they should be responsible for<br>' +
        '956<br>' +
        'maintaining them. Many of our clients have created a<br>' +
        'separate metadata database that holds these metadata<br>' +
        'tables along with any value-added content tables that are<br>' +
        'maintained by the business users. It’s not too difficult to<br>' +
        'create a .NET front end to let users manage the contents<br>' +
        'of these tables. Or, you may consider creating a Master<br>' +
        'Data Services system just to manage business metadata.<br>' +
        '5. Create programs or tools to share and synchronize<br>' +
        'metadata as needed. This primarily involves copying the<br>' +
        'metadata from its master location to whatever subsystem<br>' +
        'needs it. Fill in the description fields, the source fields,<br>' +
        'and the business name fields in all the tables, extended<br>' +
        'properties, and object models from the initial database<br>' +
        'all the way out to the front-end tools. If these are<br>' +
        'populated right from the start as part of the design and<br>' +
        'development process, they will be easier to maintain on<br>' +
        'an ongoing basis.<br>' +
        '6. Educate the DW/BI team and key business users<br>' +
        'about the importance of metadata and the metadata<br>' +
        'strategy. Assign metadata creation and updating<br>' +
        'responsibilities. Your business sponsor’s support is vital<br>' +
        'for elevating the importance of data governance<br>' +
        'throughout your organization.<br>' +
        '7. Design and implement the delivery approach for<br>' +
        'getting business metadata out to the user community.<br>' +
        'Typically, this involves creating metadata access tools,<br>' +
        'like reports and browsers. Often, you need to create a<br>' +
        'simple metadata repository for business metadata and<br>' +
        'provide users with a way to browse the repository to<br>' +
        'find out what’s available in the BI system. We describe<br>' +
        '957<br>' +
        'a simple business metadata catalog in the next section.<br>' +
        'While you may actually use several reporting tools to<br>' +
        'provide access to the various metadata sources, this<br>' +
        'should appear as seamless as possible to the users. The<br>' +
        'different metadata access tools can all be linked to form<br>' +
        'a single page in the BI Portal.<br>' +
        '8. Manage the metadata and monitor usage and<br>' +
        'compliance. Make sure people know the information is<br>' +
        'out there and are able to use it. Make sure the metadata<br>' +
        'is complete and current. Being able to view the metadata<br>' +
        'is the hardest part of monitoring — especially in the<br>' +
        'SQL Server environment — because there are a lot of<br>' +
        'different metadata sources. A large part of the baseline<br>' +
        'metadata effort is spent building reports and browsers to<br>' +
        'provide access to the metadata. Monitoring means you<br>' +
        'have to actually look at those reports on a regular basis.<br>' +
        'Even though this is the balanced strategy between nothing<br>' +
        'and too much, it is still a fair amount of work. Make sure<br>' +
        'you include time in your project plan in all development<br>' +
        'tasks to capture and manage metadata, and that you<br>' +
        'include separate tasks for the preceding steps.<br>' +
        'Business Metadata Reporting<br>' +
        'Business metadata is the most important area to address<br>' +
        'because it supports the largest and most important segment<br>' +
        'of BI stakeholders — the users — who can’t get this<br>' +
        'information any other way. In other words, the technical<br>' +
        'folks can usually dig around and find the information they<br>' +
        'need in order to understand the contents of the data<br>' +
        'warehouse. The user community, for the most part, doesn’t<br>' +
        '958<br>' +
        'have this skill set. You must provide them with an easy,<br>' +
        'accessible way to explore the contents of the DW/BI<br>' +
        'system if you want them to know what’s available. We’ll<br>' +
        'approach the task of providing business metadata to the<br>' +
        'users from the best case to the worst case.<br>' +
        'Analysis Services as Primary Query Platform<br>' +
        'If your organization’s data and analytic needs are such that<br>' +
        'Analysis Services can meet them, and you have chosen<br>' +
        'Analysis Services as your primary user access platform,<br>' +
        'delivering basic business metadata can be relatively easy.<br>' +
        'Start with your front-end tool. See what kind of metadata<br>' +
        'layer it offers. The major front-end tool vendors offer rich<br>' +
        'metadata layers that can serve as the business metadata<br>' +
        'catalog.<br>' +
        'Most of the major front-end tools pull directly from the<br>' +
        'metadata fields found in Analysis Services. This includes<br>' +
        'the descriptions of cubes, dimensions, and attributes, often<br>' +
        'shown to the user right in the tool’s interface. Therefore,<br>' +
        'filling in the few metadata fields found in Analysis<br>' +
        'Services is a critical first step to improving users’<br>' +
        'understanding of the database contents.<br>' +
        'Perhaps you’ve decided your query and reporting tool<br>' +
        'doesn’t do an adequate job of publishing Analysis Services<br>' +
        'metadata. There are several approaches to bridging this<br>' +
        'gap; some are more work than others. If Analysis Services<br>' +
        'is your primary user database, the easiest approach is to<br>' +
        'build a simple browser that allows the user to navigate the<br>' +
        'Analysis Services object model. The SQL Server Product<br>' +
        'Samples available on CodePlex include a program called<br>' +
        '959<br>' +
        'the AMO Browser, shown in Figure 15-1, which allows<br>' +
        'the user to browse the Analysis Services object model.<br>' +
        'This sample program illustrates how to retrieve the<br>' +
        'properties of various Analysis Services objects. It would<br>' +
        'be fairly easy for a C# programmer to modify (or a VB<br>' +
        'programmer to replicate) this program to limit its output to<br>' +
        'strictly those objects and properties, like name and<br>' +
        'description, that are interesting to business users. Because<br>' +
        'this approach is based on the actual Analysis Services<br>' +
        'database and its contents, it has the advantage of never<br>' +
        'being out of sync with what users will see in their tools.<br>' +
        'This makes Analysis Services the system of record for<br>' +
        'these few metadata elements. However, it is limited to the<br>' +
        'metadata properties provided in Analysis Services: name,<br>' +
        'friendly name, description, display folders, and<br>' +
        'perspective.<br>' +
        'Figure 15-1: The sample Analysis Management Objects<br>' +
        'browser<br>' +
        '960<br>' +
        'Relational Engine Extended Properties<br>' +
        'If your primary user access platform is the relational<br>' +
        'engine, or if you feel the Analysis Services properties are<br>' +
        'too limiting, you can always go back to the relational<br>' +
        'model and provide a simple set of reports that allows users<br>' +
        'to explore the metadata in the extended properties that was<br>' +
        'created from the original business process dimensional<br>' +
        'modeling spreadsheet back in Chapter 2.<br>' +
        'The problem with this approach is that it’s limited to the<br>' +
        'relational structures as defined in the relational system<br>' +
        'tables. There’s no intermediate layer that lets you group<br>' +
        'subsets of tables into business process dimensional models<br>' +
        'and no display folders to group similar attributes to<br>' +
        'simplify the user view. And, it doesn’t include any other<br>' +
        'user access platforms like Analysis Services.<br>' +
        '961<br>' +
        'DOWNLOADS<br>' +
        'On the book’s website, we have provided a<br>' +
        'simple Reporting Services report that<br>' +
        'presents the extended properties for each<br>' +
        'column in a table. This report queries the<br>' +
        'system views in the relational database.<br>' +
        'Business Metadata Schema<br>' +
        'If the extended properties browser still isn’t enough, you<br>' +
        'can create a simple business metadata schema that will<br>' +
        'support both relational and Analysis Services databases,<br>' +
        'accommodate multiple subject areas, and allow for<br>' +
        'additional metadata fields that may not exist elsewhere.<br>' +
        'Figure 15-2 shows a basic business metadata schema.<br>' +
        'Figure 15-2: An example business metadata schema<br>' +
        '962<br>' +
        'NOTE<br>' +
        'The metadata schema is essentially a<br>' +
        'transaction model meant to keep track of<br>' +
        'the relationships among the various data<br>' +
        'elements in the warehouse, and to allow<br>' +
        'maintaining and updating that data. This is<br>' +
        'not a dimensional model.<br>' +
        'The schema is a hierarchy that starts at the database level<br>' +
        'in the upper-left corner (with servers and instances<br>' +
        'collapsed into the database table). Each database can<br>' +
        'contain zero to many subject areas, each of which contains<br>' +
        'zero to many objects, each of which contains zero to many<br>' +
        'attributes. Subject areas are groupings of objects, like<br>' +
        'business process dimensional models or Analysis Services<br>' +
        'perspectives or measure groups. The three contents tables<br>' +
        'allow you to map the same subject areas into several<br>' +
        'databases, the same objects into several subject areas, and<br>' +
        'the same attributes (or columns) into several objects. Your<br>' +
        'product dimension will probably participate in several<br>' +
        'subject areas, like sales, customer care, and returns.<br>' +
        'Once you populate this metadata schema, it is easy to<br>' +
        'report on the subject areas in a database, the contents of a<br>' +
        'subject area, and the detailed attributes of a dimension or<br>' +
        'fact table (or cube). These reports constitute a simple<br>' +
        'analytic application: the metadata browser. Like any<br>' +
        'analytic application, creating the reports so they flow well<br>' +
        'and formatting them so they communicate well is a learned<br>' +
        'skill. Test your efforts on some of the users. Get their<br>' +
        '963<br>' +
        'feedback on what works and what might be improved. A<br>' +
        'good metadata browser can help users learn faster and thus<br>' +
        'accelerate the overall acceptance of the BI system.<br>' +
        'DOWNLOADS<br>' +
        'We have done a lot of this work for you.<br>' +
        'Look under Chapter 15 on the Tools page<br>' +
        'of the book’s website to find the metadata<br>' +
        'schema scripts:<br>' +
        '• The script to create a business metadata<br>' +
        'relational database illustrated in Figure<br>' +
        '15-2.<br>' +
        '• Scripts to populate the metadata database<br>' +
        'from the system tables and table and<br>' +
        'column extended properties.<br>' +
        '• Scripts to populate the metadata database<br>' +
        'from the Analysis Services database.<br>' +
        '• A set of linked reports for users to browse<br>' +
        'the business metadata catalog.<br>' +
        'The business metadata schema is generally as far as we go<br>' +
        'in terms of creating, managing, and providing access to<br>' +
        'business metadata and it is meant to be a Pretty Good<br>' +
        'Practice. We believe the metadata schema is a great<br>' +
        'example of the 80-20 rule: you get 80 percent of the value<br>' +
        'of a sophisticated metadata management system with 20<br>' +
        'percent of the effort. The four levels in this schema can be<br>' +
        'used to accommodate a range of designs, platforms, and<br>' +
        'product editions. There are plenty of enhancements that<br>' +
        'would make it more detailed and flexible. There are<br>' +
        'probably a few additional enhancements that will be<br>' +
        '964<br>' +
        'important for your environment, but beyond that, the<br>' +
        'return on increased sophistication is pretty low.<br>' +
        'Process Metadata Reporting<br>' +
        'There is more metadata work to do beyond business<br>' +
        'metadata. You will need to create a suite of reports to<br>' +
        'provide quick, easy insight into what’s going on across the<br>' +
        'BI system right now, and how it is changing over time.<br>' +
        'These process metadata reports often serve as a starting<br>' +
        'point for technical folks to assess the status of the system<br>' +
        'and investigate any problems. Savvy business users want<br>' +
        'to see if their queries are running and why the system is so<br>' +
        'slow. We’ve seen users use these reports to identify the<br>' +
        'culprit amongst their co-workers and call them up and<br>' +
        'request that they kill their queries. The data warehouse<br>' +
        'team and the DBAs will most likely turn to more<br>' +
        'sophisticated tools, like SQL Server Management Data<br>' +
        'Warehouse, to get a better sense of what’s really<br>' +
        'happening and to be able to respond to the situation<br>' +
        'appropriately.<br>' +
        'The process metadata reports include:<br>' +
        '• Reports on active processes in SQL Server: You can report<br>' +
        'information on current user activity directly from the SQL<br>' +
        'Server system views or system stored procedures. You can<br>' +
        'also set up SQL Profiler trace logs to capture current activity<br>' +
        'and create a historical log, as described in Chapter 17.<br>' +
        '• Reports on active processes in Analysis Services: You can<br>' +
        'also set up SQL Profiler trace logs to capture current activity<br>' +
        'and create a historical log. Or, use the Dynamic<br>' +
        'Management Views to report on current activity.<br>' +
        '965<br>' +
        '• Reports on currently running and historical ETL processes:<br>' +
        'These can be based on log files and audit tables as described<br>' +
        'in Table 15-1. You can also view current Integration<br>' +
        'Services activity in the SQL Server Management Studio.<br>' +
        '• Reports on Reporting Services activity: You can view<br>' +
        'history using the Reporting Services execution log schema<br>' +
        'described in Table 15-1. You can monitor current activity by<br>' +
        'adding Reporting Services events to the Performance tool.<br>' +
        'Publish these reports on the BI portal for all to see. This<br>' +
        'supports the idea of a single place to go for business<br>' +
        'information, even for the DW/BI team.<br>' +
        'What’s interesting in the here-and-now is even more<br>' +
        'interesting over time. Each of these “real-time” monitoring<br>' +
        'tools has a historical counterpart. The data warehouse team<br>' +
        'should set up systems to capture performance and usage<br>' +
        'data over time, as we describe in Chapter 17. These logs<br>' +
        'are the input data to warehouse management, performance<br>' +
        'tuning, long-term capacity planning, and educating<br>' +
        'management about the use and prevalence of the BI<br>' +
        'system.<br>' +
        'Technical Metadata Reporting<br>' +
        'Technical metadata reporting is not the first priority for the<br>' +
        'warehouse team because most of the development tools are<br>' +
        'already built with the technical user in mind. That is, they<br>' +
        'provide the developer or operations person with direct<br>' +
        'access to the technical metadata. Much of the functionality<br>' +
        'of SQL Server Management Studio is essentially creating,<br>' +
        'browsing, and managing technical metadata. The SQL<br>' +
        'Server team has also created some add-on tools that<br>' +
        'provide extended metadata browsing capabilities. These<br>' +
        '966<br>' +
        'are listed in Table 15-1 and were discussed earlier in this<br>' +
        'chapter.<br>' +
        'Ongoing Metadata Management<br>' +
        'A metadata system is more than just a set of table or cube<br>' +
        'definitions — it is also a set of processes that allow you to<br>' +
        'manage those tables and cubes, obtain and distribute their<br>' +
        'contents, and keep them current. If you build a separate<br>' +
        'metadata repository, nightly refreshes of the metadata are<br>' +
        'typically acceptable. The safest approach would be to<br>' +
        'trigger a refresh whenever any of the metadata values is<br>' +
        'changed.<br>' +
        'You will also need to build processes to extract key<br>' +
        'metadata values from the systems of record and copy them<br>' +
        'to wherever they are needed. These targets can be the<br>' +
        'business metadata schema, Analysis Services cubes, or<br>' +
        'even a Report Builder model.<br>' +
        'You will need to implement ongoing data governance that<br>' +
        'includes business users in the process of maintaining<br>' +
        'business metadata. For user participation to be successful,<br>' +
        'you must create an interface for users to edit the business<br>' +
        'metadata values. This then ties in with assigning<br>' +
        'maintenance responsibility and monitoring and notification<br>' +
        'reports. Bottom line, you are building a data stewardship<br>' +
        'system, not just filling in a few columns as part of the<br>' +
        'initial data load.<br>' +
        'Summary<br>' +
        '967<br>' +
        'Metadata is a fuzzy, complex subject. In this chapter, we<br>' +
        'defined metadata as information that defines and describes<br>' +
        'the contents, structures, and operations of the DW/BI<br>' +
        'system. We described business, technical, and process<br>' +
        'metadata. Next, we described the various sources and<br>' +
        'access methods for SQL Server metadata. The rest of the<br>' +
        'chapter was dedicated to our recommended approach for<br>' +
        'creating and managing metadata in your DW/BI system.<br>' +
        'Our approach begins with the requirement that the DW/BI<br>' +
        'team must assign the role of metadata manager to one of<br>' +
        'the team members. We then provide the metadata manager<br>' +
        'with nine steps to develop a metadata strategy. These steps<br>' +
        'are:<br>' +
        '1. Work with the data stewards to help them learn their<br>' +
        'metadata roles and responsibilities.<br>' +
        '2. Conduct a metadata inventory.<br>' +
        '3. Identify key metadata elements that you will actively<br>' +
        'use and manage.<br>' +
        '4. Identify the definitive location (system of record) for<br>' +
        'each element.<br>' +
        '5. Create tools to capture and store any needed elements<br>' +
        'that do not exist in the SQL Server system.<br>' +
        '6. Create tools to synchronize and share metadata as<br>' +
        'needed.<br>' +
        '968<br>' +
        '7. Educate the DW/BI team and key business users<br>' +
        'about metadata and their metadata roles and<br>' +
        'responsibilities.<br>' +
        '8. Determine and build the metadata delivery approach,<br>' +
        'especially for business metadata.<br>' +
        '9. Manage the metadata system and monitor usage and<br>' +
        'compliance.<br>' +
        'The education step is critical to long-term metadata<br>' +
        'success. Everyone must view the creation and maintenance<br>' +
        'of metadata as being of equal importance as any other<br>' +
        'DW/BI task.<br>' +
        'Keep in mind that metadata can evolve into an<br>' +
        'overwhelming enterprise project that will suck all the<br>' +
        'energy and enthusiasm out of anyone who gets trapped in<br>' +
        'its clutches. We described our approach as a Pretty Good<br>' +
        'Practice. It requires about 10 percent of the effort involved<br>' +
        'in a major metadata initiative, but returns a much greater<br>' +
        'percentage of the value. Use the business value measuring<br>' +
        'stick to determine how far you need to go down the<br>' +
        'metadata path.<br>' +
        '969<br>';
    document.getElementById('chapter14').innerHTML = 'Part 4: Deploying and Managing the DW/BI System<br>' +
        'Chapter 14: Designing and Implementing Security<br>' +
        'Chapter 15: Metadata Plan<br>' +
        'Chapter 16: Deployment<br>' +
        'Chapter 17: Operations and Maintenance<br>' +
        'Chapter 18: Present Imperatives and Future Outlook<br>' +
        'The last part of the Lifecycle is the most exciting.<br>' +
        'Security! Deployment! Operations! As you can see in the<br>' +
        'diagram, we’ve placed these steps at the end of the<br>' +
        'Lifecycle. But don’t wait too long to start thinking about<br>' +
        'these issues. The successful deployment and operations of<br>' +
        'your data warehouse must be built into the system from the<br>' +
        'outset.<br>' +
        'As you’re designing your plans for securing the data in the<br>' +
        'system, a good business sponsor can keep you on track.<br>' +
        'The costs of not securing the data are easily understood,<br>' +
        'but don’t underestimate the costs associated with tightly<br>' +
        'securing information. The biggest cost of restrictive<br>' +
        'security is that it hinders the analyst’s ability to combine<br>' +
        'data in unexpected ways, which can lead to valuable<br>' +
        'insights into your business.<br>' +
        'A large part of deploying the fully developed and tested<br>' +
        'system is ensuring the user community is ready to use the<br>' +
        'new tools effectively. You should augment the metadata<br>' +
        'that underlies all the system components with<br>' +
        '881<br>' +
        'business-oriented descriptions. These descriptions become<br>' +
        'part of the user-facing documentation and must be ready as<br>' +
        'soon as the system is launched. In production, you should<br>' +
        'rely on your operational procedures to load the DW/BI<br>' +
        'system with excellent data quality and system reliability.<br>' +
        'Just as your system will grow as you include new business<br>' +
        'process dimensional models, so too do we expect the<br>' +
        'Microsoft toolset to continue to grow and improve. You’re<br>' +
        'embarking on an exciting project with your DW/BI<br>' +
        'system, and we wish you the best of luck with it.<br>' +
        'The Kimball Lifecycle steps covered in Part 4<br>' +
        '882<br>' +
        'Chapter 14<br>' +
        'Designing and Implementing Security<br>' +
        'How much security is enough?<br>' +
        'Security is another one of those black holes of the DW/BI<br>' +
        'system. It seems straightforward at first glance, but it often<br>' +
        'ends up being more complicated and uses more resources<br>' +
        'than originally planned.<br>' +
        'If you’re serious about security, and take the necessary<br>' +
        'steps to educate yourself, keep up-to-date on security<br>' +
        'bulletins and software updates, and design your system to<br>' +
        'minimize your attack surface, you’ll be in a good position<br>' +
        'to run a safe system. Microsoft throws so much<br>' +
        'information and so many security options at you that the<br>' +
        'greatest risk may be that you’ll give up out of frustration<br>' +
        'and confusion. We hope this chapter helps by highlighting<br>' +
        'the most important issues for a DW/BI system.<br>' +
        'You can minimize the cost and risk of implementing<br>' +
        'security by — yes! — writing a security plan. That plan<br>' +
        'should have a section for securing the environment,<br>' +
        'including the hardware and operating system; a section for<br>' +
        'securing the operations and administration of the system;<br>' +
        'and a section for securing data. No security plan is<br>' +
        'complete without a discussion of how to test the security.<br>' +
        'Designing and implementing tests for whether the right<br>' +
        'people have access to the right data can be as hard as any<br>' +
        'other task in developing and operating the DW/BI system.<br>' +
        '883<br>' +
        'In this chapter, we talk about the major components of<br>' +
        'DW/BI system security. These are the components that<br>' +
        'should be included in your security plan. The easy part is<br>' +
        'securing the physical environment and operating systems.<br>' +
        'A serious corporate environment will lock down the<br>' +
        'physical servers and systems. Turn on only those services<br>' +
        'and features that are necessary to run your system.<br>' +
        'After you’ve slammed the security doors shut, you need to<br>' +
        'start re-opening them to allow users into the system. A<br>' +
        'DW/BI system is valuable only if people can access it. The<br>' +
        'more information that’s broadly available, the more<br>' +
        'valuable your system will be. Information is an asset. If<br>' +
        'you keep it in the equivalent of a Swiss bank account with<br>' +
        'zero interest, you’re doing your organization a disservice.<br>' +
        'Careful stewardship of data requires that you protect the<br>' +
        'information that’s truly confidential and broadly publish<br>' +
        'the rest. Some organizations’ executive teams and culture<br>' +
        'are diametrically opposed to open access, but it’s worth<br>' +
        'arguing and pushing. Let’s hope your executive sponsor<br>' +
        'can carry this battle forward.<br>' +
        'You need to ensure that only authorized users can access<br>' +
        'the DW/BI system, and limit everyone’s view of data as<br>' +
        'appropriate. There are as many ways to do this as there are<br>' +
        'possible configurations for your DW/BI system. This is<br>' +
        'especially true if you’re using non-Microsoft software in<br>' +
        'your system. The bulk of this chapter is devoted to<br>' +
        'discussing the most common configurations. The SQL<br>' +
        'Server documentation in Books Online does a good job of<br>' +
        'discussing security for Reporting Services, the relational<br>' +
        'database engine, and Analysis Services. Figuring out how<br>' +
        '884<br>' +
        'these components work together is harder, so that’s where<br>' +
        'we’ve focused our attention.<br>' +
        'After reading this chapter, you should be able to answer<br>' +
        'the following questions:<br>' +
        '• How do you secure the hardware and operating systems for<br>' +
        'all the servers in your DW/BI system?<br>' +
        '• What kinds of security will you need for different kinds of<br>' +
        'user access, from running reports to ad hoc query and<br>' +
        'analysis?<br>' +
        '• How should you implement and test security features in the<br>' +
        'various components of your DW/BI system?<br>' +
        '• How can you monitor your customers’ usage of the DW/BI<br>' +
        'system?<br>' +
        'Identifying the Security Manager<br>' +
        'The first thing you must do is to explicitly identify a team<br>' +
        'member who’s responsible for the security of the DW/BI<br>' +
        'system. If no one owns the problem, it won’t be addressed.<br>' +
        'Define the role in the organizational context: What<br>' +
        'security is the security manager responsible for? What<br>' +
        'tasks does the security manager do, and what tasks does he<br>' +
        'or she direct others to do? The security manager has to be<br>' +
        'involved in the architecture design and in verifying the<br>' +
        'actual setup and use of the DW/BI system. Every new<br>' +
        'component, upgrade, user group, indeed any system<br>' +
        'change, needs to be examined from a security perspective<br>' +
        'to make sure it doesn’t compromise the system. Many<br>' +
        'organizations require a mandatory signoff by the security<br>' +
        'manager as part of the change deployment process.<br>' +
        '885<br>' +
        'We recommend that the security manager be part of the<br>' +
        'DW/BI team. The DW/BI security manager should have a<br>' +
        'formal relationship with any enterprise security office or<br>' +
        'Internal Audit. But to be effective, the security manager<br>' +
        'must be intimately familiar with the DW/BI system. In<br>' +
        'small organizations, the DW/BI team lead may play the<br>' +
        'role of the security manager. In any case, it needs to be<br>' +
        'someone fairly senior, with a broad understanding of the<br>' +
        'end-to-end system.<br>' +
        'Securing the Hardware and Operating System<br>' +
        'The most direct way to steal access to the valuable<br>' +
        'information in the DW/BI system is to gain physical access<br>' +
        'to the computers on which the system is running. You<br>' +
        'absolutely must implement the following simple but<br>' +
        'essential recommendations for your production system,<br>' +
        'and should think very seriously about doing so for the<br>' +
        'development and test servers as well.<br>' +
        '• Place the server computers in a locked room with restricted<br>' +
        'access.<br>' +
        '• Disable the option to boot from the CD-ROM drive.<br>' +
        'Consider removing access to the USB ports.<br>' +
        '• Consider creating a power-on password, and protect the<br>' +
        'motherboard’s settings with a CMOS-access password.<br>' +
        '• Consider using a computer case that supports intrusion<br>' +
        'detection and can be locked. Don’t leave the key dangling<br>' +
        'from the computer.<br>' +
        'Securing the Operating System<br>' +
        'The second most direct way to access the DW/BI system is<br>' +
        'by way of the operating system. You should implement the<br>' +
        '886<br>' +
        'following procedures for all the servers in your<br>' +
        'development, test, and production systems:<br>' +
        '• Restrict login access. No business user needs to log on to the<br>' +
        'servers. Most DW/BI team members don’t need to log in, as<br>' +
        'their tools work remotely. Only system administrators need<br>' +
        'to log in; others can access services across the network.<br>' +
        '• Restrict network access. This includes preventing<br>' +
        'anonymous sessions and disabling unneeded services.<br>' +
        '• Ensure data folders are secure. By default, the SQL Server<br>' +
        'relational database and Analysis Services databases store<br>' +
        'data in file structures that are appropriately protected. Other<br>' +
        'sensitive information includes backups and trace logs, and<br>' +
        'Integration Services packages. Ensure all information is<br>' +
        'appropriately protected.<br>' +
        '• Keep up-to-date with security patches for the operating<br>' +
        'system. Keep up-to-date with service packs for the SQL<br>' +
        'Server components.<br>' +
        '• Secure backup media. Backup files or tapes are more<br>' +
        'portable than the underlying databases; make sure these are<br>' +
        'protected as well.<br>' +
        'Using Windows Integrated Security<br>' +
        'Microsoft SQL Server uses Windows Integrated Security<br>' +
        'as its primary security mechanism. Set up Windows users<br>' +
        'and groups in the Windows environment — this is usually<br>' +
        'a task for system administrators rather than someone on<br>' +
        'the DW team. Then in Management Studio, create a set of<br>' +
        'roles. Grant or deny permissions to database objects for<br>' +
        'each role. Then assign the Windows groups to the<br>' +
        'appropriate roles. In a large enterprise, users will be<br>' +
        'assigned to several groups and roles. Use this same general<br>' +
        'approach for the relational database, Analysis Services,<br>' +
        'and Reporting Services.<br>' +
        '887<br>' +
        'NOTE<br>' +
        'Don’t assign a Windows user directly to a<br>' +
        'SQL Server role. This creates a<br>' +
        'maintenance problem if that user leaves the<br>' +
        'organization. Use the level of indirection<br>' +
        'provided by the Windows group construct,<br>' +
        'even if you create a group with only one<br>' +
        'member.<br>' +
        'In a Microsoft-centric enterprise, the users and some or all<br>' +
        'of the groups will be defined on the domain in Active<br>' +
        'Directory. In a heterogeneous environment, someone will<br>' +
        'need to integrate Active Directory into your environment.<br>' +
        'This is a job for a system administrator rather than a DW/<br>' +
        'BI expert, so we won’t go into any details here. Buy a<br>' +
        'book or hire a consultant to help you with this project.<br>' +
        'It’s possible to connect to Reporting Services, Analysis<br>' +
        'Services, and the database engine through mechanisms<br>' +
        'other than Windows Integrated Security. These options are<br>' +
        'less secure than truly Integrated Security, so you should<br>' +
        'consider this approach only if you can’t possibly make<br>' +
        'Integrated Security work in your environment. Even if you<br>' +
        'use these alternative authentication mechanisms, you’ll<br>' +
        'still need to create Windows users and groups on the<br>' +
        'database servers, and grant privileges to those groups.<br>' +
        '888<br>' +
        'NOTE<br>' +
        'The SQL Server database engine supports<br>' +
        'database security, where you define users<br>' +
        'and groups within the SQL Server<br>' +
        'relational database. Those of us who’ve<br>' +
        'been building databases for a long time<br>' +
        'find this security model familiar and<br>' +
        'comfortable, but it’s inherently less secure<br>' +
        'than Integrated Security and should be<br>' +
        'avoided.<br>' +
        'In this chapter, when we talk about users and groups, we<br>' +
        'mean Windows Integrated Security users and groups<br>' +
        'unless we explicitly say otherwise.<br>' +
        'Securing the Development Environment<br>' +
        'It’s quite common for development teams to have fairly<br>' +
        'loose standards — or no standards at all — for<br>' +
        'management of the development environment. The<br>' +
        'development environment and servers should be managed<br>' +
        'professionally, although usually not to the same standards<br>' +
        'as the test and production systems. The data on the<br>' +
        'development servers is often sensitive, as it’s drawn from<br>' +
        'the production source systems. You certainly should<br>' +
        'secure the hardware and operating system as we’ve just<br>' +
        'described, within reason. You should have a policy<br>' +
        'against, or strict procedures for, granting access to<br>' +
        'development servers to anyone outside the development<br>' +
        'portion of the organization.<br>' +
        '889<br>' +
        'To ease deployment, make the development machines’<br>' +
        'security environment similar to the production systems. On<br>' +
        'the other hand, you don’t want to lock down the systems<br>' +
        'so tightly that the developers will have problems getting<br>' +
        'their work done.<br>' +
        'A common approach is to manage shared development<br>' +
        'resources fairly well, yet allow developers to create private<br>' +
        'databases. Institute a change control process for the shared<br>' +
        'resources, like the relational data warehouse data model.<br>' +
        'Once other team members depend on a data model, allow<br>' +
        'changes only weekly and require 24 hours advance notice.<br>' +
        'Once you’ve instituted change control on the shared<br>' +
        'resources, you’ll see private databases popping up. That’s<br>' +
        'because some team members, in a sensitive part of their<br>' +
        'development cycle, really need an unchanging database, or<br>' +
        'they need to change it more frequently. Changes to the<br>' +
        'shared database can, at times, be incredibly annoying.<br>' +
        'Because the SQL Server database software is easy to<br>' +
        'install on a desktop machine, it’s hard to prevent private<br>' +
        'databases from cropping up. If you can’t, or don’t want to,<br>' +
        'forbid private databases, make it easy for your team<br>' +
        'members to secure them. Develop a policy for private<br>' +
        'databases, including system security procedures — usually<br>' +
        'the same procedures that you implement for your shared<br>' +
        'development resources. Better yet, write a lockdown script<br>' +
        '— probably a combination of a document script and a<br>' +
        'batch file — to perform basic lockdown.<br>' +
        'Developers should use read-only access to the source<br>' +
        'transaction systems. This is especially true for the DBAs<br>' +
        'and ETL developers, who may be creating and deleting<br>' +
        '890<br>' +
        'database objects in the relational data warehouse database.<br>' +
        'It’s not impossible to imagine that they could be careless<br>' +
        'and inadvertently execute a destructive statement against a<br>' +
        'transaction system. (If you’ve ever accidentally created a<br>' +
        'table in the Master database, you know what we’re talking<br>' +
        'about.)<br>' +
        'It’s safest to use only the minimum privileges necessary to<br>' +
        'get the job done. In the case of DW/BI system<br>' +
        'development, that should mean read-only access to the<br>' +
        'source databases. In a large corporation, this is unlikely to<br>' +
        'be an issue: the DW/BI team will not have write privileges<br>' +
        'into a transaction system. In a small company where<br>' +
        'people wear many hats, it’s not uncommon for a DW/BI<br>' +
        'developer to have high privileges on a production system.<br>' +
        'Securing the Data<br>' +
        'Now that we’ve done the basics, we come to the most<br>' +
        'interesting part of the security plan: securing the data while<br>' +
        'making it available for users to query.<br>' +
        'Providing Open Access for Internal Users<br>' +
        'We strongly encourage you to develop a data access policy<br>' +
        'that is fairly open for corporate users. The best approach is<br>' +
        'to start from the position that all data should be available<br>' +
        'to internal users; any exceptions should be justified.<br>' +
        'We’ve worked with organizations that approach the<br>' +
        'problem from the other direction. Even internally, these<br>' +
        'folks’ natural reaction is to make data available only on a<br>' +
        '“need to know” basis. They say that the sales manager in<br>' +
        '891<br>' +
        'one region cannot see sales numbers for other regions. The<br>' +
        'problem with this mindset should be obvious: A sales<br>' +
        'manager can’t assess her region’s performance outside the<br>' +
        'context of the rest of the company. She may think a 10<br>' +
        'percent growth in sales is a great number, until she realizes<br>' +
        'that all other regions saw 15 percent. The more<br>' +
        'information you hide, the less valuable your DW/BI<br>' +
        'system is going to be.<br>' +
        'Regardless of who has permission to access the data, it’s a<br>' +
        'good idea to have a written data access policy. Depending<br>' +
        'on your organization, the data access policy doesn’t have<br>' +
        'to be a long statement. A reasonable statement would say<br>' +
        'something like:<br>' +
        'Open access to administrative information is provided to<br>' +
        'employees for the support of corporate functions.<br>' +
        'Inappropriate use of information is a violation of your<br>' +
        'employment agreement. Default access is: Open Access to<br>' +
        'all employees, except for data elements on the Restricted<br>' +
        'Access list. Restricted Access is a designation applied to<br>' +
        'certain data elements, and limits access because of legal,<br>' +
        'ethical, or privacy issues. Access to elements so designated<br>' +
        'can be obtained with the approval of the designated data<br>' +
        'trustee. Any request to restrict employee access must be<br>' +
        'documented to Data Administration by the designated data<br>' +
        'trustee. Any employee denied access may appeal the denial<br>' +
        'to Data Administration.<br>' +
        'Unexpected Value of Open Access<br>' +
        '892<br>' +
        'One of our clients had an enlightened data access<br>' +
        'policy — or perhaps, as a tech startup, they hadn’t<br>' +
        'gotten around to drafting a more restrictive policy.<br>' +
        'At any rate, they experienced the power of open<br>' +
        'access. A Customer Care agent — a kid just out of<br>' +
        'high school — was poking around the data and<br>' +
        'uncovered a trend. A certain kind of trouble ticket<br>' +
        'was associated with a specific supplier’s hardware,<br>' +
        'hardware which, as it turned out, was not<br>' +
        'manufactured to specification. Our client wrung<br>' +
        'several million dollars out of the supplier, and<br>' +
        'averted unsatisfactory user experiences for many<br>' +
        'customers. This for a startup, for whom several<br>' +
        'million dollars and customer satisfaction were<br>' +
        'tremendously important.<br>' +
        'Many organizations would say a Customer Care<br>' +
        'agent has no business getting a global view of the<br>' +
        'data. It sounds reasonable to limit an agent’s view<br>' +
        'only to specific tickets. The upside is unknowable<br>' +
        'in advance, but we’ve seen it happen often. The<br>' +
        'doomsday scenarios are easier to see, but many<br>' +
        'people have a tendency to overstate both the<br>' +
        'likelihood and potential financial downside of data<br>' +
        'getting out of the hands of those who “need to<br>' +
        'know.”<br>' +
        'The ideal situation is to have very little truly sensitive<br>' +
        'information, like credit card numbers or Social Security<br>' +
        'numbers, in the DW/BI system. Just don’t bring it in. In<br>' +
        '893<br>' +
        'most organizations, any data around employee<br>' +
        'compensation is highly sensitive. In a health care<br>' +
        'environment, details about an individual patient are<br>' +
        'extremely sensitive, and access is highly regulated as well.<br>' +
        'We’re not saying that you should make such sensitive<br>' +
        'information widely available. Rather, identify the<br>' +
        'information that’s sensitive and document it as an<br>' +
        'exception to the general rule of data availability.<br>' +
        'NOTE<br>' +
        'There are cases where you can solve a data<br>' +
        'sensitivity problem by applying a hashing<br>' +
        'algorithm to the data. For example, it may<br>' +
        'be important for some business users to<br>' +
        'know that a customer’s credit card number<br>' +
        'has changed, without necessarily knowing<br>' +
        'what that credit card number is. In this case<br>' +
        'the ETL system would hash the credit card<br>' +
        'number on the way into the data<br>' +
        'warehouse.<br>' +
        'For an open access policy to work, you must:<br>' +
        '• Gain executive sponsorship for the approach. You can’t<br>' +
        'change corporate culture on your own. Many healthcare and<br>' +
        'law enforcement organizations are hopelessly — and<br>' +
        'justifiably — paranoid about data security.<br>' +
        '• Develop a system use policy statement, which users must<br>' +
        'sign before gaining access. Set up a mechanism for ensuring<br>' +
        'users sign this document before accessing the DW/BI<br>' +
        '894<br>' +
        'system. We generally require people to apply for access and<br>' +
        'include the policy on the application form.<br>' +
        '• Confirm that executives are willing to carry out any<br>' +
        'sanctions against security violations implied in the policy<br>' +
        'statement.<br>' +
        '• Gain executive agreement on the list of sensitive data<br>' +
        'elements.<br>' +
        '• Review the security policy at the beginning of every training<br>' +
        'class.<br>' +
        '• Publish (on the BI portal) the detailed security policy,<br>' +
        'including the list of sensitive data elements.<br>' +
        'RESOURCES<br>' +
        'You can get a good starting draft for a<br>' +
        'complete data access policy from the<br>' +
        'internet. Many universities and government<br>' +
        'agencies post their policies online. Search<br>' +
        'for “data access policy.”<br>' +
        'Itemizing Sensitive Data<br>' +
        'Whether you approach the problem from a mindset of<br>' +
        '“most data is available” or “most data is sensitive,” you<br>' +
        'need to document what data is hidden (or available), and to<br>' +
        'whom. As you’re developing your data sensitivity<br>' +
        'document, remember that the vast majority of system use<br>' +
        'is at aggregated levels, where sensitivity is usually less.<br>' +
        'Our primary concern is read-only access to data. Any<br>' +
        'writing activities, like developing forecasts and budgets,<br>' +
        'should be securely managed by an application. Specify the<br>' +
        'level at which aggregated information becomes available.<br>' +
        '895<br>' +
        'For example, most people can’t see sales by salesperson,<br>' +
        'but anyone can see aggregated sales at the region or<br>' +
        'district level, and corporate-wide.<br>' +
        'DOWNLOADS<br>' +
        'You can find a sample data sensitivity<br>' +
        'document on the book’s website.<br>' +
        'There is a subtlety associated with allowing data access at<br>' +
        'aggregate levels, but not at the detailed level. Returning to<br>' +
        'our discussion of sales by salesperson, what if we allow<br>' +
        'aggregate reporting at the district level, by gender? That<br>' +
        'can be an interesting question, but what if a district has<br>' +
        'only one saleswoman? Anyone in the company can infer<br>' +
        'her sales figures. There’s no easy answer to the inferred<br>' +
        'data member problem, although we discuss this issue again<br>' +
        'in the upcoming sections.<br>' +
        'Securing Various Types of Data Access<br>' +
        'The more restricted data you have, the more difficult it is<br>' +
        'to provide ad hoc access to the DW/BI system. Look back<br>' +
        'at your data sensitivity document and think about how to<br>' +
        'implement open access at aggregate levels and restricted<br>' +
        'access at detailed levels. The easiest way to do that is<br>' +
        'through an application like a report. Define aggregate-level<br>' +
        'reports that everyone can see, and limit access to reports<br>' +
        'that contain sensitive information.<br>' +
        '896<br>' +
        'If you allow direct ad hoc access to the information, you<br>' +
        'may need to apply complex access rules in the database.<br>' +
        'This is difficult in the relational database, as we discuss in<br>' +
        'the section “Relational DW Security.” This is a place<br>' +
        'where Analysis Services really shines: It’s possible — and<br>' +
        'really not that difficult — to meet a wide range of access<br>' +
        'scenarios by using Analysis Services permissions. In<br>' +
        'particular, the tool addresses the difficult problem of<br>' +
        'hiding detailed data but publishing aggregated data for ad<br>' +
        'hoc access.<br>' +
        'Reporting Services and most third-party tools like<br>' +
        'Business Objects, Cognos, and Analysis Services–specific<br>' +
        'tools like Panorama all contain security features. You<br>' +
        'should carefully examine the security features of any client<br>' +
        'tool before you decide to rely on those features. If a user<br>' +
        'has login privileges to the underlying database (Analysis<br>' +
        'Services or relational), the security must be applied to the<br>' +
        'database objects. Otherwise, users can simply use Excel or<br>' +
        'any other client tool to create an ad hoc connection, log on,<br>' +
        'and browse restricted data.<br>' +
        'Many front-end tools, including Reporting Services, can<br>' +
        '— or even must — be configured to use a shared report<br>' +
        'execution service account for access to the database server.<br>' +
        'If your front-end tool manages user security this way, you<br>' +
        'typically don’t even grant database login access to the<br>' +
        'users. This is very secure, assuming you’re exceptionally<br>' +
        'careful not to compromise the password to the report<br>' +
        'execution service account.<br>' +
        '897<br>' +
        'WARNING<br>' +
        'If a user is not allowed to see certain data,<br>' +
        'then he must either be:<br>' +
        '• Denied logon privileges to the database<br>' +
        'containing that data, or<br>' +
        '• Denied read privileges to the data table,<br>' +
        'columns, or rows.<br>' +
        'Security through obfuscation is not<br>' +
        'security.<br>' +
        'External reports bring an additional layer of security<br>' +
        'issues. An external report is a report from your DW/BI<br>' +
        'system that’s available to people outside your<br>' +
        'organization. For example, you may let your suppliers see<br>' +
        'your inventory of their products, or see how well their<br>' +
        'products are selling.<br>' +
        'The easiest way to meet the security requirements for<br>' +
        'standard external reporting is to use a push model: Email<br>' +
        'reports to your partners. The data-driven subscription<br>' +
        'feature of Reporting Services (Enterprise Edition) should<br>' +
        'meet the majority of external reporting requirements. This<br>' +
        'is a preferred approach because you don’t need to provide<br>' +
        'any access into your system, and you’re completely<br>' +
        'controlling when the report is run and to whom it’s<br>' +
        'delivered.<br>' +
        'Data-driven subscriptions don’t meet all external access<br>' +
        'requirements. You probably don’t want to send a daily<br>' +
        '898<br>' +
        'email to millions of customers about their account status.<br>' +
        'We’re tempted to call this operational reporting, and wash<br>' +
        'our hands of the problem. Indeed, you should think<br>' +
        'seriously about whether you want external users accessing<br>' +
        'the same system that your employees are using to run the<br>' +
        'business. Most often you’ll decide to spin out a data mart,<br>' +
        'and associated reporting portal, dedicated to this<br>' +
        'application. You’ll need to figure out how to authenticate<br>' +
        'those external users to allow them into the portal. These<br>' +
        'users will typically access only filtered reports, using the<br>' +
        'same techniques we’ve already discussed.<br>' +
        'It’s unusual for a company to provide ad hoc access to<br>' +
        'external people. Those who do are generally in the<br>' +
        'Information Provider business. You can use SQL Server<br>' +
        'and other Microsoft technologies to develop a robust<br>' +
        'Information Provision system, but this topic is beyond the<br>' +
        'scope of this book.<br>' +
        'Most implementations have a variety of security<br>' +
        'requirements. Some reports are available for anyone in the<br>' +
        'company, some have limited access to the complete report,<br>' +
        'and some filtered reports return a different result<br>' +
        'depending on who runs the report. Beyond reporting, you<br>' +
        'need to support some ad hoc access as well. You need to<br>' +
        'define some security in Reporting Services as well as the<br>' +
        'relational database and Analysis Services, to cover all<br>' +
        'these contingencies. Each component has several kinds of<br>' +
        'security options, which means you can probably figure out<br>' +
        'a way to do anything you want. But it also means that it’s<br>' +
        'hard to know where to begin. We begin by looking at how<br>' +
        'you use SQL Server’s components together to deliver the<br>' +
        'different levels of access.<br>' +
        '899<br>' +
        'Securing the Components of the DW/BI System<br>' +
        'There are so many security features, and combinations of<br>' +
        'security features, that it seems overwhelming. The first<br>' +
        'items to knock off the list are predefined reports.<br>' +
        'Reporting Services handles these very well and easily,<br>' +
        'whether the report is sourced from the relational database<br>' +
        'or Analysis Services. Next, we turn our attention to<br>' +
        'describing how to implement security in the various<br>' +
        'components of the DW/BI system, including Analysis<br>' +
        'Services, the relational database, and even Integration<br>' +
        'Services.<br>' +
        'Reporting Services Security<br>' +
        'Reporting Services can source reports from the relational<br>' +
        'data warehouse, other relational and even non-relational<br>' +
        'sources, and from Analysis Services. Reporting Services is<br>' +
        'a client of the databases, but a special kind of client: one<br>' +
        'that is also a server, and that contains its own security<br>' +
        'features.<br>' +
        'Administrative Roles for Reporting Services<br>' +
        'Reporting Services is installed with a set of predefined<br>' +
        'roles for administrative and business users. You can<br>' +
        'modify these roles, or replace them with custom roles. But<br>' +
        'most organizations will simply assign these predefined<br>' +
        'roles to various people who are performing administrative<br>' +
        'tasks.<br>' +
        'The relational database and Analysis Services have<br>' +
        'administrative roles that are clearly the purview of the<br>' +
        '900<br>' +
        'DW/BI team, and only the DW/BI team. By contrast, your<br>' +
        'configuration and use of Reporting Services may distribute<br>' +
        'the administrative workload out to the business units. You<br>' +
        'may have some business users who need an administrative<br>' +
        'role.<br>' +
        'The predefined administrative roles include:<br>' +
        '• System Administrator. Set server level features and security,<br>' +
        'and manage jobs. Clearly this highly privileged role should<br>' +
        'be granted to only one or two members of the DW/BI team.<br>' +
        '• Content Manager. Manage report folders and items within<br>' +
        'those folders, including security on those items. Content<br>' +
        'management permissions are not necessarily system-wide. In<br>' +
        'other words, you can have one person be the content<br>' +
        'manager for one set of reports like marketing and a different<br>' +
        'person manage sales reports.<br>' +
        '• Publisher. Publish content, like a report, to a report server.<br>' +
        'Some organizations will tightly control publishing rights,<br>' +
        'linking this role with Content Manager. Others will let<br>' +
        'anyone publish a report to the enterprise. We recommend<br>' +
        'being careful about who has publishing rights.<br>' +
        'Everyone involved with administering the reporting<br>' +
        'system should understand that securing folders is the most<br>' +
        'common way to manage user security. Security is inherited<br>' +
        'through the folder structure, so you can simplify the<br>' +
        'administrative burden by grouping reports in folders<br>' +
        'according to how confidential those reports are. When you<br>' +
        'restrict a folder, you are by default restricting all the items<br>' +
        'in the folder.<br>' +
        'One sensible use of a highly restricted folder is a place to<br>' +
        'hold reports that are being tested. Testing report definitions<br>' +
        'and layouts straddles the line between an administrative<br>' +
        '901<br>' +
        'function and a user function. Often, you want a business<br>' +
        'person to sign off on the report before it’s published more<br>' +
        'broadly. Using roles and a designated test folder, you can<br>' +
        'make it easy for the testers to find the reports under<br>' +
        'development, yet hide those reports from the rest of your<br>' +
        'organization.<br>' +
        'Remember that the security assigned to a folder is the<br>' +
        'default security setting for any item, like a report, that’s<br>' +
        'created inside that folder. You can change an individual<br>' +
        'report’s settings, but that’s a second step that you (or the<br>' +
        'distributed Content Managers) may forget to take.<br>' +
        'You can use either Management Studio or the Report<br>' +
        'Manager web application to manage permissions. The<br>' +
        'DW/BI team might use Management Studio; business<br>' +
        'users who are Content Managers will almost certainly use<br>' +
        'Report Manager.<br>' +
        'User Roles for Reporting Services<br>' +
        'When a user connects to Reporting Services, the report list<br>' +
        'shows only the reports that user is allowed to see. When a<br>' +
        'report is executed on demand, it’s usually executed not<br>' +
        'with the user’s credentials, but instead with a reporting<br>' +
        'service account used only to execute reports. In this<br>' +
        'scenario, users do not have login privileges to the<br>' +
        'underlying database.<br>' +
        'Most of your reports will be standard reports: predefined<br>' +
        'reports that are available to some or all DW/BI system<br>' +
        'users. These reports are simply secured through Reporting<br>' +
        'Services or SharePoint Reporting Services Integrated<br>' +
        '902<br>' +
        'Security. Reporting Services’ security model is perfectly<br>' +
        'targeted for standard reports. It’s very easy to set<br>' +
        'permissions on folders and reports. You may have a Data<br>' +
        'Administration team manage this function, or you can<br>' +
        'distribute some administration out to the departments who<br>' +
        'design and develop the reports.<br>' +
        'Some of your reports will be filtered reports, which return<br>' +
        'a different result set depending on who runs the report.<br>' +
        'Filtered reports require some security infrastructure in the<br>' +
        'underlying database. You have several options, most of<br>' +
        'which require some sort of permissions table in the<br>' +
        'underlying database:<br>' +
        '• Use a stored procedure as the source query for the filtered<br>' +
        'report. This technique works if the source database is the<br>' +
        'relational database. You can pass the username as an input<br>' +
        'parameter to the stored procedure.<br>' +
        '• Pass the user’s credentials to the database. This technique<br>' +
        'works with either SQL Server or Analysis Services serving<br>' +
        'the query.<br>' +
        'NOTE<br>' +
        'If you pass the user’s credentials to the<br>' +
        'underlying database, you may face the<br>' +
        '“two hop” problem. Typically, when a user<br>' +
        'connects from one computer to another,<br>' +
        'Windows credentials work for only one<br>' +
        'connection. If you need to connect to a<br>' +
        'second computer, for example to use the<br>' +
        'user’s credentials to log in to a database<br>' +
        '903<br>' +
        'server, you must use one of the following<br>' +
        'strategies:<br>' +
        '• Enable Kerberos. Implementing Kerberos<br>' +
        'is a project for system administrators, and<br>' +
        'is not usually a task that the DW/BI team<br>' +
        'would tackle.<br>' +
        '• Use SQL Server authentication. This<br>' +
        'technique works only for the relational<br>' +
        'database, not Analysis Services. And in<br>' +
        'general we recommend that you avoid<br>' +
        'using SQL Server authentication as it’s less<br>' +
        'secure than Integrated Security.<br>' +
        'Reporting Services in SharePoint Integrated Mode<br>' +
        'A popular configuration for Reporting Services is<br>' +
        'integrated with SharePoint. As we describe in Chapter 12,<br>' +
        'this configuration can be a bit complicated to set up. One<br>' +
        'of those points of complication is the security model.<br>' +
        'Reporting Services and SharePoint have different security<br>' +
        'structures, which are largely but not entirely compatible<br>' +
        'with each other.<br>' +
        'You have three choices for setting up security with<br>' +
        'SharePoint in integrated mode:<br>' +
        '• Windows authentication with Kerberos: This supports<br>' +
        'passing the user’s Windows credentials to the data<br>' +
        'warehouse database (such as for the filtered reports<br>' +
        'described previously).<br>' +
        '• Windows authentication without Kerberos: This works with<br>' +
        'any authentication protocol, but does not support passing the<br>' +
        'user’s credentials to the underlying database. The users will<br>' +
        '904<br>' +
        'have to authenticate a second time in order use their own<br>' +
        'credentials to execute a filtered report.<br>' +
        '• Forms authentication: This has the same advantages and<br>' +
        'disadvantages as Windows authentication without Kerberos.<br>' +
        'Windows authentication with Kerberos is the<br>' +
        'recommended strategy, if it fits within your IT<br>' +
        'environment.<br>' +
        'You can set permissions on report definitions, models, and<br>' +
        'connections from SharePoint. These permissions are<br>' +
        'integrated with Reporting Services. The security truly is<br>' +
        'integrated, and can be managed in either place.<br>' +
        'Analysis Services Security<br>' +
        'When you install Analysis Services, all members of the<br>' +
        'Administrators local group are granted access to the server<br>' +
        'and all databases and data. Until you set up roles and<br>' +
        'explicitly grant access, no other users can even connect to<br>' +
        'the Analysis Services instance, much less browse data.<br>' +
        'Administrative Roles for Analysis Services<br>' +
        'First, set up an administrative role for each Analysis<br>' +
        'Services database. A database administrator has full<br>' +
        'control over the database, including processing,<br>' +
        'aggregation design, and security. Analysis Services<br>' +
        'database administrator is a highly privileged role, but it<br>' +
        'doesn’t require system administrative privileges on the<br>' +
        'server. If your Analysis Services server contains multiple<br>' +
        'databases, create a database administrator role for each.<br>' +
        '905<br>' +
        'The easiest way to create a role, including an<br>' +
        'administrative role, is within Management Studio. While<br>' +
        'you’re logged in with server administration privileges, use<br>' +
        'the Object Browser to navigate down to a database, then to<br>' +
        'Roles within that database. Right-click to create a new<br>' +
        'role.<br>' +
        'The person who administers security needs unrestricted<br>' +
        'access to the cube’s data. That’s because the security<br>' +
        'administrator will need to test the roles and permissions.<br>' +
        'With Analysis Services, the security administrator must<br>' +
        'have Full Control (Administrator) permissions to the<br>' +
        'database. This is more permission than a security<br>' +
        'administrator needs — a security administrator shouldn’t<br>' +
        'be able to process the database or modify aggregations —<br>' +
        'but it’s as fine grained as you can get.<br>' +
        'NOTE<br>' +
        'Management Studio is the only tool that<br>' +
        'Microsoft provides for managing Analysis<br>' +
        'Services security. This is troubling,<br>' +
        'because the person who’s creating and<br>' +
        'testing roles, and mapping roles to groups,<br>' +
        'must have database administration<br>' +
        'privileges. If the security manager is<br>' +
        'careless, she can inadvertently damage the<br>' +
        'database.<br>' +
        '906<br>' +
        'We recommend that you limit this risk in<br>' +
        'one of two ways:<br>' +
        '• Write a tool that provides the same security<br>' +
        'management functionality as in<br>' +
        'Management Studio, but no other<br>' +
        'functionality. We’re surprised that we<br>' +
        'haven’t been able to find such a tool in the<br>' +
        'market or on Codeplex.<br>' +
        '• Develop and modify roles on the test<br>' +
        'system and script the deployment to the<br>' +
        'production system. This is always a good<br>' +
        'idea, whether or not you’re worried about<br>' +
        'the security manager’s administrative<br>' +
        'privileges.<br>' +
        'User Roles for Analysis Services<br>' +
        'Now that you understand how to administer security, it’s<br>' +
        'time for the more interesting topic of how to define users’<br>' +
        'permissions. Use the Role Designer dialog box to define<br>' +
        'and test Analysis Services roles on your test server. You<br>' +
        'can find the Role Designer under the Roles node in the<br>' +
        'Object Explorer pane of Management Studio.<br>' +
        'The Roles Designer dialog box, illustrated in Figure 14-1,<br>' +
        'contains eight pages, of which only a few are interesting:<br>' +
        '• General. Use the General page to name the role.<br>' +
        '• Membership. Use the Membership page to assign Active<br>' +
        'Directory groups to the role. When you first set up a role,<br>' +
        'you don’t need to assign any groups to that role. Although<br>' +
        'you might think that the first step in defining a user role is to<br>' +
        'assign users to the role, that assignment is actually the last<br>' +
        'step.<br>' +
        '907<br>' +
        '• Data Sources. This page is seldom used. The Data Source is<br>' +
        'the source of the Analysis Services database’s data, usually a<br>' +
        'SQL Server relational database. The database and cube<br>' +
        'definition place a layer between the user and that source;<br>' +
        'very few users need access to the Data Source. The<br>' +
        'exception is that if the Analysis Services database includes a<br>' +
        'data mining model, users may need access to a data source in<br>' +
        'order to perform a predictive query.<br>' +
        '• Cubes. Use the Cubes page to grant user access to the cubes<br>' +
        'within the database. You must grant access to a cube; users<br>' +
        'are not automatically granted cube access. If you simply<br>' +
        'grant access to a cube and immediately save the role, you’ve<br>' +
        'created a role with full access to all the data in the cube. If<br>' +
        'that’s not what you want to do, continue through the pages<br>' +
        'of the Role Designer to set the appropriate cell and<br>' +
        'dimension limits for the role.<br>' +
        '• Cell Data. Use the Cell Data page to grant user access to a<br>' +
        'subset of numbers in the cube. Cell security acts by<br>' +
        'replacing some of the cube’s numbers (facts) with #N/A.<br>' +
        'Cell security is discussed in greater detail below.<br>' +
        '• Dimensions. You can use the Dimensions page to deny<br>' +
        'access to a complete dimension.<br>' +
        '• Dimension Data. Use the Dimension Data page to grant or<br>' +
        'deny access to a portion of a dimension. Dimension security<br>' +
        'acts by making the cube look smaller by hiding some<br>' +
        'dimension members. Dimension security is discussed in<br>' +
        'greater detail below.<br>' +
        '• Mining Structures. Use the Mining Structures page to grant<br>' +
        'access to data mining models.<br>' +
        'Figure 14-1: The Analysis Services Role Designer dialog<br>' +
        'box<br>' +
        '908<br>' +
        'Before you launch into the process of designing cell and<br>' +
        'dimension security rules, map out the strategy for this role.<br>' +
        'Remember that dimension security makes a report grid<br>' +
        'smaller: It limits the display of dimension attributes. Cell<br>' +
        'security doesn’t change the row or column headers of a<br>' +
        'report; instead, it replaces some numbers inside the body<br>' +
        'of the report with #N/A. It’s easiest to envision dimension<br>' +
        'versus cell security in the context of a tabular report, but of<br>' +
        'course the security holds no matter how you’re viewing the<br>' +
        'results of a query.<br>' +
        'If you want to hide descriptive information about an<br>' +
        'employee, like his Social Security number, you use the<br>' +
        'Dimension Data page. If you want to hide quantitative<br>' +
        'information, like a salesperson’s sales quota, you use the<br>' +
        'Cell Data page. Think of cell security as security on facts.<br>' +
        'Remember that a user or group can have multiple roles: A<br>' +
        'user can be in both the Marketing role and the Executive<br>' +
        'role. When Analysis Services combines multiple roles, it<br>' +
        'does so additively, with a union operation. If a user<br>' +
        'belongs to two roles, one forbidding access to a data<br>' +
        '909<br>' +
        'element but the other allowing it, the user will have access<br>' +
        'to that data element.<br>' +
        'There’s one quasi-violation of this union rule. If a user has<br>' +
        'roles with both cell security and dimension security, the<br>' +
        'dimension security trumps the cell security. In other<br>' +
        'words, even if a role is permitted to see every fact cell in<br>' +
        'the cube, that data won’t show up if the role can’t see the<br>' +
        'dimension or dimension member. This is just what you’d<br>' +
        'expect to happen because the dimension security would<br>' +
        'forbid dimension members from showing up as row and<br>' +
        'column headers.<br>' +
        'When you’re testing your role definitions, you will really<br>' +
        'come to appreciate the role impersonation feature of the<br>' +
        'cube browser in Management Studio. Figure 14-2<br>' +
        'illustrates cube browsing with a different role’s<br>' +
        'credentials. Note that you can switch users (or roles) by<br>' +
        'clicking the Change Users icon in the upper left of the<br>' +
        'browser window, highlighted by the tooltip in Figure 14-2.<br>' +
        'You can test multiple roles overlaid on top of each other.<br>' +
        'When you’re working on security, you’ll quickly get in the<br>' +
        'habit of looking at the message near the top of the window,<br>' +
        'informing you of which role or roles you’re impersonating.<br>' +
        'Figure 14-2: Test role definitions by impersonating<br>' +
        'credentials<br>' +
        '910<br>' +
        'Dimension Security<br>' +
        'There are two basic approaches to defining dimension<br>' +
        'security on a dimension attribute: Specify the members<br>' +
        'that are allowed (all others are excluded), or specify the<br>' +
        'members that are denied (all others are accessible). In most<br>' +
        'cases, your choice of whether to specify the allowed set or<br>' +
        'denied set depends on the relative size of the included and<br>' +
        'excluded sets of members. There’s a second order problem<br>' +
        'that can be really important: if a new member is added to<br>' +
        'the dimension, should it be accessible or excluded by<br>' +
        'default? The safest thing is to specify the members the role<br>' +
        'is allowed to see. Then when new members are added,<br>' +
        'they won’t be visible to restricted roles until the role<br>' +
        'definition is explicitly changed.<br>' +
        'Figure 14-3 illustrates a simple definition of dimension<br>' +
        'security on a dimension attribute. In this case, we’re using<br>' +
        '911<br>' +
        'the Deselect all members option to explicitly define the<br>' +
        'allowed set. Any new members will be excluded.<br>' +
        'Figure 14-3: Defining basic dimension security<br>' +
        'We’ve introduced the problem of new dimension members<br>' +
        'joining the dimension. If this happens rarely, it’s not too<br>' +
        'big a burden to redefine the roles to account for the new<br>' +
        'member. If the dimension changes rapidly — for example<br>' +
        'a customer dimension — you’ll want to take a different<br>' +
        'approach.<br>' +
        'One of the neatest solutions is to use MDX to define the<br>' +
        'included (or excluded) set of members. Under the covers<br>' +
        'you’re always using MDX: The pick list illustrated in<br>' +
        '912<br>' +
        'Figure 14-3 is simply a user interface that generates an<br>' +
        'MDX expression. In this case, the expression is a list of<br>' +
        'members. You can see this expression by switching over to<br>' +
        'the Advanced tab on the Dimension Data page.<br>' +
        'A common MDX security expression will include (or<br>' +
        'exclude) all the children of a parent. For example, imagine<br>' +
        'you want to create a role that can see all the product<br>' +
        'categories (Bikes, Clothing, Accessories, and<br>' +
        'Components), cannot see any of the subcategories under<br>' +
        'Bikes, but can see all the subcategories for the other three<br>' +
        'categories. Instead of listing the products that are in the<br>' +
        'brand today, define the MDX expression for the<br>' +
        'subcategory attribute of the product dimension as<br>' +
        'illustrated in Figure 14-4.<br>' +
        'MDX expressions can be a lot more complicated than this,<br>' +
        'but the Exists function illustrated here, and its close friend<br>' +
        'the Except function, cover most cases. Books Online has<br>' +
        'more examples under the topic “Granting Custom Access<br>' +
        'to Dimension Data.” For more complex examples, you<br>' +
        'should look at a reference book on MDX. Chapter 8 lists<br>' +
        'several MDX references.<br>' +
        'Figure 14-4: Using MDX expressions to define dimension<br>' +
        'security<br>' +
        '913<br>' +
        'There are a few wrinkles associated with dimension<br>' +
        'security. The first is the behavior of related attributes; the<br>' +
        'second issue is referred to as Visual Totals.<br>' +
        'We introduced related attributes in Chapter 8. One of the<br>' +
        'common uses of related attributes is to define a natural<br>' +
        'hierarchy, like product category to subcategory to brand.<br>' +
        'Related attributes have an implication for the way<br>' +
        'dimension security works for denied sets. If you deny<br>' +
        'access to a specific product category, then you’re also<br>' +
        'denying access to its children (subcategories and brands).<br>' +
        'This behavior doesn’t come from the definition of a<br>' +
        'hierarchy between these three attributes. Instead, the<br>' +
        'behavior is driven by the definition of the relationships<br>' +
        'between the attributes — which is what makes it a natural<br>' +
        'hierarchy.<br>' +
        '914<br>' +
        'TIP<br>' +
        'It really feels like the hierarchy definition<br>' +
        'should be the thing that drives the security<br>' +
        'relationship, not the related attribute<br>' +
        'definition that users never see. To help<br>' +
        'keep it straight, think of the hierarchy<br>' +
        'merely as a drilldown path for the user<br>' +
        'interface; it’s the underlying related<br>' +
        'attribute definitions that are really<br>' +
        'important. A product attribute like color<br>' +
        'that has no related attribute to the item<br>' +
        'you’re securing (product category in our<br>' +
        'example) will not be affected by the<br>' +
        'security definition. Which, after all, makes<br>' +
        'perfect sense.<br>' +
        'The second complexity with dimension security is the<br>' +
        'notion of Visual Totals, which you can see as a checkbox<br>' +
        'at bottom of the Advanced tab of the Dimension Data page<br>' +
        'in Figure 14-4. Imagine a role that grants permission only<br>' +
        'to a single product and doesn’t restrict the brand level.<br>' +
        'What should be the subtotal by brand, for the brand<br>' +
        'containing the one product the role can see? Should the<br>' +
        'subtotal be the real subtotal for the brand? Or should it be<br>' +
        'the subtotal for the visible product? The answer depends<br>' +
        'on your business user requirements. The default behavior<br>' +
        'is to not use Visual Totals: Subtotals reflect both visible<br>' +
        'and invisible cells.<br>' +
        '915<br>' +
        'TIP<br>' +
        'This is a difficult concept with a mediocre<br>' +
        'name. Visual Totals refers to what you<br>' +
        'would expect the total to be if you were<br>' +
        'looking at a report, and added all the<br>' +
        'numbers you’re allowed to see on the<br>' +
        'report.<br>' +
        'We usually prefer the default behavior for several reasons.<br>' +
        'First, the server has to work harder to support Visual<br>' +
        'Totals. It makes much less use of predefined aggregates.<br>' +
        'More important, with Visual Totals turned on, you’re<br>' +
        'sowing confusion among your business users. Two people<br>' +
        'running the same report would come up with different<br>' +
        'totals at the brand level. This is a situation you’ve built<br>' +
        'your DW/BI system to avoid.<br>' +
        'A downside of using the default behavior is that users may<br>' +
        'be able to infer a piece of hidden information. The simplest<br>' +
        'example is if you’re hiding only one product, and showing<br>' +
        'the true brand total. Any user can calculate the hidden<br>' +
        'product’s sales as the difference between the brand total<br>' +
        'and the sum of the accessible products’ sales.<br>' +
        'Cell Security<br>' +
        'Cell security affects which numbers or facts are displayed<br>' +
        'in the grid of a report, and which are blanked out (or<br>' +
        'replaced with #N/A or some other display element). Cell<br>' +
        'security, like dimension security, uses MDX expressions to<br>' +
        '916<br>' +
        'define the cells that are accessible or hidden. Unlike<br>' +
        'dimension security, the Cell Data page of the Role<br>' +
        'Designer doesn’t have any user interface other than<br>' +
        'entering MDX expressions. Cell security is so flexible that<br>' +
        'the best available UI is the MDX editor window.<br>' +
        'Figure 14-5 illustrates the Cell Data page of the Role<br>' +
        'Designer. We’ve started by granting read privileges to one<br>' +
        'node of data in the product dimension: those data elements<br>' +
        'that roll up to the “Bikes” category. Data for other<br>' +
        'categories will display #N/A.<br>' +
        'Figure 14-5: Defining cell-level security<br>' +
        'NOTE<br>' +
        '917<br>' +
        'Users in the United States are familiar with<br>' +
        'the use of #N/A to mean Not Available<br>' +
        'because that’s the convention in Microsoft<br>' +
        'Excel. If you don’t like #N/A, your client<br>' +
        'application may be able to translate this<br>' +
        'abbreviation for you. Or, you can specify a<br>' +
        'different value for the Secured Cell<br>' +
        'Value property in the user’s connection<br>' +
        'string.<br>' +
        'The expression in Figure 14-5 refers to the product<br>' +
        'dimension. Data along other dimensions in the cube is<br>' +
        'currently unrestricted. You can build up an MDX<br>' +
        'expression that refers to multiple dimensions, with clauses<br>' +
        'connected by ANDs and ORs. We recommend you start<br>' +
        'simply and build up the expression piece by piece, testing<br>' +
        'after each addition.<br>' +
        'RESOURCES<br>' +
        'The Books Online topic “Granting Custom<br>' +
        'Access to Cell Data” contains a nice set of<br>' +
        'increasingly complicated cell security<br>' +
        'definitions.<br>' +
        'Look back at Figure 14-5, and notice the option to enable<br>' +
        'read-contingent permissions. This is an advanced option<br>' +
        'that most applications will not need. Read contingency is<br>' +
        'relevant for permissions on derived cells. The definition of<br>' +
        '918<br>' +
        'read-contingent permissions is the same as for read<br>' +
        'permissions, but the behavior is different. Read-contingent<br>' +
        'permission will show a derived cell only if the role has<br>' +
        'access to all the data that goes into that derived cell. For<br>' +
        'example, if you have a derived measure for contribution to<br>' +
        'margin, the role can see that measure only if the<br>' +
        'underlying data to calculate contribution to margin is also<br>' +
        'accessible to that role. The rationale for this feature is<br>' +
        'similar to the Visual Totals discussion under dimension<br>' +
        'security: A smart user may be able to infer information<br>' +
        'that you don’t want him to have. We advise caution with<br>' +
        'implementing read-contingent security. No doubt there are<br>' +
        'cases where this finesse is important, but it’s more likely to<br>' +
        'leave everyone confused.<br>' +
        'Return to Figure 14-5, and notice the third place to enter<br>' +
        'an MDX expression for read/write permissions. Write<br>' +
        'permissions have to do with Analysis Services databases<br>' +
        'that support data writeback. These are typically financial<br>' +
        'applications like budgeting and forecasting. This book<br>' +
        'doesn’t address this kind of application, but not because<br>' +
        'it’s uninteresting or because it’s not part of business<br>' +
        'intelligence. Rather, it’s a huge topic worthy of a paper or<br>' +
        'book of its own. Within the context of our security<br>' +
        'discussion, assigning write permission to a portion of the<br>' +
        'cell data uses exactly the same kind of MDX expression<br>' +
        'we’ve already discussed. Most organizations should steer<br>' +
        'clear of developing write-enabled cubes, and instead look<br>' +
        'to purchase packaged budgeting and forecasting software<br>' +
        'that is implemented on Analysis Services.<br>' +
        '919<br>' +
        'WARNING<br>' +
        'Be careful about checking the Read<br>' +
        'Contingent or Read/Write checkboxes<br>' +
        'without adding an MDX expression to<br>' +
        'focus the permission. If you check the<br>' +
        'Read/Write checkbox but don’t add an<br>' +
        'expression, you’re giving the role Read/<br>' +
        'Write privileges to the entire cube.<br>' +
        'Similarly for Read Contingent. This is<br>' +
        'unlikely to be what you intended.<br>' +
        'Dynamic Security<br>' +
        'You can use the dimension and cell security features to<br>' +
        'implement complex security rules. However, a large<br>' +
        'organization with individualized security may find that<br>' +
        'maintaining the MDX for many groups to be burdensome.<br>' +
        'Consider the scenario where you plan to deploy ad hoc<br>' +
        'Analysis Services access to a thousand users, and each<br>' +
        'user must be allowed to see a personalized part of the<br>' +
        'cube, such as their own accounts or employees. You can<br>' +
        'be in the position of creating, testing, and maintaining<br>' +
        'hundreds or thousands of roles.<br>' +
        'An alternative approach is to use dynamic or data-driven<br>' +
        'security. This technique builds security into the structure<br>' +
        'of the cube, by creating a user dimension and a<br>' +
        'relationship fact table that describes what data elements<br>' +
        'each user is allowed to see. The UserName MDX function<br>' +
        'is your secret weapon. Creative definition of calculations<br>' +
        '920<br>' +
        'can dynamically hide access to most detailed data (except<br>' +
        'your own), and still reveal aggregated data across the<br>' +
        'enterprise.<br>' +
        'Dynamic security is more expensive at query time than<br>' +
        'standard security implementations. We’d be cautious about<br>' +
        'using the technique for terabyte-scale data volumes. The<br>' +
        'good news is that most of the dynamic security scenarios<br>' +
        'we’ve encountered, such as financial or personnel data, are<br>' +
        'good candidates for the technique.<br>' +
        'RESOURCES<br>' +
        'The book SQL Server 2008 MDX Step by<br>' +
        'Step (Microsoft Press, 2009), by Bryan<br>' +
        'Smith and Ryan Clay has more information<br>' +
        'on how to implement dynamic security.<br>' +
        'You can also find many alternative<br>' +
        'techniques on the web by searching<br>' +
        '“Analysis Services dynamic security.”<br>' +
        'PowerPivot Security<br>' +
        'As we describe in Chapter 11, the PowerPivot<br>' +
        'functionality that’s new in SQL Server 2008 R2 is<br>' +
        'basically a reporting and analysis Excel add-in. The user<br>' +
        'who’s creating a new PowerPivot model is subject to the<br>' +
        'security rules of the databases from which he’s sourcing<br>' +
        'the data (usually Analysis Services and the relational data<br>' +
        'warehouse). Once a PowerPivot model is created, it can be<br>' +
        'secured at an overall level, like any other Excel document.<br>' +
        '921<br>' +
        'There is no functionality for securing a portion of the data<br>' +
        'in a PowerPivot model.<br>' +
        'Relational DW Security<br>' +
        'If the only access to the relational data warehouse comes<br>' +
        'through Analysis Services cubes and Reporting Services,<br>' +
        'then the relational security model is simple. If you’ll allow<br>' +
        'ad hoc access into the relational data warehouse, especially<br>' +
        'if you have requirements for filtering data (also known as<br>' +
        'row-level security), the relational security model grows<br>' +
        'increasingly complex.<br>' +
        'No matter how users access the relational data warehouse,<br>' +
        'begin by thinking about the roles necessary for<br>' +
        'administering the relational database. After securing the<br>' +
        'operations, we’ll discuss issues around users’ security.<br>' +
        'Administrative Roles for the Relational Database<br>' +
        'The SQL Server database engine has predefined server and<br>' +
        'database roles. These roles work the same for a DW/BI<br>' +
        'system as for any other SQL Server database application.<br>' +
        'Compared to the administrative roles for Analysis<br>' +
        'Services, the database engine has fine-grained permissions.<br>' +
        'There are a great many server and database roles for the<br>' +
        'SQL Server relational database. There’s nothing<br>' +
        'particularly unusual about a data warehouse. You should<br>' +
        'find it easy to create server and database roles for your<br>' +
        'team members that provide exactly the permissions you<br>' +
        'want.<br>' +
        '922<br>' +
        'It’s always good practice to grant people, even people on<br>' +
        'the DW/BI team, as few privileges as they need to get their<br>' +
        'jobs done. Occasionally people are malicious, but more<br>' +
        'often they’re just careless.<br>' +
        'The processing account that runs the Integration Services<br>' +
        'packages needs to have high privileges. You should always<br>' +
        'run production operations, like the ETL system, under a<br>' +
        'service account that’s not associated with a person’s login.<br>' +
        'It’s common for the DW/BI team members to all have high<br>' +
        'privileges on the development system. The test system<br>' +
        'should be set up the same way as production. One of the<br>' +
        'things you need to test before moving into production is<br>' +
        'whether the permissions are set correctly. This is a very<br>' +
        'common task to overlook.<br>' +
        'SQL Server uses the ANSI-standard concept of a schema.<br>' +
        'Object names are fully qualified as<br>' +
        'server.database.schema.object. We usually create the data<br>' +
        'warehouse tables under a single schema, but we’ve seen<br>' +
        'people use schemas to segregate dimension, fact, utility,<br>' +
        'and metadata tables.<br>' +
        'NOTE<br>' +
        'For maximum database security, consider<br>' +
        'using the Enterprise Edition transparent<br>' +
        'database encryption feature. Transparent<br>' +
        'database encryption, or TDE, encrypts the<br>' +
        '923<br>' +
        'entire database, much as if you encrypted<br>' +
        'the file system or drives. TDE protects the<br>' +
        'entire database, and as its name implies it’s<br>' +
        'transparent to users and applications. It<br>' +
        'places a modest demand on the database<br>' +
        '(around 5 percent for most data warehouse<br>' +
        'databases). TDE is most appropriate to<br>' +
        'meet regulatory or compliance<br>' +
        'requirements, in tandem with the<br>' +
        'user-oriented security discussed in the<br>' +
        'upcoming section.<br>' +
        'User Roles for the Relational Database<br>' +
        'There are two main kinds of accounts that issue relational<br>' +
        'queries in a DW/BI system: reporting accounts used by<br>' +
        'Reporting Services, and business user accounts. You’re<br>' +
        'likely to have one or several reporting accounts.<br>' +
        'Depending on your business requirements, you may have<br>' +
        'business user accounts for individual users that can log in<br>' +
        'to the relational database.<br>' +
        'Reporting Account Permissions<br>' +
        'Depending on how users access the DW/BI system, you<br>' +
        'may have only a few user roles to worry about. If you’re<br>' +
        'using Reporting Services, you should expect to create and<br>' +
        'manage a reporting query account. Like the ETL<br>' +
        'processing account, the reporting query account should not<br>' +
        'be associated with a person. If you’re using Analysis<br>' +
        'Services, you should create a separate Analysis Services<br>' +
        '924<br>' +
        'processing account. This account may have the same<br>' +
        'privileges as the reporting account, but it’s foolish to<br>' +
        'assume they’ll always be the same.<br>' +
        'We have already said several times that all users should<br>' +
        'access the relational data warehouse through views, rather<br>' +
        'than directly querying tables. There are several reasons for<br>' +
        'this:<br>' +
        '• Views let you insulate the user experience from the physical<br>' +
        'database. It’s amazing how much the view layer lets you<br>' +
        'restructure the database with minimal disruption to the user.<br>' +
        '• Views let you hide or rename columns to suit users. You can<br>' +
        'remove the prefixes (like Dim and Fact, which we use in<br>' +
        'MDWT_AdventureWorksDW) from the view names.<br>' +
        '• Views let you define role playing dimensions with<br>' +
        'meaningful column names, such as Ship_Year and<br>' +
        'Order_Year.<br>' +
        '• Views let you add row-level security seamlessly.<br>' +
        'Create a view on every user-accessible table, even if the<br>' +
        'view simply selects all columns from the table. Grant user<br>' +
        'access to the views, not to the tables. This includes the<br>' +
        'reporting account and the Analysis Services processing<br>' +
        'account. Both of these service accounts should use views<br>' +
        'rather than the underlying tables.<br>' +
        'NOTE<br>' +
        'Create all user-oriented views and stored<br>' +
        'procedures under a single schema, and<br>' +
        'have that schema own no other objects.<br>' +
        '925<br>' +
        'This makes it easier for you to find these<br>' +
        'objects when it comes time to assign<br>' +
        'permissions.<br>' +
        'The reporting account should have read access to the<br>' +
        'appropriate views, possibly all the views. As we discussed<br>' +
        'previously in this chapter, Reporting Services secures the<br>' +
        'reports, so the reporting account will have greater read<br>' +
        'privileges than any individual user would. Some reports<br>' +
        'will use a stored procedure to deliver up the rowset. If so,<br>' +
        'the reporting account will need execution privileges on<br>' +
        'those stored procedures. The reporting account should<br>' +
        'never have write privileges on any table or view.<br>' +
        'We can’t think of a technical way for the encrypted<br>' +
        'reporting account credentials to be stolen. But the most<br>' +
        'likely way for those credentials to get out is the obvious<br>' +
        'one: Someone — presumably on the DW team — tells<br>' +
        'someone else. There’s no point in tempting fate. This is<br>' +
        'especially true for any custom reporting front-end or<br>' +
        'non-Microsoft query software that might use a reporting<br>' +
        'account. Microsoft is very careful about security these<br>' +
        'days — don’t laugh, they really are. You should<br>' +
        'thoroughly investigate the security mechanisms of any<br>' +
        'software on your system.<br>' +
        'The Analysis Services processing account should have<br>' +
        'read access to the appropriate views, possibly all the<br>' +
        'views. The Analysis Services users should not need any<br>' +
        'direct privileges in the relational database.<br>' +
        '926<br>' +
        'Business User Roles<br>' +
        'Business users will need login privileges into the relational<br>' +
        'data warehouse database if they need to perform ad hoc<br>' +
        'analyses. In this scenario, you’ll need to grant them only<br>' +
        'the appropriate permissions. This means read-only<br>' +
        'permission on the appropriate views, columns, and stored<br>' +
        'procedures. You can use the Management Studio user<br>' +
        'interface to administer security, or write SQL scripts.<br>' +
        'NOTE<br>' +
        'What if you want to hide a few columns,<br>' +
        'like Social Security number, from some<br>' +
        'users but not others? One approach is to<br>' +
        'define two views on the table, one with all<br>' +
        'public information and the other with all<br>' +
        'public and private information.<br>' +
        'The best approach is to encrypt the<br>' +
        'columns containing private information and<br>' +
        'distribute decryption keys only to the<br>' +
        'appropriate users.<br>' +
        'Create new roles that grant permissions to objects for the<br>' +
        'subset of people who can access that object. For example,<br>' +
        'say you protected the employee dimension from BIPublic,<br>' +
        'but you want everyone in human resources to have access<br>' +
        'to it. Create a new role for HR, granting access to the<br>' +
        'appropriate views. Roles are additive, so if you have a user<br>' +
        '927<br>' +
        'who’s a member of both BIPublic and HR, that user can<br>' +
        'see the employee view.<br>' +
        'Row-Level or Filtering Security<br>' +
        'You may need to build row-level or filtering security into<br>' +
        'your relational data warehouse. What we mean by<br>' +
        'row-level security is that users’ views of sensitive data<br>' +
        'differ not at the column level, as we discussed in the<br>' +
        'preceding text, but by rows. One person should see a<br>' +
        'different set of rows than another person.<br>' +
        'There’s no row-level security feature of SQL Server, but<br>' +
        'the classic solution is not difficult to implement. First,<br>' +
        'create a table that lists users’ IDs and the identifiers for the<br>' +
        'rows they’re allowed to see. For example, secure the<br>' +
        'FactOrders table from MDWT_AdventureWorksDW to<br>' +
        'limit a user’s view of the data only to a specific set of<br>' +
        'salespeople (identified by SalesRepKey).<br>' +
        'Listing 14-1 shows some DDL for this permissions table,<br>' +
        'beginning with the code to create the table:<br>' +
        'Listing 14-1: Data definition for the UserPermissions_SalesRep table<br>' +
        'IF OBJECT_ID (\'[dbo].[UserPermissions_SalesRep]\', \'U\') IS NOT NULL<br>' +
        'DROP TABLE [dbo].[UserPermissions_SalesRep];<br>' +
        'CREATE TABLE [dbo].[UserPermissions_SalesRep](<br>' +
        'UserPermissions_SalesRepKey int IDENTITY NOT NULL,<br>' +
        '[UserName] sysname NOT NULL,<br>' +
        '928<br>' +
        '[EmployeeKey] int NOT NULL<br>' +
        'CONSTRAINT [PK_UserPermisssions_SalesRep]<br>' +
        'PRIMARY KEY CLUSTERED ([UserPermissions_SalesRepKey] ASC)<br>' +
        ');<br>' +
        'Next, insert some rows into this table. Let’s say that the<br>' +
        'user Joy is allowed to see only information for employees<br>' +
        '272 and 281. Listing 14-2 inserts two rows that identify the<br>' +
        'sales reps that Joy can see:<br>' +
        'Listing 14-2: Insert rows that define which employees Joy can see<br>' +
        'INSERT UserPermissions_SalesRep VALUES (\'KimballGroup\\Joy\', 272);<br>' +
        'INSERT UserPermissions_SalesRep VALUES (\'KimballGroup\\Joy\', 281);<br>' +
        'In the case of Listing 14-2, you’re using Windows<br>' +
        'Integrated Security, and Joy is a user on the KimballGroup<br>' +
        'domain.<br>' +
        'The final step is to create a view definition that joins the<br>' +
        'fact table to this new permissions table, as illustrated in<br>' +
        'Listing 14-3:<br>' +
        'Listing 14-3: Define a view to provide row-level security<br>' +
        'IF OBJECT_ID (\'OrdersSecure\', \'V\') IS NOT NULL<br>' +
        'DROP VIEW OrdersSecure;<br>' +
        'GO<br>' +
        'CREATE VIEW OrdersSecure<br>' +
        'AS<br>' +
        'SELECT f.* FROM FactOrders f<br>' +
        '929<br>' +
        'INNER JOIN UserPermissions_SalesRep u<br>' +
        'ON (f.SalesRepKey = u.EmployeeKey)<br>' +
        'WHERE u.UserName=SYSTEM_USER;<br>' +
        'GO<br>' +
        'As you can see by looking at the view definition, the trick<br>' +
        'is to limit the view to only those rows where the user name<br>' +
        'in the permissions table is the same user name as the<br>' +
        'person issuing the query. If you maintain the permissions<br>' +
        'table, many people can access this OrdersSecure table<br>' +
        'and see only the appropriate set of rows.<br>' +
        'This solution is not as satisfactory as the dimension and<br>' +
        'cell security that you can define in Analysis Services.<br>' +
        'You’ve safely protected the detailed data, but no one can<br>' +
        'use this view to see company-wide sales. You need to<br>' +
        'create a second view, simply called Orders, that drops the<br>' +
        'SalesRepKey. The BIPublic role can query the Orders<br>' +
        'view and use that for most queries about sales volumes.<br>' +
        'The dual views create a usability problem for ad hoc<br>' +
        'business users, but it’s the best solution the relational<br>' +
        'database offers.<br>' +
        'Note that the row-level view is useful not just for ad hoc<br>' +
        'querying. You can also set up Reporting Services filtering<br>' +
        'reports that use the view. When you define that report, you<br>' +
        'need to pass credentials from the user into the database.<br>' +
        'If you need to support filtering reports and row-level<br>' +
        'security, you should consider building an applet to help<br>' +
        'maintain the UserPermissions tables. For performance<br>' +
        'reasons, you want the UserPermissions tables to use the<br>' +
        '930<br>' +
        'warehouse surrogate keys. But your requirements are<br>' +
        'probably phrased along the lines of granting permission to<br>' +
        'all the employees in a department or sales reps in a region.<br>' +
        'A simple applet can help the security administrator be<br>' +
        'more effective than writing INSERT statements, and can<br>' +
        'greatly reduce errors.<br>' +
        'Testing Relational Security<br>' +
        'Even though relational security isn’t as complicated as<br>' +
        'Analysis Services security, it’s still plenty complicated.<br>' +
        'You must thoroughly test all the roles and the<br>' +
        'combinations of roles.<br>' +
        'You can impersonate a user and then run a query. Compare<br>' +
        'the results of the queries before and after impersonation to<br>' +
        'evaluate whether the security definitions are correct.<br>' +
        'Listing 14-4 shows you how. The script assumes you’ve<br>' +
        'created the OrdersSecure view described previously, and<br>' +
        'also that you created a role BIPublic to which you granted<br>' +
        'access to OrdersSecure. We further assume that when you<br>' +
        'run this script, you have sufficiently high privileges that<br>' +
        'you can create logins and users. Sysadmin would work just<br>' +
        'fine.<br>' +
        'Listing 14-4: Create a temporary user to test security roles<br>' +
        '--Create a temporary login and user<br>' +
        'CREATE LOGIN LoginBIPublic WITH PASSWORD = \'J345#$)thb\';<br>' +
        'GO<br>' +
        'CREATE USER UserBIPublic FOR LOGIN LoginBIPublic;<br>' +
        'GO<br>' +
        '931<br>' +
        '--Display current execution context. This should come back as you.<br>' +
        'SELECT SUSER_NAME(), USER_NAME();<br>' +
        '--As a simple, not very accurate test, how many rows can you see?<br>' +
        'SELECT COUNT(*) FROM OrdersSecure<br>' +
        '--Set the execution context to LoginBIPublic.<br>' +
        'EXECUTE AS USER = \'UserBIPublic\';<br>' +
        '--Verify the execution context is now \'UserBIPublic\'.<br>' +
        'SELECT SUSER_NAME(), USER_NAME();<br>' +
        '--Select from the view. You should get a permissions error<br>' +
        'SELECT COUNT(*) FROM OrdersSecure<br>' +
        '--Revert back to yourself and add the user to the BIPublic role<br>' +
        'REVERT;<br>' +
        'EXEC<br>' +
        'sp_addrolemember @rolename=\'BIPublic\', @membername=\'UserBIPublic\'<br>' +
        'GO<br>' +
        '--Now select from the view as UserBIPublic. We expect to see zero rows<br>' +
        '--because UserBIPublic is not in the UserPermissions table.<br>' +
        'EXECUTE AS USER=\'UserBIPublic\';<br>' +
        'SELECT COUNT(*) FROM OrdersSecure<br>' +
        '--Revert back to yourself<br>' +
        'REVERT;<br>' +
        '--Remove temporary login and user<br>' +
        '932<br>' +
        'DROP LOGIN LoginBIPublic;<br>' +
        'DROP USER UserBIPublic;<br>' +
        'GO<br>' +
        'With our security hats on, we recommend that you always<br>' +
        'run impersonation tests from a script, and bracket the<br>' +
        'script with CREATE and DROP login and user, as we<br>' +
        'illustrated here.<br>' +
        'Integration Services Security<br>' +
        'Integration Services is a back-room operation, so its<br>' +
        'security story is simple. First, make sure the packages are<br>' +
        'secure, so no one can mess with package contents. You<br>' +
        'don’t want anyone to replace the package that performs an<br>' +
        'incremental load of the data warehouse with one that<br>' +
        'deletes all the data. This is a pretty far-fetched scenario.<br>' +
        'More likely someone on the team is careless and does<br>' +
        'something wrong.<br>' +
        'Packages can be stored in the file system as XML, or in<br>' +
        'SQL Server. You should secure the package location.<br>' +
        'Packages stored in SQL Server are stored in the msdb<br>' +
        'database, in the table called sysssispackages. Simply<br>' +
        'use the database engine’s security to grant limited<br>' +
        'permissions to msdb, and the package contents are<br>' +
        'automatically secured. If you store the package on the file<br>' +
        'system, use Windows security to limit access.<br>' +
        'In addition to this basic security, you can sign or encrypt<br>' +
        'packages. Digitally sign the package and set the package’s<br>' +
        'CheckSignatureOnLoad property to True to prevent<br>' +
        '933<br>' +
        'anyone from modifying a package’s contents. More<br>' +
        'accurately, what you’re doing is telling the package to<br>' +
        'check for a signature before it runs. If the package has<br>' +
        'been modified unintentionally, it wouldn’t have been<br>' +
        'signed. If someone maliciously modified the package, they<br>' +
        'should not be able to sign it.<br>' +
        'Packages contain sensitive information within them,<br>' +
        'including the connection information to an account that,<br>' +
        'usually, has very high privileges in the data warehouse<br>' +
        'database. Integration Services automatically encrypts all<br>' +
        'connection information for you, but you can go farther and<br>' +
        'encrypt more of the package contents. There are several<br>' +
        'encryption options, but they come down to requiring a<br>' +
        'password before anyone can view the package.<br>' +
        'The “Relational DW Security” section earlier in this<br>' +
        'chapter provides guidance on the kinds of relational<br>' +
        'database permissions you’ll need to provide for the<br>' +
        'connections from your Integration Services packages to the<br>' +
        'data warehouse database.<br>' +
        'Usage Monitoring<br>' +
        'A secure DW/BI system will have usage monitoring in<br>' +
        'place. In an increasingly regulated world, it’s extremely<br>' +
        'valuable to know who is connected to the system and what<br>' +
        'they’re doing.<br>' +
        'In Chapter 17 we talk about how to set up usage<br>' +
        'monitoring on Analysis Services, Reporting Services, and<br>' +
        'the relational engine. For some organizations it’s sufficient<br>' +
        'simply to collect logons: who’s accessing the database and<br>' +
        '934<br>' +
        'when? Other organizations need to know exactly who is<br>' +
        'accessing which information.<br>' +
        'Reporting Services collects usage information by default.<br>' +
        'In addition, Reporting Services provides an option to copy<br>' +
        'the usage logs from the Reporting Services catalog into a<br>' +
        'separate database. This option provides a target relational<br>' +
        'database, Integration Services packages to move the data,<br>' +
        'and a starter set of reports. This database should suffice for<br>' +
        'the majority of usage reporting requirements from<br>' +
        'Reporting Services.<br>' +
        'Usage monitoring provides other valuable benefits. It’s a<br>' +
        'valuable tool for performance tuning. Spending a bit of<br>' +
        'time analyzing how business users are accessing data is<br>' +
        'very valuable for understanding how to improve your DW/<br>' +
        'BI system.<br>' +
        'If you set up a usage monitoring system — as we strongly<br>' +
        'recommend — you should inform your business users of<br>' +
        'what you’re doing, why, and how the information will be<br>' +
        'used.<br>' +
        'Summary<br>' +
        'The goal of this chapter is to highlight the most important<br>' +
        'security issues for a DW/BI system and to help you figure<br>' +
        'out where and when to secure data. We cannot possibly<br>' +
        'cover all the security features of SQL Server or all the<br>' +
        'details of how to implement those features. We hope we<br>' +
        'have provided you with the tools to develop a security plan<br>' +
        'and security test plan.<br>' +
        '935<br>' +
        'One of the most important steps you can take for a secure<br>' +
        'DW/BI system is to identify which DW/BI team member<br>' +
        'is in charge of security. That security manager drives the<br>' +
        'development and implementation of the security plan.<br>' +
        'The easiest pieces of the security plan have to do with<br>' +
        'physical security and operating system security. There’s<br>' +
        'lots of information available about how to secure servers<br>' +
        'and the Windows operating system. You just have to<br>' +
        'commit to doing it.<br>' +
        'The harder question, and the one to which we devoted<br>' +
        'most of this chapter, has to do with securing the data. As<br>' +
        'we described, securing predefined reports in Reporting<br>' +
        'Services is easy, painless, and effective. Securing data for<br>' +
        'ad hoc analysis is a harder problem. You’ll definitely find<br>' +
        'it easier, and the user experience much better, to define<br>' +
        'security rules in Analysis Services than in the relational<br>' +
        'database. Analysis Services’ security features make this a<br>' +
        'fairly straightforward task for a wide range of<br>' +
        'requirements. And because security rules are defined in<br>' +
        'MDX, even very complicated scenarios are feasible.<br>' +
        'Finally, we struggled with how to set up the relational<br>' +
        'database security to support direct ad hoc access. It’s<br>' +
        'possible — people have been doing it for years — but it’s<br>' +
        'hardly as easy or satisfactory as we’d like.<br>' +
        'The SQL Server documentation in Books Online has a<br>' +
        'strong emphasis on security features. But the security<br>' +
        'documentation is scattered across the different<br>' +
        'components, and it isn’t always easy to find the<br>' +
        'information you need. You might be able to cut corners in<br>' +
        '936<br>' +
        'some aspects of system development, especially if your<br>' +
        'data volumes are small, but everyone needs to be careful<br>' +
        'about security.<br>' +
        '937<br>';
    document.getElementById('chapter13').innerHTML = 'Chapter 13<br>' +
        'Incorporating Data Mining<br>' +
        '“We dig up diamonds by the scoreA thousand rubies,<br>' +
        'sometimes more.”<br>' +
        '— From Snow White by Walt Disney Company, music by<br>' +
        'Frank Churchill, words by Larry Morey, ©1938<br>' +
        'Data mining is not a single topic; it’s a loosely related<br>' +
        'collection of tools, algorithms, techniques, and processes.<br>' +
        'This makes it a difficult subject area to tackle, especially in<br>' +
        'a single chapter. However, we must tackle it for two main<br>' +
        'reasons: First, data mining offers the potential of huge<br>' +
        'business impact; and second, SQL Server includes a suite<br>' +
        'of data mining tools as part of the product. In short, high<br>' +
        'value, low cost — the motivation is obvious.<br>' +
        'The first part of this chapter sets the context for data<br>' +
        'mining. We begin with a brief definition of data mining<br>' +
        'and an overview of the business motivation for using it.<br>' +
        'We then look at the Microsoft data mining architecture and<br>' +
        'environment provided as part of SQL Server, including a<br>' +
        'brief description of the data mining service, the algorithms<br>' +
        'provided, and the kinds of problems for which they might<br>' +
        'be appropriate. We next present a high-level data mining<br>' +
        'process. The process breaks into three phases: business,<br>' +
        'mining, and operations. The business phase involves<br>' +
        'identifying business opportunities and understanding the<br>' +
        'data resources. The data mining phase is a highly iterative<br>' +
        'and exploratory process whose goal is to identify the best<br>' +
        '778<br>' +
        'model possible, given the time and resource constraints.<br>' +
        'Once you identify the best model, you need to implement<br>' +
        'it in a production form where, you hope, it will provide the<br>' +
        'intended business value.<br>' +
        'The second part of the chapter puts these concepts and<br>' +
        'processes into practice by demonstrating the application of<br>' +
        'SQL Server data mining in two examples. The first<br>' +
        'example creates clusters of cities based on economic data,<br>' +
        'and the second creates a model to recommend products for<br>' +
        'the Adventure Works Cycles website.<br>' +
        'By the end of this chapter, you should have a good<br>' +
        'understanding of the following:<br>' +
        '• What data mining is and how it can be applied to a range of<br>' +
        'business opportunities<br>' +
        '• The major components of the SQL Server data mining<br>' +
        'toolset and how they work together<br>' +
        '• A high-level process for employing data mining in your<br>' +
        'organization<br>' +
        '• And, although this is not a tutorial, you should also end up<br>' +
        'with a basic idea of how to use the SQL Server data mining<br>' +
        'toolset<br>' +
        'Defining Data Mining<br>' +
        'We generally describe data mining as a process of data<br>' +
        'exploration with the intent to find patterns or relationships<br>' +
        'that can be made useful to the organization. Data mining<br>' +
        'takes advantage of a range of technologies and techniques<br>' +
        'for exploration and execution. From a business<br>' +
        'perspective, data mining helps you understand and predict<br>' +
        'behavior, identify relationships, or group items (customers,<br>' +
        '779<br>' +
        'products, and so on) into coherent sets. These models can<br>' +
        'take the form of rules or equations that you apply to new<br>' +
        'customers, products, or transactions to make a better guess<br>' +
        'as to how you should respond to them.<br>' +
        'The field of data mining is known more broadly as<br>' +
        'Knowledge Discovery and Data Mining (KDD). Both<br>' +
        'terms shed light on the purpose and process of data<br>' +
        'mining. The word “mining” is meant to evoke a specific<br>' +
        'image. Traditional mining involves digging through vast<br>' +
        'quantities of dirt to unearth a relatively small vein of<br>' +
        'valuable metallic ore, precious stones, or other substances.<br>' +
        'Data mining is the digital equivalent of this analog<br>' +
        'process. You use automated tools to dig through vast<br>' +
        'quantities of data to identify or “discover” valuable<br>' +
        'patterns or relationships that you can leverage in your<br>' +
        'business.<br>' +
        'Our brains are good examples of data mining tools.<br>' +
        'Throughout the course of our lives, we accumulate a large<br>' +
        'set of experiences. In some cases, we’re able to identify<br>' +
        'patterns within these experiences and generate models we<br>' +
        'can use to predict the future. Those who commute to work<br>' +
        'have an easy example. Over the weeks and months, you<br>' +
        'develop a sense for the traffic patterns and adjust your<br>' +
        'behavior accordingly. The freeway will be jammed at 5:00<br>' +
        'p.m., so you might leave at 4:30, or wait until 6:00, unless<br>' +
        'it’s Friday or a holiday. Going to the movies is another<br>' +
        'example of altering behavior based on experience.<br>' +
        'Deciding when to arrive at the theater is a complex<br>' +
        'equation that includes variables like when the movie<br>' +
        'opened, whether it’s a big budget film, whether it got good<br>' +
        'reviews, and what’s showing that you want to see. These<br>' +
        '780<br>' +
        'are personal examples of building a data mining model<br>' +
        'using the original neural network tool.<br>' +
        'The roots of data mining can be traced back to a<br>' +
        'combination of statistical analysis tools like SAS<br>' +
        '(Statistical Analysis System) and SPSS (Statistical<br>' +
        'Package for the Social Sciences) that took form in the<br>' +
        'academic environment in the 1960s and 1970s, and the<br>' +
        'Artificial Intelligence surge back in the 1980s. Many of the<br>' +
        'techniques from these areas were combined, enhanced, and<br>' +
        'repackaged as data mining in the 1990s. One benefit of the<br>' +
        'internet bubble of the late 1990s is that it showed how data<br>' +
        'mining could be useful. Companies like Amazon began to<br>' +
        'mine the vast quantities of data generated by millions of<br>' +
        'customers browsing their websites and making purchase<br>' +
        'selections, popularizing the phrase “Customers who<br>' +
        'bought this item also bought these items.”<br>' +
        'Data mining has finally grown up and has taken on a<br>' +
        'central role in many businesses. All of us are the subject of<br>' +
        'data mining dozens of times every day — from the junk<br>' +
        'mail in our mail boxes, to the affinity cards we use in the<br>' +
        'grocery store, to the fraud detection algorithms that<br>' +
        'scrutinize our every credit card purchase. Data mining has<br>' +
        'become so widespread for one reason: it works. Using data<br>' +
        'mining techniques can measurably and significantly<br>' +
        'increase an organization’s ability to reach its goals. Often<br>' +
        'those goals can be boiled down to “sell more stuff.” Our<br>' +
        'goal here is to describe the technology, not judge the<br>' +
        'application; how you use it is up to you.<br>' +
        'There are two common approaches to data mining. The<br>' +
        'first is usually a one-time project to help you gain an<br>' +
        '781<br>' +
        'understanding of who your customers are and how they<br>' +
        'behave. We call this exploratory or undirected data<br>' +
        'mining, where the goal is to find something interesting.<br>' +
        'The second is most often a project created to work on a<br>' +
        'specific problem or opportunity. We call this more focused<br>' +
        'activity directed data mining. Directed data mining<br>' +
        'typically leads to an ongoing effort where models are<br>' +
        'generated on a regular basis and are applied as part of the<br>' +
        'transaction system or in the ETL application. For example,<br>' +
        'you might create a model that generates a score for each<br>' +
        'customer every time you load customer data into the BI<br>' +
        'system. Models that come from the data mining process<br>' +
        'are often applied in the transaction process itself to identify<br>' +
        'opportunities or predict problems as they are happening<br>' +
        'and guide the transaction system to an appropriate<br>' +
        'response on a real-time basis.<br>' +
        'While exploratory data mining will often reveal useful<br>' +
        'patterns and relationships, this approach usually takes on<br>' +
        'the characteristics of a fishing expedition. You cast about,<br>' +
        'hoping to hook the big one; meanwhile, your guests, the<br>' +
        'business folks, lose interest. Directed data mining with a<br>' +
        'clear business purpose in mind is more appealing to their<br>' +
        'business-driven style.<br>' +
        'In Chapter 10, we defined an analytic application as a BI<br>' +
        'application that’s centered on a specific business process<br>' +
        'and encapsulates a certain amount of domain expertise. A<br>' +
        'data mining application fits this definition perfectly, and its<br>' +
        'place in the Kimball Lifecycle is clearly in the Application<br>' +
        'track boxes for BI Application design and development.<br>' +
        'Basic Data Mining Terminology<br>' +
        '782<br>' +
        'Data miners use a lot of terms that sound familiar to the<br>' +
        'general public, but have specific meaning in data mining.<br>' +
        'It’s helpful for us to define a few of these terms early on.<br>' +
        'This is not an exhaustive list of data mining terms, only the<br>' +
        'relevant ones for our discussion.<br>' +
        '• Algorithm: The programmatic technique used to identify the<br>' +
        'relationships or patterns in the data.<br>' +
        '• Model: The definition of the relationship identified by the<br>' +
        'algorithm, which generally takes the form of a set of rules, a<br>' +
        'decision tree, a set of equations, or a set of associations.<br>' +
        '• Case: The collection of attributes and relationships<br>' +
        '(variables) that are associated with an individual instance of<br>' +
        'the entity being modeled, usually a customer. The case is<br>' +
        'also known as an observation.<br>' +
        '• Case set: A group of cases that share the same attributes.<br>' +
        'Think of a case set as a table with one row per unique object<br>' +
        '(like customer). It’s possible to have a nested case set when<br>' +
        'one row in the parent table, like “customer,” joins to<br>' +
        'multiple rows in the nested table, like “purchases.” The case<br>' +
        'set is also known as an observation set.<br>' +
        '• Dependent variable(s) (or predicted attribute or predict<br>' +
        'column): The variable the algorithm will build a model to<br>' +
        'predict or classify.<br>' +
        '• Independent variable(s) (or predictive attribute or input<br>' +
        'column): The variables which provide the descriptive or<br>' +
        'behavior information used to build the model. The algorithm<br>' +
        'creates a model that uses combinations of independent<br>' +
        'variables to define a grouping or predict the dependent<br>' +
        'variable.<br>' +
        '• Discrete or continuous variables: Numeric columns that<br>' +
        'contain continuous or discrete values. A column in the<br>' +
        'Employee table called Salary that contains the actual<br>' +
        'salary values is a continuous variable. You can add a column<br>' +
        'to the table during data preparation called SalaryRange,<br>' +
        'containing integers to represent encoded salary ranges (1 =<br>' +
        '“0 to $25,000”; 2 = “between $25,000 and $50,000”; and so<br>' +
        '783<br>' +
        'on). This is a discrete numeric column. Early data mining<br>' +
        'and statistical analysis tools required the conversion of<br>' +
        'strings to numeric values like the encoded salary ranges.<br>' +
        'Most tools, including most of the SQL Server data mining<br>' +
        'algorithms, allow the use of character descriptions as<br>' +
        'discrete values. The string “0 to $25,000” is easier to<br>' +
        'understand than the number 1. Discrete variables are also<br>' +
        'known as categorical. This distinction between discrete and<br>' +
        'continuous is important to the underlying algorithms in data<br>' +
        'mining, although its significance is less obvious to those of<br>' +
        'us who are not statisticians.<br>' +
        '• Regression: A statistical technique that creates a best-fit<br>' +
        'formula based on a data set. The formula can be used to<br>' +
        'predict values based on new input variables. In linear<br>' +
        'regression, the formula is the equation for a line.<br>' +
        '• Deviation: A measure of how well the regression formula<br>' +
        'fits the actual values in the data set from which it was<br>' +
        'created.<br>' +
        '• Mining structure: A Microsoft data mining term used as a<br>' +
        'name for the definition of a case set in Analysis Services.<br>' +
        'The mining structure is essentially a metadata layer on top of<br>' +
        'a Data Source View that includes additional data<br>' +
        'mining–related flags and column properties, such as the field<br>' +
        'that identifies a column as input, predict, both, or ignore. A<br>' +
        'mining structure can be used as the basis for multiple mining<br>' +
        'models.<br>' +
        '• Mining model: The specific application of an algorithm to a<br>' +
        'particular mining structure. You can build several mining<br>' +
        'models with different parameters or different algorithms<br>' +
        'from the same mining structure.<br>' +
        'Business Uses of Data Mining<br>' +
        'Data mining terminology has not yet become completely<br>' +
        'standardized. There are terms that describe the business<br>' +
        'task and terms that describe the data mining techniques<br>' +
        'applied to those tasks. The problem is, the same terms are<br>' +
        '784<br>' +
        'used to describe both tasks and techniques, sometimes with<br>' +
        'different meanings.<br>' +
        'The terms in this section are drawn from the book Data<br>' +
        'Mining Techniques: For Marketing, Sales, and Customer<br>' +
        'Relationship Management by Michael J. A. Berry and<br>' +
        'Gordon S. Linoff, Second Edition (Wiley, 2004). Berry<br>' +
        'and Linoff list six basic business tasks that are served by<br>' +
        'data mining techniques: classification, estimation,<br>' +
        'prediction, affinity grouping, clustering, and description<br>' +
        'and profiling. We’ve added a seventh business task to the<br>' +
        'list called anomaly detection. We describe each of these<br>' +
        'business task areas in the following sections, along with<br>' +
        'lists of the relevant algorithms included in SQL Server<br>' +
        'Data Mining. A word of warning: Some of these tasks<br>' +
        'overlap in what seems to be odd ways to the uninitiated<br>' +
        'because the distinctions between the areas are more<br>' +
        'mathematical than practical.<br>' +
        'Classification<br>' +
        'Classification is the task of assigning each item in a set to<br>' +
        'one of a predetermined set of discrete choices based on its<br>' +
        'attributes or behaviors. Consumer goods are classified in a<br>' +
        'standard hierarchy down to the SKU level. If you know the<br>' +
        'attributes of a product, you can determine its classification.<br>' +
        'You can use attributes like size, sugar content, flavor, and<br>' +
        'container type to classify a soda. Typical classes in<br>' +
        'business include Yes and No; High, Medium, and Low;<br>' +
        'Silver, Gold, and Platinum. What these are classes of<br>' +
        'depends on the business context; Good Credit Risk classes<br>' +
        'might be Yes and No. Classification helps organizations<br>' +
        'and people simplify their dealings with the world. If you<br>' +
        '785<br>' +
        'can classify something, you then know how to deal with it.<br>' +
        'If you fly often with the same airline, you have no doubt<br>' +
        'been classified as an elite level, or Platinum, customer.<br>' +
        'Knowing this classification allows the airline employees to<br>' +
        'work with you in a way that is appropriate for top<br>' +
        'customers, even if they have never met you before. The<br>' +
        'key differentiating factors of classification are the limited<br>' +
        '(discrete) number of entries in the class set and the fact<br>' +
        'that the class set is predefined.<br>' +
        'A common example of classification is the assignment of a<br>' +
        'socioeconomic class to customers or prospects in a<br>' +
        'marketing database. Companies like Nielsen Claritas, with<br>' +
        'its PRIZM system, have built an industry around<br>' +
        'classification. These systems identify classes of consumers<br>' +
        'who have common geographic, demographic, economic,<br>' +
        'and behavioral attributes and can be expected to respond to<br>' +
        'certain opportunities in a similar way.<br>' +
        'Classification algorithms predict the class or category of<br>' +
        'one or more discrete variables, based on the other variables<br>' +
        'in the case set. Determining whether someone is likely to<br>' +
        'respond to a direct mail piece involves putting them in the<br>' +
        'category of Likely Responder or not. Microsoft Decision<br>' +
        'Trees, Microsoft Neural Network, and Microsoft Naïve<br>' +
        'Bayes are the first choice algorithms for classification<br>' +
        'when the predict column is a discrete variable.<br>' +
        'Estimation (Regression)<br>' +
        'Estimation is the continuous version of classification. That<br>' +
        'is to say, where classification returns a discrete value,<br>' +
        'estimation returns a continuous number. In practice, most<br>' +
        '786<br>' +
        'classification is actually estimation. The process is<br>' +
        'essentially the same: A set of attributes is used to<br>' +
        'determine a relationship. A direct mail marketing company<br>' +
        'could estimate customers’ likelihood to respond to a<br>' +
        'promotion based on past responses. Estimating a<br>' +
        'continuous variable called Response_Likelihood that<br>' +
        'ranges from zero to one is more useful when creating a<br>' +
        'direct marketing campaign than a discrete classification of<br>' +
        'High, Medium, or Low. The continuous value allows the<br>' +
        'marketing manager to determine the size of the campaign<br>' +
        'by changing the cutoff point of the Response_Likelihood<br>' +
        'estimate. For example, a promotions manager with a<br>' +
        'budget for 200,000 pieces and a list of 12 million prospects<br>' +
        'would use the predicted Response_Likelihood variable to<br>' +
        'limit the target subset. Including only those prospects with<br>' +
        'a Response_Likelihood greater than some number, say<br>' +
        '0.80, would give the promotions manager a target list of<br>' +
        'the top 200,000 prospects. The continuous variable allows<br>' +
        'the user to more finely tune the application of the results.<br>' +
        'Estimation algorithms estimate a continuously valued<br>' +
        'variable based on the other variables in the case set.<br>' +
        'Microsoft has built several algorithms that can be used for<br>' +
        'either discrete or continuous variables. Microsoft Decision<br>' +
        'Trees and Microsoft Neural Network are good choices for<br>' +
        'estimating a continuous variable.<br>' +
        'Most of the estimation algorithms are based on regression<br>' +
        'analysis techniques. As a result, this category is often<br>' +
        'called regression, especially when the algorithm is used<br>' +
        'for prediction. Microsoft includes a separate linear<br>' +
        'regression algorithm in its list of algorithms, but it is a<br>' +
        'specially-parameterized version of the decision trees<br>' +
        '787<br>' +
        'algorithm. You will also see the Microsoft Logistic<br>' +
        'Regression algorithm on the list; it is based on the neural<br>' +
        'network algorithm.<br>' +
        'Prediction<br>' +
        'Where classification and estimation are assignment of<br>' +
        'values that are “correct” by definition, prediction is the<br>' +
        'application of the same techniques to assign a value that<br>' +
        'can be validated at some future date. For example, you<br>' +
        'might use a classification algorithm to classify your<br>' +
        'customers as male or female based on their purchasing<br>' +
        'behaviors. You can use this classification as an input to<br>' +
        'designing various marketing programs.<br>' +
        'TIP<br>' +
        'Be careful not to reveal your guess to your<br>' +
        'customers because it could adversely affect<br>' +
        'your relationship with them. For example,<br>' +
        'it would be unwise to use this variable by<br>' +
        'itself to send out promotional pieces for a<br>' +
        '“For Women Only” sale. However, the<br>' +
        'variable is useful for the business even<br>' +
        'though you will never know for certain<br>' +
        'which customers are actually male or<br>' +
        'female.<br>' +
        'Prediction, on the other hand, seeks to determine a class or<br>' +
        'estimate as accurately as possible before the value is<br>' +
        'known. This future-oriented element is what places<br>' +
        '788<br>' +
        'prediction in its own category. The input variables exist or<br>' +
        'occur before the predicted variable. For example, a lending<br>' +
        'company offering mortgages might want to predict the<br>' +
        'market value of a piece of property before it’s sold. This<br>' +
        'value would give them an upper limit for the amount<br>' +
        'they’d be willing to lend the property owner, regardless of<br>' +
        'the actual amount the owner has offered to pay for the<br>' +
        'given property. In order to build a predictive data mining<br>' +
        'model, the company needs a training set that includes<br>' +
        'predictive attributes that are known prior to the sale, such<br>' +
        'as total square footage, number of bathrooms, city, school<br>' +
        'district, and the actual sale price of each property in the<br>' +
        'training set. The data mining algorithm uses this training<br>' +
        'set to build a model based on the relationships between the<br>' +
        'predictive variables and the known historical sale price.<br>' +
        'The model can then be used to predict the sale price of a<br>' +
        'new property based on the known input variables about<br>' +
        'that property.<br>' +
        'One interesting feature of predictive models is that their<br>' +
        'accuracy can be tested. At some point in the future, the<br>' +
        'actual sale amount of the property will become known and<br>' +
        'can be compared to the predicted value. In fact, the data<br>' +
        'mining process described later in this chapter recommends<br>' +
        'splitting the historical data into two sets: one to build or<br>' +
        'train the model and one to test its accuracy against known<br>' +
        'historical data that was not part of the training process.<br>' +
        'TIP<br>' +
        '789<br>' +
        'The real estate sale price predictor is a<br>' +
        'good example of how data mining models<br>' +
        'tend to go stale over time. Real estate<br>' +
        'prices in a given area can be subject to<br>' +
        'significant, rapid fluctuations. The<br>' +
        'mortgage company would want to re-build<br>' +
        'the data mining model with recent sales<br>' +
        'transactions on a regular basis.<br>' +
        'Microsoft Decision Trees and Microsoft Neural Network<br>' +
        'are the first choice algorithms for regression when the<br>' +
        'predict column is a continuous variable. When prediction<br>' +
        'involves time series data, it is often called forecasting.<br>' +
        'Microsoft Time Series is the first choice algorithm for<br>' +
        'predicting time series data, like monthly sales forecasts.<br>' +
        'Association or Affinity Grouping<br>' +
        'Association looks for correlations among the items in a<br>' +
        'group of sets. E-commerce systems are big users of<br>' +
        'association models in an effort to increase sales. This can<br>' +
        'take the form of an association modeling process known as<br>' +
        'market basket analysis. The online retailer first builds a<br>' +
        'model based on the contents of recent shopping carts and<br>' +
        'makes it available to the web server. As the shopper adds<br>' +
        'products to the cart, the system feeds the contents of the<br>' +
        'cart into the model. The model identifies items that<br>' +
        'commonly appear with the items currently in the cart.<br>' +
        'Most recommendation systems are based on association<br>' +
        'algorithms.<br>' +
        '790<br>' +
        'Microsoft Association is an association, or affinity<br>' +
        'grouping algorithm. Other algorithms, like Microsoft<br>' +
        'Decision Trees, can also be used to create association<br>' +
        'rules.<br>' +
        'Clustering (Segmentation)<br>' +
        'Clustering can be thought of as auto-classification.<br>' +
        'Clustering algorithms group cases into clusters that are as<br>' +
        'similar to one another, and as different from other clusters,<br>' +
        'as possible. The clusters are not predetermined, and it’s up<br>' +
        'to the data miner to examine the clusters to understand<br>' +
        'what makes them unique. When applied to customers, this<br>' +
        'process is also known as customer segmentation. The idea<br>' +
        'is to segment the customers into smaller, homogenous<br>' +
        'groups that can be targeted with customized promotions<br>' +
        'and even customized products. Naming the clusters is a<br>' +
        'great opportunity to show your creativity. Clever names<br>' +
        'can succinctly communicate the nature and content of the<br>' +
        'clusters. They can also give the data mining team<br>' +
        'additional credibility with the business folks.<br>' +
        'Once the clustering model has been trained, you can use it<br>' +
        'to categorize new cases. It often helps to first cluster<br>' +
        'customers based on their buying patterns and<br>' +
        'demographics, and then run predictive models on each<br>' +
        'cluster separately. This allows the unique behaviors of<br>' +
        'each cluster to show through rather than be overwhelmed<br>' +
        'by the overall average behaviors.<br>' +
        'One form of clustering involves ordered data, usually<br>' +
        'ordered temporally or physically. The goal is to identify<br>' +
        'frequent sequences or episodes (clusters) in the data. The<br>' +
        '791<br>' +
        'television industry does extensive analysis of TV viewing<br>' +
        'sequences to determine the order of programs in the<br>' +
        'lineup. Companies with significant public websites may<br>' +
        'use sequence analysis to understand how visitors move<br>' +
        'through their website. For example, a consumer electronics<br>' +
        'product manufacturer’s website might identify several<br>' +
        'clusters of users based on their browsing behavior. Some<br>' +
        'users might start with the Sale Items page, then browse the<br>' +
        'rest of the e-commerce section, but rarely end with a<br>' +
        'purchase (Bargain Hunters). Others may enter through the<br>' +
        'home page, and then go straight to the support section,<br>' +
        'often ending by sending a help request e-mail (Clueless).<br>' +
        'Others may go straight to the e-commerce pages, ending<br>' +
        'with a purchase, but rarely visit the account management<br>' +
        'or support pages (Managers). Another group might go to<br>' +
        'the account management pages, checking order statuses<br>' +
        'and printing invoices (Administrators). A Sequence<br>' +
        'Clustering model like this one can be used to classify new<br>' +
        'visitors and customize content for them, and to predict<br>' +
        'future page hits for a given visitor.<br>' +
        'Microsoft Clustering and Microsoft Sequence Clustering<br>' +
        'are segmentation algorithms. The Microsoft Sequence<br>' +
        'Clustering algorithm is primarily designed for sequence<br>' +
        'analysis (hence the clever name).<br>' +
        'The Power of Naming<br>' +
        'When Nielsen Claritas originally created its<br>' +
        'customer segmentation system called PRIZM, it<br>' +
        '792<br>' +
        'likely used clustering techniques to identify about<br>' +
        '60 different groups of consumers. The resulting<br>' +
        'clusters, called lifestyle types, were numbered 1<br>' +
        'through 60+. It’s clear that someone at Nielsen<br>' +
        'Claritas realized that numbers were not descriptive<br>' +
        'and would not make good marketing. So, they<br>' +
        'came up with a clever name for each cluster; a<br>' +
        'shorthand way to communicate its unique<br>' +
        'characteristics. A few of the names are: 02. Blue<br>' +
        'Blood Estates (old money, big mansions), 51.<br>' +
        'Shotguns and Pickups (working class, large<br>' +
        'families, mobile homes), and 60. Park Bench<br>' +
        'Seniors (modest income, sedentary, daytime TV<br>' +
        'watchers).<br>' +
        'Anomaly Detection<br>' +
        'Several business processes rely on the identification of<br>' +
        'cases that deviate from the norm in a significant way.<br>' +
        'Fraud detection in consumer credit is a common example<br>' +
        'of anomaly detection. Anomaly detection can take<br>' +
        'advantage of any of the data mining algorithms. Clustering<br>' +
        'algorithms can be tuned to create a cluster that contains<br>' +
        'data outliers, separate from the rest of the clusters in the<br>' +
        'model.<br>' +
        'Anomaly detection involves a few extra twists in the data<br>' +
        'mining process. Often it’s necessary to bias the training set<br>' +
        'in favor of the exceptional events. Otherwise, there may be<br>' +
        'too few of them in the historical data for the algorithm to<br>' +
        '793<br>' +
        'detect. After all, they are anomalies. We provide an<br>' +
        'example of this in the case studies later in this chapter.<br>' +
        'Description and Profiling<br>' +
        'The business task Berry and Linoff call description and<br>' +
        'profiling is essentially the same activity we earlier called<br>' +
        'undirected data mining. The task is to use the various data<br>' +
        'mining techniques to gain a better understanding of the<br>' +
        'complexities of the data. Decision trees, clustering, and<br>' +
        'affinity grouping can reveal relationships that would<br>' +
        'otherwise be undetectable. For example, a decision tree<br>' +
        'might reveal that women purchase certain products much<br>' +
        'more than men. In some cases, like women’s shoes, this<br>' +
        'would be stereotypically obvious, but in others, like<br>' +
        'hammers, the reasons are less clear and the behavior would<br>' +
        'prompt additional investigation. Data mining, like all<br>' +
        'analytic processes, often opens doors to whole new areas<br>' +
        'of investigation.<br>' +
        'Description and profiling can also be used as an extension<br>' +
        'to the data profiling tasks we described in previous<br>' +
        'chapters. You can use data mining to identify specific data<br>' +
        'error anomalies and broader patterns of data problems that<br>' +
        'would not be obvious to the unaided eye.<br>' +
        'Business Task Summary<br>' +
        'The definitions of the various business tasks that are<br>' +
        'suitable for data mining and the list of which algorithms<br>' +
        'are appropriate for which tasks can be a bit confusing.<br>' +
        'Table 13-1 gives a few examples of common business<br>' +
        '794<br>' +
        'tasks and the associated data mining algorithms that can<br>' +
        'help accomplish these tasks.<br>' +
        'Table 13-1: Examples of business tasks and associated<br>' +
        'algorithms<br>' +
        'Business Task Example Microsoft Algorithms<br>' +
        'Classifying<br>' +
        'customers into<br>' +
        'discrete classes<br>' +
        'Assigning each customer to an<br>' +
        'Activity Level with discrete<br>' +
        'values of Disinterested, Casual,<br>' +
        'Recreational, Serious, or<br>' +
        'Competitor.<br>' +
        'Decision TreesNaïve<br>' +
        'BayesClusteringNeural<br>' +
        'Network<br>' +
        'Predicting a<br>' +
        'discrete attribute<br>' +
        'Predicting a variable like<br>' +
        'ServiceStatus with discrete<br>' +
        'values of Cancelled or Active<br>' +
        'might form the core of a<br>' +
        'customer retention program.<br>' +
        'Decision TreesNaïve<br>' +
        'Bayes<br>' +
        'ClusteringNeural<br>' +
        'Network<br>' +
        'Predicting a<br>' +
        'continuous<br>' +
        'attribute<br>' +
        'Predicting the sale price of a real<br>' +
        'estate listing or forecasting next<br>' +
        'year’s sales.<br>' +
        'Decision TreesTime<br>' +
        'SeriesNeural Network<br>' +
        'Making<br>' +
        'recommendations<br>' +
        'based on a<br>' +
        'sequence<br>' +
        'Predicting website usage<br>' +
        'behavior. The order of events is<br>' +
        'important in this case. A<br>' +
        'customer support website might<br>' +
        'use common sequences to<br>' +
        'suggest additional support pages<br>' +
        'that might be helpful based on<br>' +
        'the page path the customer has<br>' +
        'already followed.<br>' +
        'Sequence Clustering<br>' +
        'Making<br>' +
        'recommendations<br>' +
        'based on a set<br>' +
        'Suggesting additional products<br>' +
        'for a customer to purchase based<br>' +
        'on items they’ve already selected<br>' +
        'or pages they’ve viewed. In this<br>' +
        'case, order is not important.<br>' +
        '(“People who bought this book<br>' +
        'also bought . . .”).<br>' +
        'AssociationDecision<br>' +
        'Trees<br>' +
        'Segmenting<br>' +
        'customers<br>' +
        'Creating groups of customers<br>' +
        'with similar behaviors,<br>' +
        'ClusteringSequence<br>' +
        'Clustering<br>' +
        '795<br>' +
        'Business Task Example Microsoft Algorithms<br>' +
        'demographics, and product<br>' +
        'preferences. This allows you to<br>' +
        'create targeted products and<br>' +
        'promotions designed to appeal to<br>' +
        'specific segments.<br>' +
        'Roles and Responsibilities<br>' +
        'Microsoft’s Data Mining tools have been designed to be<br>' +
        'usable by just about anyone who can install BIDS. After a<br>' +
        'little data preparation, a competent user can fire up the<br>' +
        'Data Mining Wizard and start generating data mining<br>' +
        'models. Data mining is an iterative, exploratory process. In<br>' +
        'order to get the most value out of a model, the data miner<br>' +
        'must conduct extensive research and testing. The data<br>' +
        'mining person (or team) will need the following skills:<br>' +
        '• Good business sense and good-to-excellent working<br>' +
        'relationships with the business folks: This skill set is used to<br>' +
        'form the foundation of the data mining model. Without it,<br>' +
        'the data miner can build a sophisticated model that is<br>' +
        'meaningless to the business.<br>' +
        '• Good-to-excellent knowledge of Integration Services and/or<br>' +
        'SQL: These skills are crucial to creating the data<br>' +
        'transformations needed to build the case sets and packaging<br>' +
        'them up in repeatable modules.<br>' +
        '• A good understanding of statistics and probability: This<br>' +
        'knowledge helps in understanding the functionality,<br>' +
        'parameters, and output of the various algorithms. It also<br>' +
        'helps to understand the data mining literature and<br>' +
        'documentation — most of which seems to have been written<br>' +
        'by statisticians. Microsoft has tried hard to minimize the<br>' +
        'amount of statistics you need to know, but the more you<br>' +
        'know, the better.<br>' +
        '• Data mining experience: Much of what is effective data<br>' +
        'mining comes from having seen a similar problem before<br>' +
        '796<br>' +
        'and knowing which approaches might work best to solve it.<br>' +
        'Obviously, you have to start somewhere. If you don’t have a<br>' +
        'lot of data mining experience, it’s a good idea to find a local<br>' +
        'or online data mining special interest group you can use to<br>' +
        'validate your ideas and approach.<br>' +
        '• Programming skills: To incorporate the resulting data<br>' +
        'mining model into the organization’s transaction systems,<br>' +
        'someone on the team or elsewhere in the organization will<br>' +
        'need to learn the appropriate APIs.<br>' +
        'SQL Server Data Mining Architecture Overview<br>' +
        'Microsoft SQL Server Data Mining offers a rich,<br>' +
        'well-tuned, integrated, and easy-to-use data mining<br>' +
        'environment. In this section, we give an overview of the<br>' +
        'data mining environment using the high-level architecture<br>' +
        'drawing presented in Figure 13-1 as a guide.<br>' +
        'From a system point of view, integrating data mining into<br>' +
        'the overall SQL Server product allows the data mining<br>' +
        'service to take advantage of the functionality offered by<br>' +
        'the rest of the system. For example, point A in Figure 13-1<br>' +
        'shows how data mining models are built using the<br>' +
        'Analysis Services dimensional engine, leveraging its<br>' +
        'ability to load data and quickly perform the base statistical<br>' +
        'calculations like sums, averages, and counts. The data<br>' +
        'mining server can easily pull case data through the Data<br>' +
        'Source View from a wide variety of data sources including<br>' +
        'relational and Analysis Services, as seen at point B in<br>' +
        'Figure 13-1.<br>' +
        'Figure 13-1: The SQL Server data mining architecture<br>' +
        '797<br>' +
        'From a system point of view, integrating data mining into<br>' +
        'the overall SQL Server product allows the data mining<br>' +
        'service to take advantage of the functionality offered by<br>' +
        'the rest of the system. For example, point A in Figure 13-1<br>' +
        'shows how data mining models are built using the<br>' +
        'Analysis Services dimensional engine, leveraging its<br>' +
        'ability to load data and quickly perform the base statistical<br>' +
        'calculations like sums, averages, and counts. The data<br>' +
        'mining server can easily pull case data through the Data<br>' +
        'Source View from a wide variety of data sources including<br>' +
        'relational and Analysis Services, as seen at point B in<br>' +
        'Figure 13-1.<br>' +
        'Point C in Figure 13-1 shows how the typical data miner<br>' +
        'will first experience data mining by creating an Analysis<br>' +
        'Services project in BIDS and then using the Data Mining<br>' +
        'Wizard to create a new data mining structure and an initial<br>' +
        'data mining model. The mining structure is a construct that<br>' +
        '798<br>' +
        'provides a metadata layer allowing several mining models<br>' +
        'to work with the same input data. Each mining model in a<br>' +
        'mining structure can have different algorithms and<br>' +
        'parameters. The wizard provides model building guidance<br>' +
        'with auto selection and adjustment of variables based on<br>' +
        'the algorithm selected. The wizard also helps you create<br>' +
        'case sets, including complex, nested queries.<br>' +
        'The Data Mining Design Environment<br>' +
        'When the wizard is finished building the mining structure<br>' +
        'and the initial data mining model, it drops the developer<br>' +
        'into the data mining design environment. At this point, the<br>' +
        'mining model has not been built; the project contains only<br>' +
        'the metadata that defines the model. The Data Mining<br>' +
        'Designer is broken up into five tabs to support the data<br>' +
        'mining process. Several of these tabs work with the<br>' +
        'completed mining model as it exists on Analysis Services,<br>' +
        'so they are not available until the model has been built and<br>' +
        'deployed. The first tab shows the Mining Structure with its<br>' +
        'underlying Data Source View. The second tab is the<br>' +
        'Mining Models tab, showing the source mining structure<br>' +
        'and all the data mining models that have been defined<br>' +
        'based on this structure. The third tab is the Mining Model<br>' +
        'Viewer that lets you select a model and a viewer type, and<br>' +
        'then provides several sub-tabs, each with a different<br>' +
        'graphic or tabular representation of the contents of the<br>' +
        'model. The Mining Model Viewer tab is the primary tool<br>' +
        'the data miner uses to explore the various models. The<br>' +
        'fourth tab is the Mining Accuracy Chart. This tab provides<br>' +
        'three ways to compare the relative accuracy of certain<br>' +
        'kinds of predictive models: the Lift Chart, the<br>' +
        'Classification Matrix, and Cross Validation. Finally, the<br>' +
        '799<br>' +
        'fifth tab is the Mining Model Prediction tab that allows the<br>' +
        'data miner to specify a prediction query using a<br>' +
        'rudimentary query builder interface. You will see<br>' +
        'examples of most of these tabs in the screenshots<br>' +
        'throughout this chapter.<br>' +
        'Build, Deploy, and Process<br>' +
        'Most of the functions in the Data Mining Designer work<br>' +
        'with the actual model as it exists in Analysis Services. This<br>' +
        'means once the wizard is complete, the developer must<br>' +
        'build and deploy the project (which includes processing<br>' +
        'the model cubes) before any more progress can be made.<br>' +
        'Building the project writes the metadata out to project files<br>' +
        'in the development environment. The actual model does<br>' +
        'not come into being until the project is deployed to an<br>' +
        'Analysis Services instance. At that point, BIDS creates a<br>' +
        'database for the project in Analysis Services. It writes out<br>' +
        'the mining structure metadata and the definition of each<br>' +
        'model. Finally, it creates a cube for each mining structure<br>' +
        'and processes the models, inserting the training data so the<br>' +
        'algorithm can calculate the rules, correlations, and other<br>' +
        'relationships. Until the project is deployed and a model is<br>' +
        'processed, it cannot be viewed in the viewers.<br>' +
        'TIP<br>' +
        'It is possible to process a single model<br>' +
        'rather than all models in a mining structure<br>' +
        'by selecting the model in the Mining<br>' +
        '800<br>' +
        'Models tab, and then selecting Process<br>' +
        'Model from the Mining Model menu. This<br>' +
        'can save a lot of time if you are working<br>' +
        'with large case sets and complex models.<br>' +
        'Accessing the Mining Models<br>' +
        'As you see at point D in Figure 13-1, the Data Mining<br>' +
        'eXtensions to SQL language (DMX) is at the core of all<br>' +
        'the Microsoft data mining APIs. As the name suggests,<br>' +
        'DMX is an extension to SQL designed to create, train,<br>' +
        'modify, and query data mining models. DMX was<br>' +
        'introduced with SQL Server 2000 and enhanced in 2005<br>' +
        'and 2008 as part of the OLE DB for Data Mining APIs. An<br>' +
        'easy way to begin learning about DMX is to use the<br>' +
        'Mining Model Prediction tab in the Data Mining Designer<br>' +
        'and examine the syntax it generates for DMX queries. The<br>' +
        'code can be copied to a DMX query window in SQL<br>' +
        'Studio for further exploration. There are also some data<br>' +
        'mining schema rowsets in Analysis Services that act like<br>' +
        'the predefined system views in the relational system.<br>' +
        'Although DMX is an extension to SQL, queries are<br>' +
        'submitted to the Analysis Services server — that’s where<br>' +
        'the data mining services are.<br>' +
        'The development environment works with Analysis<br>' +
        'Services mostly using Analysis Management Objects<br>' +
        '(AMO) to define and build the underlying cubes. Once the<br>' +
        'models are in place, they are available to any application<br>' +
        'as a web service by using SOAP protocols because<br>' +
        'Analysis Services is a native XMLA server. It is still<br>' +
        '801<br>' +
        'possible to access the server with OLE DB APIs, ADO,<br>' +
        'ADO.NET, or ADOMD.NET.<br>' +
        'Point E is where many analysts, and some data miners,<br>' +
        'will experience Microsoft data mining, often without even<br>' +
        'realizing it. Microsoft has created a set of add-ins for Excel<br>' +
        'and Visio. The Excel add-ins bring some of the core<br>' +
        'algorithms into the Excel analytic environment and<br>' +
        'provide an Excel friendly interface to create, access, and<br>' +
        'manage Analysis Services-based data mining models. The<br>' +
        'Excel data mining add-ins are an excellent way for many<br>' +
        'analysts to get started exploring data mining without<br>' +
        'having to first conquer the Visual Studio development<br>' +
        'environment. You’ll see this Excel interface in the first<br>' +
        'data mining case study later in this chapter.<br>' +
        'Integration Services and Data Mining<br>' +
        'Integration Services can play a major role in the data<br>' +
        'mining process as shown in Figure 13-1. Many of the<br>' +
        'standard transforms used for data cleaning and data<br>' +
        'integration are particularly valuable for building the<br>' +
        'training and test data sets. Besides the obvious tasks, like<br>' +
        'Data Conversion and Derived Column, tasks like the<br>' +
        'Percentage Sampling, Row Sampling, Conditional Split,<br>' +
        'Lookup, and Merge Join are powerful components the data<br>' +
        'miner can use to build a set of packages to prepare the case<br>' +
        'sets for the data mining process. This is shown at point F<br>' +
        'in Figure 13-1. In addition to the standard tasks, there are<br>' +
        'two Integration Services tasks that directly interact with<br>' +
        'the data mining models, shown at point G in Figure 13-1.<br>' +
        'The Data Mining Model Training destination<br>' +
        'transformation feeds the data flow into a training<br>' +
        '802<br>' +
        'command for a mining model. This capability is perfect for<br>' +
        'the ongoing re-training required to keep certain mining<br>' +
        'models current — recommendation models, for example.<br>' +
        'The Data Mining Query task is specifically designed to do<br>' +
        'prediction joins against a model in the SSIS pipeline, once<br>' +
        'the model has been trained and tested. The task passes<br>' +
        'query values to a model and receives the results, which<br>' +
        'could be used to add scores to the Customer table or<br>' +
        'identify significant data anomalies during the nightly ETL<br>' +
        'process. It could also be used in a low latency mode to flag<br>' +
        'transactions that were potentially fraudulent.<br>' +
        'Additional Features<br>' +
        'There are several additional features that will be important<br>' +
        'for certain applications. Most of these can be found at<br>' +
        'point H in Figure 13-1 and are listed briefly here:<br>' +
        '• Extensibility: Microsoft has provided a set of COM APIs<br>' +
        'that allow developers to integrate additional data mining<br>' +
        'algorithms into the data mining engine. They can integrate<br>' +
        'custom viewers into the Data Mining Designer as well.<br>' +
        'Someone could even create a new viewer for an existing<br>' +
        'Microsoft algorithm.<br>' +
        '• Analysis Management Objects (AMO): AMO is an API for<br>' +
        'managing the creation and maintenance of data mining<br>' +
        'objects, including creating, processing, backing up,<br>' +
        'restoring, and securing.<br>' +
        '• Stored procedures and user-defined functions: A developer<br>' +
        'can create what are essentially stored procedures or<br>' +
        'user-defined functions and load them as managed assemblies<br>' +
        'into Analysis Services. This allows clients to work with<br>' +
        'large mining models through the intermediate layer of the<br>' +
        'server-based managed assembly.<br>' +
        '803<br>' +
        '• Text mining: It is possible to do some interesting data mining<br>' +
        'on unstructured text data, like the text in HTML files in a set<br>' +
        'of web directories, or even text fields in a database. For<br>' +
        'example, use the Integration Services Term Extraction<br>' +
        'transformation to build a dictionary of terms found in the<br>' +
        'text files. Next, use the Term Lookup transform to convert<br>' +
        'the contents of the unstructured document data into term<br>' +
        'vectors. Then use data mining to create classification rules to<br>' +
        'categorize the documents according to the terms they<br>' +
        'contain. This is essentially a data mining application, not a<br>' +
        'new data mining algorithm, but it has value in dealing with<br>' +
        'unstructured data.<br>' +
        'RESOURCES<br>' +
        'To find more information about the text<br>' +
        'mining technique, see the Books Online<br>' +
        'topic “Term Extraction Transformation,” or<br>' +
        'visit www.sqlserverdatamining.com for a text<br>' +
        'mining tutorial.<br>' +
        'Architecture Summary<br>' +
        'Our goal in this section is to show how data mining fits<br>' +
        'into the overall SQL Server architecture, and to show how<br>' +
        'well data mining has been integrated into the SQL Server<br>' +
        'environment. It should be clear by now that data mining is<br>' +
        'a serious component of the BI toolset, and that it leverages<br>' +
        'the capabilities of other SQL Server components. Data<br>' +
        'mining and application developers will have a lot more to<br>' +
        'learn before they are proficient with all the data mining<br>' +
        'components. Fortunately, Microsoft’s documentation is<br>' +
        'heavily weighted toward the development community.<br>' +
        '804<br>' +
        'Additional help should be easy to find. A good place to<br>' +
        'start is a website called www.sqlserverdatamining.com,<br>' +
        'maintained by members of the SQL Server data mining<br>' +
        'development team.<br>' +
        'Microsoft Data Mining Algorithms<br>' +
        'Data mining algorithms are the logic used to create the<br>' +
        'mining models. Several standard algorithms in the data<br>' +
        'mining community have been carefully tested and honed<br>' +
        'over time. One of the algorithms used to calculate decision<br>' +
        'trees uses a Bayesian method to determine the score used<br>' +
        'to split the branches of the tree. The roots of this method<br>' +
        '(so to speak) trace back to its namesake, Thomas Bayes,<br>' +
        'who first established a mathematical basis for probability<br>' +
        'inference in the 1700s.<br>' +
        'The Data Mining group at Microsoft has been working<br>' +
        'diligently to expand the number of algorithms offered in<br>' +
        'SQL Server and to improve their accuracy. SQL Server<br>' +
        'Data Mining includes seven core algorithms that cover a<br>' +
        'large percentage of the common data mining application<br>' +
        'areas. The seven core algorithms are:<br>' +
        '• Decision Trees (and Linear Regression)<br>' +
        '• Naïve Bayes<br>' +
        '• Clustering<br>' +
        '• Sequence Clustering<br>' +
        '• Time Series<br>' +
        '• Association<br>' +
        '• Neural Network (and Logistic Regression)<br>' +
        'The two regression algorithms set parameters on the main<br>' +
        'algorithm to generate the regression results. Some of these<br>' +
        '805<br>' +
        'higher-level algorithms include parameters the data miner<br>' +
        'can use to choose from several underlying algorithms to<br>' +
        'generate the model. If you plan to do serious data mining,<br>' +
        'you need to know what these algorithms are and how they<br>' +
        'work so you can apply them to the appropriate problems<br>' +
        'and are able to get the best performance. We briefly<br>' +
        'describe each of these algorithms in the following list. The<br>' +
        'Books Online topic “Data Mining Algorithms” is a good<br>' +
        'starting point for additional information about how each of<br>' +
        'these algorithms work.<br>' +
        'RESOURCES<br>' +
        'For more detailed information on<br>' +
        'Microsoft’s algorithms, see the following<br>' +
        'resources:<br>' +
        '• Data Mining with Microsoft SQL Server<br>' +
        '2008 (Wiley, 2008) by Jamie MacLennan,<br>' +
        'ZhaoHui Tang, and Bogdan Crivat; all<br>' +
        'current or former members of the<br>' +
        'Microsoft SQL Server Data Mining team.<br>' +
        '• Search in SQL Server Books Online for<br>' +
        '“data mining algorithms” for descriptions<br>' +
        'and links to technical details, query guides,<br>' +
        'and model content.<br>' +
        'Decision Trees<br>' +
        'The Microsoft Decision Trees algorithm supports both<br>' +
        'classification and estimation. It works well for predictive<br>' +
        'modeling for both discrete and continuous attributes.<br>' +
        '806<br>' +
        'The process of building a decision tree starts with the<br>' +
        'dependent variable to be predicted and runs through the<br>' +
        'independent variables to see which one most effectively<br>' +
        'divides the population. The goal is to identify the variable<br>' +
        'that splits the cases into groups where the predicted<br>' +
        'variable (or class) either predominates or is faintly<br>' +
        'represented. The best starting variable for the tree is the<br>' +
        'one that creates groups that are the most different from<br>' +
        'each other — the most diverse.<br>' +
        'For example, if you’re creating a decision tree to identify<br>' +
        'couples who are likely to form a successful marriage,<br>' +
        'you’d need a training set of input attributes for both<br>' +
        'members of the couple and an assessment of whether or<br>' +
        'not the partnership is successful. The input attributes might<br>' +
        'include the age of each individual, religion, political views,<br>' +
        'gender, relationship role (husband or wife), and so on. The<br>' +
        'predictable attribute might be MarriageOutcomewith the<br>' +
        'values of Success or Fail. The algorithm might determine<br>' +
        'that the first split in the decision tree — the one that<br>' +
        'creates the biggest split — is based on a variable called<br>' +
        'PoliticalViews: a discrete variable with the values of<br>' +
        'Similar and Different. Figure 13-2 shows that this initial<br>' +
        'split results in one group (PoliticalViews=Similar) that<br>' +
        'has a much higher percentage of successful marriages than<br>' +
        'the other (PoliticalViews=Different). The next split<br>' +
        'might be different for each of the two branches. In Figure<br>' +
        '13-2, the top branch splits based on the height difference<br>' +
        'between the two (calculated as height of husband minus<br>' +
        'height of wife). The lower branch also splits on height<br>' +
        'difference, but uses different cutoff points. It seems that<br>' +
        'couples can better tolerate a greater height difference if<br>' +
        'their political views are similar.<br>' +
        '807<br>' +
        'Figure 13-2: A simple decision tree to predict relationship<br>' +
        'success<br>' +
        'The bottom branch of this tree indicates that the chances of<br>' +
        'having a successful marriage are more likely when the<br>' +
        'couple shares similar political views (.72). Following<br>' +
        'down that same branch, the chances get better when the<br>' +
        'husband is at least as tall as, but not more than 10 inches<br>' +
        'taller than the wife (.81). The group with the lowest<br>' +
        'probability of success (.08) is characterized by different<br>' +
        'political views and a height difference where the husband<br>' +
        'is either less than 2 inches taller or 8 inches or more taller<br>' +
        'than the wife. Once you build a decision tree like this, you<br>' +
        'could use it to predict the success of a given relationship<br>' +
        'by entering the appropriate attributes for both partners. At<br>' +
        'that point, you’d be well on your way to building a<br>' +
        'matching engine for a dating website.<br>' +
        '808<br>' +
        'Naïve Bayes<br>' +
        'The Microsoft Naïve Bayes algorithm is a good starting<br>' +
        'point for many data mining projects. It is a simplified<br>' +
        'version of Decision Trees, and can be used for<br>' +
        'classification and prediction for discrete attributes. If you<br>' +
        'select the Naïve Bayes algorithm in the Data Mining<br>' +
        'Wizard, it will offer to ignore any continuous variables in<br>' +
        'the input case set. You can always use the mining structure<br>' +
        'to “discretize” these variables.<br>' +
        'The Naïve Bayes algorithm is fairly simple, based on the<br>' +
        'relative probabilities of the different values of each<br>' +
        'attribute, given the value of the predictable attribute. For<br>' +
        'example, if you had a case set of individuals with their<br>' +
        'occupations and income ranges, you could build a Naïve<br>' +
        'Bayes model to predict Income Range given an<br>' +
        'Occupation. The decision tree in Figure 13-2 could have<br>' +
        'been generated by the Naïve Bayes algorithm because all<br>' +
        'the variables are discrete (although the Tree Viewer is not<br>' +
        'available for Naïve Bayes models). The required<br>' +
        'probability calculations are almost all done as part of the<br>' +
        'process of building the mining model cube, so the results<br>' +
        'are returned quickly.<br>' +
        'Clustering<br>' +
        'The clustering algorithm is designed to meet the clustering<br>' +
        'or segmentation business need described earlier. Clustering<br>' +
        'is generally considered a density estimation problem with<br>' +
        'the assumption that there are multiple populations in a set,<br>' +
        'each with its own density distribution. (Sentences like the<br>' +
        'preceding one serve to remind us that statisticians speak a<br>' +
        '809<br>' +
        'different language.) It’s easier to understand clustering<br>' +
        'visually: A simple spreadsheet chart of data points serves<br>' +
        'as an eyeball clustering tool — especially with only two<br>' +
        'variables. It’s easy to see where the dense clusters are<br>' +
        'located. For example, a graph showing per-capita income<br>' +
        'versus per-capita national debt for each country in the<br>' +
        'world would quickly reveal several obvious clusters of<br>' +
        'countries. We explore the idea of graphically identifying<br>' +
        'clusters further in the next section. The challenge is<br>' +
        'finding these clusters when there are more than two<br>' +
        'variables, or when the variables are discrete and<br>' +
        'non-numeric rather than continuous.<br>' +
        'NOTE<br>' +
        'The Microsoft Clustering algorithm uses<br>' +
        'what’s known as an<br>' +
        'Expectation-Maximization (EM) approach<br>' +
        'to identifying clusters. An alternative,<br>' +
        'distance-based clustering mechanism called<br>' +
        'K-means is available by setting parameters<br>' +
        'on the model.<br>' +
        'Sequence Clustering<br>' +
        'Sequence clustering adds another level of flexibility to the<br>' +
        'clustering problem by including an ordering attribute. The<br>' +
        'algorithm can identify common sequences and use those<br>' +
        'sequences to predict the next step in a new sequence.<br>' +
        'Using the website example, sequence clustering can<br>' +
        'identify common click-paths and predict the next page (or<br>' +
        '810<br>' +
        'pages) someone will visit, given the pages they have<br>' +
        'already visited.<br>' +
        'Time Series<br>' +
        'The Microsoft Time Series algorithms can be used to<br>' +
        'predict continuous variables, like Sales, over time. The<br>' +
        'algorithms include both ARIMA- and ARTxp-based<br>' +
        'algorithms to provide the best prediction over variable<br>' +
        'time spans. The algorithm includes time-variant factors<br>' +
        'like seasonality and can predict one or more variables from<br>' +
        'the case set. It also has the ability to generate predictions<br>' +
        'using cross-variable correlations. For example, product<br>' +
        'returns in the current period may be a function of product<br>' +
        'sales in the prior period. (It’s just a guess, but we’d bet that<br>' +
        'high sales in the week leading up to December 25 may<br>' +
        'lead to high returns in the week following.)<br>' +
        'Association<br>' +
        'The Microsoft Association algorithm is designed to meet<br>' +
        'the business tasks described as association, affinity<br>' +
        'grouping, or market basket analysis. Association works<br>' +
        'well with the concept of nested case sets, where the parent<br>' +
        'level is the overall transaction and the child level is the<br>' +
        'individual items involved in the transaction. The algorithm<br>' +
        'looks for items that tend to occur together in the same<br>' +
        'transaction. The number of times a combination occurs is<br>' +
        'called its support. The MINIMUM_SUPPORT parameter<br>' +
        'allows the data miner to set a minimum number of<br>' +
        'occurrences before a given combination is considered<br>' +
        'significant. The Association algorithm goes beyond item<br>' +
        'pairs by creating rules that can involve several items. In<br>' +
        '811<br>' +
        'English, the rule sounds like “When Item A and Item B<br>' +
        'exist in the item set, then the probability that Item C is also<br>' +
        'in the item set is X.” The rule is displayed in this form: A,<br>' +
        'B → C (X). In the same way the data miner can specify a<br>' +
        'minimum support level, it is also possible to specify a<br>' +
        'minimum probability in order for a rule to be considered.<br>' +
        'Neural Network<br>' +
        'Neural network algorithms mimic our understanding of the<br>' +
        'way neurons work in the brain. The attributes of a case are<br>' +
        'the inputs to a set of interconnected nodes, each of which<br>' +
        'generates an output. The output can feed another layer of<br>' +
        'nodes (known as a hidden layer) and eventually feeds out<br>' +
        'to a result. The goal of the Microsoft Neural Network<br>' +
        'algorithm is to minimize the error of the result compared<br>' +
        'with the known value in the training set. Through some<br>' +
        'fancy footwork known as back propagation, the errors are<br>' +
        'fed back into the network, modifying the weights of the<br>' +
        'inputs. Then the algorithm makes additional passes<br>' +
        'through the training set, feeding back the results, until it<br>' +
        'converges on a solution. All this back and forth means the<br>' +
        'Neural Network algorithm is the slowest of the algorithms<br>' +
        'in terms of the time it takes to build a model. The<br>' +
        'algorithm can be used for classification or prediction on<br>' +
        'both continuous and discrete variables.<br>' +
        'Using these seven core algorithms, separately or in<br>' +
        'combination, you can create solutions to most common<br>' +
        'data mining problems.<br>' +
        'The Data Mining Process<br>' +
        '812<br>' +
        'There are probably as many ways to approach data mining<br>' +
        'as there are data mining practitioners. Much like<br>' +
        'dimensional modeling, starting with the goal of adding<br>' +
        'business value leads to a clear series of steps that just make<br>' +
        'sense.<br>' +
        'You’ll be shocked to hear that our data mining process<br>' +
        'begins with an understanding of the business opportunities.<br>' +
        'Figure 13-3 shows the three major phases of the data<br>' +
        'mining process — Business, Data Mining, and Operations<br>' +
        '— and the major task areas within those phases.<br>' +
        'RESOURCES<br>' +
        'We didn’t invent this process; we just<br>' +
        'stumbled on it through trial and error.<br>' +
        'Others who’ve spent their careers entirely<br>' +
        'on data mining have arrived at similar<br>' +
        'approaches to data mining. We’re fortunate<br>' +
        'that they have documented their processes<br>' +
        'in detail in their own publications. In<br>' +
        'particular, three sources have been valuable<br>' +
        'to us. The book Data Mining Techniques,<br>' +
        '2nd Ed. by Michael J. A. Berry and Gordon<br>' +
        'S. Linoff (Wiley, 2004) describes a process<br>' +
        'Berry and Linoff call the Virtuous Cycle of<br>' +
        'Data Mining. Another, similar approach<br>' +
        'comes from a special interest group that<br>' +
        'was formed in the late 1990s to define a<br>' +
        'data mining process. The result was<br>' +
        '813<br>' +
        'published as Cross Industry Standard<br>' +
        'Process for Data Mining (CRISP). Visit<br>' +
        'www.crisp-dm.org for more information.<br>' +
        'Also, search SQL Server Books Online for<br>' +
        '“Data Mining Concepts” to see Microsoft’s<br>' +
        'version of the approach.<br>' +
        'Like most of the processes in the DW/BI system, the data<br>' +
        'mining process is iterative. The arrows that point back to<br>' +
        'previous processes in Figure 13-3 are the most common<br>' +
        'iteration points. There are additional iteration points; for<br>' +
        'instance, it is also common to return to the business phase<br>' +
        'tasks based on what is learned in the data mining phase. In<br>' +
        'this section, we examine the three phases and their task<br>' +
        'areas in order, beginning with the business phase.<br>' +
        'Figure 13-3: The data mining process<br>' +
        '814<br>' +
        'The Business Phase<br>' +
        'The business phase is a much more focused version of the<br>' +
        'overall requirements gathering process. The goal is to<br>' +
        'identify an opportunity, or a list of opportunities and their<br>' +
        'relative priorities, that can have a significant impact on the<br>' +
        'business. The business opportunities and data<br>' +
        'understanding tasks in Figure 13-3 connect to each other<br>' +
        'because the process of identifying opportunities must be<br>' +
        'bounded by the realities of the data world. By the same<br>' +
        'token, the data itself may suggest business opportunities.<br>' +
        'Identifying Business Opportunities<br>' +
        'As always, the most important step in successful business<br>' +
        'intelligence is not about technology; it’s about<br>' +
        'understanding the business. In data mining, this usually<br>' +
        '815<br>' +
        'takes the form of a set of discussions between the business<br>' +
        'folks and the data miner about potential opportunities, and<br>' +
        'the associated relationships and behaviors that are captured<br>' +
        'in the data. The purpose of these meetings is to identify<br>' +
        'several high value opportunities and think through each<br>' +
        'one carefully. First, identify the overall business value goal<br>' +
        'of the data mining project. It helps to describe this in as<br>' +
        'narrow and measurable a way as possible. A goal like<br>' +
        '“increase sales” is too broad. A goal like “reduce the<br>' +
        'monthly cancellation, or churn, rate” is a bit more<br>' +
        'manageable. Next, think about what factors influence the<br>' +
        'goal. What might indicate that someone is likely to churn,<br>' +
        'or how can we tell if someone would be interested in a<br>' +
        'given product? While you’re discussing these factors, try<br>' +
        'to translate them into specific attributes and behaviors that<br>' +
        'are known to exist in a usable, accessible form. The data<br>' +
        'miner may hold several of these meetings with different<br>' +
        'groups to identify a range of opportunities. At the end of<br>' +
        'these meetings, the data miner should work with the<br>' +
        'business folks to prioritize the various opportunities based<br>' +
        'on the estimated potential for business impact and the<br>' +
        'difficulty of implementation. These priorities will change<br>' +
        'as you learn more about the data, but this is an important<br>' +
        'starting point.<br>' +
        'The data miner then takes the top-priority business<br>' +
        'opportunity and its associated list of potential variables<br>' +
        'back to the BIDS for further exploration.<br>' +
        'Understanding the Data<br>' +
        'The data miner typically spends a significant amount of<br>' +
        'time exploring the various data sets that might be relevant<br>' +
        '816<br>' +
        'to the business opportunities discussed. At this stage, the<br>' +
        'goal is to be reasonably confident that the data needed to<br>' +
        'support the business opportunity is available and clean<br>' +
        'enough to be usable. This exploration is generally not<br>' +
        'much more complex than the data exploration and data<br>' +
        'profiling that took place during the data modeling step in<br>' +
        'Chapter 2, and in designing the ETL process in Chapter 7.<br>' +
        'Any problems identified at this point should be noted so<br>' +
        'they can be included in the data mining opportunity<br>' +
        'document.<br>' +
        'Describing the Data Mining Opportunity<br>' +
        'The data mining opportunity document describes the<br>' +
        'top-priority opportunity discussed with the business folks.<br>' +
        'The opportunity description should include the following<br>' +
        'sections:<br>' +
        '• Business Opportunity Description<br>' +
        '• Data Sources, Transformations, and Potential Data Issues<br>' +
        '• Modeling Process Description<br>' +
        '• Implementation Plan<br>' +
        '• Maintenance Plan<br>' +
        'It’s important to document the opportunity, and have the<br>' +
        'business folks review it to make sure you understand their<br>' +
        'needs, and they understand how you intend to meet them.<br>' +
        'The data mining opportunity document is also a milestone<br>' +
        'in the data mining process. Once the data miner has a<br>' +
        'solid, clearly described, approved business opportunity, the<br>' +
        'data mining process enters the second phase: the data<br>' +
        'mining phase.<br>' +
        'The Data Mining Phase<br>' +
        '817<br>' +
        'Once you understand the business opportunities and<br>' +
        'supporting data, you can move into the data mining phase<br>' +
        'of the project. The data mining phase is where the data<br>' +
        'miner works through the tasks of preparing the data,<br>' +
        'developing alternative models, comparing their accuracy,<br>' +
        'and validating the final model. As Figure 13-3 shows, this<br>' +
        'is a highly iterative process. Data preparation feeds the<br>' +
        'model development task, which often identifies the need<br>' +
        'for further data preparation. By the same token, the process<br>' +
        'of validating a model commonly indicates the need for<br>' +
        'further improvements, which loops the data miner right<br>' +
        'back to the model development task, and potentially back<br>' +
        'to data preparation. In some cases, when serious problems<br>' +
        'arise, the loop goes all the way back to the business<br>' +
        'opportunity step. We ignore all this iteration in our<br>' +
        'description and move sequentially through the tasks.<br>' +
        'Data Preparation<br>' +
        'The first task in the data mining phase is to build the data<br>' +
        'mining case sets. Recall that a case set includes one row<br>' +
        'per instance or event. For many data mining models, this<br>' +
        'means a data set with one row per customer. Models based<br>' +
        'on simple customer attributes, like gender and marital<br>' +
        'status, work at the one-row-per-customer level. Models<br>' +
        'that include behaviors like purchasing work at the<br>' +
        'one-row-per-event level. A case set for customer purchases<br>' +
        'would have one row for each product purchased by a<br>' +
        'customer. This is called a nested case set with two<br>' +
        'components — the customer case set with one row per<br>' +
        'customer and all the applicable customer attributes, and the<br>' +
        'nested product data set, which includes the customer key<br>' +
        'and the products purchased by the given customer.<br>' +
        '818<br>' +
        'Building the case set involves creating SQL scripts, MDX<br>' +
        'scripts, and/or Integration Services packages to clean and<br>' +
        'transform the data, and copy it into the data sets needed to<br>' +
        'support the model building process.<br>' +
        'Cleaning and Transforming<br>' +
        'Ideally, the variables in the data set are fully populated<br>' +
        'with only the appropriate values and no outliers or null<br>' +
        'values. The bulk of this book describes the incredible<br>' +
        'amount of work it takes to create that cleaned, conformed<br>' +
        'information infrastructure we call the data warehouse. This<br>' +
        'is why the data warehouse is the ideal source for data<br>' +
        'mining case data. In the easiest case, many of the variables<br>' +
        'identified in the business opportunity already exist as<br>' +
        'attributes in the data warehouse database. This is often true<br>' +
        'with fields like CustomerType, or ProductColor. The data<br>' +
        'miner’s world gets even better when demographics and<br>' +
        'other external data are already loaded as part of the<br>' +
        'standard ETL process. While these variables can be<br>' +
        'integrated directly into the data mining training set from<br>' +
        'their sources, it is always a good idea to verify that basic<br>' +
        'data quality rules have been appropriately applied.<br>' +
        'Unfortunately, directly selecting variables from the data<br>' +
        'warehouse database rarely provides us with a rich enough<br>' +
        'data set to build a solid mining model. You may have to<br>' +
        'apply additional transformations to the data to make it<br>' +
        'more relevant to the business opportunity. This might<br>' +
        'include converting variables into more useful forms, such<br>' +
        'as combining fields or creating standard discrete ranges for<br>' +
        'continuous variables. Your organization or industry may<br>' +
        'already have them — audience age range in television<br>' +
        '819<br>' +
        'programming and advertising is a common discrete range.<br>' +
        'If no standard ranges exist, the data mining designer can be<br>' +
        'used to automatically discretize these variables based on<br>' +
        'different methods, like a histogram of the values, or an<br>' +
        'even distribution. This can also be done by preprocessing<br>' +
        'the data in a SQL statement using the CASE function. Some<br>' +
        'attributes might need multiple conversions, like birth date<br>' +
        'might be converted to age, which could then be converted<br>' +
        'to age range.<br>' +
        'As you work through the data mining phase, you may<br>' +
        'discover that these descriptive variables are generally not<br>' +
        'enough to build a highly predictive model, even after they<br>' +
        'have been transformed into more relevant forms. The most<br>' +
        'influential variables in a data mining model are typically<br>' +
        'behavior-based, not descriptive. Behaviors are generally<br>' +
        'captured as facts. What did customers do, how often did<br>' +
        'they do it, how much did they do it, and when did they do<br>' +
        'it are basic behavior questions. For example, knowing<br>' +
        'which web pages someone has viewed, what products they<br>' +
        'bought, what services they used, what problems they<br>' +
        'complained about, when the last time they complained<br>' +
        'was, and how many complaints they had in the last two<br>' +
        'months can help you build a clear picture of the status of<br>' +
        'your relationship with that customer.<br>' +
        'These behavioral variables are painstakingly extracted<br>' +
        'from the detailed fact tables as part of the data preparation<br>' +
        'process. The initial choice of which behavioral variables to<br>' +
        'create is based on the business’s understanding of<br>' +
        'behavior. Note that many of these behavior-based<br>' +
        'attributes require full table scans of the fact tables to<br>' +
        'create.<br>' +
        '820<br>' +
        'Integrating External Variables<br>' +
        'Unfortunately, behavioral variables may still not be<br>' +
        'enough. Building an effective model often means bringing<br>' +
        'in additional data. These attributes, like demographics,<br>' +
        'come from various systems around the company or even<br>' +
        'from external sources. They will need to be merged<br>' +
        'together to make a single case set. When the source tables<br>' +
        'share a common key, this typically means joining them<br>' +
        'together to create a single row per case (usually per<br>' +
        'customer). However, it is not uncommon to have to map<br>' +
        'keys from the external source to the transaction system’s<br>' +
        'natural keys, and then to the dimension’s surrogate keys.<br>' +
        'In the worst case, the external data will not have a known<br>' +
        'key that ties to any field in the data warehouse database.<br>' +
        'When this happens, the relationship will need to be<br>' +
        'determined using matching tools such as Master Data<br>' +
        'Services or third party software. You can also use<br>' +
        'PowerPivot and the data mining add-ins for Excel to pull<br>' +
        'data together. You still need an instance of Analysis<br>' +
        'Services to process the mining models and you need to<br>' +
        'flatten the pivot tables and convert them to regular tables.<br>' +
        'In organizations with large analytic communities, such as<br>' +
        'insurance companies or large web retailers, a tension may<br>' +
        'exist between professional data miners and the DW/BI<br>' +
        'team. Data miners sometimes reach for the raw source<br>' +
        'system data, clean it, analyze it, and then go directly to<br>' +
        'management with recommendations for decision making.<br>' +
        'The resulting models will not include all the data cleaning<br>' +
        'and conforming that goes into the data warehouse, but will<br>' +
        'contain whatever changes each data miner felt was<br>' +
        '821<br>' +
        'appropriate. This results in another variety of the multiple<br>' +
        'versions of the truth problem.<br>' +
        'warning Accurately tracking history is critical to<br>' +
        'successful data mining. If your DW/BI system or external<br>' +
        'sources overwrite changes in a type 1 fashion, your model<br>' +
        'will be associating current attribute values with historical<br>' +
        'behavior. This is particularly dangerous when integrating<br>' +
        'external data that might have only current attribute values.<br>' +
        'See the section “Slowly Changing Dimensions” in Chapter<br>' +
        '2 for an example of how this might create problems.<br>' +
        'Building the Case Sets<br>' +
        'Build these data sets by defining data cleaning and<br>' +
        'transformation steps that build a data structure made up of<br>' +
        'individual observations or cases. Cases often contain<br>' +
        'repeating nested or child structures. These case sets are<br>' +
        'then fed into the data mining service. It’s helpful to<br>' +
        'manage these tables independent of the data warehouse<br>' +
        'itself. Keep the data mining case sets in their own<br>' +
        'database, on their own server if necessary, where the data<br>' +
        'miner has permission to create tables. The process of<br>' +
        'building the case sets is typically very similar to the<br>' +
        'regular ETL process. It usually involves a set of<br>' +
        'transformations and full table scans that actually generate a<br>' +
        'resulting data set that gets loaded into the data mining<br>' +
        'database.<br>' +
        'There are two main approaches to data preparation in the<br>' +
        'SQL Server environment. Folks who come from a SQL/<br>' +
        'relational background will be inclined to write SQL<br>' +
        '822<br>' +
        'scripts, saving the results to separate case set tables that<br>' +
        'become inputs to the data mining process.<br>' +
        'NOTE<br>' +
        'Creating case sets can also be done through<br>' +
        'views if you are creative enough with your<br>' +
        'SQL. We don’t recommend this because if<br>' +
        'the underlying data changes (like when<br>' +
        'new rows are added), the mining model<br>' +
        'may change for no apparent reason.<br>' +
        'Folks who come from an ETL background will be more<br>' +
        'comfortable using SSIS to merge, clean, and prepare the<br>' +
        'data mining case sets. The Integration Services approach<br>' +
        'has some advantages in that there are many transformation<br>' +
        'components built into the SSIS toolbox. Integration<br>' +
        'Services can also more easily pull data from external<br>' +
        'sources and in different formats and can be used to deposit<br>' +
        'the prepared case set wherever it is needed, in a variety of<br>' +
        'formats.<br>' +
        'Depending on the business opportunity and the data<br>' +
        'mining algorithms employed, creating the initial data sets<br>' +
        'often involves creating separate subsets of the data for<br>' +
        'different purposes. Table 13-2 lists two common data sets<br>' +
        'used for data mining. The Microsoft mining structure has<br>' +
        'the ability to automatically divide its input case set into<br>' +
        'training and test subsets. The data miner’s main task is to<br>' +
        'identify the appropriate source data sample to serve as the<br>' +
        'input case training set to the data mining process. In some<br>' +
        '823<br>' +
        'cases, the Percentage Sampling and Row Sampling tasks<br>' +
        'are particularly well suited to creating the input case set.<br>' +
        'Table 13-2: The primary data mining data sets<br>' +
        'Set Purpose<br>' +
        'Training Used as input to the algorithm to develop the initial model.<br>' +
        'Test Data not included in the training sets — often called holdout data.<br>' +
        'Used to verify the accuracy or effectiveness of the model.<br>' +
        'One last advantage Integration Services offers is that it<br>' +
        'allows you to build a package or project that contains all of<br>' +
        'the steps needed to prepare data for a given data mining<br>' +
        'project. Put this SSIS project under source control, and<br>' +
        're-use it to create new data sets to keep the model current.<br>' +
        'In our opinion, SSIS is the best choice for data mining data<br>' +
        'preparation. SQL plays a role in defining the initial<br>' +
        'extracts and some of the transformations, and will be part<br>' +
        'of any data preparation effort, but building the whole flow<br>' +
        'generally works best in Integration Services.<br>' +
        'Model Development<br>' +
        'The first step in developing the data mining model is to<br>' +
        'create the mining model structure in BIDS. The mining<br>' +
        'model structure is essentially a metadata layer that<br>' +
        'separates the data from the algorithms. The Data Mining<br>' +
        'Wizard creates the initial mining model structure, which<br>' +
        'can then be edited as needed.<br>' +
        'Once the mining structure is defined, the data miner builds<br>' +
        'as many mining models and versions as time allows, trying<br>' +
        'different algorithms, parameters, and variables to see<br>' +
        '824<br>' +
        'which combination yields the greatest impact or is most<br>' +
        'accurate. Usually this involves going back and redefining<br>' +
        'the data preparation task to add new variables or change<br>' +
        'existing transformations. These iterations are where SQL<br>' +
        'Server Data Mining shines. The flexibility, ease of use,<br>' +
        'range of algorithms, and integration with the rest of the<br>' +
        'SQL Server toolset allows the data miner to run through<br>' +
        'more variations than many other data mining environments<br>' +
        'in a given time period. Generally, the more variations<br>' +
        'tested, the better the final model.<br>' +
        'Model Validation (Evaluation)<br>' +
        'There are two kinds of model validation in data mining.<br>' +
        'The first involves comparing models created with different<br>' +
        'algorithms, parameters, and inputs to see which is most<br>' +
        'effective at predicting the target variable. The second is a<br>' +
        'business review of the proposed model to examine its<br>' +
        'contents and assess its value. We will look at both<br>' +
        'validation steps in this section.<br>' +
        'Comparing Models<br>' +
        'Creating the best data mining model is a process of<br>' +
        'triangulation. Attack the data with several algorithms like<br>' +
        'Decision Trees, Neural Network, and Naïve Bayes. You’d<br>' +
        'like to see several models point to similar results. This is<br>' +
        'especially helpful in those cases where the tool spits out an<br>' +
        'answer but doesn’t provide an intuitive foundation for why<br>' +
        'the answer was chosen. Neural Network models are<br>' +
        'notorious for this kind of result. Triangulation gives all the<br>' +
        'observers (especially end users and management)<br>' +
        'confidence that the predictions mean something.<br>' +
        '825<br>' +
        'Analysis Services Data Mining provides three tools for<br>' +
        'comparing the effectiveness of certain types of data mining<br>' +
        'models — a lift chart, a classification matrix, and cross<br>' +
        'validation. These can be found under the Mining Accuracy<br>' +
        'Chart tab in the Data Mining Designer. To use the<br>' +
        'Accuracy tab tools, first select the mining structure that<br>' +
        'supports the models you want to compare and join it to<br>' +
        'your test data set. The lift chart works in a couple of<br>' +
        'different ways, depending on the models being compared.<br>' +
        'The basic idea is to run the test cases through all of the<br>' +
        'models, and compare the predicted results with the known<br>' +
        'actual results from the test data set. The lift chart then plots<br>' +
        'the percentage of correct predictions for a given<br>' +
        'percentage of the overall test population, beginning with<br>' +
        'the most accurate portions of the model (the cases with the<br>' +
        'highest predicted probability). Figure 13-4 shows a lift<br>' +
        'chart that compares two simple models used to predict<br>' +
        'Income Range based on the other non-income<br>' +
        'demographics in the AdventureWorksDW Customer<br>' +
        'table.<br>' +
        'Figure 13-4: An example lift chart comparing two models<br>' +
        'designed to predict Income Range<br>' +
        '826<br>' +
        'The lines representing the two models are bounded by an<br>' +
        'upper limit that represents the best possible prediction. In<br>' +
        'the best case, the model would be 100 percent correct for<br>' +
        'whatever percent of the population it processed. The best<br>' +
        'case is represented by the heavy, straight line between 0<br>' +
        'and 100. The worst case would be a random guess.<br>' +
        'Because this model has only six possible values of<br>' +
        'predicted income ranges, a random guess would be right<br>' +
        '1/6 or 16.67 percent of the time. The Decision Trees model<br>' +
        'called Income-DT is clearly more predictive than the<br>' +
        'Naïve Bayes model called Income-NB. At the 100 percent<br>' +
        'point, the Decision Trees model accurately predicts 90.39<br>' +
        'percent of the cases while the Naïve Bayes has tailed off to<br>' +
        'only 72.28 percent of the cases. The Lift Chart tab also<br>' +
        'includes the ability to add in incremental costs, fixed costs,<br>' +
        '827<br>' +
        'and revenue to create a profit chart that helps determine<br>' +
        'the optimal percentage of the overall population to target.<br>' +
        'The second tool, called the classification matrix, is a<br>' +
        'matrix with the actual values of the data set on the columns<br>' +
        'and the values predicted by the model on the rows. Ideally,<br>' +
        'you’d like to see a single vector down the diagonal of the<br>' +
        'matrix with values, and the rest of the cells should be<br>' +
        'zeros. This would represent the outcome where all of the<br>' +
        'cases the model predicted to be in a certain Income Range<br>' +
        'actually were in that range. Figure 13-5 shows the<br>' +
        'classification matrices for the Decision Trees and Naïve<br>' +
        'Bayes models from Figure 13-4.<br>' +
        'In this example, the Naïve Bayes model clearly is incorrect<br>' +
        'more often than the Decision Trees model. For example,<br>' +
        'for cases where the actual range is 70,812.1–98,020.4 (the<br>' +
        'third data column), the Naïve Bayes model incorrectly<br>' +
        'predicts an Income Range of 39,272.8–70,812.1 for 105<br>' +
        'cases, while the Decision Trees model makes this error<br>' +
        'only 54 times.<br>' +
        'Figure 13-5: Example classification matrices for the<br>' +
        'Income Predictor models<br>' +
        '828<br>' +
        'The fourth Mining Accuracy Chart tab is called Cross<br>' +
        'Validation. It breaks up the input data set into a number of<br>' +
        'subsets and creates models based on those subsets for each<br>' +
        'of the mining models in the mining structure. It then<br>' +
        'generates a report with a set of statistics about the accuracy<br>' +
        'of each data subset within each mining model. The results<br>' +
        'are not so visually interesting to the average business<br>' +
        'person, but do have value to an experienced data miner/<br>' +
        'statistician.<br>' +
        'Unfortunately, the mining accuracy tools work only for<br>' +
        'single-valued results at this point. They don’t work for<br>' +
        'result lists, like recommendation lists. In those cases, you<br>' +
        'will need to build your own comparison tests using the<br>' +
        'Integration Services tasks or the Excel data mining add-in<br>' +
        'to query the mining model with the test data set and<br>' +
        '829<br>' +
        'compare the results. We show an example of this in one of<br>' +
        'the case studies later in this chapter.<br>' +
        'RESOURCES<br>' +
        'For more information about using lift<br>' +
        'charts, classification matrices, and cross<br>' +
        'validation, search for “Validating Data<br>' +
        'Mining Models” in SQL Server Books<br>' +
        'Online.<br>' +
        'Business Review<br>' +
        'The data mining phase ends with the selection of the<br>' +
        '“best” model based on its performance in the model<br>' +
        'comparison process and its implementation cost.<br>' +
        'Ultimately this is a business decision, so you need to<br>' +
        'review the contents and performance of the model with the<br>' +
        'business folks to make sure that it makes sense.<br>' +
        'Prepare for this review by carefully combing through the<br>' +
        'selected model to understand and document the rules it<br>' +
        'uses, the relationships it defines, and the impact you expect<br>' +
        'it to have. The various tabs in the Data Mining Designer<br>' +
        'are particularly helpful in developing this understanding.<br>' +
        'Or, consider using the model rendering template for Visio,<br>' +
        'which is part of the Office Data Mining Addins. Present<br>' +
        'this documentation to the business users and carefully<br>' +
        'walk them through the logic behind the recommended<br>' +
        'model. This presentation also includes evidence of the<br>' +
        'model’s performance from the Mining Accuracy Chart tab<br>' +
        '830<br>' +
        'tools and other sources. This helps all participants<br>' +
        'understand the expected impact of the model. Once the<br>' +
        'business review is complete, the next step is to move the<br>' +
        'model out into the real world.<br>' +
        'The Operations Phase<br>' +
        'The operations phase is where the rubber meets the road.<br>' +
        'At this point, you have the best possible model (given the<br>' +
        'time, data, and technology constraints) and you have<br>' +
        'business approval to proceed. Now you get to put it into<br>' +
        'production and see what kind of impact it has. The<br>' +
        'operations phase involves three main tasks:<br>' +
        'implementation, impact assessment, and maintenance.<br>' +
        'Implementation<br>' +
        'After all participants have approved the final model and<br>' +
        'the implementation plan, the team can move the model<br>' +
        'into production in the implementation task. Production can<br>' +
        'range from using the model once a quarter to assess the<br>' +
        'effectiveness of various promotions, to classifying<br>' +
        'customers as part of the nightly ETL process, to<br>' +
        'interactively making product recommendations as part of<br>' +
        'the web server or customer care transaction system.<br>' +
        'Each of these alternatives involves a different cast of<br>' +
        'characters. At one end of the spectrum, the quarterly<br>' +
        'update of the customer dimension may involve only the<br>' +
        'data miner and the ETL developer. At the other end of the<br>' +
        'spectrum, making online recommendations will clearly<br>' +
        'involve the production systems folks. And, depending on<br>' +
        'the transaction volume, they will likely want a production<br>' +
        '831<br>' +
        'Analysis Services server (or cluster) dedicated to providing<br>' +
        'recommendations. Moving the data mining model into<br>' +
        'production may also involve significant changes to the<br>' +
        'transaction system applications to incorporate the data<br>' +
        'mining query and results into the business process and user<br>' +
        'interface. This is usually a big deal. You must figure out<br>' +
        'who needs to be involved in the implementation task for<br>' +
        'your data mining model and let them know as early as<br>' +
        'possible, so they can help determine the appropriate<br>' +
        'timeframes and resources. Deploy in phases, starting with<br>' +
        'a test version, to make sure the data mining server doesn’t<br>' +
        'gum up the transaction process.<br>' +
        'Assess Impact<br>' +
        'Determining the impact of the data mining model can be<br>' +
        'high art. In some areas, like direct mail and web-based<br>' +
        'offers, the process of tuning and testing the marketing<br>' +
        'offers and collateral and the target prospect lists is often<br>' +
        'full-time work for a large team. They do test and control<br>' +
        'sets with different versions of the mailing before they do<br>' +
        'the full mass mailing. Even in the full campaign, often<br>' +
        'several phases with different versions and control sets are<br>' +
        'built in. The results of each phase help the team tweak<br>' +
        'subsequent phases for best results. One method, known as<br>' +
        'A/B testing, involves comparing the responses of people<br>' +
        'who are randomly selected into subgroups that receive<br>' +
        'different versions of an offer. A/B testing is a simple and<br>' +
        'powerful way to minimize the influence of external<br>' +
        'variables.<br>' +
        'In general, the data miner should adopt as much of this<br>' +
        'careful assessment process as possible.<br>' +
        '832<br>' +
        'Maintain the Model<br>' +
        'Almost all data mining models will need to be re-trained,<br>' +
        'or be completely rebuilt over some period of time. As the<br>' +
        'world changes, the behaviors that have been captured in<br>' +
        'the model become outdated. This is particularly noticeable<br>' +
        'in a fast changing industry like retail where new fashions,<br>' +
        'products, and models are announced on a daily basis. A<br>' +
        'recommendation engine that didn’t include the most recent<br>' +
        'behavior and latest models would be less than useful to the<br>' +
        'customer. In a case like this, the basic model structure may<br>' +
        'still apply, but the rules and relationships must be<br>' +
        're-generated based on new behavior data.<br>' +
        'Metadata<br>' +
        'In the best of all possible worlds, the final data mining<br>' +
        'model should be documented with a detailed history of<br>' +
        'how it came into being. What sources contributed to the<br>' +
        'case set? What kinds of transformations were applied to<br>' +
        'the variables, and at what points in the process were they<br>' +
        'applied? What was the initial model and what were the<br>' +
        'intermediate versions considered and discarded? What<br>' +
        'parameter values were used for which versions of the<br>' +
        'model? A professional data miner will want to know<br>' +
        'exactly what went into creating a model in order to explain<br>' +
        'its value, to avoid repeating the same errors, and to<br>' +
        're-create it if need be. The data miner should also keep<br>' +
        'track of how and when the model is used, and when it<br>' +
        'should be maintained.<br>' +
        'The problem with tracking the history of a mining model is<br>' +
        'that because Analysis Services makes it so easy to create<br>' +
        '833<br>' +
        'additional versions of the model, it takes much more time<br>' +
        'to document each iteration than it does to actually do the<br>' +
        'work. Nonetheless, you still need to keep track of what<br>' +
        'you have and where it came from. We recommend keeping<br>' +
        'a basic set of metadata to track the contents and derivation<br>' +
        'of all the transformed data sets and resulting mining<br>' +
        'models you decide to keep around. This can get much<br>' +
        'more complex if you like, but the simplest approach is to<br>' +
        'use a spreadsheet, as pictured in Figure 13-6.<br>' +
        'Figure 13-6: A simple spreadsheet for tracking data<br>' +
        'mining models<br>' +
        'NOTE<br>' +
        '834<br>' +
        'The data mining process takes place across<br>' +
        'the various DW/BI system platforms and<br>' +
        'relies on their security mechanisms. The<br>' +
        'data miner needs to have enough privileges<br>' +
        'on the data source servers to create new<br>' +
        'tables and/or cubes. Additionally, to create<br>' +
        'and modify data mining models, the data<br>' +
        'miner must be a member of the Analysis<br>' +
        'Services Administrators group on the data<br>' +
        'mining computer.<br>' +
        'Data Mining Examples<br>' +
        'For many of us, the best way to learn something is by<br>' +
        'doing it. This is especially true for technical folks, and<br>' +
        'even more so for data mining. You really need to work<br>' +
        'through the SQL Server basic data mining tutorial before<br>' +
        'you run through the following examples. This will give<br>' +
        'you a chance to get familiar with the tools and the user<br>' +
        'interface, which will make these examples easier to<br>' +
        'understand and follow. If you haven’t already done so,<br>' +
        'please take the time to work through the data mining<br>' +
        'tutorials now.<br>' +
        'In this section, we start with a simple example to get a feel<br>' +
        'for the data mining tool environment and the data mining<br>' +
        'process. In fact, the first example is so simple that its case<br>' +
        'set of economic data can be presented on a single page. It’s<br>' +
        'perfect to show the power and accessibility of the data<br>' +
        'mining add-ins for Microsoft Office Excel. Then we dig<br>' +
        'into a more detailed example based on the Adventure<br>' +
        '835<br>' +
        'Works Cycles data warehouse data. Both of these<br>' +
        'examples are based on business scenarios and will follow<br>' +
        'the general flow of the data mining process presented in<br>' +
        'the previous section.<br>' +
        'Case Study: Categorizing Cities<br>' +
        'This first example is a small, simplified problem designed<br>' +
        'to provide a clear understanding of the process. The data<br>' +
        'set has only 48 cases total — not nearly enough to build a<br>' +
        'robust data mining model. However, the small size allows<br>' +
        'us to examine the inputs and outputs to see if the resulting<br>' +
        'model makes sense. The scenario is based on a large<br>' +
        'non-governmental organization (NGO) with a mission and<br>' +
        'operations much like that of the World Bank:<br>' +
        '(T)o fight poverty and improve the living standards of<br>' +
        'people in the developing world. It is a development Bank<br>' +
        'that provides loans, policy advice, technical assistance and<br>' +
        'knowledge sharing services to low and middle income<br>' +
        'countries to reduce poverty. The Bank promotes growth to<br>' +
        'create jobs and to empower poor people to take advantage<br>' +
        'of these opportunities.<br>' +
        'http://web.worldbank.org/<br>' +
        'Categorizing Cities: Business Opportunity<br>' +
        'The data miner on the NGO’s DW/BI team held meetings<br>' +
        'with key directors and managers from around the<br>' +
        'organization to identify business opportunities that are<br>' +
        'supported by available data and can be translated into data<br>' +
        'mining opportunities. During these meetings it became<br>' +
        '836<br>' +
        'clear that there were not enough resources to properly<br>' +
        'customize the NGO’s programs. The NGO had historically<br>' +
        'focused its efforts to provide financial aid at the country<br>' +
        'level. Several economists felt that targeting policies and<br>' +
        'programs at the city level would be more effective,<br>' +
        'allowing for the accommodation of unique regional and<br>' +
        'local considerations that are not possible at the country<br>' +
        'level.<br>' +
        'A switch to the city level would mean the people who<br>' +
        'implemented the NGO’s programs would need to deal with<br>' +
        'potentially thousands of cities rather than the 208 countries<br>' +
        'they were working with around the world. This switch<br>' +
        'would significantly expand the complexities of managing<br>' +
        'the economic programs in an organization that was already<br>' +
        'resource limited.<br>' +
        'The group discussed the possibility of designing programs<br>' +
        'for groups of cities that have similar characteristics. If<br>' +
        'there were a relatively small number of city groups, the<br>' +
        'economic analysts felt it would be possible to design and<br>' +
        'manage much more appropriate economic programs.<br>' +
        'Categorizing Cities: Data Understanding<br>' +
        'During the meetings, there was discussion about what<br>' +
        'variables might be useful inputs. The analysts had a gut<br>' +
        'feel for what the city groups might be, and some initial<br>' +
        'guesses about which variables would be most important<br>' +
        'and how they could be combined. The group came up with<br>' +
        'a list of likely variables. The data miner combed through<br>' +
        'the organization’s databases to see if these inputs could be<br>' +
        'found, and to see what other relevant information was<br>' +
        '837<br>' +
        'available. This effort turned up 54 variables that the<br>' +
        'organization tracked, from total population to the number<br>' +
        'of fixed line and mobile phones per 1,000 people. This<br>' +
        'wealth of data seemed too good to be true, and it was.<br>' +
        'Further investigation revealed that much of this data was<br>' +
        'not tracked at the city level. In fact, there were only ten<br>' +
        'city-level variables available. The data miner reviewed this<br>' +
        'list with the business folks and the group determined that<br>' +
        'of the ten variables, only three were reliably measured.<br>' +
        'Fortunately, the group also felt these three variables were<br>' +
        'important measures of a city’s economic situation. The<br>' +
        'three variables were average hourly wages (local<br>' +
        'currency), average hours worked per year, and the average<br>' +
        'price index.<br>' +
        'At this point, the data miner wrote up a description of the<br>' +
        'data mining opportunity, its expected business impact, and<br>' +
        'an estimate of the time and effort it would take to<br>' +
        'complete. The project was described in two phases; the<br>' +
        'goal of the first phase was to develop a model that clusters<br>' +
        'cities based on the data available. If this model made sense<br>' +
        'to the business folks, phase two would use that model to<br>' +
        'assign new cities to the appropriate clusters as they contact<br>' +
        'the NGO for financial support. The data mining<br>' +
        'opportunity description was reviewed by the economists<br>' +
        'and all agreed to proceed.<br>' +
        'Categorizing Cities: Data Preparation<br>' +
        'The data miner reviewed the data set and realized it needed<br>' +
        'some work. First, the average hourly wages were in local<br>' +
        'currencies. After some research on exchange rates and<br>' +
        'discussion with the economists to decide on the<br>' +
        '838<br>' +
        'appropriate rates and timing, the data miner created a<br>' +
        'package to load and transform the data. The package<br>' +
        'extracted the data from the source system and looked up<br>' +
        'the exchange rate for each city and applied it to the wages<br>' +
        'data. Because the consumer price data was already indexed<br>' +
        'relative to Zurich, using Zurich as 100, this package also<br>' +
        'indexed the wage data in the same fashion. Finally, the<br>' +
        'package wrote the updated data set out to a separate table.<br>' +
        'The resulting data set to be used for training the cluster<br>' +
        'model, containing three economic measures for 46 cities<br>' +
        'around the world, is shown in Figure 13-7 (split into two<br>' +
        'tables in the figure for easier viewing). Note that the total<br>' +
        'data set has 48 cities, but we held out two cities to use later<br>' +
        'as examples in the implementation section.<br>' +
        'NOTE<br>' +
        'This data comes from the Economics<br>' +
        'Research Department of the Union Bank of<br>' +
        'Switzerland. The original set contains<br>' +
        'economic data from 48 cities around the<br>' +
        'globe in 1991. It has been used as example<br>' +
        'data for statistics classes and can be found<br>' +
        'on many websites. You can find this data<br>' +
        'set by searching for “cities prices and<br>' +
        'earnings economic (1991) globe.” You can<br>' +
        'find more current versions by searching for<br>' +
        '“UBS Prices and Earnings.” The current<br>' +
        'version includes 73 cities.<br>' +
        '839<br>' +
        'Observe that two of the cities do not have complete data.<br>' +
        'The data miner might opt to exclude these cities from the<br>' +
        'data set, or include them to see if the model can identify<br>' +
        'them as outliers. This would help spot any bad data that<br>' +
        'might come through in the future.<br>' +
        'Figure 13-7: The city economic dataset after data<br>' +
        'preparation<br>' +
        'Categorizing Cities: Model Development<br>' +
        'Because the goal in the first phase was to identify groups<br>' +
        'of cities that have similar characteristics, and those<br>' +
        'groupings were not predetermined, the data miner decided<br>' +
        'to use the Microsoft Clustering algorithm. Once the Data<br>' +
        '840<br>' +
        'Mining Add-in for Microsoft Office was installed, the data<br>' +
        'miner copied the data into a table in a worksheet, and then<br>' +
        'selected Create Mining Structure under the Advanced<br>' +
        'button in the Data Modeling section of the Data Mining<br>' +
        'Ribbon as shown in Figure 13-8. (The Cluster button<br>' +
        'would be a more obvious choice, but the add-in assumes<br>' +
        'you want to include the city name in the clustering process.<br>' +
        'The Advanced button requires a two step process, but it is<br>' +
        'still pretty easy.)<br>' +
        'This opened a wizard to create a new mining structure. The<br>' +
        'wizard asks for a source, defaulting to the data set in the<br>' +
        'local table. On the Select Columns screen, the data miner<br>' +
        'set the Usage of the RowID to Do Not Use, and set the<br>' +
        'usage of the CityName to Key. On the “Split data into<br>' +
        'training and testing sets” screen, the data miner set the<br>' +
        'percentage of data for testing to 0, so data from all the<br>' +
        'cities will be used to determine the clusters. The structure<br>' +
        'name was entered on the last screen, and then the data<br>' +
        'miner clicked the Finish button to write the new mining<br>' +
        'structure out to Analysis Services.<br>' +
        'Figure 13-8: Data Mining Ribbon in Excel<br>' +
        '841<br>' +
        'With the mining structure metadata layer in place, the data<br>' +
        'miner then selected the Advanced button again, and<br>' +
        'selected “Add model to structure.” A wizard asked which<br>' +
        'data mining technique should be used. The data miner<br>' +
        'chose Microsoft Clustering and specified the city name as<br>' +
        'the key and the other columns as input and changed the<br>' +
        'model name on the final wizard screen. After the wizard<br>' +
        'finished processing the model on Analysis Services, it<br>' +
        'opened up a model browser window and presented the<br>' +
        'cluster diagram shown in Figure 13-9.<br>' +
        'Figure 13-9: Cluster diagram for the city economic data<br>' +
        '842<br>' +
        'At first glance, this diagram isn’t that helpful. Without<br>' +
        'knowing anything about the problem, the Microsoft<br>' +
        'Clustering algorithm is limited to names like Cluster 1 and<br>' +
        'Cluster 2. At this point, the data miner can explore the<br>' +
        'model by using the other tabs in the model browser, and by<br>' +
        'using the drill-through feature to see which cities have<br>' +
        'been assigned to which nodes. The data miner can also use<br>' +
        'the other tabs in the model browser to examine the<br>' +
        'underlying rules and distributions. The data in Figure<br>' +
        '13-10 is grouped by cluster and was created by<br>' +
        'right-clicking each cluster in the model browser and<br>' +
        'selecting Drill Through Model Columns.<br>' +
        'Figure 13-10: City economic data listed by cluster<br>' +
        '843<br>' +
        'The clusters in Figure 13-10 make general sense given a<br>' +
        'basic understanding of the world economic situation in<br>' +
        '1991. Many of the poorer cities of South and Central<br>' +
        'America and Africa ended up in Cluster 2. Similar cities<br>' +
        'ended up in Cluster 5, except Cluster 5 has significantly<br>' +
        'more work hours on average. The two cities that had only<br>' +
        'price data, Cairo and Jakarta, are on their own in Cluster 8,<br>' +
        'and that cluster was not connected to the other seven in<br>' +
        '844<br>' +
        'Figure 13-9. Cluster 8 seems to be the data anomalies<br>' +
        'cluster.<br>' +
        'While you can make some sense of the raw tabular output,<br>' +
        'SQL Server also provides tools to improve your<br>' +
        'understanding. For example, the Cluster Profiles tab of the<br>' +
        'model browser shown in Figure 13-11 makes it easier to<br>' +
        'identify the characteristics of the individual clusters. The<br>' +
        'table shows the average value and range of each cluster for<br>' +
        'each of the three input variables. The clusters are listed<br>' +
        'from largest to smallest starting with the distribution for all<br>' +
        '46 cities. Cluster 1 includes cities whose wages and prices<br>' +
        'are relatively high, and have relatively low work hours.<br>' +
        'These are the cities that make up the day-to-day workforce<br>' +
        'of their respective countries — we might call this cluster<br>' +
        'the “Heartland” cities, although Paris might not like that<br>' +
        'name. The next cluster, Cluster 3, is even more upscale,<br>' +
        'with the highest average prices, second highest average<br>' +
        'wages, and the lowest work hours. Cluster 2, on the other<br>' +
        'hand, has extremely low wages, higher work hours than<br>' +
        'most, and relatively lower prices. This is similar to Cluster<br>' +
        '5, which has even higher work hours. These are the newly<br>' +
        'developing cities where labor is cheap and people must<br>' +
        'work long hours to survive. You might call this cluster the<br>' +
        '“Hard Knock Life” cities. The NGO would likely call it<br>' +
        'something a bit more politically correct, like the<br>' +
        '“Developing Cities” cluster. As we mentioned earlier,<br>' +
        'good names can help crystallize the defining<br>' +
        'characteristics of each cluster.<br>' +
        'Figure 13-11: The Cluster Profiles tab of the model<br>' +
        'browser<br>' +
        '845<br>' +
        'Categorizing Cities: Model Validation<br>' +
        'Some data mining models are easier to test than others. For<br>' +
        'example, when you’re building a prediction model, you<br>' +
        'can create it based on one historical data set and test it on<br>' +
        'another. Having historical data means you already know<br>' +
        'the right answer, so you can feed the data through the<br>' +
        'model, see what value it predicts, and compare it to what<br>' +
        'actually happened.<br>' +
        'In the City cluster example, validating the model is not so<br>' +
        'straightforward. You don’t know the categorization ahead<br>' +
        'of time, so you can’t compare the assigned cluster with the<br>' +
        'right cluster — there is no such thing as “right” in this<br>' +
        'case. Validation of this model came in two stages. First,<br>' +
        'the data miner went back to the economic analysts and<br>' +
        'reviewed the model and the results with them. This was a<br>' +
        'reasonableness test where the domain experts compared<br>' +
        'the model with their own understanding of the problem.<br>' +
        'The second test came when the model was applied to the<br>' +
        'original business problem. In this case, the team monitored<br>' +
        '846<br>' +
        'the categorization of new cities to make sure they<br>' +
        'continued to stand up to the reasonableness test.<br>' +
        'Ultimately, the question was “Do the categorizations seem<br>' +
        'right, and do they help simplify how the organization<br>' +
        'works at the city level?” Is it easier and more effective to<br>' +
        'work with eight clusters rather than 48 (or 4,800) cities?<br>' +
        'Categorizing Cities: Implementation<br>' +
        'Once the team decided the model would work, or at least<br>' +
        'that it was worth testing in an operational environment, it<br>' +
        'was time to move it into production. This can mean a<br>' +
        'whole range of activities, depending on the nature of the<br>' +
        'model and the business process to which it’s being applied.<br>' +
        'In this city categorization example, new economic data at<br>' +
        'the city level can arrive at any time. While is it easy to<br>' +
        'assign new cities to clusters using the Excel data mining<br>' +
        'add-in, the team decided to have this new data entered into<br>' +
        'a table in the database and assign clusters to it in a batch<br>' +
        'process once a night.<br>' +
        'The data miner wrote a simple Integration Services<br>' +
        'package that reads in unassigned cities, submits them to<br>' +
        'the data mining model for cluster assignment, and writes<br>' +
        'the full record out to a table called CityMaster with a<br>' +
        'batch date identifying when the cluster was assigned.<br>' +
        'Figure 13-12 shows what the data flow for this simple<br>' +
        'package might look like. This flow has a data viewer<br>' +
        'inserted right after the Data Mining Query transformation<br>' +
        'showing its output. Compare the cluster assignments for<br>' +
        'Kuala Lumpur (Cluster 5), and Lisbon (Cluster 6), with the<br>' +
        'other cities in those clusters in Figure 13-10. Do these<br>' +
        'assignments pass the reasonableness test?<br>' +
        '847<br>' +
        'Implementation would also integrate this package into the<br>' +
        'rest of the nightly ETL process. The package should<br>' +
        'include the standard data and process audit functions<br>' +
        'described in Chapter 7. Ultimately, the process should be<br>' +
        'part of the package that manages the City dimension.<br>' +
        'This nightly data mining batch process is a common one in<br>' +
        'many DW/BI systems, using a data mining model to<br>' +
        'populate a calculated field like a Default Risk score across<br>' +
        'all loan records or a Credit Rating across all customers.<br>' +
        'These scores can change depending on customer<br>' +
        'behaviors, like late payments or deposit balances. Every<br>' +
        'loan and customer may have to be re-scored every night.<br>' +
        'The same batch process can be used on a one-time basis to<br>' +
        'address opportunities like identifying customers who are<br>' +
        'likely to respond to a new product offering or who are at<br>' +
        'risk of canceling their service.<br>' +
        'NOTE<br>' +
        'The Integration Services Data Mining<br>' +
        'Query transformation is an Enterprise<br>' +
        'Edition feature.<br>' +
        'Figure 13-12: An Integration Services package to assign<br>' +
        'clusters to new cities<br>' +
        '848<br>' +
        'Categorization Cities: Maintenance and Assessment<br>' +
        'As the data changes, the data mining model will likely<br>' +
        'change as well. In this case, the data mining model should<br>' +
        'be re-built as existing data is updated or data about<br>' +
        'additional cities is collected. The team would then review<br>' +
        'the resulting clusters to make sure they make sense from a<br>' +
        'business perspective. Given the nature of the business<br>' +
        'process and the changing data, this review should probably<br>' +
        'happen on a regularly scheduled basis — perhaps monthly<br>' +
        'to start, then quarterly as the model and data stabilize.<br>' +
        'This example shows how clustering can be used to<br>' +
        'improve an organization’s business processes. Although<br>' +
        'this data set is too small to produce reliable results, it does<br>' +
        'generate a model that makes sense and shows how that<br>' +
        'model can be applied to new incoming data.<br>' +
        'Case Study: Product Recommendations<br>' +
        '849<br>' +
        'The ability to recommend products that might be<br>' +
        'particularly interesting to a given customer can have a<br>' +
        'huge impact on how much your customers purchase. This<br>' +
        'example follows the data mining process from identifying<br>' +
        'the business requirements through to the implementation<br>' +
        'of a product recommendation data mining model. It’s<br>' +
        'based on the SQL Server Adventure Works Cycles data<br>' +
        'set. Recall from Chapter 1 that Adventure Works Cycles is<br>' +
        'a manufacturer, wholesaler, and internet retailer of<br>' +
        'bicycles and accessories.<br>' +
        'The SQL Server data mining tutorial steps you through the<br>' +
        'process of building a model that predicts whether or not<br>' +
        'someone will be a bike buyer. While this is interesting<br>' +
        'information, it doesn’t help you figure out what to display<br>' +
        'on the web page. Even if you’re pretty certain someone<br>' +
        'will be a bike buyer, you don’t know which bike to show<br>' +
        'them. Also, what products should you show all those folks<br>' +
        'who are not bike buyers? Our goal in this example is to<br>' +
        'create a data mining model that produces a custom list of<br>' +
        'specific products that you can show to any given website<br>' +
        'visitor based on demographic information provided by the<br>' +
        'visitor. To the extent that the custom product list is more<br>' +
        'appealing than a random list of products, the visitor is<br>' +
        'more likely to make a purchase.<br>' +
        'In this example, we try to give you enough information to<br>' +
        'work through the model creation process yourself. It’s not<br>' +
        'a complete step-by-step tutorial, but if you’ve already<br>' +
        'worked through the SQL Server data mining tutorial, you<br>' +
        'should be able to follow along and see how it works<br>' +
        'firsthand.<br>' +
        '850<br>' +
        'Product Recommendations: The Business Phase<br>' +
        'Recall that the business phase of the data mining process<br>' +
        'involves identifying business opportunities, and building<br>' +
        'an understanding of the available data and its ability to<br>' +
        'support the data mining process. The Adventure Works<br>' +
        'Cycles data is not real, but it is more complete than many<br>' +
        'real-world systems we’ve seen, and it has more than<br>' +
        'enough customers and purchases to be interesting.<br>' +
        'Product Recommendations: Business Opportunities<br>' +
        'In many companies, the DW/BI team has to sell the idea of<br>' +
        'incorporating data mining into the business processes. This<br>' +
        'is usually either because the business folks don’t<br>' +
        'understand the potential or because the business will have<br>' +
        'to change the transaction system, which is a big and scary<br>' +
        'task. Sometimes, the team gets lucky and data mining<br>' +
        'starts with a request from the business community. This is<br>' +
        'how it worked in the Adventure Works Cycles example.<br>' +
        'One of the folks in marketing who is responsible for<br>' +
        'e-commerce marketing came to the DW/BI team asking<br>' +
        'for ways to boost web sales. Web sales accounted for<br>' +
        'about $9,000,000 in the first half of 2008, or about<br>' +
        'one-third of Adventure Works Cycles total sales. The<br>' +
        'marketing group has created a three-part strategy for<br>' +
        'growing the online business: Bring in more visitors<br>' +
        '(Attract), turn more visitors into customers (Convert), and<br>' +
        'develop long-term relationships with customers (Retain).<br>' +
        'The marketing person who came to the DW/BI team is<br>' +
        'responsible for the Convert strategy — that is, for<br>' +
        'converting visitors to customers.<br>' +
        '851<br>' +
        'TIP<br>' +
        'In a case like this, if the marketing person<br>' +
        'has a PowerPoint presentation that goes<br>' +
        'into detail on the marketing strategy, the<br>' +
        'data miner should review it. We feel sure<br>' +
        'they have such a presentation.<br>' +
        'Because the marketing person is responsible only for<br>' +
        'conversion, the team will investigate alternatives for<br>' +
        'increasing the conversion of visitors to customers. They<br>' +
        'will check to see if the final model also increases the<br>' +
        'average dollars per sale as a beneficial side effect. After<br>' +
        'some discussion, the team decided that influencing<br>' +
        'purchasing behavior (conversion) with relevant<br>' +
        'recommendations is likely to be the best way to achieve<br>' +
        'their goals. They translated the idea of recommendations<br>' +
        'into specifics by deciding to dedicate one section of the<br>' +
        'left-hand navigation bar on the e-commerce website to<br>' +
        'hold a list of six recommended products that will be<br>' +
        'tailored to the individual visitor. While the marketing<br>' +
        'person investigated the level of effort required to make this<br>' +
        'change on the website, the data miner dug into the<br>' +
        'availability of relevant data.<br>' +
        'Product Recommendations: Data Understanding<br>' +
        'After some research, the data miner discovered that when<br>' +
        'visitors come to the Adventure Works Cycles website, they<br>' +
        'are asked to fill out an optional demographics form (“to<br>' +
        'better serve them”). A few queries revealed that about<br>' +
        '852<br>' +
        'two-thirds of all visitors actually do fill out the form.<br>' +
        'Because the form is a required part of the purchase<br>' +
        'process, the information is also available for all customers.<br>' +
        'In either case, this demographic information is placed in a<br>' +
        'database and in cookies in the visitor or customer’s<br>' +
        'browser. The DW/BI system also has all historical<br>' +
        'purchasing behavior for each customer at the individual<br>' +
        'product and line item level. Based on this investigation, the<br>' +
        'data miner felt that sufficient data was available to create a<br>' +
        'useful mining model for product recommendations.<br>' +
        'The data miner knew from experience that demographic<br>' +
        'based models are generally not as predictive as behavior<br>' +
        'based models (like purchases or page views). However,<br>' +
        'there were several opportunities to make recommendations<br>' +
        'where no product related behavioral data was available but<br>' +
        'demographic data was available. As a result, the data<br>' +
        'miner believed that two data mining models might be<br>' +
        'appropriate: one to provide recommendations on the home<br>' +
        'page and any non-product pages, and one to provide<br>' +
        'recommendations on any product related pages. The first<br>' +
        'model would be based on demographics and would be<br>' +
        'used to predict what a visitor might be interested in given<br>' +
        'their demographic profile. The second model would be<br>' +
        'based on product interest as indicated by the product<br>' +
        'associated with each web page they visit or any products<br>' +
        'added to their cart.<br>' +
        'At this point, the data miner wrote up a Data Mining<br>' +
        'Opportunity document to capture the goals, decisions, and<br>' +
        'approach. The overall business goal was to increase<br>' +
        'conversion rates with an ancillary goal of increasing the<br>' +
        'average dollars per sale. The strategy was to offer products<br>' +
        '853<br>' +
        'that have a higher probability of being interesting to any<br>' +
        'given visitor to the website. This strategy breaks down into<br>' +
        'two separate data mining models, one based on<br>' +
        'demographics and one based on product purchases. This<br>' +
        'decision was considered a starting point with the<br>' +
        'understanding that it would likely change during the data<br>' +
        'mining phase. This example goes through the creation of<br>' +
        'the demographics-based model. The product-based model<br>' +
        'is left as an exercise for the reader.<br>' +
        'The team also agreed on metrics to measure the impact of<br>' +
        'the program. They would compare before and after data,<br>' +
        'looking at the change in the ratio of the number of new<br>' +
        'customers (conversions) to the total unique visitor count in<br>' +
        'the same time periods. They would also examine the<br>' +
        'change in the average shopping cart value at the time of<br>' +
        'checkout. A third impact measure would be to analyze the<br>' +
        'web logs to see how often customers viewed and clicked<br>' +
        'on a recommended link. Some of these comparisons could<br>' +
        'be done by randomly assigning visitors either to a server<br>' +
        'on the web farm that offers recommendations or to other<br>' +
        'servers that are unchanged. This A/B comparison evens<br>' +
        'out the influence that any external factor, such as a big<br>' +
        'holiday, may have.<br>' +
        'Product Recommendations: The Data Mining Phase<br>' +
        'With the opportunity document as a guide, the data miner<br>' +
        'began with the demographics-based model. This section<br>' +
        'follows the development of the model from data<br>' +
        'preparation to model development and validation.<br>' +
        'Product Recommendations: Data Preparation<br>' +
        '854<br>' +
        'The data miner decided the data source should be the<br>' +
        'AdventureWorksDW2008R2 relational database. The<br>' +
        'advantage of sourcing the data from the data warehouse is<br>' +
        'that it has already been through a rigorous ETL process<br>' +
        'where it was cleaned, transformed, and aligned to meet<br>' +
        'basic business needs. While this is a good starting point,<br>' +
        'it’s often not enough for data mining.<br>' +
        'The data miner’s first step was to do a little data<br>' +
        'exploration. This involved running some data profiling<br>' +
        'reports and creating some queries that examined the<br>' +
        'contents of the source tables in detail. The goal is to relate<br>' +
        'customer information to product purchases based on the<br>' +
        'theory that if someone bought something, they must have<br>' +
        'had an interest in it. Customers have a one-to-many<br>' +
        'relationship with purchases, which results in two levels of<br>' +
        'granularity to the case sets. The demographic case set is<br>' +
        'generally made up of one row per observation: in this<br>' +
        'example, one row per customer. Each row has the<br>' +
        'customer key and all available demographics and other<br>' +
        'derived fields that might be useful. The product sales<br>' +
        'nested table is at a lower level of detail, involving<br>' +
        'customers and the products they bought. This is a master/<br>' +
        'detail relationship, where each row in this case set has the<br>' +
        'customer key and the product model name<br>' +
        '(DimProduct.ModelName) of the purchased product<br>' +
        'along with any other information that might be useful.<br>' +
        'Thus, each customer case set row has a one-to-many<br>' +
        'relationship with the product sales case set (called a nested<br>' +
        'case set). You could create a single case set by joining the<br>' +
        'demographics and sales together up front and creating a<br>' +
        'denormalized table, but we prefer to rely on the data<br>' +
        'mining structure to do that for us.<br>' +
        '855<br>' +
        'After reviewing the source data in the<br>' +
        'AdventureWorksDW database, the data miner decided to<br>' +
        'pull the demographic case data from the DimCustomer<br>' +
        'table and combine it with other descriptive information<br>' +
        'from the DimGeography and DimSalesTerritory<br>' +
        'tables, and to pull the product purchasing case data from<br>' +
        'the InternetSalesFact table along with some product<br>' +
        'description fields from DimProduct and related tables.<br>' +
        'The data exploration also helped the data miner identify<br>' +
        'several additional transformations that might be useful in<br>' +
        'creating the data mining model. These are shown in Table<br>' +
        '13-3.<br>' +
        'Table 13-3: Additional transformations used to create the<br>' +
        'example case set<br>' +
        'Transformation Purpose<br>' +
        'Convert<br>' +
        'BirthDate to<br>' +
        'Age<br>' +
        'Reduce the number of distinct values by moving from day<br>' +
        'to year, and provide a more meaningful value (Age) versus<br>' +
        'YearOfBirth.<br>' +
        'Calculate<br>' +
        'YearsAsCust<br>' +
        'DATEDIFF the DateFirstPurchase from the<br>' +
        'GetDate() to determine how many years each case has<br>' +
        'been a customer. This may help as an indicator of<br>' +
        'customer loyalty.<br>' +
        'Create bins for<br>' +
        'YearlyIncome<br>' +
        'Create a discrete variable called IncomeGroup to use as<br>' +
        'input to algorithms that cannot handle continuous data.<br>' +
        'Note: This is optional when creating the case set because<br>' +
        'binning can also be accomplished in the Mining Structure<br>' +
        'tab, or through the Data Mining Wizard.<br>' +
        'The size of the case set depends on the nature of the<br>' +
        'model. In this example, we are really building a model for<br>' +
        'each product model in the product dimension, so we need<br>' +
        'enough data to support up to 119 data mining models, not<br>' +
        'just one. After a bit of customer count experimentation, the<br>' +
        '856<br>' +
        'data miner decided to build an Integration Services<br>' +
        'package to randomly select 18,000 customers from the<br>' +
        'customer dimension. As a critical part of data preparation,<br>' +
        'the data miner recognized the need to split the historical<br>' +
        'data set into two subsets: one to train the initial model, and<br>' +
        'one to test its effectiveness. SQL Server 2008’s mining<br>' +
        'structure metadata layer can be set to automatically select a<br>' +
        'random subset of the input data set to use as a test set. The<br>' +
        'data flow shown in Figure 13-13 is the part of this package<br>' +
        'that selects the customers, adds the derived columns, and<br>' +
        'writes them out to a table called DMCustInput.<br>' +
        'Figure 13-13: An Integration Services data flow to create<br>' +
        'the input data set<br>' +
        'TIP<br>' +
        '857<br>' +
        'This approach works for the Adventure<br>' +
        'Works Cycles customer data set because<br>' +
        'there are only 18,484 customers total. If<br>' +
        'you have millions of customers, you might<br>' +
        'look for a more efficient way to extract<br>' +
        'your data mining input set. One possible<br>' +
        'approach is to use the last few digits of the<br>' +
        'customer key (as long as it is truly<br>' +
        'random). For example, a WHERE clause<br>' +
        'limiting the last two digits to “42” will<br>' +
        'return a 1 percent subset.<br>' +
        'Another data flow later in this package joins the selected<br>' +
        'customer input set to the FactInternetSales table and<br>' +
        'writes their purchases out to a table called DMCustPurch.<br>' +
        'This is the nested product data set. Depending on how<br>' +
        'rapidly the product list changes, it might make sense to<br>' +
        'limit the data sets to only those products that have been<br>' +
        'purchased in the last year and their associated customers.<br>' +
        'You can see the tables for the input data set and the nested<br>' +
        'product data set in the Data Source View in Figure 13-14.<br>' +
        'Figure 13-14: The product recommendation input data<br>' +
        'sets, presented as a Data Source View<br>' +
        '858<br>' +
        'The nested product case set has one or more rows for each<br>' +
        'customer. Just the fact that someone with a certain set of<br>' +
        'demographics bought a certain product is all the<br>' +
        'information you need. Notice from Figure 13-14 that the<br>' +
        'data miner decided to include some additional fields from<br>' +
        'the orders fact table that will not play a role in making<br>' +
        'recommendations but may be helpful in troubleshooting<br>' +
        'the data set.<br>' +
        'TIP<br>' +
        'Integration Services makes it easy to create<br>' +
        'separate physical tables of the exact data<br>' +
        'sets and relationships at a point in time.<br>' +
        'You can then use these tables to build and<br>' +
        'test many different mining models over a<br>' +
        'period of time without tainting the process<br>' +
        'with changing data. It’s common to set up a<br>' +
        '859<br>' +
        'separate database or even a separate server<br>' +
        'to support the data mining process and<br>' +
        'keep the production databases clear of the<br>' +
        'toxic tailings of data mining debris.<br>' +
        'The SQL Server Data Mining Tutorial uses<br>' +
        'views to define the case sets. This helps<br>' +
        'keep the proliferation of physical tables in<br>' +
        'the database to a minimum, but it’s not our<br>' +
        'preferred approach. The views need to be<br>' +
        'defined carefully; otherwise their contents<br>' +
        'will change as the data in the underlying<br>' +
        'database is updated nightly. It also creates<br>' +
        'the burden of re-executing potentially<br>' +
        'complex, full table scans every time the<br>' +
        'model is re-processed.<br>' +
        'At this point, the data miner has enough data in the proper<br>' +
        'form to move on to the data mining model development<br>' +
        'process.<br>' +
        'Product Recommendations: Model Development<br>' +
        'The data miner began the model development process by<br>' +
        'creating a new Analysis Services project in BIDS called<br>' +
        'Data Mining Projects. She added a data source that pointed<br>' +
        'to the DMWorkingDB relational database created to store<br>' +
        'data mining data sets. She then created a Data Source<br>' +
        'View that included the two tables created in the Integration<br>' +
        'Services package: DMCustInput and DMCustPurch. After<br>' +
        'adding the relationships between the tables, the project and<br>' +
        '860<br>' +
        'Data Source View looked like the screen capture shown in<br>' +
        'Figure 13-14. The keys shown in the tables are logical<br>' +
        'primary keys assigned in the Data Source View.<br>' +
        'Next, the data miner used the Data Mining Wizard to<br>' +
        'create a new mining structure by right-clicking the Mining<br>' +
        'Structures folder and selecting New Mining Structure. The<br>' +
        'data is coming from an existing relational data warehouse,<br>' +
        'and the data miner chose the Microsoft Decision Trees<br>' +
        'data mining technique in order to predict the probability of<br>' +
        'purchasing a given product based on a set of known<br>' +
        'attributes (the demographics from the registration process).<br>' +
        'In the Specify Table Types dialog window of the wizard,<br>' +
        'the data miner checked DMCustInput as the Case table<br>' +
        'and DMCustPurch as the Nested table.<br>' +
        'The Specify the Training Data window can be a bit tricky<br>' +
        'because it’s unclear what columns should be used in what<br>' +
        'ways. For this model, you would check the input column<br>' +
        'of all of the demographic variables because they are the<br>' +
        'independent variables — you’re trying to use known<br>' +
        'demographics such as age or gender to predict an interest<br>' +
        'in products.<br>' +
        'Also, you need to correctly specify which column or<br>' +
        'columns you’d like to predict. These are product related<br>' +
        'and can be found in the nested DMCustPurch table. First<br>' +
        'you need to specify a key for the nested data set.<br>' +
        'In this example, ProductModel is the appropriate key for<br>' +
        'the DMCustPurch data, although it’s not enforced in the<br>' +
        'creation of the table. You can now specify which column<br>' +
        'or columns to predict. ProductModel is the obvious<br>' +
        '861<br>' +
        'choice because it contains the description of the products<br>' +
        'to recommend. The data miner also included<br>' +
        'EnglishProductCategoryName as a predicted column<br>' +
        'because it groups the product models and makes it easier<br>' +
        'to navigate later on in the Model Viewer. Finally, the data<br>' +
        'miner did not include the quantity and amount fields<br>' +
        'because they are not relevant to this model. Remember,<br>' +
        'these are the nested purchases for each customer case.<br>' +
        'With a new visitor, you’ll know their demographics and<br>' +
        'can use that as input to the model, but you won’t have any<br>' +
        'purchase information so it makes no sense to include it as<br>' +
        'available input data. The bottom section of the completed<br>' +
        'Specify the Training Data window is illustrated in Figure<br>' +
        '13-15.<br>' +
        'The next step in the Data Mining Wizard is meant to<br>' +
        'specify the content and data types of the columns in the<br>' +
        'mining structure. The data miner accepted the defaults at<br>' +
        'this point and went to the next screen, called Create<br>' +
        'Testing Set. By entering 10 percent, the data miner set the<br>' +
        'mining structure to automatically hold out 1,800 rows from<br>' +
        'the model training process to use as test data. On the final<br>' +
        'screen, the data miner changed the mining structure name<br>' +
        'to ProductRecs1, and the mining model name to<br>' +
        'ProductRecs1-DT (for Decision Trees). When the Finish<br>' +
        'button was clicked, the wizard completed the creation of<br>' +
        'the mining structure and the definition of the Decision<br>' +
        'Trees data mining model. The data miner could then view<br>' +
        'and verify the model definitions by selecting the Mining<br>' +
        'Models tab.<br>' +
        'Figure 13-15: The nested table portion of the Specify the<br>' +
        'Training Data window<br>' +
        '862<br>' +
        'The next step is to deploy and process the model.<br>' +
        'Typically, a data miner works with one model at a time to<br>' +
        'avoid the overhead of processing all the models in the<br>' +
        'project (even though there is only one at this point, there<br>' +
        'will be more).<br>' +
        'TIP<br>' +
        '863<br>' +
        'To deploy and process the model, select<br>' +
        'any column in the ProductRecs1-DT<br>' +
        'model, right-click, and select Process<br>' +
        'Model. Select Yes to build and deploy the<br>' +
        'project, Run at the bottom of the Process<br>' +
        'Mining Model window, Close in the<br>' +
        'Process Progress window (when it’s<br>' +
        'finished), and finally Close back in the<br>' +
        'Process Mining Model window. This<br>' +
        'should all look very familiar if you’ve built<br>' +
        'Analysis Services cubes before (since<br>' +
        'that’s exactly what you are doing).<br>' +
        'The Decision Trees algorithm generates a separate tree for<br>' +
        'each predicted value (each product model), determining<br>' +
        'which variables historically have had a relationship with<br>' +
        'the purchase of that particular product. This means it will<br>' +
        'build 40 trees, one for each distinct value of the predicted<br>' +
        'variable, ProductModel, found in the DMCustPurch<br>' +
        'table. Once the processing is complete, the data miner is<br>' +
        'finally able to explore the results. Selecting the Model<br>' +
        'Viewer tab automatically brings up the currently selected<br>' +
        'mining model in the appropriate viewer — in this case, the<br>' +
        'Decision Trees sub-tab of the Microsoft Tree Viewer. The<br>' +
        'tree that appears is for the first item alphabetically in the<br>' +
        'predicted results set: the tree for the All-Purpose Bike<br>' +
        'Stand, which seems to have a slight bias toward females<br>' +
        '(at least in our random subset). Selecting a more<br>' +
        'interesting product like the Mountain-200 mountain bike<br>' +
        'from the Tree pull-down menu at the top of the window<br>' +
        '864<br>' +
        'brings up a more interesting tree — or at least one with<br>' +
        'more nodes. Figure 13-16 shows an example of the<br>' +
        'Mountain-200 decision tree later in the data mining<br>' +
        'process.<br>' +
        'The first time you run through this example on your own<br>' +
        'machine, you will notice that the first split in the initial<br>' +
        'Mountain-200 tree is on DateFirstPurchase, and then<br>' +
        'several other fields come into play at each of the<br>' +
        'sub-branches. The data miner immediately recognized a<br>' +
        'problem with this model. The DateFirstPurchase field<br>' +
        'was included in the case set inadvertently because it is an<br>' +
        'attribute of the customer dimension. However, it’s not a<br>' +
        'good choice for an input field for this model because<br>' +
        'visitors who have not been converted to customers will not<br>' +
        'have a DateFirstPurchase by definition. Even worse,<br>' +
        'after looking at several trees for other bicycle products, it<br>' +
        'is clear that DateFirstPurchase is always a strong<br>' +
        'splitter — perhaps because the longer someone has been a<br>' +
        'customer, the more products they have purchased, and the<br>' +
        'more likely they are to have purchased a bike. If you<br>' +
        'included the YearsAsCust field, you will notice the same<br>' +
        'problem because it is a function of DateFirstPurchase,<br>' +
        'and contains essentially the same information. The data<br>' +
        'miner decided to remove these fields from the model and<br>' +
        'reprocess it.<br>' +
        'One easy way to do this is to delete the fields from the<br>' +
        'Mining Structure by right-clicking the field and selecting<br>' +
        'Delete. The more cautious way is to leave them in the<br>' +
        'structure, but remove them from the mining model by<br>' +
        'changing their type from Input to Ignore in the dropdown<br>' +
        'menu on each field. This keeps the fields in the mining<br>' +
        '865<br>' +
        'structure, just in case. After changing the type to Ignore on<br>' +
        'these two fields and reprocessing the model, the decision<br>' +
        'tree for the Mountain-200 now looks something like the<br>' +
        'one shown in Figure 13-16.<br>' +
        'After exploring the model in the Decision Tree tab for a<br>' +
        'bit, it’s useful to switch over to the Dependency Network<br>' +
        'tab. This tool provides a graphical way to see which<br>' +
        'demographic variables are predictive of which product<br>' +
        'models. The dependency network shows the relationships<br>' +
        'between the input variables and the predicted variables.<br>' +
        'But the meaning of the initial view of the dependency<br>' +
        'network for this example, shown in Figure 13-17, is not<br>' +
        'immediately obvious. Each node in the network stands for<br>' +
        'one of the variables or products in the mining model. Some<br>' +
        'input variables are predictive of many product models,<br>' +
        'others only a few. Because the model contains 19 input<br>' +
        'variables and 40 individual product models and so many<br>' +
        'relationships among those variables, the dependency<br>' +
        'network looks like a spider web. In fact, the viewer<br>' +
        'automatically limits the number of nodes it will display so<br>' +
        'as not to overwhelm the user.<br>' +
        'Figure 13-16: The Mountain-200 decision tree<br>' +
        '866<br>' +
        'Figure 13-17: The default Dependency Network drawing<br>' +
        'for the ProductRecs1 Decision Trees model<br>' +
        '867<br>' +
        'Fortunately, there’s more to the Dependency Network tab<br>' +
        'than just this view. Zooming in to see the actual names of<br>' +
        'the variables is a good way to start. Selecting a node<br>' +
        'highlights the nodes with which it has relationships. The<br>' +
        'tool uses color and arrow directions to show the nature of<br>' +
        'those relationships. Finally, the slider on the left of the<br>' +
        'pane allows the user to limit the number of relationships<br>' +
        'shown based on the strength of the relationship. The<br>' +
        'default in this viewer is to show all the relationships.<br>' +
        'Moving the slider down toward the bottom of the pane<br>' +
        'removes the weakest relationships in ascending order of<br>' +
        'strength.<br>' +
        'Working with the Tree Viewer<br>' +
        'It’s worth taking a few minutes to discuss the<br>' +
        'elements of the tree viewer and how to work with<br>' +
        'it. The main pane where the picture of the tree is<br>' +
        'presented holds a lot of information. Starting with<br>' +
        'the parameter settings at the top of the Decision<br>' +
        'Tree tab in Figure 13-16, you can see the<br>' +
        'Mountain-200 is the selected tree. (Okay, you can’t<br>' +
        'really see the whole description unless you select<br>' +
        'the dropdown menu. This is an argument in favor<br>' +
        'of using very short names for your case tables and<br>' +
        'columns.) The Default Expansion parameter box to<br>' +
        'the right shows that you’re viewing All Levels of<br>' +
        'the tree — in this case, there are four levels but<br>' +
        'only three are visible on the screen. Some trees<br>' +
        'have too many levels to display clearly so the<br>' +
        '868<br>' +
        'Default Expansion control lets you limit the<br>' +
        'number of levels shown.<br>' +
        'The tree itself is made up of several linked boxes<br>' +
        'or nodes. Each node is essentially a class with<br>' +
        'certain rules that would determine whether<br>' +
        'someone belongs in that class. (This is how<br>' +
        'decision trees can be used for classification.) The<br>' +
        'shading of each node in Figure 13-16 is determined<br>' +
        'by the number of people who have Mountain-200s<br>' +
        'in that node divided by the total number of people<br>' +
        'who meet the rules for that node. The darker the<br>' +
        'node, the higher the probability that a person<br>' +
        'classified in that node owns a Mountain-200. You<br>' +
        'can change the shading calculation to divide by the<br>' +
        'total number of cases to show the overall<br>' +
        'distribution of cases. Then, the darkest nodes<br>' +
        'would have the most Mountain-200 bikes,<br>' +
        'regardless of the probability calculation.<br>' +
        'Selecting a node reveals the counts and<br>' +
        'probabilities for that node along with its<br>' +
        'classification rules. In Figure 13-16, the node at the<br>' +
        'top of the second column labeled Yearly Income<br>' +
        '>= 122000 has been selected. As a result, its values<br>' +
        'and rules are displayed in the Mining Legend<br>' +
        'window in the lower right of the screen. We see<br>' +
        'that 697 cases meet the classification rules for this<br>' +
        'node. Of these 697 cases, 241 have Mountain-200<br>' +
        'bikes, which results in a probability of 241/697 =<br>' +
        '869<br>' +
        '34.63%. In English, this reads “If you are one of<br>' +
        'our customers and your income is $122,000 or<br>' +
        'greater, the chances are about 1 out of 3 that you’ll<br>' +
        'own a Mountain-200.”<br>' +
        'When the model is used for predicting, the theory<br>' +
        'is that probabilities based on existing customers<br>' +
        'can be applied to the folks who are not yet<br>' +
        'customers. That is, the chances are about 1 out of 3<br>' +
        'that someone with an income of $122,000 or<br>' +
        'greater would purchase a Mountain-200. All you<br>' +
        'need to do to use this model is feed your web<br>' +
        'visitor’s demographic input variables in and it will<br>' +
        'find the nodes that the visitor classifies into and<br>' +
        'return the trees (ProductModels) that have the<br>' +
        'nodes with the highest probabilities.<br>' +
        'One way to get a better sense of the relationships in the<br>' +
        'Dependency Network tab is to drag the predictive (input)<br>' +
        'variables over to one corner of the screen. Figure 13-18<br>' +
        'shows the model from Figure 13-17 after the data miner<br>' +
        'dragged the predictive variables to the upper-left corner.<br>' +
        'Figure 13-18: The Dependency Network with predictive<br>' +
        'variables dragged to the upper-left corner<br>' +
        '870<br>' +
        'This is still not very helpful. By zooming in on the<br>' +
        'upper-left corner, as shown in Figure 13-19, you can see<br>' +
        'that these, in fact, are some of the input variables. Note<br>' +
        'that there are only 12 shown in the dependency network.<br>' +
        'Variables such as First Name, Last Name, and<br>' +
        'StateProvinceName did not play enough of a role in the<br>' +
        'model to make it onto the graph. Figure 13-19 has had<br>' +
        'another adjustment as well: The slider on the left side was<br>' +
        'moved down to about one-third of the way up from the<br>' +
        'bottom. This shows that most of the relationships, and the<br>' +
        'strongest relationships, come from only a few variables.<br>' +
        'This comes as no surprise to an experienced data miner.<br>' +
        'Often there are only a few variables that really make a<br>' +
        'difference — it’s just difficult to figure out ahead of time<br>' +
        'which ones they’ll be. (Actually, it’s not so difficult for<br>' +
        'certain models. When you specify the input and predicted<br>' +
        'variables in the Data Mining Wizard, there is a Suggest<br>' +
        '871<br>' +
        'button at the bottom of the window that will calculate the<br>' +
        'strength of the relationship between the predictable<br>' +
        'variable and the input variables. This lets you narrow the<br>' +
        'model down right from the start.)<br>' +
        'Figure 13-19: The Dependency Network zoomed in on the<br>' +
        'predictive variables<br>' +
        'The input variables with the strongest relationships shown<br>' +
        'in Figure 13-19 are Yearly Income, English Country<br>' +
        'Region Name, Number Cars Owned, Age, and Total<br>' +
        'Children. True to the iterative data mining process, this<br>' +
        'brings up an opportunity. Removing some of the weaker<br>' +
        'variables will allow the model to explore more<br>' +
        'combinations among the stronger variables and to generate<br>' +
        'a more predictive model. For example, Figure 13-20 shows<br>' +
        'the decision tree for the Women’s Mountain Shorts<br>' +
        'product based on the initial model.<br>' +
        '872<br>' +
        'There is clearly a variation in preference for these shorts<br>' +
        'by country. More than 15.5 percent of the Canadians<br>' +
        'bought a pair, but about one-half of one percent of the<br>' +
        'Germans bought a pair. Given this information,<br>' +
        'recommending Women’s Mountain Shorts to a German<br>' +
        'website visitor is probably a waste of time.<br>' +
        'Figure 13-20: The initial decision tree for Women’s<br>' +
        'Mountain Shorts<br>' +
        'Figure 13-21 shows the decision tree for the same product<br>' +
        'after the model has been narrowed down to the five<br>' +
        'strongest input variables shown in Figure 13-19.<br>' +
        'Figure 13-21: The expanded decision tree for Women’s<br>' +
        'Mountain Shorts after reducing the number of input<br>' +
        'variables<br>' +
        '873<br>' +
        'The first split is still based on English Country Region<br>' +
        'Name, but now there is a second split for three of the<br>' +
        'country nodes. Canada can be split out by income,<br>' +
        'showing that Canadian customers making >= $74,000 are<br>' +
        'more likely to own a pair of Women’s Mountain Shorts<br>' +
        '(probability 23.65 percent) — much higher than the 15.5<br>' +
        'percent we saw for Canada based on the English<br>' +
        'Country Region Name split alone in Figure 13-20.<br>' +
        'The process of building a solid data mining model involves<br>' +
        'exploring as many iterations of the model as possible. This<br>' +
        'could mean adding variables, taking them out, combining<br>' +
        'them, adjusting the parameters of the algorithm itself, or<br>' +
        'trying one of the other algorithms that is appropriate for<br>' +
        'the problem. This is one of the strengths of the SQL Server<br>' +
        'Data Mining workbench — it is relatively easy and quick<br>' +
        'to make these changes and explore the results.<br>' +
        '874<br>' +
        'Moving back to the case study, once the data miner<br>' +
        'worked through several iterations and identified the final<br>' +
        'candidate, the next step in the process would be to validate<br>' +
        'the model.<br>' +
        'Product Recommendations: Model Validation<br>' +
        'As we described in the data mining process section, the lift<br>' +
        'chart, classification matrix, and cross validation sub-tabs in<br>' +
        'the Mining Accuracy Chart tab are designed to help<br>' +
        'compare and validate models that predict single, discrete<br>' +
        'variables. The recommendations model in this example is<br>' +
        'difficult to validate using these tools because, rather than<br>' +
        'one value per customer, the recommendations data mining<br>' +
        'model generates a probability for each ProductModel for<br>' +
        'each customer.<br>' +
        'Another problem with validating the model is that the data<br>' +
        'miner doesn’t really have historical data to test it with. The<br>' +
        'test data available, and the data used to build the model, is<br>' +
        'actually purchasing behavior, not responses to<br>' +
        'recommendations. For many data mining models, the<br>' +
        'bottom line is you won’t know if it works until you try it.<br>' +
        'Meanwhile, the data miner wants to be a bit more<br>' +
        'comfortable that the model will have a positive impact.<br>' +
        'One way to see how well the model predicts actual buying<br>' +
        'behavior is to simulate the lift chart idea in the context of<br>' +
        'recommendations. At the very least, the data miner could<br>' +
        'generate a list of the top six recommended products for<br>' +
        'each customer in the test case set and compare that list to<br>' +
        'the list of products the person actually bought. Any time a<br>' +
        'customer has purchased a product on their recommended<br>' +
        '875<br>' +
        'list, the data miner would count that as a hit. This approach<br>' +
        'provides a total number of hits for the model, but it doesn’t<br>' +
        'indicate if that number is a good one. You need more<br>' +
        'information: You need a baseline indication of what sales<br>' +
        'would be without the recommendations.<br>' +
        'In order to create a baseline number for the<br>' +
        'recommendations model, the data miner also created a list<br>' +
        'of six random products for each customer in the test case<br>' +
        'set. Table 13-4 shows the results for these two tests. As it<br>' +
        'turns out, the random list isn’t a realistic baseline. You<br>' +
        'wouldn’t really recommend random products; you would<br>' +
        'at least use some simple data mining in the form of a query<br>' +
        'and recommend your six top-selling products to everyone<br>' +
        '— people are more likely to want popular products. Table<br>' +
        '13-4 includes the results for the top six list as well.<br>' +
        'Table 13-4: Recommendations model validation data<br>' +
        'points<br>' +
        'The data miner and marketing manager learn from Table<br>' +
        '13-4 that the model is reasonably effective at predicting<br>' +
        'what customers bought — it’s not great, but it’s better than<br>' +
        'listing the top six products, and a lot better than nothing at<br>' +
        'all. Note that the hit rate in Table 13-4 has very little to do<br>' +
        'with the click-through rate you’d expect to see on the<br>' +
        'website. The real number will likely be significantly lower.<br>' +
        '876<br>' +
        'However, based on these results, the data miner and the<br>' +
        'marketing manager decided to give the model a try and<br>' +
        'carefully assess its impact on the original goals of the<br>' +
        'project, increasing the percentage of visitors who become<br>' +
        'customers, and increasing the average sale amount.<br>' +
        'Product Recommendations: The Operations Phase<br>' +
        'The decision to go forward moved the project into the<br>' +
        'operations phase of the data mining process. The<br>' +
        'implementation details are well beyond the scope of this<br>' +
        'book, but the high-level steps would involve making the<br>' +
        'data mining model available to the web server, and writing<br>' +
        'the ADOMD.NET calls to submit the visitor’s<br>' +
        'demographic information and to receive and post the<br>' +
        'recommendation list. Figure 13-22 shows an example of a<br>' +
        'DMX query to return the top 6 products for an individual<br>' +
        'from the ProductRecs1-DT mining model.<br>' +
        'In this case, for a 43-year-old person from France who<br>' +
        'makes $70,000 per year, has no children, and owns one<br>' +
        'car, the model recommendations include a Mountain-200,<br>' +
        'a Road-250, and a Touring-1000. This is good — you like<br>' +
        'seeing those high-revenue bikes in the recommendations<br>' +
        'list.<br>' +
        'Assessing the impact of the model would involve several<br>' +
        'analyses. First, the team would look at the number of<br>' +
        'unique visitors over time, and the percentage of visitors<br>' +
        'that actually become customers before and after the<br>' +
        'introduction of the recommendation list. Increasing this<br>' +
        'percentage is one of the goals of providing<br>' +
        'recommendations in the first place. This analysis would<br>' +
        '877<br>' +
        'look at the average purchase amount before and after as<br>' +
        'well. It may be that the conversion rate is not significantly<br>' +
        'affected, but the average purchase amount goes up because<br>' +
        'current customers are more interested in the<br>' +
        'recommendations and end up purchasing more.<br>' +
        'Figure 13-22: Sample DMX for a data mining query to get<br>' +
        'product recommendations based on an individual’s<br>' +
        'demographics<br>' +
        'The second analysis would look at the web browsing data<br>' +
        'to see how many people click one of the recommendation<br>' +
        'links. The analysis would follow this through to see how<br>' +
        'many people actually made a purchase. In a large enough<br>' +
        'organization, it might be worth testing the<br>' +
        'recommendation list against the top six list, or with no<br>' +
        'recommendations at all, using the random assignment<br>' +
        'process described earlier in this chapter.<br>' +
        '878<br>' +
        'The model would also have to be maintained on a regular<br>' +
        'basis because it is built on purchasing behaviors. New<br>' +
        'product offerings and changes in fashion, preference, and<br>' +
        'price can have a big impact on the purchasing behaviors —<br>' +
        'you don’t want your recommendations to go stale.<br>' +
        'Summary<br>' +
        'Congratulations for making it this far. You should have a<br>' +
        'basic understanding of the data mining concepts and how<br>' +
        'data mining can have an impact on your organization. Data<br>' +
        'mining is not just a tool; it’s a process of understanding<br>' +
        'business needs and data issues, working through various<br>' +
        'alternative models, testing and validating their viability,<br>' +
        'rolling them out into production, and making sure they<br>' +
        'address the opportunity and don’t get stale.<br>' +
        'Early in the chapter we reviewed some basic data mining<br>' +
        'concepts, describing how data mining is used for several<br>' +
        'different business tasks: classification, estimation or<br>' +
        'regression, prediction, association or affinity grouping,<br>' +
        'clustering or segmentation, anomaly detection, and<br>' +
        'description and profiling. We then discussed SQL Server’s<br>' +
        'data mining architecture and toolset, reviewing the key<br>' +
        'components and showing how they fit together. Digging<br>' +
        'down into the technology, we briefly described the seven<br>' +
        'algorithms provided with the product and how they applied<br>' +
        'to the various business tasks.<br>' +
        'Next we went into some detail on the process of data<br>' +
        'mining, outlining a step-by-step approach starting with<br>' +
        'identifying business opportunities and the associated data,<br>' +
        'moving through the actual data mining phase with its data<br>' +
        '879<br>' +
        'preparation, model development, and model validation<br>' +
        'steps, and ending with the operations phase with the<br>' +
        'implementation of the model, maintenance, and an<br>' +
        'assessment of its impact.<br>' +
        'Most of the second part of the chapter walked through this<br>' +
        'process based on two data mining scenarios: a large<br>' +
        'international lending organization that wants to cluster and<br>' +
        'classify cities, and an Adventure Works Cycles marketing<br>' +
        'person who wants to increase the number of website<br>' +
        'visitors who become customers by offering targeted<br>' +
        'product recommendations.<br>' +
        'By this point, we hope it’s clear that data mining is a<br>' +
        'powerful set of tools the DW/BI team can use to add<br>' +
        'significant, measurable business value. And that the SQL<br>' +
        'Server Data Mining toolset is effective, easy to use, and<br>' +
        'easy to incorporate into your overall system environment.<br>' +
        'We encourage the data miner on the team or in the<br>' +
        'organization to explore SQL Server’s data mining<br>' +
        'capabilities. The Excel add-in is a particularly easy way to<br>' +
        'get started. We also encourage everyone who uses the data<br>' +
        'mining tools to be careful with them. Data mining is a true<br>' +
        'power tool. As with a chainsaw, you can do amazing<br>' +
        'things, but you can also hurt yourself. It doesn’t help your<br>' +
        'credibility if you roll out a model based on the clever<br>' +
        'finding that customers who have been with you longer tend<br>' +
        'to have purchased more items.<br>' +
        '880<br>';
    document.getElementById('chapter12').innerHTML = 'Chapter 12<br>' +
        'The BI Portal and SharePoint<br>' +
        'Thinking outside the box.<br>' +
        'Reporting Services and PowerPivot bring significant<br>' +
        'functionality to the BI user community. However, these<br>' +
        'are not the complete solution to the problem of delivering<br>' +
        'business value. As you add more and more reports and<br>' +
        'analyses to the DW/BI system, you will need to provide<br>' +
        'some means for organizing and structuring them. This is<br>' +
        'the role of the BI portal. The BI portal is the primary<br>' +
        'starting point in the information quest for a large part of<br>' +
        'the business community. It needs to be structured in a way<br>' +
        'that allows people to find what they are looking for within<br>' +
        'an ever increasing number of reports and analyses. Ideally,<br>' +
        'it will be more than just a directory structure; it will<br>' +
        'provide additional useful features such as search,<br>' +
        'customization, collaboration, metadata access, and user<br>' +
        'support.<br>' +
        'For many smaller organizations, the Report Manager<br>' +
        'component of Reporting Services, along with basic web<br>' +
        'development skills, can be made to serve as a crude BI<br>' +
        'portal. However, DW/BI teams who want to provide rich<br>' +
        'portal experience, especially in larger organizations, will<br>' +
        'need a portal environment to serve as the BI portal<br>' +
        'platform.<br>' +
        'SharePoint is Microsoft’s offering in the portal and<br>' +
        'web-based application platform category. Microsoft has<br>' +
        '737<br>' +
        'chosen SharePoint 2010 to serve as the hosting<br>' +
        'environment for several BI related components, including<br>' +
        'Reporting Services, Excel Services, PowerPivot, Office<br>' +
        'Web Applications, Visio Services, and PerformancePoint<br>' +
        'Services. SharePoint also includes a broad set of portal<br>' +
        'functions to help you create a powerful BI portal<br>' +
        'experience.<br>' +
        'The first part of this chapter is a discussion of the BI portal<br>' +
        'concept, including design guidelines and a simple<br>' +
        'example. In the second part, we take a high level look at<br>' +
        'SharePoint as a BI portal platform and provide a summary<br>' +
        'of what it takes to get SharePoint going with a set of<br>' +
        'BI-related functionality including Reporting Services and<br>' +
        'PowerPivot for SharePoint.<br>' +
        'In this chapter, you will learn the following:<br>' +
        '• The major design principles and guidelines for a BI portal<br>' +
        '• The general structure and organization of a BI portal<br>' +
        '• What SharePoint is and how it works as a BI portal<br>' +
        '• An approach to setting up SharePoint as a BI portal<br>' +
        'The last section of the chapter summarizes the major steps<br>' +
        'involved in setting up a test SharePoint environment with<br>' +
        'its major BI-related components. We have provided a more<br>' +
        'detailed walk through of these steps on the book’s website,<br>' +
        'but decided to spare you the pain of reading through those<br>' +
        'steps here.<br>' +
        'The BI Portal<br>' +
        'In Chapter 10, we introduced the concept of the navigation<br>' +
        'framework as the organizing structure for the standard<br>' +
        '738<br>' +
        'report set. Any time we use the word portal, it invokes<br>' +
        'visions of a major enterprise effort to collect and<br>' +
        'categorize all structured and unstructured information<br>' +
        'throughout the organization and make it available through<br>' +
        'a rich user interface with intelligent search capabilities and<br>' +
        'the ability to personalize the experience. Building the<br>' +
        'enterprise information portal may be a useful and<br>' +
        'important task, but in most cases, it is someone else’s task.<br>' +
        'What you need to worry about is the BI portal, not the<br>' +
        'overall enterprise portal. Think of the BI portal as the<br>' +
        'central place where people can find the analytic<br>' +
        'information they need.<br>' +
        'The success of the DW/BI system is determined by<br>' +
        'whether or not the organization gets value out of it. For the<br>' +
        'organization to get value from the DW/BI system, people<br>' +
        'have to use it. Since the BI portal is the primary interaction<br>' +
        'most people have with the DW/BI system, the DW/BI<br>' +
        'team needs to do everything in its power to make sure the<br>' +
        'BI portal provides the best possible experience.<br>' +
        'As you begin the design process, keep in mind that a<br>' +
        'significant component of the work a DW/BI team does is<br>' +
        'about managing organizational change (which is just<br>' +
        'another way to say “politics”). The BI portal plays a<br>' +
        'significant role in this change process, so it has to work at<br>' +
        'several levels. It must be:<br>' +
        '• Usable: People have to be able to find what they need.<br>' +
        '• Content-rich: It should include much more than just the<br>' +
        'reports. It should include as much support information,<br>' +
        'documentation, help, tutorials, examples, and advice as<br>' +
        'possible.<br>' +
        '739<br>' +
        '• Clean: It should be nicely laid out so people are not<br>' +
        'confused or overwhelmed by it.<br>' +
        '• Current: It needs to be someone’s job to keep the content<br>' +
        'up-to-date. No broken links or 12-month-old items labeled<br>' +
        '“New!” allowed.<br>' +
        '• Interactive: It should include functions that engage the users<br>' +
        'and encourage them to return to the portal. A good search<br>' +
        'tool, a metadata browser, maybe even a support-oriented<br>' +
        'discussion group are all ways for people to interact with the<br>' +
        'portal. The ability for users to personalize their report home<br>' +
        'page, and to save reports or report links to it, makes it<br>' +
        'directly relevant to them. It also helps to have new items<br>' +
        'appear every so often. Surveys, class notices, and even data<br>' +
        'problem warnings all help keep it fresh.<br>' +
        '• Value-oriented: This is the organizational change goal. We<br>' +
        'want everyone who comes to the BI portal to end up with the<br>' +
        'feeling that the DW/BI system is a valuable resource,<br>' +
        'something that helps do their jobs better. In a way, the BI<br>' +
        'portal is one of the strongest marketing tools the DW/BI<br>' +
        'team has and you need to make every impression count.<br>' +
        'In short, the design principles that apply to any good<br>' +
        'website apply to the BI portal.<br>' +
        'Planning the BI Portal<br>' +
        'The process of creating the BI portal requires the careful<br>' +
        'combination of two basic design principles: density and<br>' +
        'structure.<br>' +
        '• Density: The human mind can take in an incredible amount<br>' +
        'of information. The human eye is able to resolve images at a<br>' +
        'resolution of about 530 pixels per inch at a distance of 20<br>' +
        'inches. Even though computer monitor resolutions have been<br>' +
        'improving over the years they still don’t come close to this<br>' +
        'number. Typical desktop LCD monitors have a resolution of<br>' +
        'about 100 pixels per inch. Our brains have evolved to<br>' +
        'rapidly process all this information looking for the relevant<br>' +
        '740<br>' +
        'elements. The browser gives us such a low-resolution<br>' +
        'platform that we have to use it as carefully and efficiently as<br>' +
        'possible. Every pixel counts.<br>' +
        '• Structure: Although we need to fill the BI portal home page<br>' +
        'with information, it doesn’t work if we jam it full of<br>' +
        'hundreds of unordered descriptions and links. Your brain<br>' +
        'can handle all this information only if it’s well organized.<br>' +
        'For example, a typical major daily newspaper has an<br>' +
        'incredible amount of information but you can handle it<br>' +
        'because it’s structured in a way that helps you find what you<br>' +
        'need. At the top level, the paper is broken up into sections. If<br>' +
        'you’re looking for certain kinds of information, you know<br>' +
        'which section to start with. Some readers look at every<br>' +
        'section, but most skip a few that they deem irrelevant to their<br>' +
        'lives. At the next level down, each section may be divided<br>' +
        'into subsections and all sections use headlines as their<br>' +
        'common organizing structure. Headlines (at least<br>' +
        'non-tabloid headlines) attempt to communicate the content<br>' +
        'of the article in as few words as possible. These headlines<br>' +
        'are the “relevant elements” that allow readers to quickly<br>' +
        'parse through the newspaper to find information that is<br>' +
        'interesting to them.<br>' +
        'RESOURCES<br>' +
        'For interesting reading about image<br>' +
        'density, see the work of R. N. Clark: Visual<br>' +
        'Astronomy of the Deep Sky, Cambridge<br>' +
        'University Press and Sky Publishing, 1990.<br>' +
        '• http://www.clarkvision.com/articles/<br>' +
        'eye-resolution.html.<br>' +
        'Edward Tufte’s three-volume series<br>' +
        'provides a good general reference for<br>' +
        '741<br>' +
        'structure and information display. Tufte has<br>' +
        'described the three books as being about,<br>' +
        'respectively, “pictures of numbers, pictures<br>' +
        'of nouns, and pictures of verbs.”:<br>' +
        '• The Visual Display of Quantitative<br>' +
        'Information, Second Edition, Graphics<br>' +
        'Press, May, 2001.<br>' +
        '• Envisioning Information, Graphics Press,<br>' +
        'May, 1990.<br>' +
        '• Visual Explanations: Images and<br>' +
        'Quantities, Evidence and Narrative,<br>' +
        'Graphics Press, February, 1997.<br>' +
        'Impact on Design<br>' +
        'The idea of density translates to the BI portal in a couple<br>' +
        'of ways. Primarily, it means we flatten the information<br>' +
        'hierarchy. Categories are often represented as hierarchies<br>' +
        'in the browser. You see a list of choices, each representing<br>' +
        'a topic. Click on a topic, and you’re taken to a page with<br>' +
        'another list of choices, and so on until you finally reach<br>' +
        'some content. Flattening the hierarchies means bringing as<br>' +
        'much information to the top-level pages as possible.<br>' +
        'Information that was hidden in the sub-pages is now pulled<br>' +
        'up to an indented list of category and subcategory headings<br>' +
        'on a single page.<br>' +
        'Figure 12-1 translates these concepts into the world of<br>' +
        'Adventure Works Cycles. The BI portal shown here<br>' +
        'demonstrates how two levels of report categories have<br>' +
        'been collapsed into one page. The portal is easy to<br>' +
        'navigate because you can identify major categories of<br>' +
        '742<br>' +
        'information based on the headings and ignore them if they<br>' +
        'don’t apply to your current needs, or examine them more<br>' +
        'closely if they seem relevant. Having the two levels on the<br>' +
        'same page actually gives the user more information<br>' +
        'because the two levels help define each other. For<br>' +
        'example, Sales helps group the sales-related subcategories<br>' +
        'together, but at the same time, the subcategory descriptions<br>' +
        'help the user understand what activities are included in<br>' +
        'Sales.<br>' +
        'Figure 12-1: The Adventure Works Cycles BI portal home<br>' +
        'page<br>' +
        '743<br>' +
        'You can see other examples of this dense design with<br>' +
        'flattened hierarchies in the management pages in<br>' +
        'SharePoint. When you get to the SharePoint section later<br>' +
        'in this chapter, take a look at the Site Settings page on any<br>' +
        'SharePoint site. The entire SharePoint Central<br>' +
        'Administration site follows this highly dense, flattened<br>' +
        'hierarchy approach. (See Figure 12-7.) It’s not so easy to<br>' +
        'use for someone who is new to SharePoint because the<br>' +
        'categories and descriptions are not obvious to the<br>' +
        'uninitiated.<br>' +
        'Business Process Categories<br>' +
        'Every word you include on the portal — every header,<br>' +
        'description, function, and link — all need to communicate<br>' +
        'what content people will find behind it. The categories you<br>' +
        'choose as the top level of your taxonomy will determine<br>' +
        'how understandable the BI portal is to your users.<br>' +
        'Generally, the best way to organize the portal is to use<br>' +
        'your organization’s business processes as the main outline.<br>' +
        'Look at Figure 12-1 from a business process perspective.<br>' +
        'The left column under Standard Reports includes<br>' +
        'Adventure Works Cycles’ major business processes. You<br>' +
        'can also think about this as the organization’s value chain.<br>' +
        'In Adventure Works Cycles’ case, marketing and sales<br>' +
        'business processes come early in the value chain, working<br>' +
        'to bring in new customers and new orders. Once the<br>' +
        'company has orders, they purchase materials from their<br>' +
        'suppliers, manufacture the bikes, and ship them out to the<br>' +
        'customers. Customer support may interact with the<br>' +
        'customers at any point along the way, and even after the<br>' +
        'product has been shipped. There are also internal business<br>' +
        'processes that generate information that is useful across<br>' +
        '744<br>' +
        'the organization, like headcount data from HR, or cost data<br>' +
        'from finance.<br>' +
        'Beyond business process categories, the BI portal needs to<br>' +
        'have a standard layout so people can easily find what<br>' +
        'they’re looking for. If your organization has a standard<br>' +
        'page layout that you can adapt for the BI portal, use it.<br>' +
        'Your users won’t have to learn a new interface when they<br>' +
        'come to the BI portal.<br>' +
        'Additional Functions<br>' +
        'Although one of the main purposes of the BI portal is to<br>' +
        'provide access to the standard reports, it must offer much<br>' +
        'more than just reports. In addition to the categories and<br>' +
        'reports lists, you need to provide several common<br>' +
        'functions:<br>' +
        '• Search: The search tool serves as an alternative report<br>' +
        'locator if the business process categories aren’t helpful. A<br>' +
        'good search tool that indexes every report name and<br>' +
        'description, document, and page on the BI website can<br>' +
        'dramatically shorten the amount of time it takes users to find<br>' +
        'what they want.<br>' +
        '• Metadata browser: A metadata browser can be as simple as<br>' +
        'a few ASP.NET pages or even Reporting Services reports<br>' +
        'that allow users to browse through the metadata’s<br>' +
        'descriptions of the databases, schemas, tables, columns,<br>' +
        'business rules, load statistics, report usage, report content,<br>' +
        'and so on. You could also build this using Master Data<br>' +
        'Services and export the taxonomy into SharePoint. However<br>' +
        'you build it, interested users will learn a lot about the DW/BI<br>' +
        'system through the metadata browser.<br>' +
        '• Forum: It may make sense to host a support-oriented forum<br>' +
        'or discussion on the BI portal. This can be a good way for<br>' +
        'users to find help when they need it. It can also create a<br>' +
        '745<br>' +
        'record of problems and their solutions for future reference. It<br>' +
        'takes a fairly large user community to generate the critical<br>' +
        'mass of activity needed to make a forum successful. The<br>' +
        'DW/BI team should be active participants.<br>' +
        '• Personalization: Users should be able to save reports or<br>' +
        'report links to their personal pages. This personalization can<br>' +
        'be a powerful incentive for people to return to the portal<br>' +
        'every day.<br>' +
        '• Announcements and calendars: It helps keep things<br>' +
        'interesting to have new items appear on a regular basis.<br>' +
        'Offer a survey, have people sign up for tool training, or post<br>' +
        'a notice of an upcoming User Forum meeting.<br>' +
        '• Feedback: You need to provide a direct means for your user<br>' +
        'community to submit requests and suggestions. You should<br>' +
        'also enable social media style feedback mechanisms such as<br>' +
        'document ratings and rankings.<br>' +
        'There is also a whole set of support and administration<br>' +
        'content the BI portal needs to provide. This includes online<br>' +
        'training/tutorials, help pages, metadata browser, example<br>' +
        'reports, data cheat sheets, help request forms, and contact<br>' +
        'info for the DW/BI team. This information all goes in the<br>' +
        'lower right corner, the least valuable real estate on the<br>' +
        'screen (at least for languages that read from left to right<br>' +
        'and top to bottom). We discuss this supporting content<br>' +
        'again in Chapters 15 and 17.<br>' +
        'Building the BI Portal<br>' +
        'Creating and maintaining the BI portal is much more work<br>' +
        'than most DW/BI teams expect. However, the effort is<br>' +
        'worth it because the BI portal is the farthest-reaching tool<br>' +
        'the DW/BI team has for delivering value to the<br>' +
        'organization. It is also one of your best tools for marketing<br>' +
        'the DW/BI system.<br>' +
        '746<br>' +
        '• Line up the resources: You will need to have resources on<br>' +
        'the BI team, or dedicated to the BI team, who are proficient<br>' +
        'at web development and web content creation. This includes<br>' +
        'some facility with the server operating system, user<br>' +
        'authentication, and the portal software.<br>' +
        'It’s also a brilliant idea to get a good graphic designer<br>' +
        'involved; someone who has a clean, practical sense of<br>' +
        'design. Do not blindly copy our examples because they<br>' +
        'were designed to be shrunk down and printed in black and<br>' +
        'white. Besides, we are not the best graphic designers.<br>' +
        '• Learn the tools: How much learning you will need depends on<br>' +
        'what expertise you already have and what areas you will be<br>' +
        'managing. At the very least, someone on the BI team will need<br>' +
        'to know how to create and maintain the BI portal pages and the<br>' +
        'reports they reference. You may also need to know how to set<br>' +
        'up a portal site and enable the various functions you’d like to<br>' +
        'provide, such as search, help requests, and discussion forums. In<br>' +
        'the worst case, you may need to develop expertise in installing<br>' +
        'and maintaining the BI portal software itself. This can be a<br>' +
        'significant effort, as you will see in the SharePoint section later<br>' +
        'in this chapter.<br>' +
        'If you don’t have any recent experience with web<br>' +
        'development, plan to spend some time learning how to be<br>' +
        'productive and efficient in the portal development<br>' +
        'environment. There’s a whole lot more to it than just<br>' +
        'writing some HTML code. Templates, cascading style<br>' +
        'sheets, scripting languages, and graphic design tools can<br>' +
        'help you create a flexible, easy to maintain website. But it<br>' +
        'takes time to figure out how they work and how to use<br>' +
        'them well.<br>' +
        '• Create a code management process: Most of the portal tools<br>' +
        'have change management systems built in. After all, they are<br>' +
        'primarily web content management tools. Spend a little time<br>' +
        '747<br>' +
        'learning how you put a directory or site under management,<br>' +
        'how you check a page out and in, and how you revert to a<br>' +
        'previous version.<br>' +
        'You will need to get the basics of your portal security in<br>' +
        'place at this point as well. From a code management<br>' +
        'perspective, you need to have security groups to determine<br>' +
        'who can see, edit, and create or delete which pages. This<br>' +
        'becomes especially important once you have users<br>' +
        'uploading their own reports to the portal.<br>' +
        '• Create a report submission process: You will have a set of<br>' +
        'reports that are your organization’s official BI reports. They<br>' +
        'have been tested by the BI team and bear the official DW/BI<br>' +
        'logo. BI professionals in other parts of the organization will<br>' +
        'want to add their work to the BI portal. This has pros and cons.<br>' +
        'On the pro side, the reports they create will likely be useful to<br>' +
        'many people in their groups and you want to encourage these<br>' +
        'folks to develop their skills and share their expertise. On the con<br>' +
        'side, they may not have the same attention to detail and quality<br>' +
        'control that you have. Their reports may be wrong, and<br>' +
        'ultimately undermine the DW/BI system’s credibility.<br>' +
        'Consider having separate locations for user submitted<br>' +
        'reports in the portal. They can have a specific directory or<br>' +
        'site for their department where they keep local reports.<br>' +
        'These reports are not created, tested, or maintained by the<br>' +
        'BI team, and should not sport the DW/BI system logo. A<br>' +
        'certain percentage of these reports will prove to have value<br>' +
        'to a broader audience. Move those reports into the official<br>' +
        'reports section after you have verified their contents,<br>' +
        'structure, and calculations.<br>' +
        '• Create the core BI portal pages: Design the BI portal home<br>' +
        'page and sub-pages based on the navigation framework from the<br>' +
        'BI applications design process. The BI portal home page<br>' +
        'typically corresponds to the overall bus matrix. It represents<br>' +
        '748<br>' +
        'information from across the enterprise. Sub-pages usually<br>' +
        'correspond to individual business processes, such as orders or<br>' +
        'shipments, and typically include several standard report groups,<br>' +
        'including time series, time comparisons, geographic<br>' +
        'comparisons, key indicators, or KPIs.<br>' +
        'You will usually need to start your BI portal at the<br>' +
        'business process sub-page level because you do not have<br>' +
        'data for the rest of the bus matrix. If you start at the home<br>' +
        'page level, most of the links will be inoperative. Such a<br>' +
        'limited home page would undermine your credibility.<br>' +
        'Over time as you add more rows on the bus matrix and the<br>' +
        'associated BI applications your home page will gradually<br>' +
        'move toward the full BI portal shown in Figure 12-1.<br>' +
        '• Manage expectations: As you learn more about your portal<br>' +
        'tools, the BI applications, and your user community, the BI<br>' +
        'portal will change. You should let people know this early on,<br>' +
        'and even enlist them in helping design new generations of the<br>' +
        'BI portal. People can deal with changes they have been a part of<br>' +
        'creating, but unexpected change will almost always generate<br>' +
        'resistance.<br>' +
        'These principles, guidelines, and preparation steps will<br>' +
        'help you create a BI portal in any web environment. We<br>' +
        'now shift our focus to delivering a BI portal in a specific<br>' +
        'portal environment: Microsoft’s SharePoint.<br>' +
        'Using SharePoint as the BI Portal<br>' +
        'While it is possible to create a workable BI portal using<br>' +
        'HTML editors, there are several major enterprise portal<br>' +
        'players on the market offering an extended set of web<br>' +
        'functionality (search www.wikipedia.org for “Enterprise<br>' +
        'Portal” to see a list of vendors and products). SharePoint is<br>' +
        '749<br>' +
        'Microsoft’s full service portal and web application<br>' +
        'platform with multi-tier, workload-balanced, distributed<br>' +
        'application support. SharePoint is an enterprise class tool.<br>' +
        'It is complex and multi-faceted, and can be a challenge to<br>' +
        'install and manage.<br>' +
        'You can use SharePoint strictly as the website<br>' +
        'environment for the BI portal. However, SharePoint<br>' +
        'provides much more than just website hosting. It offers a<br>' +
        'range of features including connection and collaboration<br>' +
        'tools, an application platform, data capture, and workflow.<br>' +
        'Many parts of your organization could use SharePoint<br>' +
        'functionality but may have limited interest in the BI portal.<br>' +
        'Let’s be clear right from the start: SharePoint is big.<br>' +
        'Ideally, your organization already has SharePoint installed<br>' +
        'and working. If that’s the case, see if the folks who have<br>' +
        'the scars of experience will help you get the incremental<br>' +
        'components needed to support BI installed and working. If<br>' +
        'your organization does not have SharePoint up and<br>' +
        'running, see if you can get some other group in IT to own<br>' +
        'SharePoint and be responsible for its broader use in the<br>' +
        'organization. This will likely be the team who already<br>' +
        'provides intranet support. This may slow things down, but<br>' +
        'it is generally a good idea because of SharePoint’s<br>' +
        'complexity and broad applicability. In the worst case, if<br>' +
        'you need to install SharePoint and get it working yourself,<br>' +
        'be prepared for a few weeks of work. It may not take that<br>' +
        'long, but the probability of it taking longer than you expect<br>' +
        'is very high.<br>' +
        'Because SharePoint is so big, we don’t have the room (or<br>' +
        'the patience, or frankly, the expertise) to give you<br>' +
        '750<br>' +
        'step-by-step guidance on installing all the pieces you will<br>' +
        'likely need. This section points out the major landmarks in<br>' +
        'the SharePoint ecosystem, but you still need to do a lot of<br>' +
        'work to get them going. We will list out the major<br>' +
        'BI-related components, offer general guidance on how to<br>' +
        'get those components working, and provide references on<br>' +
        'where to get more details. If you want real hands-on<br>' +
        'experience, which we strongly encourage, we have posted<br>' +
        'a guide to installing the test system we describe here on the<br>' +
        'book’s website at http://www.kimballgroup.com/<br>' +
        'html/booksMDWTtools.html.<br>' +
        'RESOURCES<br>' +
        'If you are going to manage SharePoint<br>' +
        'yourself, you will need more detailed<br>' +
        'instruction. One useful resource is<br>' +
        'Professional SharePoint 2010<br>' +
        'Administration, by Todd Klindt, Steve<br>' +
        'Caravajal, and Shane Young (Wiley, 2010).<br>' +
        'Architecture and Concepts<br>' +
        'This section provides an overview of the SharePoint<br>' +
        'product, including a description of the software editions,<br>' +
        'the architecture, and key terminology.<br>' +
        'There are three major editions of SharePoint 2010:<br>' +
        'SharePoint Foundation, SharePoint Server Standard, and<br>' +
        'SharePoint Server Enterprise. Each level includes the<br>' +
        'functionality of the previous level. Microsoft SharePoint<br>' +
        '751<br>' +
        'Foundation 2010 was known as SharePoint Services. It<br>' +
        'provides the core website, service management, and<br>' +
        'document sharing features. Microsoft SharePoint Server<br>' +
        '2010 Standard edition extends SharePoint Foundation<br>' +
        '2010 to provide a full-featured business collaboration<br>' +
        'platform that scales from the enterprise to the Web. It<br>' +
        'provides additional collaboration, content management,<br>' +
        'and more robust search features. Microsoft SharePoint<br>' +
        'Server 2010 Enterprise edition includes many of the BI<br>' +
        'features such as Excel Services and PerformancePoint<br>' +
        'Insight. SharePoint uses Internet Information Service (IIS)<br>' +
        'as its underlying web server in all its editions.<br>' +
        'RESOURCES<br>' +
        'You can learn more about SharePoint’s<br>' +
        'features and capabilities, and the various<br>' +
        'SharePoint editions at Microsoft’s main<br>' +
        'SharePoint marketing site:<br>' +
        'http://sharepoint.microsoft.com.<br>' +
        'SharePoint’s Three-Tier Architecture and Topology<br>' +
        'Microsoft SharePoint Server 2010 and SharePoint<br>' +
        'Foundation Server 2010 provide the infrastructure for<br>' +
        'hosting services. SharePoint services and functions<br>' +
        'generally map to one of three roles that relate to each other<br>' +
        'in a three-tier structure. In Figure 12-2, the user-facing role<br>' +
        'is assigned to the web server tier. The primary function of<br>' +
        'the servers in this layer, also known as Web Front Ends or<br>' +
        '752<br>' +
        'WFEs, is to serve web pages and process requests for<br>' +
        'services from the farm.<br>' +
        'Figure 12-2: SharePoint’s three-tier architecture<br>' +
        'Most of the services are found in the application server tier<br>' +
        'in the middle of Figure 12-2. You deploy only the services<br>' +
        'you need, and a deployed service is known as a service<br>' +
        'application. Service applications are services that are<br>' +
        'shared across sites within a farm (for example, Search and<br>' +
        'Excel Calculation Services) or in some cases across<br>' +
        'multiple farms. You deploy service applications by starting<br>' +
        'the associated services on the desired server computers<br>' +
        '753<br>' +
        'using the Services on Server page on the SharePoint<br>' +
        'Central Administration site. Service applications can be<br>' +
        'co-hosted on a single computer, deployed on a dedicated<br>' +
        'server, or activated on multiple servers across the farm,<br>' +
        'depending on the scale required.<br>' +
        'Some services include multiple components and<br>' +
        'deployment of these components requires planning. For<br>' +
        'example, the PowerPivot for SharePoint feature mentioned<br>' +
        'in Chapter 11 includes multiple application components<br>' +
        'and multiple databases.<br>' +
        'The third tier is the database. SharePoint sets up several<br>' +
        'databases to store technical metadata and content. The<br>' +
        'search function, for example, starts with three separate<br>' +
        'databases: administration, crawl, and property. You can<br>' +
        'create more crawl and property databases as you scale out<br>' +
        'the search function. For a list of SharePoint databases,<br>' +
        'search technet.microsoft.com for “using DBA-created<br>' +
        'databases.”<br>' +
        'SharePoint will automatically create the necessary<br>' +
        'databases for you as part of creating a new service<br>' +
        'application. Since the default names are based on GUIDs,<br>' +
        'you might want to override this and provide more friendly<br>' +
        'names. You can find a guide to creating SharePoint 2010<br>' +
        'database names by searching technet.microsoft.com for<br>' +
        '“Introduction to the Microsoft SharePoint 2010 Database<br>' +
        'Layer.”<br>' +
        'SharePoint Terminology<br>' +
        '754<br>' +
        'Understanding all the terms used in the SharePoint world<br>' +
        'can give you a big advantage. Starting with the big picture,<br>' +
        'a SharePoint farm is the collection of all servers running in<br>' +
        'a SharePoint deployment and the services running on those<br>' +
        'servers. You can have a single server farm, or spread your<br>' +
        'service applications and websites across many servers. The<br>' +
        'point of the farm is to provide load balancing, scalability,<br>' +
        'and availability.<br>' +
        'From the web perspective, a top-level domain URL is<br>' +
        'known as a web application, and it corresponds to an<br>' +
        'Internet Information Services (IIS) website. A web<br>' +
        'application contains one or more site collections, which is<br>' +
        'a set of one or more websites that have the same owner<br>' +
        'and administration settings. The top-level site collection in<br>' +
        'a web application contains the default site for the web<br>' +
        'application itself. For example, entering http://finance/<br>' +
        'will display the default home page from the top-level site<br>' +
        'in the http://finance web application.<br>' +
        'A site collection always has a top-level site and can<br>' +
        'contain many additional sub-sites. A site is a coherent set<br>' +
        'of pages with a home page, libraries, lists, a common<br>' +
        'layout template and theme, all based off the same root<br>' +
        'URL. Each site can have multiple sub-sites. A typical<br>' +
        'company might have several web applications at the top<br>' +
        'level of its SharePoint implementation, each with a<br>' +
        'different base URL. For example, SharePoint may be<br>' +
        'supporting the organization’s extranet site, and several<br>' +
        'independent intranet sites. The associated web applications<br>' +
        'in SharePoint might be as follows:<br>' +
        'Root URL Purpose<br>' +
        '755<br>' +
        'http://www.adventureworks.com/ Extranet<br>' +
        'http://finance Finance<br>' +
        'http://hrweb Human Resources<br>' +
        'http://AWweb General organization intranet<br>' +
        'This hierarchy-based approach gives significant flexibility<br>' +
        'to the SharePoint administrator because it allows the<br>' +
        'assignment of servers, services, databases, and other<br>' +
        'resources at multiple levels. It also allows different groups<br>' +
        'and individuals to create and maintain their own section of<br>' +
        'the overall SharePoint environment. Figure 12-3 shows a<br>' +
        'hierarchy for the http://enterprise web application we create<br>' +
        'later in this chapter.<br>' +
        'Figure 12-3: An example web application hierarchy<br>' +
        '756<br>' +
        'Note that the default URL for a web application would<br>' +
        'typically start with the name of the SharePoint server. You<br>' +
        'would need to use your Domain Name Server (DNS) to<br>' +
        'map a friendly name such as http://enterprise to the actual<br>' +
        'web application name which might be<br>' +
        'http://ServerGrpShrPtProd01:59732. If this organization only<br>' +
        'had one SharePoint server, each of the preceding web<br>' +
        'applications would map to different ports on the same<br>' +
        'server. This is known as alternate access mapping, and it<br>' +
        'makes the websites easier to remember.<br>' +
        'Once you have your sites set up, you have to have some<br>' +
        'content for people to work with. When you direct your<br>' +
        '757<br>' +
        'browser to a website, the web server displays a page (or<br>' +
        'invokes a program that generates the page). SharePoint has<br>' +
        'several types of content that are often displayed, entered,<br>' +
        'and edited on SharePoint pages: lists, libraries, and Web<br>' +
        'Parts.<br>' +
        'Lists are collections of items of a single type. SharePoint<br>' +
        'lists include tasks, calendars, announcements, links, and<br>' +
        'contacts. Think of a list like a spreadsheet table with one<br>' +
        'row per entry and columns of attributes appropriate for that<br>' +
        'type of list. For example, a calendar is a list of events with<br>' +
        'attributes such as event date, event description, start time,<br>' +
        'and duration. A calendar list is displayed in a Web Part<br>' +
        'that provides the known context of a calendar: days,<br>' +
        'weeks, and months.<br>' +
        'Views are ways to view the contents of a library or list, sort<br>' +
        'of like a report definition. They provide a display format<br>' +
        'for the list and allow you to filter it. For example, you can<br>' +
        'change the view from Calendar to the All Events view to<br>' +
        'see the individual items in the calendar list.<br>' +
        'Libraries are a special type of list that store files as well as<br>' +
        'information about files (essentially directories). Libraries<br>' +
        'may be set to hold certain content types, such as pages,<br>' +
        'reports, or images. You can have different content types in<br>' +
        'a library, but you may have to enable this in the library’s<br>' +
        'properties (via the Library Settings button in the Library<br>' +
        'Ribbon under Library Tools). You can add content types<br>' +
        'from the set of types enabled on the site. The library’s<br>' +
        'property page is also where you identify what types of new<br>' +
        'documents a user can create in this library. If you want to<br>' +
        'enable a new content type on the site, you need to activate<br>' +
        '758<br>' +
        'it through the Manage site features under Site Actions, or<br>' +
        'Site collection features under Site Collection<br>' +
        'Administration in Site Settings.<br>' +
        'The kinds of lists and libraries (and sites) available to you<br>' +
        'in the Site Actions/More Options list depend on the site<br>' +
        'features you have activated in the Site Settings menu (both<br>' +
        'under Site Actions for the BI Portal site, and under Site<br>' +
        'Collection Administration (Site collection features) for the<br>' +
        'overall set of sites). The initial template you select to<br>' +
        'create your BI Portal site will determine the initial feature<br>' +
        'set available. For example, the Business Intelligence<br>' +
        'Center template only includes a few libraries and lists, and<br>' +
        'some sample PerformancePoint pages. The PowerPivot<br>' +
        'site template includes announcements, calendar, links, and<br>' +
        'task lists; several empty document libraries; and a team<br>' +
        'discussion list. We have created a site template that<br>' +
        'includes all these features and more.<br>' +
        'Web Parts are reusable code modules that allow you to<br>' +
        'display various types of content on your pages. You may<br>' +
        'use a Web Part to show a mini version of the calendar in<br>' +
        'the corner of the page, for example. Two of the most used<br>' +
        'Web Parts in BI are the Excel viewer and Reporting<br>' +
        'Services report viewer. These allow you to embed reports<br>' +
        'in a web page, and provide some level of interaction for<br>' +
        'the users.<br>' +
        'Setting Up SharePoint<br>' +
        'If you plan to use SharePoint as your BI portal, now is as<br>' +
        'good a time as any to dig in and get your hands dirty. This<br>' +
        'is one case where actually working with the product will<br>' +
        '759<br>' +
        'help you understand what it is and how it works. If you<br>' +
        'don’t have SharePoint 2010 already running in your<br>' +
        'company, and if the BI team will be responsible for<br>' +
        'providing its own SharePoint functionality, we strongly<br>' +
        'recommend getting started with a simple test installation of<br>' +
        'SharePoint and supporting components. Find an available<br>' +
        '64-bit server, set up a 64-bit virtual machine, and get<br>' +
        'started. The earlier you do this the better because you have<br>' +
        'a lot to learn about how to get SharePoint working in your<br>' +
        'DW/BI system environment, and you will make a lot of<br>' +
        'mistakes.<br>' +
        'Installing SharePoint is very much like the old example of<br>' +
        'bad documentation for shutting down a nuclear power<br>' +
        'plant where the instructions read:<br>' +
        '1. Pull the red lever.<br>' +
        '2. But first, push the green button.<br>' +
        'A lot of pieces must fall neatly into place to get SharePoint<br>' +
        'working, but sometimes they step on each other. There is<br>' +
        'no perfect install order, and you often need to go back and<br>' +
        'redo some steps before you can move forward with the<br>' +
        'next step. Because of this uncertainty, we can only provide<br>' +
        'a summary of the install process for a test system here. We<br>' +
        'have posted a more detailed set of instructions on the<br>' +
        'book’s website if you are inclined to actually set up a test<br>' +
        'system.<br>' +
        'We strongly recommend using a virtual machine for your<br>' +
        'test environment. One huge advantage is that you can take<br>' +
        'snapshots at the end of each major install step, once you’ve<br>' +
        '760<br>' +
        'verified the successful completion of that step. This will let<br>' +
        'you go back to a known good point rather than starting all<br>' +
        'over if something goes wrong.<br>' +
        'The Installation Process<br>' +
        'Figure 12-4 illustrates the flow of the install steps you will<br>' +
        'need to take to get a test installation of SharePoint going<br>' +
        'along with the other products, add-ins, and SharePoint<br>' +
        'services that help provide a full featured BI portal. You<br>' +
        'can install the various components in a different order, but<br>' +
        'you will have a different set of configurations needed for<br>' +
        'each install order. There are a lot of steps to this process in<br>' +
        'the initial release of SharePoint Server 2010 and SQL<br>' +
        'Server 2008 R2. We hope Microsoft will quickly make this<br>' +
        'process less cumbersome, and we suggest search<br>' +
        'technet.microsoft.com for “SharePoint Server installation and<br>' +
        'deployment” for more recent information.<br>' +
        'Figure 12-4: Installation steps for SharePoint and related<br>' +
        'components<br>' +
        '761<br>' +
        'Our test system install process is based on a clean version<br>' +
        'of Windows Server 2008 R2 installed on a dedicated<br>' +
        'server in an existing domain. Create a virtual machine on<br>' +
        'this server, also running Windows Server 2008 R2, and<br>' +
        'make sure you can allocate plenty of memory; 4 or more<br>' +
        'gigabytes would be good and at least two CPU cores.<br>' +
        'Note: If you need to create a true standalone installation<br>' +
        'with its own domain controller, go to<br>' +
        'http://www.powerpivot-info.com/ and search for “single<br>' +
        'machine.”<br>' +
        'Following Figure 12-4, the remainder of this section starts<br>' +
        'with a few planning steps, and then summarizes the install<br>' +
        '762<br>' +
        'steps for SharePoint and its supporting products and<br>' +
        'add-ins, including SQL Server Analysis Services in<br>' +
        'SharePoint mode and PowerPivot for SharePoint, SQL<br>' +
        'Server Reporting Services in SharePoint integrated mode<br>' +
        'for reports. We’ll also mention some supporting steps<br>' +
        'around access and security and enabling various<br>' +
        'SharePoint services. Once you have the base infrastructure<br>' +
        'in place, you can do some simple experimentation with<br>' +
        'PowerPivot and Reporting Services.<br>' +
        'In the last part of this install summary, we offer you two<br>' +
        'options to build out the rest of SharePoint’s BI portal<br>' +
        'related services, both of which require a download from<br>' +
        'the book’s website. The first option is to create a new site<br>' +
        'based on a special template created from the site shown in<br>' +
        'Figure 12-1. The second option is to follow a separate<br>' +
        'guide to build out the full BI portal yourself. Wait until<br>' +
        'you’ve had some experience with SharePoint before you<br>' +
        'decide which option to take.<br>' +
        'Plan SharePoint for BI<br>' +
        'Before you slip the install disks into your DVD drive, take<br>' +
        'some time to plan out your installation. Here are several<br>' +
        'prerequisites for you to think through and get in place first.<br>' +
        'Verify software versions and editions. Most of the<br>' +
        'features we describe here require the enterprise editions of<br>' +
        'both SharePoint 2010 and SQL Server 2008 R2, or higher.<br>' +
        'Many BI features will not work with the standard editions,<br>' +
        'and most will not work with prior versions of these two<br>' +
        'products. Some features, such as PowerPivot for<br>' +
        '763<br>' +
        'SharePoint 2010, also require Windows Server 2008 SP2<br>' +
        'or Windows Server 2008 R2.<br>' +
        'Obtain install files and product keys. Download all the<br>' +
        'software install files or gather the DVDs you will need<br>' +
        'ahead of time and have your product keys handy. This<br>' +
        'includes 64-bit versions of the following:<br>' +
        '• Windows Server 2008 R2 (unless your server OS is already<br>' +
        'running)<br>' +
        '• SQL Server 2008 R2 Enterprise edition<br>' +
        '• SharePoint 2010 Enterprise edition<br>' +
        'If you have an MSDN license, you can get the install files<br>' +
        'and keys online from the download section of<br>' +
        'http://msdn.microsoft.com. There are a few other components<br>' +
        'and add-ins you will download along the way.<br>' +
        'Create service accounts in the domain. SharePoint<br>' +
        'server(s) must be part of a Windows domain to manage<br>' +
        'network security and accounts. Out test system install had<br>' +
        'eight service accounts to support various SharePoint and<br>' +
        'SQL Server services. The list of accounts we used is<br>' +
        'included in the walk through guide on the book’s website.<br>' +
        'Your real install should include additional accounts to<br>' +
        'support additional SharePoint services and IIS application<br>' +
        'pools, depending on which services you enable and how<br>' +
        'you want to scale out your farm. You may want to adopt<br>' +
        'your server group’s account naming conventions.<br>' +
        'Our recommendation is to set these up in Active Directory<br>' +
        'on your domain and give them appropriate permissions on<br>' +
        'your test machine right away. When you set up your<br>' +
        'official system, you should also create separate service<br>' +
        '764<br>' +
        'accounts for each SQL Server service and SharePoint<br>' +
        'service application to properly isolate them. Search<br>' +
        'technet.microsoft.com for “service accounts required for initial<br>' +
        'SharePoint deployment” for more detail about the<br>' +
        'permissions these accounts will need.<br>' +
        'Plan authentication and secure store. Windows default<br>' +
        'authentication mode, called NTLM, does not support the<br>' +
        'pass-through of user credentials resulting in a login failure<br>' +
        'if more than two machines are involved in an interaction.<br>' +
        'This phenomenon, known as the double-hop problem, is<br>' +
        'common in the DW/BI system environment when the<br>' +
        'database is on a separate machine from the report access<br>' +
        'server. This is almost always the case when SharePoint is<br>' +
        'introduced into the DW/BI system environment because<br>' +
        'the SQL Server data warehouse database is usually located<br>' +
        'on a separate server from SharePoint. In this case, a user<br>' +
        'connects from client machine A to the SharePoint server<br>' +
        'B, and then selects a report from the BI portal. SharePoint<br>' +
        'then attempts to connect to SQL Server on server C, at<br>' +
        'which point the login fails because SharePoint cannot<br>' +
        'provide enough information for SQL Server to authenticate<br>' +
        'the user. To avoid this, you must switch to a different<br>' +
        'authentication method called Kerberos if you want to have<br>' +
        'user-level authentication. Kerberos is considered more<br>' +
        'secure than the default Windows authentication system and<br>' +
        'has been part of the Windows environment since 2000. It<br>' +
        'can be a challenge to configure, but it is getting easier as<br>' +
        'Windows improves its implementation.<br>' +
        'Kerberos will solve the double-hop problem whether<br>' +
        'SharePoint is part of your DW/BI system or not. If the<br>' +
        'problem is limited to data access from SharePoint, you<br>' +
        '765<br>' +
        'could address it with Secure Store Services, which replaces<br>' +
        'Single Sign On from SharePoint 2007. Secure Store<br>' +
        'Services provides an option to store credentials in a Secure<br>' +
        'Store database on the SharePoint server. Credentials are<br>' +
        'then mapped as user-to-user or Domain Group-to-user and<br>' +
        'can be renewed at the SharePoint server. Secure Store<br>' +
        'Services are used in SharePoint to manage PowerPivot<br>' +
        'data refreshes and other unattended account activity. They<br>' +
        'require storing a copy of user name and password<br>' +
        'information in the secure store database.<br>' +
        'You don’t need to get Kerberos working for your test<br>' +
        'SharePoint install. You can avoid the double hop problem<br>' +
        'by doing your testing on the SharePoint server, perhaps via<br>' +
        'remote desktop, rather than using a browser from a<br>' +
        'separate client machine. You will need to figure out your<br>' +
        'authentication strategy when you build out your<br>' +
        'SharePoint development and production systems. If you<br>' +
        'control all the servers you are working with, this is a<br>' +
        'manageable problem. If others are involved, you are<br>' +
        'probably facing some negotiations. You also don’t need<br>' +
        'Kerberos if you are only querying PowerPivot for<br>' +
        'SharePoint databases. This is because they are hosted<br>' +
        'within the SharePoint environment and do not require<br>' +
        'additional authentication.<br>' +
        'There are a lot of online guides to getting Kerberos<br>' +
        'working with SharePoint. This one from the SQL Server<br>' +
        'Customer Advisory Team, called “Configuring Kerberos<br>' +
        'Authentication for Microsoft SharePoint 2010 Products,”<br>' +
        'is particularly helpful: http://go.microsoft.com/fwlink/<br>' +
        '?LinkID=196600.<br>' +
        '766<br>' +
        'Installing the Test System<br>' +
        'This section summarizes our test installation based on the<br>' +
        'steps in Figure 12-4.<br>' +
        'Set Up the Test Server<br>' +
        'Once you’ve done the BI planning, you need to set up the<br>' +
        'test system itself. Figure 12-5 shows the topology and<br>' +
        'environment for the SharePoint test server installation we<br>' +
        'created. This is by no means a model or recommended<br>' +
        'topology for anything you might put into production. All<br>' +
        'three layers of the SharePoint architecture are on one<br>' +
        'machine. Even if you choose to access data on an existing<br>' +
        'database machine, the SharePoint test machine will still<br>' +
        'have SQL Server to host the SharePoint databases. A more<br>' +
        'robust version of this farm would have two load-balanced<br>' +
        'front end servers to host the Web Front End (WFE), two<br>' +
        'load-balanced application servers to host the services<br>' +
        'layer, a failover cluster setup for the database layer, and<br>' +
        'perhaps a separate cluster for the Reporting Services<br>' +
        'component. We set our goals a little lower to get it all<br>' +
        'working on one machine first.<br>' +
        'Figure 12-5: Test SharePoint environment topology and<br>' +
        'services<br>' +
        '767<br>' +
        'The test server doesn’t start out with all the servers and<br>' +
        'services installed. Each of the following steps adds one or<br>' +
        'more servers and services to the environment. It’s this<br>' +
        'incremental process that can cause conflicts because each<br>' +
        'new layer may reconfigure the system in a way that breaks<br>' +
        'one of the previous layers.<br>' +
        'SQL Server in particular plays several roles in the<br>' +
        'SharePoint world. It acts as the database engine for the<br>' +
        'SharePoint admin, content, and property databases. In the<br>' +
        'case of this book, SQL Server is the data warehouse<br>' +
        'platform for the relational dimensional model and for the<br>' +
        'Analysis Services database. SQL Server also provides a<br>' +
        '768<br>' +
        'separate Analysis Services engine for PowerPivot for<br>' +
        'SharePoint. Finally, SQL Server Reporting Services is<br>' +
        'commonly used in an integrated mode with SharePoint to<br>' +
        'access data kept in a SQL Server data warehouse. All of<br>' +
        'these roles are installed and configured at different points,<br>' +
        'and have the potential to cause problems for SharePoint<br>' +
        'and for the other SQL Server roles.<br>' +
        'Install SharePoint Server 2010<br>' +
        'SharePoint has several prerequisites that must be in place<br>' +
        'before the actual product can be installed. Install the<br>' +
        'prerequisites from the SharePoint installer, then install<br>' +
        'SharePoint itself. Because of the install order we chose for<br>' +
        'our test system, we ran the installation, but did not run the<br>' +
        'SharePoint Products Configuration Wizard. As a result,<br>' +
        'this initial install step only enables the Web Front End and<br>' +
        'a few of the service applications. You can’t view a<br>' +
        'SharePoint site or the central administration site yet.<br>' +
        'Once the SharePoint install has completed without errors,<br>' +
        'this is a good point to take a snapshot of your virtual<br>' +
        'machine.<br>' +
        'Install PowerPivot for SharePoint<br>' +
        'The approach we took for the test server lets the<br>' +
        'PowerPivot installer configure SharePoint server for us.<br>' +
        'This part of the installation is run from the SQL Server<br>' +
        '2008 R2 installer. Once you get to the Setup Role step,<br>' +
        'select SQL Server PowerPivot for SharePoint, and make<br>' +
        'sure the Add PowerPivot for SharePoint To option is set to<br>' +
        'New Server.<br>' +
        '769<br>' +
        'The PowerPivot installation adds the PowerPivot service,<br>' +
        'the PowerPivot SQL database instance, and the Analysis<br>' +
        'Services engine in SharePoint mode. It also configures the<br>' +
        'top-level site.<br>' +
        'You can find install guides for other scenarios, such as<br>' +
        'installing PowerPivot on an existing SharePoint farm, on<br>' +
        'MSDN. Search msdn.microsoft.com for “How to:<br>' +
        'Install PowerPivot for SharePoint.”<br>' +
        'Verify the SharePoint Install<br>' +
        'Once you have the PowerPivot for SharePoint components<br>' +
        'installed, there are several things you can do to make sure<br>' +
        'your server is working as it should be. First, open a<br>' +
        'browser and navigate to the top-level site of your test<br>' +
        'server. Figure 12-6 shows the homepage for this site which<br>' +
        'was automatically configured using the PowerPivot site<br>' +
        'template.<br>' +
        'At this point, you can upload a PowerPivot for Excel<br>' +
        'workbook and see how it is displayed in the PowerPivot<br>' +
        'Gallery (which we saw back in Figure 11-10 in the chapter<br>' +
        'on PowerPivot). You can also take a look at the SQL<br>' +
        'Server instance in SQL Server Management Studio and see<br>' +
        'what databases SharePoint has created.<br>' +
        'It’s a good idea to visit the SharePoint Central<br>' +
        'Administration Site to see how it works and check the<br>' +
        'status of your server. This site is command central for<br>' +
        'running your SharePoint farm. From here you can enable<br>' +
        'new service applications, manage servers and services<br>' +
        'running on those servers, monitor usage, and much more.<br>' +
        '770<br>' +
        'Figure 12-7 shows the Central Administration site’s<br>' +
        'homepage. To get to the Central Administration site, enter<br>' +
        'the URL with the port number in your browser. If you<br>' +
        'neglected to memorize the port number, you can always<br>' +
        'find a link to the Central Administration site in Start ⇒ All<br>' +
        'Programs ⇒ Microsoft SharePoint 2010 Products ⇒<br>' +
        'SharePoint 2010 Central Administration.<br>' +
        'Note that the Central Administration site follows the portal<br>' +
        'design patterns we described in the first part of this<br>' +
        'chapter, with a dense set of links grouped into a collapsed<br>' +
        'hierarchy of categories. If you know what the categories<br>' +
        'mean in the SharePoint context, you should be able to find<br>' +
        'what you are looking for. If you’re not familiar with<br>' +
        'SharePoint terminology, it’s a bit mysterious.<br>' +
        'Figure 12-6: The default PowerPivot site template<br>' +
        'homepage<br>' +
        '771<br>' +
        'Figure 12-7: SharePoint Central Administration home<br>' +
        'page<br>' +
        'Install, Configure, and Verify Reporting Services<br>' +
        '772<br>' +
        'The next step for the test server is to install Reporting<br>' +
        'Services along with the default instance of SQL Server to<br>' +
        'give you a chance to create some reports and display them<br>' +
        'on a SharePoint page. Remember, the relational database<br>' +
        'and Reporting Services would typically be installed on a<br>' +
        'separate machine (or machines). In fact, the SharePoint<br>' +
        'Health Analyzer will flag SQL Server on the same<br>' +
        'machine as a problem.<br>' +
        'The final installation step is to configure Reporting<br>' +
        'Services to work in SharePoint integrated mode from<br>' +
        'SharePoint Central Administration. Verify that the<br>' +
        'integration was successful by uploading a reporting<br>' +
        'services RDL file to the Shared Documents library. Once it<br>' +
        'is in SharePoint, click on the file name to execute the<br>' +
        'report.<br>' +
        'Completing the BI Portal<br>' +
        'At this point, you have the core capabilities enabled in<br>' +
        'SharePoint to build a basic BI portal. You can offer<br>' +
        'Reporting Services reports and PowerPivot reports to your<br>' +
        'user community from a common interface. You can learn a<br>' +
        'lot about SharePoint by experimenting with your test<br>' +
        'server in its current state. You did good work.<br>' +
        'However, you still have a long way to go before you have<br>' +
        'a fully functioning BI portal to unveil at the first user<br>' +
        'deployment education session. If you want to learn all the<br>' +
        'details, you can continue from this point and install<br>' +
        'PerformancePoint, enable a whole range of useful services<br>' +
        'and content types, and create your portal home page along<br>' +
        'with the site theme and layout. This is the best path to take<br>' +
        '773<br>' +
        'if managing SharePoint will be one of your jobs, or if you<br>' +
        'are truly interested in understanding how SharePoint works<br>' +
        'and what you can do with it.<br>' +
        'The individual steps for this detailed installation are in a<br>' +
        'document titled SharePoint BI Portal Detailed<br>' +
        'Install Completion Step-by-step.doc in the zip<br>' +
        'file you can download from the book’s website. We<br>' +
        'encourage you to pull them out and work through them to<br>' +
        'get the full SharePoint experience.<br>' +
        'Alternatively, if you don’t need to learn all the portal<br>' +
        'creation details, you can still get the full experience by<br>' +
        'creating a new SharePoint site with the BIPortal template<br>' +
        'also included in the zip file, which will do most of the<br>' +
        'setup and configuration for you.<br>' +
        'The steps for installing the template are also included in a<br>' +
        'file in the zip file called SharePoint BI Portal<br>' +
        'Template Install Steps.doc.<br>' +
        'The end result of either path is a full featured BI portal<br>' +
        'site.<br>' +
        'The Added Functionality of the BIPortal Site Template<br>' +
        'Both the BI Portal site template and the step-by-step guide<br>' +
        'enable many features that are not grouped together in the<br>' +
        'standard templates found in SharePoint. We started with<br>' +
        'the BI Center template and added on the following<br>' +
        'features, each of which requires several steps to enable<br>' +
        'within SharePoint:<br>' +
        '774<br>' +
        '• PerformancePoint dashboard and scorecard capabilities<br>' +
        '• Search across the portal environment<br>' +
        '• Announcements to provide information to users<br>' +
        '• Web-based surveys<br>' +
        '• User support discussion groups<br>' +
        '• User support request database<br>' +
        '• Alerts to notify users when documents or other items of<br>' +
        'interest change<br>' +
        '• User personalization capabilities (my reports, custom<br>' +
        'layouts, and so on)<br>' +
        '• A workflow to manage user submission and approval of<br>' +
        'reports and PowerPivot analytics<br>' +
        '• Group work lists including calendars and team resource<br>' +
        'scheduling tools<br>' +
        'In addition to these features, there are examples of the<br>' +
        'actual pages that make up the portal: the home page,<br>' +
        'sub-pages for the enterprise business process reports, and<br>' +
        'supporting pages for documentation and user support,<br>' +
        'along with the navigation links on the left side of the site.<br>' +
        'The point is, once you get the core SharePoint and SQL<br>' +
        'Server components installed, there is still a lot more work<br>' +
        'to do. By using the BIPortal site template, you will be able<br>' +
        'to experiment with all this added SharePoint functionality,<br>' +
        'but you will not gain a good understanding of how it got<br>' +
        'there. If you are going to use SharePoint as your BI portal<br>' +
        'and application delivery platform, you, or someone else on<br>' +
        'the BI team, will have to learn the details of how to make<br>' +
        'it work. That’s where the detailed step-by-step approach<br>' +
        'will help.<br>' +
        'Exploring SharePoint<br>' +
        '775<br>' +
        'Once you get the BI Portal site template loaded, or go<br>' +
        'through the steps to build it out yourself, you should spend<br>' +
        'some time playing around with it. Upload some of your<br>' +
        'own PowerPivot examples and see what they look like in<br>' +
        'SharePoint. Try to create a report in Report Builder. Try to<br>' +
        'edit a SharePoint page in SharePoint Designer. Experiment<br>' +
        'with the PerformancePoint Dashboard Designer. Work<br>' +
        'with security groups a bit to see how you tie them to<br>' +
        'Active Directory.<br>' +
        'The best way to do this would be to create a beta version<br>' +
        'of your own BI portal. Start working on your standard<br>' +
        'page layouts, color themes, navigation bar, user<br>' +
        'documentation, and user interactions. This will likely take<br>' +
        'a couple of weeks, so make sure it is built into the project<br>' +
        'plan.<br>' +
        'Summary<br>' +
        'Your BI portal will provide the first impression of the DW/<br>' +
        'BI system for most users in your organization. You need to<br>' +
        'make sure it’s a good impression. It needs to be usable,<br>' +
        'fast, clear, and complete. It also should look appealing to<br>' +
        'the eye. Building and maintaining an effective BI portal is<br>' +
        'a lot more work than most teams imagine. You can judge<br>' +
        'your success by the number of people who use the portal,<br>' +
        'and how often they use it. These should be part of your<br>' +
        'standard DW/BI system metrics.<br>' +
        'The obvious choice for a BI portal platform in the<br>' +
        'Microsoft world is SharePoint. One of the main goals of<br>' +
        'this chapter was to take you far enough into SharePoint to<br>' +
        'give you a sense for the capabilities of the product and the<br>' +
        '776<br>' +
        'effort involved in getting it to work. If you installed<br>' +
        'SharePoint following the guide on the book’s website, and<br>' +
        'your SharePoint experience has been anything close to<br>' +
        'ours, you must be exhausted. Nonetheless, we encourage<br>' +
        'you to continue on and create a BI portal site with the site<br>' +
        'template, or build out all the features yourself following<br>' +
        'the step-by-step guide from the book’s website.<br>' +
        'Microsoft has created a flexible, distributed, web-based<br>' +
        'application platform that can be scaled out to meet the<br>' +
        'information and collaboration needs of large enterprises.<br>' +
        'But all this power and flexibility is a two-edged sword. It<br>' +
        'is a fair amount of work to install and maintain, and much<br>' +
        'of the complexity of SharePoint is overkill for most<br>' +
        'small-to-medium sized businesses.<br>' +
        'However, Microsoft has chosen SharePoint as its<br>' +
        'application delivery vehicle for its enterprise BI offerings.<br>' +
        'Using SharePoint is appealing because of the add-on<br>' +
        'connections it has to many of Microsoft’s BI products and<br>' +
        'the core portal and collaboration functions it provides.<br>' +
        '777<br>';
    document.getElementById('chapter11').innerHTML = 'Chapter 11<br>' +
        'PowerPivot and Excel<br>' +
        '“Easy” is a relative term.<br>' +
        'Depending on how you count, Excel is arguably the most<br>' +
        'popular reporting and analysis tool on the planet today.<br>' +
        'This is not to say it is the best tool, but its broad<br>' +
        'availability, powerful expression language, programming<br>' +
        'capabilities, fine grained formatting functions, and data<br>' +
        'accessibility make it the starting point for most business<br>' +
        'analysts.<br>' +
        'Microsoft’s PowerPivot add-in for Excel 2010 takes Excel<br>' +
        'reporting and analytics to a whole new level. PowerPivot<br>' +
        'for Excel is an in-memory database add-in that allows<br>' +
        'Excel users to work with millions of rows of data at<br>' +
        'memory speeds. This evokes a high level of enthusiasm<br>' +
        'among its supporters. One of Microsoft’s white papers on<br>' +
        'PowerPivot includes the line: “The ultimate goal of<br>' +
        'PowerPivot for Excel is to make data analysis really easy.”<br>' +
        'It goes on to describe PowerPivot as “…a new product that<br>' +
        'provides self-service BI (Business Intelligence)<br>' +
        'functionality for users of Microsoft Office.” It sounds like<br>' +
        'all you really need to do is hand out Office 2010 licenses<br>' +
        'and set free all that self-service BI. Once you’ve heard<br>' +
        'these kinds of statements enough times, you might start to<br>' +
        'question your efforts to create a full-scale DW/BI system.<br>' +
        'In fact, you’re probably wondering why you read this far<br>' +
        'in the first place.<br>' +
        '693<br>' +
        'Of course, reality is seldom as rosy as marketing would<br>' +
        'have you believe. PowerPivot is the equivalent of pivot<br>' +
        'tables on steroids. You can load and effectively work with<br>' +
        'much larger data sets, with millions of rows of data. You<br>' +
        'can join data from multiple, disparate sources in the<br>' +
        'PowerPivot database. You can employ an expression<br>' +
        'language to create complex calculations and measures that<br>' +
        'calculate correctly in any context within the pivot table.<br>' +
        'How easy all this is depends on your background and<br>' +
        'experience level with Excel, databases, the structure and<br>' +
        'complexity of the data you are working with, and BI in<br>' +
        'general.<br>' +
        'We start this chapter with a brief description of Excel as an<br>' +
        'analysis and reporting tool. We then take a look at<br>' +
        'PowerPivot and its product architecture. The bulk of the<br>' +
        'chapter is dedicated to working through an example to give<br>' +
        'you a sense for what it takes to apply PowerPivot to a<br>' +
        'simple, realistic problem. We’ll finish up with a brief<br>' +
        'discussion of PowerPivot in the SharePoint environment<br>' +
        'and its role the overall DW/BI system.<br>' +
        'In this chapter, you learn the following:<br>' +
        '• Options for using Excel as a reporting and analysis tool<br>' +
        '• The functionality and architecture of the PowerPivot add-in<br>' +
        'for Excel and the supporting components in SharePoint<br>' +
        '• The main steps in creating a PowerPivot database and an<br>' +
        'associated PivotTable<br>' +
        '• A basic understanding of how to create calculations and<br>' +
        'measures in a PowerPivot database<br>' +
        '• A sense for how PowerPivot works in the SharePoint<br>' +
        'environment<br>' +
        '694<br>' +
        '• Guidance on how to include PowerPivot as part of your<br>' +
        'managed DW/BI system strategy<br>' +
        'Using Excel for Analysis and Reporting<br>' +
        'Excel has clearly staked its claim as one of the leading<br>' +
        'tools for business intelligence analysis and reporting. The<br>' +
        'ability it provides to create formulas, tables, and to<br>' +
        'manipulate data is the heart of many ad hoc BI style<br>' +
        'investigations. In many organizations, Excel is also the<br>' +
        'standard delivery vehicle for enterprise reports.<br>' +
        'Excel does have basic functionality built in to support<br>' +
        'reporting and analysis. Data connections allow authorized<br>' +
        'users to access both the SQL and Analysis Services<br>' +
        'databases in the data warehouse. The relational query<br>' +
        'designer is not very sophisticated: It returns a single, flat<br>' +
        'record set rather than separate dimensions and facts. The<br>' +
        'Analysis Services connection leverages the cube metadata<br>' +
        'to group columns in the field list by the dimension or fact<br>' +
        'table they came from.<br>' +
        'While it is possible to access this dataset using Excel<br>' +
        'commands, the Excel PivotTable control is the primary<br>' +
        'data manipulation tool for analytics. It allows the user to<br>' +
        'create basic row and column matrix reports by dragging<br>' +
        'and dropping attributes from the field list. Excel 2007<br>' +
        'added report filter fields to the PivotTable Field List, and<br>' +
        'Excel 2010 enhanced this filtering capability with slicers.<br>' +
        'A slicer is a visual list of the available values for a given<br>' +
        'field. You can filter the PivotTable display by selecting<br>' +
        'values in the slicer control. A single slicer can control<br>' +
        'multiple PivotTables and charts. Figure 11-1 shows an<br>' +
        'Excel dashboard with three slicers in the lower-left<br>' +
        '695<br>' +
        'quadrant that allow the report user to filter the report based<br>' +
        'on country, year, and customer type.<br>' +
        'Figure 11-1: A simple Sales Summary dashboard in Excel<br>' +
        'Creating flexible, parameter-driven reports in Excel is<br>' +
        'often much harder than it might first appear because our<br>' +
        'organizations, products, and customers are not<br>' +
        'symmetrical. For example, if you chose a country other<br>' +
        'than the United States in the report in Figure 11-1, the<br>' +
        'regional sales section falls apart because other countries<br>' +
        'have different numbers of regions. The report has to be<br>' +
        'able to expand and contract based on the user’s parameter<br>' +
        'choices. While it is possible to do almost anything in<br>' +
        'Excel, these complexities often lead to programming in<br>' +
        'Excel macros — an activity that is not for everyone.<br>' +
        'Excel, and especially the PivotTable control, is an<br>' +
        'excellent tool for ad hoc projects where the analyst needs<br>' +
        '696<br>' +
        'to analyze data in a new way, potentially bringing in data<br>' +
        'from multiple sources. This ad hoc access works best<br>' +
        'against a well designed Analysis Services cube because of<br>' +
        'the nature of the PivotTable control. However, interactive<br>' +
        'pivot table browsing of Analysis Services data is not very<br>' +
        'practical against cubes with large dimensions. It’s too easy<br>' +
        'to drag in the wrong column and get stuck while Excel<br>' +
        'tries to load 20 million customers into the PivotTable. As<br>' +
        'you will see later in this chapter, PowerPivot for Excel<br>' +
        'greatly expands the limits of what’s possible in Excel in<br>' +
        'terms of analytic capabilities and raw data volumes.<br>' +
        'The PowerPivot Architecture: Excel on Steroids<br>' +
        'There are actually two major components to PowerPivot:<br>' +
        'PowerPivot for Excel 2010 and PowerPivot for<br>' +
        'SharePoint.<br>' +
        'PowerPivot for Excel is a free add-in for Microsoft Office<br>' +
        'Excel 2010 that provides greatly enhanced data<br>' +
        'management and query capabilities to the Excel users’<br>' +
        'desktops. Users can install PowerPivot for Excel and use it<br>' +
        'on a standalone basis; it does not require a separate server<br>' +
        'component to function. As Figure 11-2 shows, the add-in<br>' +
        'includes the assembly and the database, called the<br>' +
        'VertiPaq engine. The VertiPaq database is designed to load<br>' +
        'the data into memory in a highly compressed,<br>' +
        'column-oriented format that is especially well suited to<br>' +
        'analytic slicing and dicing. This engine, known as an<br>' +
        'in-memory, column-store database, was developed by the<br>' +
        'Analysis Services product team.<br>' +
        '697<br>' +
        'Figure 11-2: PowerPivot for Excel application<br>' +
        'architecture<br>' +
        'The data itself is stored in a compressed form in a section<br>' +
        'of the spreadsheet file. You may see some large<br>' +
        'spreadsheets when you start using PowerPivot, but they are<br>' +
        'surprisingly small relative to the number of rows in the<br>' +
        'tables because of the compression. The PowerPivot add-in<br>' +
        'and its data are loaded into the Excel process address<br>' +
        'space. This is particularly important for 32-bit Excel<br>' +
        'because it must share the available 2GB of memory with<br>' +
        'Excel and any other add-ins.<br>' +
        'PowerPivot for SharePoint is a SharePoint add-in that is<br>' +
        'the enterprise version of PowerPivot, allowing Excel users<br>' +
        '698<br>' +
        'to publish their PowerPivot workbooks and share them<br>' +
        'with others in the organization. Users and applications can<br>' +
        'access PowerPivot workbooks via two main Service<br>' +
        'applications, shown in the middle of Figure 11-3. Users<br>' +
        'who do not have Excel and PowerPivot installed can come<br>' +
        'in through a web<br>' +
        'Figure 11-3: PowerPivot for SharePoint system<br>' +
        'architecture<br>' +
        'browser via SharePoint Excel services and get most of the<br>' +
        'functionality of the desktop PowerPivot. PowerPivot for<br>' +
        'SharePoint also adds its own web service so PowerPivot<br>' +
        'for Excel and other clients can access PowerPivot for<br>' +
        'SharePoint data as a data source.<br>' +
        '699<br>' +
        'The SharePoint implementation allows IT to monitor and<br>' +
        'manage these shared PowerPivot workbooks through the<br>' +
        'PowerPivot Management Dashboard. It also supports an<br>' +
        'automated data refresh function and provides a graphical<br>' +
        'interface called a gallery that lets users view screen shots<br>' +
        'of worksheets in PowerPivot files without having to open<br>' +
        'them up. Unlike the free PowerPivot for Excel add-in,<br>' +
        'PowerPivot for SharePoint requires licensing for<br>' +
        'SharePoint and SQL Server 2008 R2.<br>' +
        'PowerPivot is positioned as a tool that can enable business<br>' +
        'users familiar with Excel to easily create powerful<br>' +
        'analytics on their desktops, and share those analytics with<br>' +
        'other users via SharePoint. IT helps in this process by<br>' +
        'setting up the SharePoint environment to support<br>' +
        'enterprise access to PowerPivot-based reports and<br>' +
        'analyses. IT will also ensure performance by monitoring<br>' +
        'report usage and managing the PowerPivot update and<br>' +
        'maintenance process.<br>' +
        'We agree with this positioning in theory, but it’s important<br>' +
        'for you to have a good understanding of what it takes to<br>' +
        'make PowerPivot work in your environment before you<br>' +
        'decide what role it plays in your DW/BI system. Let’s dive<br>' +
        'in to PowerPivot at this point and create a simple analysis.<br>' +
        'Creating and Using PowerPivot Databases<br>' +
        'The best way to understand PowerPivot is to work with it<br>' +
        'hands-on. We’ll run through the process of creating a<br>' +
        'PowerPivot database and an associated analysis. If you<br>' +
        'haven’t used PowerPivot before, you’ll get the most out of<br>' +
        'this section by installing it on your computer, downloading<br>' +
        '700<br>' +
        'the example files from the book’s website, and following<br>' +
        'along. (If you don’t have Office 2010, you can always set<br>' +
        'up a virtual machine and download a trial version.)<br>' +
        'Our scenario is a simple one: we’re going to load<br>' +
        'population and income data from an external source and<br>' +
        'join it to the customer dimension. Our goal is to explore<br>' +
        'the data for relationships between our customers and the<br>' +
        'demographics of the geographies in which they live. The<br>' +
        'initial exploration will be to see if there is a correlation<br>' +
        'between customers and per capita income. If there is a<br>' +
        'relationship, we could use it to tune our marketing<br>' +
        'campaigns. For example, if it turns out our products are<br>' +
        'more popular in higher income states, we can target our<br>' +
        'advertising toward those states, or to media channels that<br>' +
        'focus on higher income households across all states.<br>' +
        'This kind of ad hoc integration of disparate data sources is<br>' +
        'generally known as an analytic mashup, a term we borrow<br>' +
        'from the web development space. In this example, we<br>' +
        'might get demographic data from the U.S. Census Bureau.<br>' +
        'It offers extensive demographic data down to the zip code<br>' +
        'level and lower in some cases. The example we’ll work<br>' +
        'with in this section uses small tables of simplified data so<br>' +
        'you can see what PowerPivot is doing. We are not out to<br>' +
        'test its ability to load millions of rows here.<br>' +
        'DOWNLOADS<br>' +
        '701<br>' +
        'You can download the source files and<br>' +
        'resulting Excel workbook from the book’s<br>' +
        'website at kimballgroup.com/html/<br>' +
        'booksMSDWTtools.html. You can also<br>' +
        'download a few other useful tables, such as<br>' +
        'a Date table from 2000 to 2020, and<br>' +
        'census data at the zip code level from the<br>' +
        '2000 census.<br>' +
        'Getting Started<br>' +
        'If you are starting from scratch, you need to have Office<br>' +
        '2010 and the PowerPivot for Excel add-in installed.<br>' +
        'Installation of PowerPivot is fairly straightforward. Once<br>' +
        'you have Office 2010 installed, you download and run the<br>' +
        'PowerPivot for Excel installation package from<br>' +
        'Microsoft’s PowerPivot website: http://www.powerpivot.com.<br>' +
        'You need to install the 32-bit version if you have 32-bit<br>' +
        'Office installed, or 64-bit if you have 64-bit Office<br>' +
        'installed. They are not interchangeable. Note that the<br>' +
        '64-bit version will allow you to access more memory,<br>' +
        'which is important to support larger data sets. If your data<br>' +
        'doesn’t fit in memory, PowerPivot won’t be able to load it.<br>' +
        'Once you’ve installed the add-in, Excel may still ask if you<br>' +
        'want to install it when you open Excel. Once it’s fully<br>' +
        'installed, you should see a message indicating that Excel is<br>' +
        'loading the add-in every time you start the program.<br>' +
        'Start by exploring the add-in a bit to get familiar with its<br>' +
        'functions and interface. When you select the new<br>' +
        '702<br>' +
        'PowerPivot Ribbon tab, you’ll see around 10 buttons<br>' +
        'arranged in categories. The one you will use most often is<br>' +
        'the PowerPivot Window button on the left of the Ribbon,<br>' +
        'which launches a separate window used to design and load<br>' +
        'data into the PowerPivot database. You’ll see two main<br>' +
        'ribbon tabs in the PowerPivot window: Home and Design.<br>' +
        'The Home Ribbon includes the buttons for getting external<br>' +
        'data. This is where you will typically start building your<br>' +
        'PowerPivot database.<br>' +
        'PowerPivot Table Design<br>' +
        'You can load data from a range of sources including<br>' +
        'relational databases, Analysis Services, flat files, and<br>' +
        'Excel spreadsheets. In our example, we load data into<br>' +
        'PowerPivot from a table already in the Excel spreadsheet<br>' +
        'and from an external flat file. These data sets are small and<br>' +
        'don’t prove PowerPivot’s ability to handle large data, but<br>' +
        'they do make it easier to understand what PowerPivot is<br>' +
        'doing when we start adding calculations and measures.<br>' +
        'First let’s get a sense for the analysis we want to create.<br>' +
        'Figure 11-4 shows the final layout of the report with the<br>' +
        'appropriate calculated columns.<br>' +
        'Figure 11-4: Target Analysis — Customer counts,<br>' +
        'population, and per capita income by state<br>' +
        '703<br>' +
        'This analysis brings together customer counts by state and<br>' +
        'lines them up with external income and population<br>' +
        'measures. The two data columns in the middle measure<br>' +
        'how popular we are in each state. CustPer100MPop is a<br>' +
        'relative measure of how far we’ve penetrated into the<br>' +
        'population of each state. We are doing relatively well in<br>' +
        'Alaska, for example. The two customers we have in a state<br>' +
        'with a population of 4 million people gives us a relative<br>' +
        'penetration rate of 50 customers per 100 million<br>' +
        'population. The math looks like this: (2/4) × 100 = 50.0.<br>' +
        'The NormPenRate column simply normalizes the<br>' +
        'CustPer100MPop column by dividing each row’s value by<br>' +
        'the overall number. For Alaska, the math looks like this:<br>' +
        '50.0/17.5 = 2.857. This says our penetration rate is above<br>' +
        'average if the normalized penetration rate is greater than<br>' +
        'one, and below average if it is less than one. We will use<br>' +
        'this column on the chart at the end of this exploration.<br>' +
        'The per capita income, the last column on the right,<br>' +
        'provides the values for the X axis in the final chart. Let’s<br>' +
        'deal with generating the numbers since that’s the hard part.<br>' +
        '704<br>' +
        'Loading the Data<br>' +
        'If you’d like to work through this exercise, start with the<br>' +
        'spreadsheet named 1 Simple Cust and Pop Example<br>' +
        'Starting Point.xlsx and follow these steps to load<br>' +
        'the example customer data from an Excel table into a<br>' +
        'PowerPivot table:<br>' +
        '1. Open the spreadsheet and select any cell in the<br>' +
        'Customer table. Normally you would load in customer<br>' +
        'data from a clean, trustworthy, reliable source, such as<br>' +
        'the data warehouse. In this case the Customer table is<br>' +
        'only 18 rows so you can load it into PowerPivot directly<br>' +
        'from Excel.<br>' +
        '2. Select the PowerPivot Ribbon tab at the top of the<br>' +
        'Excel window and select the Create Linked Table button<br>' +
        'in the Excel Data section of the Ribbon.<br>' +
        '3. This should open the PowerPivot window and copy<br>' +
        'the customer data over into a PowerPivot table called<br>' +
        'Customer. That’s one table loaded — you’re half way<br>' +
        'there! Examine the PowerPivot window to see what<br>' +
        'functions are available.<br>' +
        'Next, you’ll load in some data from an external source, in<br>' +
        'this case, from the U.S. Census Bureau. Again, the data set<br>' +
        'is abbreviated so you can more easily understand what<br>' +
        'PowerPivot is doing.<br>' +
        '1. In the PowerPivot window, on the Home Ribbon,<br>' +
        'select the From Text button in the Get External Data<br>' +
        '705<br>' +
        'section. This will open a Table Import Wizard to help<br>' +
        'you provide the parameters of the import.<br>' +
        '2. Change the Friendly connection name to PopByState.<br>' +
        '3. Click the Browse button next to the File Path box and<br>' +
        'navigate to the Simple Population Data.txt file.<br>' +
        'Select the file and select Open. PowerPivot will do its<br>' +
        'best to parse the file with the default settings.<br>' +
        '4. Change the Column Separator from Comma (,) to Tab<br>' +
        '(t).<br>' +
        '5. Finally, check the “Use first row as column headers”<br>' +
        'box. If everything looks good, click Finish. You should<br>' +
        'now have two tables in separate tabs in the PowerPivot<br>' +
        'window.<br>' +
        '6. Notice that the new table name is Simple<br>' +
        'Population Data. Select the name by<br>' +
        'double-clicking, or right-clicking and selecting Rename.<br>' +
        'Change the name to CensusData. PowerPivot can<br>' +
        'handle names with spaces, but this simpler name with<br>' +
        'no spaces will make our formulas a bit easier later.<br>' +
        'Creating the Relationships<br>' +
        'Now that you have these two tables in place, you have the<br>' +
        'data you need to do the calculations in Figure 11-4.<br>' +
        'However, you still need to define the relationship between<br>' +
        'the two tables in order for PowerPivot to be able to join<br>' +
        'them together.<br>' +
        '706<br>' +
        '1. Select the Design Ribbon at the top of the PowerPivot<br>' +
        'window. Select Manage Relationships from the<br>' +
        'Relationships section. The Manage Relationships dialog<br>' +
        'box should open with no relationships. Sometimes<br>' +
        'PowerPivot can figure out the relationships but in this<br>' +
        'case, you need to help it.<br>' +
        '2. Click the Create button at the top of the Manage<br>' +
        'Relationships dialog box. You should see a Create<br>' +
        'Relationship window like the one shown in Figure 11-5.<br>' +
        'The order of the tables in this dialog box is important<br>' +
        'because you are defining a one-to-many relationship. In<br>' +
        'this case, the “many” table is Customer and the join<br>' +
        'column is StateCode. The “one” table is CensusData<br>' +
        'and the join column is also StateCode. This says a<br>' +
        'single customer from a given state should only return<br>' +
        'one row from the CensusData table, but a row from the<br>' +
        'CensusData table for a single state may return many<br>' +
        'customers. Click Create to create the relationship, and<br>' +
        'then close the Manage Relationships window.<br>' +
        'Figure 11-5: The Create Relationship window<br>' +
        '707<br>' +
        'PowerPivot will use this relationship to properly align data<br>' +
        'from the two tables in the same query. Note that the<br>' +
        'relationship definition can use only a single column from<br>' +
        'each table, and any given column can be used in only one<br>' +
        'relationship. This means you may need to create a single<br>' +
        '“key” column for some of your input tables. One way to<br>' +
        'do this is by adding a calculated column to the table that<br>' +
        'concatenates multiple fields from the table together to<br>' +
        'create a unique join field.<br>' +
        'PowerPivot does help define these table relationships by<br>' +
        'importing existing primary key/foreign key relationships,<br>' +
        'and by recommending relationships it has auto-detected<br>' +
        'using common naming semantics, cardinality, and data<br>' +
        'patterns.<br>' +
        'PowerPivot does not support the concept of a role-playing<br>' +
        'dimension. For example, if you have OrderDate and<br>' +
        '708<br>' +
        'RequestedShipDate in your fact table, you will need to<br>' +
        'load two separate copies of your date dimension.<br>' +
        'At this point, you’ve brought data from multiple sources<br>' +
        'into a single, high performance, “mashup” database.<br>' +
        'Creating Analytics with PowerPivot<br>' +
        'Now that the data’s loaded (that was easy, wasn’t it?), you<br>' +
        'can start doing some analytics in a PivotTable back in the<br>' +
        'Excel worksheet. This section goes through the steps<br>' +
        'needed to create a PivotTable and the calculations used to<br>' +
        'generate our target analysis.<br>' +
        'Creating a PowerPivot PivotTable<br>' +
        'First, insert a PivotTable that is tied to the PowerPivot<br>' +
        'database you just created into a new worksheet:<br>' +
        '1. Select the Home Ribbon in the PowerPivot window<br>' +
        'and then select the PivotTable button in the Reports<br>' +
        'section. PowerPivot should shift control back to Excel<br>' +
        'and you should see a Create PivotTable window. Click<br>' +
        'OK to let the PivotTable default to a New Worksheet.<br>' +
        'You should see a PowerPivot Field List and a<br>' +
        'PivotTable results area in the new worksheet.<br>' +
        '2. Take a look at some data. Expand the CensusData<br>' +
        'table in the field list and check StateName, or drag it to<br>' +
        'the Row Labels well at the bottom of the field list. Next,<br>' +
        'check the Pop field, or drag it to the Values well. Your<br>' +
        'spreadsheet should now look similar to Figure 11-6.<br>' +
        '709<br>' +
        'Explore the PivotTable a bit at this point. You can work<br>' +
        'with the CensusData table, but the Customer table<br>' +
        'doesn’t add much value yet. In particular, you need a<br>' +
        'customer count as part of the calculation for several of the<br>' +
        'columns shown in Figure 11-4.<br>' +
        'DAX — Yet Another Expression Language<br>' +
        'PowerPivot has its own language for creating<br>' +
        'custom calculations and measures called the Data<br>' +
        'Analysis Expressions language, or DAX for short.<br>' +
        'DAX uses a syntax that was explicitly designed to<br>' +
        'be familiar to Excel formula experts, but because it<br>' +
        'works with the PowerPivot database, its arguments<br>' +
        'are columns and tables, not cells or arrays. There<br>' +
        'are more than 80 functions, operators, and<br>' +
        'constants in the initial version of DAX. Most of<br>' +
        'these functions will be familiar to Excel users; a<br>' +
        'few are unique to PowerPivot and are critical to<br>' +
        'creating correct calculations across multiple tables.<br>' +
        'Figure 11-6: Initial PivotTable results<br>' +
        '710<br>' +
        'There are a couple of concepts that will be helpful in<br>' +
        'working with PowerPivot data. Users can define two kinds<br>' +
        'of computed columns: measures and calculated columns.<br>' +
        'Measures are computed columns that get added to the field<br>' +
        'list in the PowerPivot field list in Excel. Measures are<br>' +
        'calculated according to the row and filter context in which<br>' +
        'they appear in any given PivotTable report. Calculated<br>' +
        'columns are computed columns that get added to the base<br>' +
        'tables in the PowerPivot window. Calculated columns are<br>' +
        'populated with values when they are created, and act like<br>' +
        'any other data element in the database. When they are<br>' +
        'reported in a PivotTable, they can only be aggregated<br>' +
        'using the core aggregate functions (Sum, Count, Min, Max,<br>' +
        'and Average). Calculated columns generate row level<br>' +
        'detail data and are not as flexible or as sensitive to the row<br>' +
        'and filter context as measures.<br>' +
        'Adding New Measures to the PivotTable<br>' +
        'The best way to create calculations and measures in<br>' +
        'PowerPivot is to break long, complex formulas up into<br>' +
        '711<br>' +
        'their component parts and build them incrementally. This<br>' +
        'allows you to experiment with the data and quickly see the<br>' +
        'results of your formulas. We’ll start with the CustCount<br>' +
        'measure and build up from there.<br>' +
        '1. Right-click on the Customer table in the PivotTable<br>' +
        'Field List in Excel and select Add New Measure (or<br>' +
        'select Customer and then select New Measure from the<br>' +
        'PowerPivot Ribbon).<br>' +
        '2. Change the Measure Name to CustCount.<br>' +
        '3. Since COUNT() seems like the right function, enter<br>' +
        'the following in the Formula box:<br>' +
        '=COUNT(Customer[Customer_ID])<br>' +
        '4. Click OK and you should see a new column appear in<br>' +
        'your PivotTable and at the end of the Customer field<br>' +
        'list. It should match the CustCount column in Figure<br>' +
        '11-4: a total of 18 customers with none in New Mexico.<br>' +
        'That was easy! But let us share a few words of warning.<br>' +
        'The COUNT() function counts cells in a column that<br>' +
        'contain numbers. Since Customer_ID is a number, this<br>' +
        'works in this case. If you needed to count cells with any<br>' +
        'values in them, you would use the COUNTA() function. The<br>' +
        'real world begins to creep in when you realize your<br>' +
        'Customer table will likely have multiple rows per<br>' +
        'customer because you have type 2 change tracking in place<br>' +
        'in your data warehouse. In that case, you could use the<br>' +
        'DISTINCT() function on the Customer_ID column to<br>' +
        'only count the distinct Customer_IDs. However, the<br>' +
        '712<br>' +
        'DISTINCT() function returns a table that contains a<br>' +
        'one-column list of distinct values, and the COUNT()<br>' +
        'functions expect a column as an argument. Fortunately,<br>' +
        'there is a COUNTROWS() function that counts the number of<br>' +
        'rows in a table. As a result, the following formula is more<br>' +
        'likely to give you the count you want across a variety of<br>' +
        'data structures:<br>' +
        '= COUNTROWS(DISTINCT(Customer[Customer_ID]))<br>' +
        'You might want to edit the CustCount column to give this<br>' +
        'a try even though it won’t make a difference in our simple<br>' +
        'example.<br>' +
        'Explore the PivotTable a bit more at this point. Add in<br>' +
        'CustType from the Customer table. Note that the state<br>' +
        'population is repeated, independent of customer.<br>' +
        'PowerPivot does an outer join to include all values of<br>' +
        'CustType and displays the lowest grain of population it<br>' +
        'has: state population. The customer count, which is based<br>' +
        'on the Customer table, is only shown where a customer<br>' +
        'exists.<br>' +
        'The CALCULATE() Function<br>' +
        'Next you want a calculation that shows how many<br>' +
        'customers you have per 100 million people<br>' +
        '(CustPer100MPop); this is your penetration rate. The<br>' +
        'basic formula would be (Customer Count *100)/<br>' +
        'Population. Naively converting this to the DAX syntax<br>' +
        'you’ve learned thus far, you might create a formula like<br>' +
        'this:<br>' +
        '=(Customer[CustCount]*100)/CensusData[Pop]<br>' +
        '713<br>' +
        'Unfortunately, this formula will give you an error claiming<br>' +
        'the value for Pop cannot be determined in the current<br>' +
        'context. The problem here is CustCount is a measure that<br>' +
        'creates an aggregate based on the row and filter context,<br>' +
        'but Pop is a detail level field. You need to apply some<br>' +
        'form of aggregation to the Pop column so PowerPivot can<br>' +
        'properly roll it up to the row and filter context.<br>' +
        'We’ve found it to be a useful practice to create a measure<br>' +
        'that automatically applies the appropriate aggregation; we<br>' +
        'then use that measure in subsequent calculations. This is<br>' +
        'the root of our earlier recommendation to build measures<br>' +
        'incrementally. In this case you’ll create a TotalPop<br>' +
        'measure, and then use it to create the CustPer100MPop<br>' +
        'measure.<br>' +
        '1. Right-click on the CensusData table in the<br>' +
        'PivotTable field list and select Add New Measure.<br>' +
        '2. Change the Measure Name to TotalPop.<br>' +
        '3. Enter the following formula in the Formula box and<br>' +
        'click OK:<br>' +
        '=SUM(CensusData[Pop])<br>' +
        '4. Add another new measure to the CensusData table<br>' +
        'called CustPer100MPop with the following formula:<br>' +
        '=(Customer[CustCount]*100)/CensusData[TotalPop]<br>' +
        'At this point, your PivotTable should look something like<br>' +
        'the one shown in Figure 11-7, assuming you removed the<br>' +
        'CustType column. Note that the total under<br>' +
        '714<br>' +
        'CustPer100MPop is a division of the sums, not a sum of<br>' +
        'the divisions.<br>' +
        'Figure 11-7: Intermediate PivotTable results<br>' +
        'The formula for the CustPer100MPop measure is actually<br>' +
        'a shorthand version of a very powerful DAX function<br>' +
        'called CALCULATE(). The full syntax is<br>' +
        'CALCULATE(<aggregate expression>, <filter 1>, <filter 2>,...)<br>' +
        'We provided the aggregate expression, but we didn’t use<br>' +
        'any filters in the formula, relying entirely on the row<br>' +
        'context to determine the appropriate inputs. (You are also<br>' +
        'allowed to leave off the word CALCULATE() as we did.)<br>' +
        '715<br>' +
        'The next computed column is the normalized penetration<br>' +
        'rate value, called NormPenRate, which will serve as the Y<br>' +
        'axis column. The challenge here is that every row must be<br>' +
        'divided by the overall average to normalize the values<br>' +
        'around 1. Fortunately, DAX offers a particularly useful<br>' +
        'filter called the All() function. The filter list in the<br>' +
        'CALCULATE function allows you to override the existing<br>' +
        'row and filter setting in the context of the cell. The generic<br>' +
        'formula should be something like this: CustPer100MPop/<br>' +
        'Average Total CustPer100MPop. You use the All()<br>' +
        'function in the CALCULATE filter to return the total<br>' +
        'CustPer100MPop over the entire CensusData table.<br>' +
        '1. Add a new measure to the CensusData table called<br>' +
        'NormPenRate with the following formula:<br>' +
        '=CensusData[CustPer100MPop]/<br>' +
        'CALCULATE(CensusData[CustPer100MPop],<br>' +
        'ALL(CensusData))<br>' +
        '2. Take a look at this new column in the PivotTable. The<br>' +
        'largest penetration rate should be around 2.86 in Alaska.<br>' +
        'The Grand Total value should be 1.<br>' +
        'So far, all your calculations have been associated with<br>' +
        'population and customer counts. As the last step in your<br>' +
        'PivotTable, bring in the per capita income numbers which<br>' +
        'will become the X axis in our chart.<br>' +
        '1. Check the box next to the PerCapitaIncome field in<br>' +
        'the field list under the CensusData table, or drag it<br>' +
        'down to the Values well.<br>' +
        '716<br>' +
        '2. Notice the name of the new column in the PivotTable<br>' +
        'is Sum of PerCapitaIncome. This default<br>' +
        'aggregation isn’t right because per capita income is not<br>' +
        'directly additive. The total number in this column of<br>' +
        '275.6 has no useful meaning.<br>' +
        '3. Try changing the aggregation to Average by clicking<br>' +
        'the Sum of PerCapitaIncome field in the Values<br>' +
        'well and selecting Edit Measure from the pop-up menu<br>' +
        'and then selecting Average in the Measure Settings<br>' +
        'window. Changing from sum to average is better, but<br>' +
        'it’s still not right. You’ll fix this next.<br>' +
        'Adding a Computed Column to the PivotTable Database<br>' +
        'The 34.45 total value is the sum of each state’s per capita<br>' +
        'income divided by the number of states. Instead it should<br>' +
        'be the total income across all states, divided by the total<br>' +
        'number of people in all states. Since you already have a<br>' +
        'TotalPop column, all you need is a total income column<br>' +
        'to do this right. You’ll create a TotalInc measure in two<br>' +
        'steps, by first creating a TotalIncome calculated column<br>' +
        'and then creating a measure on that column that uses the<br>' +
        'SUM() function. Finally, you’ll finish by creating a<br>' +
        'PerCapitaInc measure that is TotalInc/TotalPop.<br>' +
        '1. To create the TotalIncome calculated column, open<br>' +
        'the PowerPivot window and select the CensusData<br>' +
        'table tab. Remember, a calculated column is defined in<br>' +
        'the PowerPivot window and is part of the underlying<br>' +
        'table, as opposed to the measures you’ve been creating<br>' +
        'out in the PivotTable field list.<br>' +
        '717<br>' +
        '2. Select the first cell in the Add Column column (or the<br>' +
        'entire column), and enter the following formula:<br>' +
        '=CensusData[Pop]*CensusData[PerCapitaIncome]<br>' +
        '3. Change the name of the new column from<br>' +
        'CalculatedColumn1 to TotalIncome.<br>' +
        '4. Return to the PivotTable field list in Excel and add a<br>' +
        'measure called TotalInc to the CensusData table with<br>' +
        'the following formula:<br>' +
        '=SUM(CensusData[TotalIncome])<br>' +
        '5. Add the final measure called PerCapitaInc to the<br>' +
        'CensusData table with the following formula:<br>' +
        '=CensusData[TotalInc]/CensusData[TotalPop]<br>' +
        'This calculation gives you an opportunity to learn the<br>' +
        'importance of verifying your calculations. Compare the<br>' +
        'PerCapitaInc column with the Average of<br>' +
        'PerCapitaIncome column. The value in the total line for<br>' +
        'PerCapitaInc is 36.35. The raw difference is only 1.9,<br>' +
        'but in the business context, this translates to a difference in<br>' +
        'per capita income of almost $2,000 per person compared<br>' +
        'to the Average value. Over the 103 million people in our<br>' +
        'simple data set, this is a total of $196 billion. This is not an<br>' +
        'amount you would care to lose due to an incorrect<br>' +
        'calculation.<br>' +
        'You’ve created all the calculations you need. Wasn’t that<br>' +
        'easy? The rest of this example is standard Excel work and<br>' +
        'is left as an exercise to the student. You might want to<br>' +
        '718<br>' +
        'remove the unused columns, format the remaining<br>' +
        'columns, and add in a scatter chart. Figure 11-8 offers a<br>' +
        'completed PivotTable and chart, along with a correlation<br>' +
        'calculation.<br>' +
        'Try adding a few slicers to the report; drag CustType and<br>' +
        'StateGroup down to the Slicers Horizontal well and see<br>' +
        'how that changes the user experience. If you add multiple<br>' +
        'pivot tables from the PowerPivot window, the slicers are<br>' +
        'automatically configured so they apply to all of the pivot<br>' +
        'tables. In other words, when you change the values<br>' +
        'selected in a slicer, all of the pivot tables change. Slicers<br>' +
        'are pretty cool, but they work best with low cardinality<br>' +
        'columns.<br>' +
        'Note that the scatter chart control does not work directly<br>' +
        'with PivotTable data. You can trick it by choosing Select<br>' +
        'Data in scatter chart, then clicking the Add button in the<br>' +
        'Select Data Source window. Name the series in the Edit<br>' +
        'Series window that pops up, then select the numbers in the<br>' +
        'PerCapitaInc column for the Series X values, and the<br>' +
        'numbers in the NormPenRate column for the Series Y<br>' +
        'values. By the way, when you work through the chart,<br>' +
        'you’ll figure out you need to go back and add an<br>' +
        'IF(ISBLANK(),0,…) around the NormPenRate formula<br>' +
        'in order to show a zero for New Mexico. This will result in<br>' +
        'data points for all eight states on the chart.<br>' +
        'Figure 11-8: Final PivotTable and chart<br>' +
        '719<br>' +
        'To finish the business scenario, Figure 11-8 shows a clear<br>' +
        'correlation between the penetration rate and median<br>' +
        'income. You don’t know cause and effect and your data set<br>' +
        'is a little small to be conclusive, but you have shown an<br>' +
        'interesting relationship that you can explore in more detail.<br>' +
        'Observations and Guidelines on PowerPivot for Excel<br>' +
        'The tag line for this chapter is “Easy is a relative term.”<br>' +
        'Look back over the steps you just went through to create a<br>' +
        'fairly simple analysis. Our conclusion after this and several<br>' +
        'other efforts is that using PowerPivot is not easy. True, it is<br>' +
        'a very powerful tool, and it is easier to load data and query<br>' +
        'it than in earlier versions of Excel. And it’s faster, and you<br>' +
        'can load much more data. However, like other BI tools,<br>' +
        'there are a lot of nuances and subtleties to master before<br>' +
        'you can fully apply the tool. Most organizations will find<br>' +
        'that the people who will be able to do self-service BI using<br>' +
        '720<br>' +
        'PowerPivot are the same people who were already able to<br>' +
        'query data sources, create macros, and use complex<br>' +
        'functions in Excel.<br>' +
        'Whatever your opinion is, our main point is, don’t oversell<br>' +
        'the power of PowerPivot. It has a role in the DW/BI<br>' +
        'system, and can add significant value, but it is not a<br>' +
        'miracle tool.<br>' +
        'Here are a few guidelines for working with PowerPivot for<br>' +
        'Excel, some of which we’ve already mentioned:<br>' +
        '• Work from a dimensional model: PowerPivot is a<br>' +
        'dimensional engine under the covers, and it is most easily<br>' +
        'understood if you create the PowerPivot database using a<br>' +
        'dimensional model. That is, design your PowerPivot<br>' +
        'database with a fact table that joins to a set of dimension<br>' +
        'tables on a strict many-to-one basis.<br>' +
        '• Create incremental calculations: As we described in the<br>' +
        'example, it’s very helpful to build simple measures first, and<br>' +
        'then use those measures to create more complex<br>' +
        'calculations.<br>' +
        '• Check your work: Work through any calculations you do<br>' +
        'manually to verify they are correct. Drag in attributes from<br>' +
        'all of the tables in the PivotTable to verify your calculations<br>' +
        'work across all dimensions as expected. Look for common<br>' +
        'problems like edge condition failures, such as a<br>' +
        'year-over-year calculation in the first year of data, or blanks<br>' +
        'or zeros in the denominator of a ratio.<br>' +
        '• Create a robust date dimension: This is a table with one row<br>' +
        'for every date in your fact table. You can build this in Excel,<br>' +
        'download one from the book’s website, or simply extract it<br>' +
        'from the data warehouse. The date dimension is important<br>' +
        'because PowerPivot has 35 functions expressly designed for<br>' +
        'working with dates. For example, there are functions for<br>' +
        'comparison, such as PREVIOUSYEAR() or<br>' +
        '721<br>' +
        'SAMEPERIODLASTYEAR(), and for selecting data on specific<br>' +
        'dates, such as STARTOFMONTH() or ENDOFYEAR().<br>' +
        '• Use naming conventions: It’s a good idea to establish some<br>' +
        'simple naming conventions for calculated columns and<br>' +
        'measures to help distinguish data that comes directly from<br>' +
        'the source versus data that has been manipulated locally in<br>' +
        'some way. You may also want to hide some of the<br>' +
        'underlying fields if you create measures on top of them, in<br>' +
        'order to reduce the level of complexity other users see in the<br>' +
        'PowerPivot Field List.<br>' +
        'The goal of this section was to give you a good sense for<br>' +
        'what PowerPivot for Excel is and how it works. You can<br>' +
        'make your own assessment of how easy it is, especially<br>' +
        'after you’ve had a chance to get your own hands on the<br>' +
        'tool and try it out.<br>' +
        'The other part of the PowerPivot story is the centralized<br>' +
        'SharePoint component. The next section takes a high level<br>' +
        'look at the main capabilities of PowerPivot for SharePoint.<br>' +
        'PowerPivot for SharePoint<br>' +
        'SharePoint is Microsoft’s web portal and application<br>' +
        'platform. One of the services SharePoint can host is<br>' +
        'PowerPivot. This service is called the PowerPivot add-in<br>' +
        'for SharePoint and it provides three main functions. First it<br>' +
        'allows a PowerPivot for Excel developer to upload a<br>' +
        'PowerPivot-based spreadsheet and share it with others in<br>' +
        'the organization in conjunction with Excel Services.<br>' +
        'Second, it brings server-level resources to bear on<br>' +
        'PowerPivot applications. And third, it allows IT to monitor<br>' +
        'and manage these shared resources, including<br>' +
        'automatically refreshing PowerPivot databases.<br>' +
        '722<br>' +
        'The PowerPivot SharePoint User Experience<br>' +
        'Users work with PowerPivot for SharePoint in three<br>' +
        'different modes: publishing, viewing, and data sourcing.<br>' +
        'We’ll look at each of these in turn.<br>' +
        'PowerPivot Publishing<br>' +
        'PowerPivot for SharePoint is primarily a PowerPivot for<br>' +
        'Excel “viewer” application. The SharePoint user<br>' +
        'experience begins when a user publishes a PowerPivot<br>' +
        'report or analysis. It’s easy for users to upload a<br>' +
        'PowerPivot workbook to the BI Portal site in one of two<br>' +
        'ways. From within the Excel workbook on the user’s<br>' +
        'desktop, open the PowerPivot window and select Publish<br>' +
        'from the File menu. Enter the URL to the appropriate<br>' +
        'directory in the BI Portal and click the Save button. Figure<br>' +
        '11-9 shows the resulting Save As window. Users can<br>' +
        'upload to any SharePoint document library with the<br>' +
        'appropriate permission. Commonly used PowerPivot<br>' +
        'workbooks will usually end up in a special type of<br>' +
        'SharePoint document library called a PowerPivot gallery.<br>' +
        'Figure 11-9: Publishing a PowerPivot workbook from<br>' +
        'Excel to SharePoint<br>' +
        '723<br>' +
        'The Excel publish capability offers two advantages. First,<br>' +
        'it is asynchronous and restartable, so a user can start a<br>' +
        'large upload and proceed with other work. Second, users<br>' +
        'can publish a subset of objects from within an Excel file,<br>' +
        'such as certain worksheets, or even selected objects within<br>' +
        'a worksheet.<br>' +
        'Alternatively, users can upload their PowerPivot<br>' +
        'workbooks by navigating to the appropriate SharePoint<br>' +
        'directory in a web browser and selecting Upload<br>' +
        'Document from the Documents Ribbon under Library<br>' +
        'Tools. A third approach is to use the WebDAV facility,<br>' +
        'where a SharePoint site is made to look like a file share. In<br>' +
        'this case the user would navigate to the SharePoint<br>' +
        'directory in Windows Explorer, and copy in the<br>' +
        'PowerPivot file.<br>' +
        '724<br>' +
        'Once the PowerPivot workbook is in SharePoint, users can<br>' +
        'browse available workbooks using one of several list<br>' +
        'formats, known as views. The Gallery view is the default<br>' +
        'view for a PowerPivot workbook stored in a PowerPivot<br>' +
        'gallery. The Gallery view uses Silverlight to give users a<br>' +
        'more visual sense for the contents of the available<br>' +
        'PowerPivot workbooks. Figure 11-10 shows the thumbnail<br>' +
        'pictures from a PowerPivot report in the Gallery view.<br>' +
        'The ability to visually browse through the PowerPivot<br>' +
        'workbooks and the sheets within those workbooks is<br>' +
        'particularly helpful when you don’t know exactly what<br>' +
        'you are looking for. However, it may not be the best<br>' +
        'interface for infrequent users who need to look through<br>' +
        'hundreds of workbooks for a report they might not<br>' +
        'recognize visually.<br>' +
        'Figure 11-10: SharePoint PowerPivot Gallery view<br>' +
        'By the way, much of the SharePoint user interface,<br>' +
        'including the PowerPivot gallery, uses Silverlight to<br>' +
        'achieve its visual impact. This means PowerPivot users<br>' +
        'will need to view the SharePoint BI portal using a 32-bit<br>' +
        'browser until Microsoft releases a 64-bit version of<br>' +
        'Silverlight.<br>' +
        '725<br>' +
        'PowerPivot Viewing<br>' +
        'Once you click a PowerPivot workbook in the gallery,<br>' +
        'SharePoint invokes the Excel Services web access control<br>' +
        'and opens up the workbook in the browser. Excel Services<br>' +
        'does not offer full Excel functionality, therefore<br>' +
        'PowerPivot for SharePoint offers a very limited set of the<br>' +
        'full featured desktop version of PowerPivot. SharePoint<br>' +
        'users can slice and dice the data using the slicer controls<br>' +
        'and can set or change filters. They cannot make changes to<br>' +
        'the report layout or format, or access the PowerPivot field<br>' +
        'list.<br>' +
        'This server-based functionality is fine for reports and<br>' +
        'analytics that can be driven by a relatively small set of user<br>' +
        'specified parameters. This makes PowerPivot for Excel<br>' +
        'and SharePoint a good candidate for developing the<br>' +
        'enterprise set of standard reports that are often the starting<br>' +
        'point for most business inquiries.<br>' +
        'PowerPivot as a Data Source<br>' +
        'In addition to interacting with a PowerPivot workbook,<br>' +
        'Excel users can use the data set as a starting point for their<br>' +
        'own analyses. There are several ways this can be<br>' +
        'accomplished, each with its own capabilities. The most<br>' +
        'powerful and flexible approach is to simply download the<br>' +
        'Excel workbook and go from there. If you have<br>' +
        'permission, you can do this by selecting the workbook, and<br>' +
        'then selecting Download a Copy in the Documents Ribbon<br>' +
        'under Library Tools. Of course, this copy is now on your<br>' +
        'desktop and will no longer be automatically updated,<br>' +
        'tracked, or managed by SharePoint. You also need Excel<br>' +
        '726<br>' +
        '2010 to manipulate the workbook, and the PowerPivot for<br>' +
        'Excel add-in to edit PowerPivot.<br>' +
        'Users can also extract data from an existing PowerPivot<br>' +
        'data set and use it to create a local PowerPivot data set by<br>' +
        'using the Get External Data from Database function in the<br>' +
        'PowerPivot window. This brings up the same Analysis<br>' +
        'Services report designer we saw in Reporting Services.<br>' +
        'The resulting data set is imported into the workbook and<br>' +
        'becomes a flattened, single table in the PowerPivot<br>' +
        'window. In other words, all the fact and dimension fields<br>' +
        'you select are joined together to create a single table in the<br>' +
        'new PowerPivot window. This can then be refreshed like<br>' +
        'any other PowerPivot data source by clicking the Refresh<br>' +
        'button in the PowerPivot window.<br>' +
        'Finally, the PowerPivot workbook can be used as a data<br>' +
        'source for either an Excel pivot table, a Reporting Services<br>' +
        'report, or any other application that can connect to and<br>' +
        'query SQL Server Analysis Services. Users can have<br>' +
        'SharePoint set this up by selecting the new document<br>' +
        'button in the upper right of the Gallery view for each<br>' +
        'PowerPivot workbook. This creates a workbook<br>' +
        'connection in Excel, which allows the user to access a<br>' +
        'PowerPivot data set hosted in SharePoint much like<br>' +
        'accessing an Analysis Services cube (which is actually<br>' +
        'what it is doing). In this case PowerPivot for Excel does<br>' +
        'not need to be installed locally, although the SQL 2008 R2<br>' +
        'Analysis Services OLE DB provider does need to be<br>' +
        'installed. No data is downloaded to the Excel workbook<br>' +
        'except the PowerPivot table definitions, which are fed into<br>' +
        'a pivot table control. The PowerPivot cube is re-queried<br>' +
        'for every change in the pivot table definition. Since this is<br>' +
        '727<br>' +
        'not PowerPivot, you are limited to the standard pivot table<br>' +
        'functionality.<br>' +
        'Server-Level Resources<br>' +
        'SharePoint brings enterprise functionality to PowerPivot<br>' +
        'through its server infrastructure. This includes workload<br>' +
        'management, data caching, automated data refreshing, and<br>' +
        'security.<br>' +
        'SharePoint can scale out PowerPivot across multiple<br>' +
        'servers in the SharePoint farm to support large numbers of<br>' +
        'users. Incoming user queries can be allocated across<br>' +
        'available servers based on a round robin or server<br>' +
        'health–based methodologies.<br>' +
        'The PowerPivot System service also caches data on<br>' +
        'PowerPivot servers rather than re-extracting the data from<br>' +
        'content servers to speed query response times. There is<br>' +
        'also an associated caching discovery and maintenance<br>' +
        'function that unloads cached data that is no longer needed,<br>' +
        'or that may not be valid.<br>' +
        'Keeping data in spreadsheets current is typically a manual<br>' +
        'process. Fortunately, PowerPivot for SharePoint can be set<br>' +
        'to automatically refresh PowerPivot data. Each PowerPivot<br>' +
        'workbook can be set to refresh its data on its own<br>' +
        'schedule. The data refresh facility is fairly simple. The<br>' +
        'refresh is set to run based on day and time of day, and the<br>' +
        'most frequent refresh interval available is once a day.<br>' +
        'Security within SharePoint does not have the fine grain<br>' +
        'that can be defined in an Analysis Services database, but it<br>' +
        '728<br>' +
        'is much better than a standalone Excel workbook.<br>' +
        'SharePoint security is integrated with the Windows Active<br>' +
        'Directory structures, and can be administered at the site<br>' +
        'and document level. The IT or DW/BI system security<br>' +
        'manager can monitor who is accessing which workbook.<br>' +
        'There is a whole list of SharePoint and third party<br>' +
        'capabilities you can leverage for PowerPivot files. Since<br>' +
        'these files are SharePoint documents, you can enable<br>' +
        'version management, set up approval workflows, create<br>' +
        'email alerts for content changes, and define record<br>' +
        'retention rules.<br>' +
        'PowerPivot Monitoring and Management<br>' +
        'PowerPivot for SharePoint includes a usage logging<br>' +
        'function and a set of BI dashboards to allow the DW/BI<br>' +
        'team to monitor usage and system functions. There will be<br>' +
        'a set of management activities including system resource<br>' +
        'allocation, performance tuning, and security. The DW/BI<br>' +
        'team can leverage some of SharePoint’s workflow<br>' +
        'functions to create a content validation and approval<br>' +
        'process to manage the submission of workbooks for<br>' +
        'publishing, rather than allow users to publish directly.<br>' +
        'PowerPivot Monitoring<br>' +
        'The PowerPivot Management dashboards are built using<br>' +
        'PowerPivot and provide a nice example of how the tool<br>' +
        'can be used. You can view these reports by selecting the<br>' +
        'PowerPivot Management Dashboards link under General<br>' +
        'Application Settings on the SharePoint Central<br>' +
        'Administration home page.<br>' +
        '729<br>' +
        'The PowerPivot Management dashboard includes a whole<br>' +
        'set of reports that look at several measures of user and<br>' +
        'server activity across several dimensions, including date,<br>' +
        'user, server, and workbook. The measures include CPU<br>' +
        'and memory usage, user counts, query counts and response<br>' +
        'times, workbook size, and refresh duration.<br>' +
        'Figure 11-11 shows the Activity view of one of the<br>' +
        'sub-reports on the home page of the PowerPivot<br>' +
        'Management dashboard. This report takes advantage of<br>' +
        'Excel 2010’s conditional formatting and data bars to<br>' +
        'highlight peak days across a range of measures including<br>' +
        'connections, queries, data loaded and unloaded, and user<br>' +
        'counts.<br>' +
        'Figure 11-11: A sub-report in the SharePoint PowerPivot<br>' +
        'Management dashboard<br>' +
        '730<br>' +
        'The PowerPivot Management dashboard home page<br>' +
        'includes a report with a Silverlight slider control that gives<br>' +
        'an animated view of how usage for each workbook is<br>' +
        'changing over time. Very sexy. There are a few additional<br>' +
        'reports that can be accessed from the Reports list in the<br>' +
        'lower-right area of the management dashboard home page.<br>' +
        'Figure 11-12 shows the Workbook Activity report, which<br>' +
        'lists each workbook and shows the users, queries, and<br>' +
        'maximum load size.<br>' +
        'Figure 11-12: The Workbook Activity report from the<br>' +
        'SharePoint PowerPivot Management dashboard<br>' +
        'PowerPivot Management dashboard reports can be a big<br>' +
        'help in identifying performance problems and tracking<br>' +
        'down the likely causes. SharePoint collects and aggregates<br>' +
        'the usage data every night and puts it in a PowerPivot<br>' +
        'workbook. Because the data’s in PowerPivot, you can add<br>' +
        'your own reports to the dashboard, or create separate<br>' +
        '731<br>' +
        'reports that do not require SharePoint Central<br>' +
        'Administration access rights.<br>' +
        'RESOURCES<br>' +
        'Search the internet for “custom powerpivot<br>' +
        'management dashboard” to find several<br>' +
        'Microsoft and third party documents on<br>' +
        'customizing the dashboard.<br>' +
        'PowerPivot Workbook Publishing Process<br>' +
        'Even though it’s pretty easy for a user to publish a<br>' +
        'PowerPivot workbook in SharePoint, you may want to<br>' +
        'help them do it right. This has to do with making sure the<br>' +
        'report contents and calculations are correct, verifying that<br>' +
        'there is no hidden data in the workbook, and determining<br>' +
        'the appropriate security settings.<br>' +
        'RESOURCES<br>' +
        'Search technet.microsoft.com for “Secure a<br>' +
        'PowerPivot Workbook on SharePoint” to<br>' +
        'find additional guidance.<br>' +
        'You can set up a SharePoint workflow and alerts to allow<br>' +
        'users to submit a PowerPivot workbook (or Reporting<br>' +
        'Services report). The workflow would notify the<br>' +
        'appropriate member of the DW/BI team, and guide them<br>' +
        '732<br>' +
        'through the review and approval process. You can start<br>' +
        'creating an approval workflow by selecting the Library<br>' +
        'Settings button in the Library Ribbon of the PowerPivot<br>' +
        'gallery library. Then select Workflow Settings under the<br>' +
        'Permissions and Management heading. The setup forms<br>' +
        'are fairly self-explanatory.<br>' +
        'RESOURCES<br>' +
        'Search the internet for “PowerPivot<br>' +
        'approval workflow” to find several<br>' +
        'Microsoft and third party documents on<br>' +
        'creating SharePoint workflows to manage<br>' +
        'PowerPivot publishing.<br>' +
        'PowerPivot’s Role in a Managed DW/BI Environment<br>' +
        'Since PowerPivot for Excel is a standalone tool, Excel<br>' +
        '2010 users can download and install it without knowledge<br>' +
        'or permission of the IT organization, for free. Regardless<br>' +
        'of what you would prefer, PowerPivot will be part of your<br>' +
        'BI environment.<br>' +
        'While there are limits to what PowerPivot can do, and how<br>' +
        'easy it is to do it, many Excel experts will be able to use it<br>' +
        'to create useful analytics they never could have created<br>' +
        'before. Many of these, like many Excel-based BI<br>' +
        'applications, will be one-off analyses to help understand a<br>' +
        'unique situation, or explore a new idea; or they will only<br>' +
        'be of individual or departmental interest. Some of them<br>' +
        'will prove to have broader value and will rise to the top,<br>' +
        '733<br>' +
        'perhaps identified through SharePoint’s PowerPivot<br>' +
        'Management dashboard.<br>' +
        'Once the broader appeal of these popular reports is known,<br>' +
        'the data sources they use that are not in the data warehouse<br>' +
        'should be brought in through your rigorous ETL process so<br>' +
        'they match the structure, quality, and availability of the<br>' +
        'rest of the DW/BI system. In a way, this is a usage-based<br>' +
        'method for identifying the business value of data. It should<br>' +
        'not be the only method you use to determine what goes<br>' +
        'into the DW/BI system, but it can play a role. The dark<br>' +
        'side of this monitoring is it can be used to police data<br>' +
        'usage and confront users with evidence of their<br>' +
        'transgressions. Do not do this. One of the fastest ways to<br>' +
        'drive your users away is to use these Big Brother tactics.<br>' +
        'On its own, PowerPivot can offer significant value to the<br>' +
        'individual Excel advanced user. It does bring many of the<br>' +
        'same costs Excel has always had in terms of data quality,<br>' +
        'data updates, business rules, and multiple versions.<br>' +
        'PowerPivot for SharePoint helps mitigate some of those<br>' +
        'costs by bringing these reports out into the open where<br>' +
        'they can be validated, shared, and better managed.<br>' +
        'However, SharePoint itself is a big cost, both in terms of<br>' +
        'money and resources.<br>' +
        'Summary<br>' +
        'PowerPivot is powerful analytic tool that allows advanced<br>' +
        'users of Excel to pull together large, disparate data sets and<br>' +
        'explore them to find new relationships and insights that<br>' +
        'can add significant business value. PowerPivot can act as a<br>' +
        'platform to support the development of reports and<br>' +
        '734<br>' +
        'analyses that would otherwise take too long, or would not<br>' +
        'be possible to create at all.<br>' +
        'PowerPivot for SharePoint provides a way for creators of<br>' +
        'PowerPivot databases and analytics to share those with<br>' +
        'others in the organization who may be able to use the same<br>' +
        'analyses. PowerPivot for SharePoint also allows IT to<br>' +
        'monitor and manage these PowerPivot reports, and to<br>' +
        'automatically update the data in the underlying PowerPivot<br>' +
        'databases.<br>' +
        'We see PowerPivot adding value to the DW/BI system in<br>' +
        'the following major ways:<br>' +
        '• Database prototyping: The DW/BI team can use PowerPivot<br>' +
        'to test out dimensional model design options based on real<br>' +
        'data.<br>' +
        '• Data profiling: The DW/BI team can pull data from the<br>' +
        'source systems into PowerPivot and quickly create a set of<br>' +
        'data profile reports and an issues list.<br>' +
        '• Report/Dashboard prototyping and delivery: The DW/BI<br>' +
        'team can use PowerPivot to design complex reports and<br>' +
        'analyses that can be distributed across the enterprise via<br>' +
        'SharePoint. Business users may also create analytics that<br>' +
        'migrate into the DW/BI standard report set.<br>' +
        '• Business user analytics: Power users can create more<br>' +
        'advanced ad hoc analyses, combining data from multiple<br>' +
        'sources with relative ease. This could include external data,<br>' +
        'or new attributes or hierarchies.<br>' +
        'PowerPivot does not remove the need for the DW/BI<br>' +
        'system. All the work that goes into creating a robust ETL<br>' +
        'system and data warehouse, with its data cleaning,<br>' +
        'dimensionalization, and quality controls, is work that must<br>' +
        'be done to support accurate analytics, whether or not<br>' +
        '735<br>' +
        'PowerPivot is part of the system. PowerPivot users will<br>' +
        'generally source their internal data, such as customer,<br>' +
        'account, product, or promotion, straight out of the data<br>' +
        'warehouse database.<br>' +
        'PowerPivot does not replace other components in the BI<br>' +
        'layer either. You will still need Reporting Services to<br>' +
        'manage the execution and distribution of standard reports,<br>' +
        'and to provide interactive execution of those reports as<br>' +
        'needed. Users who are not Excel experts, but who want ad<br>' +
        'hoc access to the data warehouse, may prefer a query and<br>' +
        'reporting tool such as Report Builder 3.0.<br>' +
        'PowerPivot does enable rapid data combination and<br>' +
        'exploration. For those who can use it, it will open up a<br>' +
        'whole new level of analytic possibilities. It will likely be<br>' +
        'the source of much of the unforeseen value of the DW/BI<br>' +
        'system. PowerPivot users are good for you and you need to<br>' +
        'make sure they are well supported.<br>' +
        '736<br>';
    document.getElementById('chapter10').innerHTML = 'Part 3: Developing the BI Applications<br>' +
        'Chapter 10: Building BI Applications in Reporting<br>' +
        'Services<br>' +
        'Chapter 11: PowerPivot and Excel<br>' +
        'Chapter 12: The BI Portal and SharePoint<br>' +
        'Chapter 13: Incorporating Data Mining<br>' +
        'We’ve reached the point in the DW/BI Lifecycle where we<br>' +
        'can actually start delivering value to the business<br>' +
        'community. It turns out this delivery step is more<br>' +
        'important than you might think. We believe the business<br>' +
        'folks should be wildly enthusiastic about getting at their<br>' +
        'data and understanding it better.<br>' +
        'The problem with this belief is that most business people<br>' +
        'do not seem to agree. In fact, based on our experience, you<br>' +
        'will be lucky to get 10 percent of your user base to actually<br>' +
        'build their own reports from scratch. We suspect this is<br>' +
        'because learning the tools and the data is just too far<br>' +
        'outside the comfort zone of most business people.<br>' +
        'Therefore, a critical part of every DW/BI project is<br>' +
        'providing the other 90 percent of the user community with<br>' +
        'a more structured, and easier, way to access the data<br>' +
        'warehouse.<br>' +
        'This section is about the components in the SQL Server<br>' +
        'platform, and in the broader Microsoft product line, which<br>' +
        'you will use to close the last gap between the DW/BI<br>' +
        '607<br>' +
        'system and the business users. These components include<br>' +
        'Reporting Services and Report Builder, Excel and<br>' +
        'PowerPivot, SharePoint, and SQL Server data mining.<br>' +
        'The Kimball Lifecycle steps covered in Part 3<br>' +
        '608<br>' +
        'Chapter 10<br>' +
        'Building BI Applications in Reporting Services<br>' +
        'Building a bridge for those who don’t want to swim.<br>' +
        'We refer to all the tools, reports, and applications used to<br>' +
        'access the DW/BI system for business purposes as BI<br>' +
        'applications. This covers a range of concepts and<br>' +
        'technologies, which we will describe in the first part of this<br>' +
        'chapter.<br>' +
        'One of the BI application categories is called standard or<br>' +
        'enterprise reports. These are the core DW/BI system<br>' +
        'reports, usually created and maintained by the DW/BI<br>' +
        'team, that provide the business with official numbers for a<br>' +
        'given business process. Reporting Services is SQL<br>' +
        'Server’s standard report delivery platform. Therefore, most<br>' +
        'of this chapter is focused on creating and delivering<br>' +
        'standard reports on the Reporting Services platform.<br>' +
        'Every user of your DW/BI system will access it through BI<br>' +
        'applications and especially standard reports. The vast<br>' +
        'majority of those users — typically between 70 and 90<br>' +
        'percent — will use only standard reports. To them, the<br>' +
        'standard reports and the portal they live in are the DW/BI<br>' +
        'system. After working through the relational and OLAP<br>' +
        'database designs and the ETL system to populate them,<br>' +
        'creating reports seems easy. If you’ve built a solid,<br>' +
        'dimensional information infrastructure, creating reports is<br>' +
        'the fun part where all that work finally pays off.<br>' +
        '609<br>' +
        'Reporting Services has been well received by the<br>' +
        'Microsoft customer base and has been successfully<br>' +
        'implemented in many large organizations. Given its<br>' +
        'reasonable level of functionality and its more than<br>' +
        'reasonable incremental cost, we expect that a large<br>' +
        'percentage of folks reading this book will choose<br>' +
        'Reporting Services as the delivery vehicle for their<br>' +
        'standard reports and analytic applications.<br>' +
        'This chapter provides the basic information you need to<br>' +
        'understand the range of BI applications available to you<br>' +
        'and to build your core set of BI applications. We start with<br>' +
        'an introduction to BI applications in general. We then offer<br>' +
        'an overview of Reporting Services as a platform for<br>' +
        'creating and distributing standard reports. Moving down to<br>' +
        'the practical level, we provide a development process for<br>' +
        'creating standard reports in the context of the Kimball<br>' +
        'Lifecycle. This development process applies equally to the<br>' +
        'creation of other kinds of BI applications. The last section<br>' +
        'walks through the creation of a standard report in<br>' +
        'Reporting Services.<br>' +
        'By the end of this chapter, you should be able to answer<br>' +
        'the following questions:<br>' +
        '• What are the various types of BI applications and why are<br>' +
        'they important?<br>' +
        '• What is Reporting Services? How does it work, and where<br>' +
        'does it fit in the overall DW/BI system?<br>' +
        '• What process and techniques should one employ to design<br>' +
        'and develop standard reports or other BI applications?<br>' +
        '• What tools does Reporting Services provide for report<br>' +
        'development?<br>' +
        '610<br>' +
        '• What does it take to create a standard report in the Reporting<br>' +
        'Services environment?<br>' +
        '• What does it take to manage and maintain the standard<br>' +
        'report set?<br>' +
        '• What are the additional components of Report Builder 3.0<br>' +
        'that help make it a reasonable ad hoc query and reporting<br>' +
        'tool?<br>' +
        'Learning Reporting Services<br>' +
        'This chapter is not a tutorial on Reporting Services.<br>' +
        'If you’ve been charged with the task of building<br>' +
        'the initial set of reports, you should first install<br>' +
        'Reporting Services and the samples on your<br>' +
        'development machine. Then review the<br>' +
        'documentation in Microsoft’s Books Online and<br>' +
        'work through the tutorials. You may also want to<br>' +
        'get one of the many Reporting Services books<br>' +
        'available, or even take one of the many classes<br>' +
        'offered on Reporting Services. Visit the book’s<br>' +
        'website for references to a few current books.<br>' +
        'A Brief Overview of BI Applications<br>' +
        'Before we describe how to build a set of BI applications,<br>' +
        'we should be clear about what they are and why they are<br>' +
        'important. This section defines several types of BI<br>' +
        'applications and covers why BI applications are important.<br>' +
        'Types of BI Applications<br>' +
        '611<br>' +
        'There are many tools and report types that pull data from<br>' +
        'the data warehouse, from canned, pre-run reports to<br>' +
        'custom coded applications. In an effort to bring some<br>' +
        'structure to this confusion, Figure 10-1 lists several<br>' +
        'categories of BI applications along with the roles they<br>' +
        'play, the consumer types who typically use them, and the<br>' +
        'Microsoft tools used to build them. These categories<br>' +
        'include:<br>' +
        '• Direct access query and reporting tools: These applications<br>' +
        'allow users to query the dimensional model directly and<br>' +
        'allow users to define a results set. Simple ad hoc tools<br>' +
        'deliver only tabular results sets, while more advanced tools<br>' +
        'allow the creation of fully realized, complex reports. These<br>' +
        'more sophisticated ad hoc tools also serve as the<br>' +
        'development tools for standard reports that other users can<br>' +
        'run themselves.<br>' +
        '• Data mining: Data mining applications are included in the<br>' +
        'direct access box in Figure 10-1 because the process of<br>' +
        'developing a data mining model involves a highly iterative<br>' +
        'direct access data exploration process. The models that are<br>' +
        'the outcome of the data mining process are often embedded<br>' +
        'in other BI applications. We’ve dedicated a chapter (Chapter<br>' +
        '13) to data mining because it is such a powerful component<br>' +
        'of the SQL Server platform.<br>' +
        'Figure 10-1: BI applications, consumer modes, and<br>' +
        'associated Microsoft tools<br>' +
        '612<br>' +
        '• Standard reports: These are predefined, preformatted reports<br>' +
        'that generally provide some level of user interaction, like the<br>' +
        'ability to enter a parameter, drill down to a lower level of<br>' +
        'detail, and link to related reports.<br>' +
        '• Analytic applications: These applications are managed sets<br>' +
        'of reports that usually embed domain expertise about how to<br>' +
        'analyze a particular business process. Most analytic<br>' +
        'applications require an interface layer for user access.<br>' +
        'Analytic applications that include forecasting or prediction,<br>' +
        'such as a promotion analysis system or a sales rep<br>' +
        'dashboard, often take advantage of data mining models.<br>' +
        '• Dashboards and scorecards: These applications generally<br>' +
        'involve a combination of multiple reports and charts in a<br>' +
        'seamless interface that use exception highlighting and<br>' +
        'drill-down capabilities to analyze data from multiple<br>' +
        'business processes.<br>' +
        '• Operational BI and closed loop applications: These include<br>' +
        'the use of applications that are more sophisticated than<br>' +
        'typical operational reports. These applications leverage the<br>' +
        '613<br>' +
        'rich historical context across multiple business processes<br>' +
        'available in the data warehouse to guide operational decision<br>' +
        'making. Operational BI applications often include data<br>' +
        'mining models to help identify patterns and opportunities,<br>' +
        'and make recommendations, at the operational level.<br>' +
        '• The BI portal: The portal is the business’s primary interface<br>' +
        'to the BI applications. It provides an organizing framework<br>' +
        'to help people find the information they need. We describe<br>' +
        'the BI portal in Chapter 12.<br>' +
        'There is a significant overlap across these categories. For<br>' +
        'example, a dashboard may be essentially a collection of<br>' +
        'standard reports integrated into a single interface with a<br>' +
        'common set of parameters. Or, you may use dashboard<br>' +
        'tools to build an analytic application. We recommend you<br>' +
        'concentrate on doing whatever you need to do to deliver<br>' +
        'business value, and don’t get too caught up on the terms.<br>' +
        'The Value of Business Intelligence Applications<br>' +
        'Before we dive into the details of creating these<br>' +
        'applications, it’s worth reviewing the value you get from<br>' +
        'them to help justify the effort. As it turns out, they add<br>' +
        'significant value in several ways.<br>' +
        '• Business value: The process of identifying and creating BI<br>' +
        'applications based on business requirements almost<br>' +
        'guarantees that you will provide something of significant<br>' +
        'value to the business.<br>' +
        '• Broad access: The BI applications provide data warehouse<br>' +
        'access for a broad, important user community. Remember,<br>' +
        '80 percent or more of your knowledge workers will not<br>' +
        'develop the technical skills and data acumen needed to build<br>' +
        'their own reports. You must provide them with a means to<br>' +
        'get the information they need to make informed decisions.<br>' +
        '614<br>' +
        '• Early impact: BI applications built in the development phase<br>' +
        'of the Lifecycle demonstrate the value of the DW/BI system<br>' +
        'from day one. Business users across the organization can<br>' +
        'take advantage of the initial business process dimensional<br>' +
        'model as soon as you deploy it.<br>' +
        '• Data validation: BI applications help validate the data<br>' +
        'warehouse content because they represent real-world<br>' +
        'analyses that bring together dimensions and facts in a way<br>' +
        'that hasn’t happened prior to this point. Typically, you will<br>' +
        'uncover some data irregularities, in spite of your rigorous<br>' +
        'data quality and design efforts.<br>' +
        '• Query performance: Similar to data validation, the BI<br>' +
        'applications generate more complex queries than the basic<br>' +
        'testing that has taken place prior to this point. So much so,<br>' +
        'you should capture the SQL and MDX queries from the BI<br>' +
        'applications and use them to generate some of the ongoing<br>' +
        'performance metrics.<br>' +
        '• Tool functionality: Because the BI applications are real<br>' +
        'business analyses, it is important that your front-end tool be<br>' +
        'able to handle them easily. Building BI applications during<br>' +
        'development provides an opportunity to test the ability of the<br>' +
        'tools to meet the business needs. You can bring in a<br>' +
        'development expert from the vendor’s consulting<br>' +
        'organization (surely, you negotiated this as part of your<br>' +
        'purchase) to show you how to work around some of the<br>' +
        'rough edges of the product.<br>' +
        '• Relationship building: Including your power users in the BI<br>' +
        'application development process is a great way to keep them<br>' +
        'excited about the DW/BI system and motivated to climb the<br>' +
        'learning curve. The users get early, supervised training on<br>' +
        'the reporting tool and the data, and the team gets extra help<br>' +
        'building the applications. (So maybe it isn’t so helpful in<br>' +
        'terms of actually getting reports built, but the relationship<br>' +
        'part is worth the extra effort.) Make this process more fun by<br>' +
        'setting up a development lab where users and team members<br>' +
        'can all work and learn together. Bring donuts.<br>' +
        '• Feedback: Finally, building BI applications helps the DW/BI<br>' +
        'team experience the impact of their design decisions. Many<br>' +
        '615<br>' +
        'of the tradeoffs that were made during the design phase<br>' +
        'surface at this point. For example, it is now possible to<br>' +
        'estimate the full cost of decisions about pre-calculating a<br>' +
        'value versus letting the users calculate it in their reports.<br>' +
        'Consider having your data modelers and ETL developers<br>' +
        'participate in creating some of the BI applications.<br>' +
        'Experience is the best teacher.<br>' +
        'We hope you were already planning to include BI<br>' +
        'applications as part of your DW/BI system development<br>' +
        'effort and this section has served only to highlight the<br>' +
        'wisdom of your plans. Now that you are appropriately<br>' +
        'motivated, let’s dig into the process of designing and<br>' +
        'developing the standard reports set of BI applications on<br>' +
        'the Reporting Services platform, starting with the<br>' +
        'product’s overall architecture.<br>' +
        'A High-Level Architecture for Reporting<br>' +
        'With an understanding of BI applications in place, we now<br>' +
        'turn our attention to the technology in SQL Server used to<br>' +
        'create and deliver them. Remember, the initial step in any<br>' +
        'development effort is to understand the business<br>' +
        'requirements. The Lifecycle shows that business<br>' +
        'requirements determine the architecture, and the<br>' +
        'architecture defines product requirements. Standard report<br>' +
        'users’ business requirements should determine the<br>' +
        'capabilities your BI applications need to provide, and the<br>' +
        'specific functionality of the tool you use.<br>' +
        'Following the Lifecycle flow, this section begins with a<br>' +
        'list of high-level business requirements for standard<br>' +
        'reporting and the architectural or functional implications of<br>' +
        'each of these requirements. Next we present an<br>' +
        '616<br>' +
        'architectural overview of Reporting Services, Microsoft’s<br>' +
        'enterprise reporting platform. Then, we examine Reporting<br>' +
        'Services to see how well it maps back to the general<br>' +
        'reporting requirements.<br>' +
        'This is a good time for you to pause and consider your<br>' +
        'organization’s reporting and analysis requirements because<br>' +
        'it may turn out that Reporting Services doesn’t provide all<br>' +
        'the functionality your users need. You will need to gather<br>' +
        'detailed requirements for reporting and analysis as part of<br>' +
        'the requirements definition process. The functional list we<br>' +
        'provide here is not enough for you to do a rigorous product<br>' +
        'evaluation.<br>' +
        'Reviewing Business Requirements for Reporting<br>' +
        'The real, detailed business requirements will come from<br>' +
        'the requirements-gathering process. The steps outlined in<br>' +
        'this section are not a substitute for that process. However,<br>' +
        'it’s possible to identify some common, high-level business<br>' +
        'requirements for reporting. Create a mental image of your<br>' +
        'user community. The group includes people at all levels of<br>' +
        'the organization, with a broad range of technical skills.<br>' +
        'The common element is that they’re business focused. As<br>' +
        'a result, they’re generally not that excited by technology<br>' +
        'and will rarely build their own queries from scratch.<br>' +
        'They’re more interested in getting a quick answer to a<br>' +
        'specific business question than in working through an<br>' +
        'analysis process or figuring out the correct SQL or MDX<br>' +
        'syntax for a query.<br>' +
        'Table 10-1 summarizes the major, high-level requirements<br>' +
        'of this group related to standard reports. A few of the<br>' +
        '617<br>' +
        'major functional implications are listed for each<br>' +
        'requirement. Look at the second row of the table for<br>' +
        'example: In order to meet the business need to find reports,<br>' +
        'the DW/BI team will need to provide navigation, metadata,<br>' +
        'and search functions. Table 10-1 serves as a roadmap for<br>' +
        'describing the basic requirements for reporting and their<br>' +
        'architectural implications.<br>' +
        'Table 10-1: User requirements and functional implications<br>' +
        'Business<br>' +
        'Requirement<br>' +
        'Functional Implications<br>' +
        'Create reports<br>' +
        '• Powerful, easy, fast report development tool • Variety of<br>' +
        'presentation formats (tables, charts, matrices, interactive<br>' +
        'pivots, maps, and so on)<br>' +
        '• Compound reports with shared controls and parameters<br>' +
        'Find reports<br>' +
        '• Navigation framework • Metadata<br>' +
        '• Search<br>' +
        '• Personalization (“My Reports”)<br>' +
        'View reports<br>' +
        '• Access through a variety of methods and devices •<br>' +
        'User-initiated (for example, browser-based)<br>' +
        '• System-initiated (for example, auto email)<br>' +
        'Receive<br>' +
        'results in most<br>' +
        'useful way<br>' +
        '• Output to a variety of file types, formats, and locations<br>' +
        'Change report<br>' +
        'as needed<br>' +
        '• Parameters • Drill down/additional attributes<br>' +
        '• Linking<br>' +
        'Solid, reliable<br>' +
        'system<br>' +
        '• Performance • Scalability<br>' +
        '• Management<br>' +
        'In practice, you need to back up each functional<br>' +
        'implication with a more detailed description of the<br>' +
        'required functionality. For example, if you ask any tool<br>' +
        'vendor a question like “Do you provide a variety of<br>' +
        'presentation formats?” the answer, spoken in a loud,<br>' +
        '618<br>' +
        'confident manner, will be “Absolutely!” Instead, you need<br>' +
        'to list several detailed examples of how people need to see<br>' +
        'information in your organization. Make them tough,<br>' +
        'real-world examples and use them to test out the tool’s<br>' +
        'functionality and to give you a chance to see how difficult<br>' +
        'it is to work with the tool.<br>' +
        'Examining the Reporting Services Architecture<br>' +
        'The primary intent of Reporting Services is to provide a<br>' +
        'solid, scalable, extensible enterprise reporting<br>' +
        'infrastructure. This helps us understand the pieces of the<br>' +
        'architecture and how they fit together to become Reporting<br>' +
        'Services. Figure 10-2 shows the high-level architecture of<br>' +
        'Reporting Services and its operating environment.<br>' +
        'Reporting Services is a Windows service that contains<br>' +
        'three applications: Report Manager, Reporting Services<br>' +
        'Web service, and a background processing application.<br>' +
        'Each of these applications call on a set of shared<br>' +
        'processing extensions that provide specific functionality.<br>' +
        'Reporting Services uses HTTP.SYS to provide web<br>' +
        'functionality, which means the service is accessible either<br>' +
        'through a browser pointed to the Report Server URL, or<br>' +
        'through an application using the SOAP API. The SOAP<br>' +
        'API allows developers to integrate reports seamlessly into<br>' +
        'other applications. It also means Reporting Services no<br>' +
        'longer has a dependency on Microsoft’s web server, IIS.<br>' +
        'The Report Server communicates with its metadata store<br>' +
        'hosted in a SQL Server database called ReportServer. The<br>' +
        'ReportServer database stores all the information needed to<br>' +
        'define and manage the reports and the report server. It also<br>' +
        '619<br>' +
        'is used to cache data to improve performance. It is not the<br>' +
        'source for report data.<br>' +
        'At the core of the report server is a processing engine that<br>' +
        'supports functions like sorting, filtering, aggregations, and<br>' +
        'conditional formatting. It has several components that are<br>' +
        'designed to be extensible: Data, Rendering, Security, and<br>' +
        'Delivery.<br>' +
        '• Data: A data extension allows the server to connect to a data<br>' +
        'source. Reporting Services ships with several data<br>' +
        'extensions, including SQL Server, Analysis Services,<br>' +
        'Oracle, SAP NetWeaver BI, Teradata, Hyperion Essbase,<br>' +
        'and ADO.NET. ADO.NET indirectly provides access to a<br>' +
        'wide range of data sources that have OLE DB or ODBC<br>' +
        'drivers. Microsoft provides a set of APIs in the data<br>' +
        'extension space if you need to add a data extension of your<br>' +
        'own. If you have invested in an ADO.NET data extension,<br>' +
        'you can plug it into the Report Server.<br>' +
        'Figure 10-2: The Reporting Services 2008 R2 architecture<br>' +
        '620<br>' +
        '• Rendering: Rendering extensions allow the processing<br>' +
        'engine to take a report defined in the Report Definition<br>' +
        'Language (RDL) and output it to any number of formats<br>' +
        'including HTML, Excel, PDF, CSV, images, and others.<br>' +
        'There is also a rendering extension that generates<br>' +
        'Atom-compliant data feeds that can be read by an<br>' +
        'application. For example, an easy way to get a chunk of data<br>' +
        'to PowerPivot might be via a Reporting Services Atom data<br>' +
        'feed. Beyond this, you can write your own rendering<br>' +
        'extension and add it to the list, but it is non-trivial because of<br>' +
        'the complexity of the formatting options in RDL.<br>' +
        '• Security: The standard edition of Reporting Services relies<br>' +
        'on existing Windows authentication for security. If you have<br>' +
        'an application that is not using Windows Integrated security,<br>' +
        'you can support it through an extensible security component<br>' +
        'included in SQL Server Enterprise Edition.<br>' +
        '• Delivery: Reporting Services supports several ways to<br>' +
        'distribute reports in addition to direct access to the web<br>' +
        'server. The delivery function allows you to send reports<br>' +
        'through file shares and email. This, too, is extensible, and<br>' +
        '621<br>' +
        'partners have built other delivery options like fax and<br>' +
        'networked printers.<br>' +
        'From a user access perspective, the upper-right corner of<br>' +
        'Figure 10-2 shows how Reporting Services provides three<br>' +
        'major methods for directly interacting with the server.<br>' +
        'Most users access the server through the web browser<br>' +
        'using a URL that points to the reporting service. As we<br>' +
        'described earlier, it is also possible to access the server<br>' +
        'through an application using SOAP APIs. The<br>' +
        'management functions are accessible through the<br>' +
        'Windows Management Instrumentation (WMI) provider<br>' +
        'objects. This is what the Report Manager uses from the<br>' +
        'browser or from the Management Studio to manage<br>' +
        'Reporting Services.<br>' +
        'Overall, the Reporting Services architecture accomplishes<br>' +
        'its primary intent — it is an extensible, scalable reporting<br>' +
        'infrastructure designed to meet the technical needs of a<br>' +
        'broad range of organizations, from small companies to<br>' +
        'large enterprises. While good technology is important,<br>' +
        'technical products succeed only if they meet business<br>' +
        'users’ needs. The next section compares this architecture<br>' +
        'to see how it maps to the business requirements for<br>' +
        'reporting.<br>' +
        'Using Reporting Services as a Standard Reporting Tool<br>' +
        'The goal here is to make sure Reporting Services provides<br>' +
        'the necessary functionality to meet a general set of<br>' +
        'business requirements for reporting. We compare<br>' +
        'Reporting Services to the list of general requirements from<br>' +
        'Table 10-1 to see how it does.<br>' +
        '622<br>' +
        'Creating Reports<br>' +
        'Reporting Services has two primary tools for authoring<br>' +
        'reports: the Report Designer in BI Development Studio,<br>' +
        'and Report Builder 3.0. As we discussed in Chapter 3,<br>' +
        'BIDS lives in, and leverages the power of, the Visual<br>' +
        'Studio development environment. This means the person<br>' +
        'creating the reports in Report Designer is working in a<br>' +
        'software development environment. The report creator<br>' +
        'needs to know about words like debug, build, and deploy.<br>' +
        'He’ll need to be able to create a data connection and write<br>' +
        'SQL. It’s not a place for the faint of heart, and certainly<br>' +
        'not a place for the vast majority of end users. Report<br>' +
        'Designer allows developers to create reports, manage them<br>' +
        'in source control, and publish them up to the report server.<br>' +
        'Report Builder 3.0 is a stand-alone report authoring tool.<br>' +
        'You can access it by selecting the Report Builder button<br>' +
        'on the menu bar in the Report Manager, or you can<br>' +
        'download and install it as a stand-alone reporting tool. It<br>' +
        'has a more Office-like Ribbon interface, and offers<br>' +
        'essentially the same report components and design<br>' +
        'experience as the Report Designer in BIDS. In addition,<br>' +
        'Report Builder can use shared datasets and pre-defined<br>' +
        'report parts that it draws from a library on the report<br>' +
        'server. Report Builder can read and write reports directly<br>' +
        'to the report server, or save them locally.<br>' +
        'After the report is defined in Report Designer or Report<br>' +
        'Builder, the definition is saved in Report Definition<br>' +
        'Language (RDL), which is an open XML schema for<br>' +
        'defining all the components of the report. This includes the<br>' +
        'definition of the datasets; calculations, expressions,<br>' +
        '623<br>' +
        'conditional formatting, sorts, and filters; and layout of<br>' +
        'information including tables, pivots, charts, text, and<br>' +
        'formatting. When you publish, or deploy, a report to the<br>' +
        'Report Server, it writes the RDL out to the Reporting<br>' +
        'Services database in an XML data type field in one of the<br>' +
        'metadata tables. Any tool that creates RDL files can<br>' +
        'leverage Reporting Services as their enterprise report<br>' +
        'distribution and execution engine.<br>' +
        'Report Definition Language<br>' +
        'You can see what the Report Definition Language<br>' +
        'looks like by opening up a report in a browser<br>' +
        '(look for files ending in .rdl). If you want to see the<br>' +
        'full XML schema, go to the beginning of the .rdl<br>' +
        'file and look for the URL of the namespace. It<br>' +
        'should be right after “xmlns=”. Copy the URL you<br>' +
        'see into the address box of another browser<br>' +
        'window. You can see the entire XML schema of<br>' +
        'the Report Definition Language.<br>' +
        'Report Designer and Report Builder include tools to create<br>' +
        'complex cross-tab reports, charts, maps, sparklines, data<br>' +
        'bars, and various indicators. Because Report Designer is<br>' +
        'oriented more toward programmers, the solution to many<br>' +
        'problems is to write code rather than use the GUI to make<br>' +
        'a selection, drag and drop, or check a box, as with some of<br>' +
        'the third party tools used for creating reports. Report<br>' +
        'Builder can also be fairly complex to use. Of course, this is<br>' +
        'the universal tradeoff: power and flexibility versus ease of<br>' +
        'use. The nature of this tradeoff will become clearer later in<br>' +
        '624<br>' +
        'this chapter when we create an example report. It will<br>' +
        'become obvious when you begin working with the tool<br>' +
        'firsthand to create real reports.<br>' +
        'Finding Reports<br>' +
        'After the developers have created and deployed a set of<br>' +
        'standard reports, users need to be able to find a report they<br>' +
        'want when they want it. This begins when you organize<br>' +
        'the reports into categories that make sense to users, as we<br>' +
        'described in the section on creating the navigation<br>' +
        'framework earlier.<br>' +
        'Microsoft has included a basic navigation framework<br>' +
        'system as part of the Reporting Services product called<br>' +
        'Report Manager. The Report Manager plays two roles. It<br>' +
        'allows the developer to set some of the parameters and<br>' +
        'properties of the report server environment and the reports<br>' +
        'themselves. It can also serve as a simple vehicle to<br>' +
        'organize standard reports and deliver them to the users.<br>' +
        'Figure 10-3 shows the top level of a simple Report<br>' +
        'Manager home page. The interface is essentially a list of<br>' +
        'directories that follow the basic file system tree structure<br>' +
        'of the projects that have been deployed to the report server.<br>' +
        'This simple three-level hierarchy (Home/Project/Report)<br>' +
        'serves as the rough navigation framework for the user.<br>' +
        'You can add levels to this hierarchy by adding folders in<br>' +
        'the Report Manager or in the TargetReportFolder property<br>' +
        'of the Project Properties under the View menu of the<br>' +
        'Report Designer. When the report is deployed to the<br>' +
        'server, the default target folder is based on the BIDS<br>' +
        'project name. To view a report, the user navigates the<br>' +
        '625<br>' +
        'folders starting with the Report Manager home and then<br>' +
        'selects the desired report name.<br>' +
        'Figure 10-3: Report Manager home page<br>' +
        'Figure 10-4 shows the three Sales by Product reports<br>' +
        'available when the user clicks on the 01 - Product Mgmt -<br>' +
        'Sales by Product directory shown in Figure 10-3.<br>' +
        'Figure 10-4: Reports in the Sales by Product directory<br>' +
        '626<br>' +
        'Under the covers, Report Manager is a web application<br>' +
        'that uses the SOAP and WMI protocols to access to the<br>' +
        'Report Server and its contents. SQL Server Management<br>' +
        'Studio also provides some operational report management<br>' +
        'functionality. This primarily includes defining security<br>' +
        'roles and shared schedules, and monitoring of report<br>' +
        'execution. Management Studio does not display the<br>' +
        'contents of the reports or provide the other user-oriented<br>' +
        'functions that Report Manager in the browser does.<br>' +
        'Report Manager does provide many useful functions.<br>' +
        'Because it’s based on the file system directory structure,<br>' +
        'users can view reports as soon as they’ve been deployed. It<br>' +
        'also provides some means for users to subscribe to reports<br>' +
        'and publish their own reports if they have appropriate<br>' +
        'permission. There is a search capability within the report<br>' +
        'site, which searches both the file names and description<br>' +
        'metadata in the Reporting Services catalog. Report<br>' +
        'Manager displays parameter entry boxes and dropdown<br>' +
        'choice lists in the Report Manager header area, enabling<br>' +
        'users to enter their own choices and rerun the report. There<br>' +
        'is a Find function to search within the body of a report,<br>' +
        'which can be particularly helpful in large reports. Finally,<br>' +
        'users have the ability to export the report to a file in any of<br>' +
        'several formats.<br>' +
        'In spite of all this useful stuff, Report Manager is a limited<br>' +
        'report delivery solution. You can customize its appearance<br>' +
        'by changing the color scheme and displaying your own<br>' +
        'logo. But at the end of the day the reports are still grouped<br>' +
        'by project and ordered by name. If Report Manager<br>' +
        'doesn’t offer the functionality you need, you can build<br>' +
        '627<br>' +
        'your own portal. The sample set includes a sample called<br>' +
        'RSExplorer that shows you how.<br>' +
        'On the other hand, this is why SharePoint is often part of<br>' +
        'the overall BI delivery solution, in spite of the extra effort.<br>' +
        'You can completely customize the look and feel of the BI<br>' +
        'portal in SharePoint. We begin to explore this option in<br>' +
        'Chapter 12. All of the report management capabilities are<br>' +
        'available when you run Reporting Services in SharePoint<br>' +
        'integrated mode.<br>' +
        'While it is not a full-featured information portal, Report<br>' +
        'Manager does provide enough functionality to be<br>' +
        'considered a viable delivery solution. Even in the face of<br>' +
        'these (and other) limitations, many companies have<br>' +
        'successfully employed Report Manager as their standard<br>' +
        'report delivery vehicle. In any case, it does give you an<br>' +
        'out-of-the-box starting point you can use to get going<br>' +
        'quickly and verify your business users’ needs before<br>' +
        'building a custom portal.<br>' +
        'Viewing Reports<br>' +
        'After a user has found a report that seems like it might<br>' +
        'contain the needed information, he has to have a way to<br>' +
        'view its contents. This is known as a “pull” model, where<br>' +
        'the user finds the reports and interactively pulls the data<br>' +
        'from the server.<br>' +
        'Users can view reports through any application that can<br>' +
        'access the report server, either through a URL with an<br>' +
        'Internet browser or through an application that uses the<br>' +
        'SOAP methods, like the Report Manager interface. Both<br>' +
        '628<br>' +
        'allow easy integration of Reporting Services reports into<br>' +
        'existing portals or into custom-built or third-party<br>' +
        'applications.<br>' +
        'The browser is the most popular tool for viewing reports.<br>' +
        'Using a browser means users don’t need additional<br>' +
        'software installed on their machines and IT doesn’t need to<br>' +
        'manage the software distribution and upgrade process.<br>' +
        '(Unless, of course, your users have browsers other than<br>' +
        'Internet Explorer, or they have a version of Internet<br>' +
        'Explorer not supported by Reporting Services.)<br>' +
        'Figure 10-5 shows what the user would see in the browser<br>' +
        'as a result of clicking on the Product Subcategory Trend<br>' +
        'link shown in Figure 10-4.<br>' +
        'This example shows the report in the Report Manager<br>' +
        'interface. You can view the report directly in the browser<br>' +
        'without the Report Manager’s organizing structure. You’ll<br>' +
        'see this direct access when we include a report in a simple<br>' +
        'portal structure later in this chapter.<br>' +
        'The custom application approach to accessing reports<br>' +
        'might be something like an ASP.NET application built to<br>' +
        'integrate reports into a larger system that provides<br>' +
        'additional functionality. For example, a customer care<br>' +
        'system might use Reporting Services to display a<br>' +
        'customer’s purchasing and returns history to the customer<br>' +
        'care agent. On the same screen, there can be a button to<br>' +
        'allow the agent to select a recent order and submit it to the<br>' +
        'transaction system to generate a Returned Merchandise<br>' +
        'Authorization.<br>' +
        '629<br>' +
        'Figure 10-5: The Product Subcategory Sales Trend report<br>' +
        'Receiving Results<br>' +
        'Reporting Services offers several delivery methods and file<br>' +
        'formats other than the pull approach described in the<br>' +
        'Viewing Reports section. It’s helpful to look at these<br>' +
        'options in two separate categories: first in terms of<br>' +
        'delivery methods, and then in terms of formats.<br>' +
        'The idea of the push model is to deliver the report to the<br>' +
        'user based on a predefined schedule. Selecting<br>' +
        '“Subscribe…” from a given report’s drop-down menu in<br>' +
        '630<br>' +
        'the Report Manger takes the user to a page for defining the<br>' +
        'nature and timing of the subscription. The reports data<br>' +
        'sources must use the proper credential settings to allow it<br>' +
        'to be executed in a batch fashion. The subscription<br>' +
        'includes a schedule based on a time event or a data event.<br>' +
        'In the case of a time event, the report is distributed at a<br>' +
        'certain time on a periodic basis (at 8:00 a.m. every<br>' +
        'weekday, for example). In the case of a data event, the<br>' +
        'report is distributed whenever the underlying report<br>' +
        'snapshot is updated. Users can create their own<br>' +
        'subscriptions to reports they’d like pushed to them, or the<br>' +
        'report administrator can create shared schedules that send<br>' +
        'reports to a list of users. Reporting Services supports<br>' +
        'pushing reports out through email or to a file share.<br>' +
        'However, because this function is extensible, several<br>' +
        'companies are offering more push-style delivery options,<br>' +
        'like faxing and printing.<br>' +
        'Reporting Services provides several output formats for<br>' +
        'exporting or subscribing to a report. These formats include<br>' +
        'Excel, XML, TIFF, PDF, CSV, images, and various<br>' +
        'HTML versions. Like the delivery mode, the format<br>' +
        'choices are extensible.<br>' +
        'Changing Reports<br>' +
        'Static reports based on carefully identified business<br>' +
        'requirements often provide enough information to answer<br>' +
        'commonly asked questions. In most organizations,<br>' +
        'standard report users want the ability to make changes to a<br>' +
        'report without having to ask someone else to do it, or<br>' +
        'having to learn the complexities of the data model and the<br>' +
        'ad hoc tool. In these cases, it makes sense to provide users<br>' +
        '631<br>' +
        'with the ability to customize the standard reports to meet<br>' +
        'their individual needs. Reporting Services includes several<br>' +
        'common functions that provide users with the ability to<br>' +
        'change a report, including parameters, report linking,<br>' +
        'drill-down, and content modification.<br>' +
        'The report in Figure 10-5 includes a parameter in the<br>' +
        'control section above the body of the report section labeled<br>' +
        'Select Product Category. The parameter has been set to the<br>' +
        'value Bikes using a pull-down menu. A user can select a<br>' +
        'particular product category and then click the View Report<br>' +
        'button to re-execute the report. We discuss some of the<br>' +
        'other functions for allowing users to interact with reports<br>' +
        'in the second half of this chapter when we step through the<br>' +
        'process of building a report.<br>' +
        'The Limits of Security<br>' +
        'Once someone removes a report from the<br>' +
        'Reporting Services environment, either through<br>' +
        'email or by directly exporting the report, access to<br>' +
        'the data is no longer managed by Reporting<br>' +
        'Services. Once users have the information in a file<br>' +
        'format, there’s very little the DW/BI team can do<br>' +
        'to control what they do with it. This is one of the<br>' +
        'reasons the DW/BI system must have a clear<br>' +
        'security and privacy policy, and compliance with<br>' +
        'that policy must be monitored. See Chapter 14 for<br>' +
        'more details.<br>' +
        'Solid, Reliable System<br>' +
        '632<br>' +
        'Having a solid, reliable system is not the kind of business<br>' +
        'requirement you like to hear from your users. The only<br>' +
        'time they specifically mention this requirement is when<br>' +
        'previous efforts haven’t performed well or have been<br>' +
        'unreliable. Regardless of the history, you want your DW/<br>' +
        'BI system to meet expectations. This involves setting up<br>' +
        'the standard reporting process infrastructure, securing<br>' +
        'access to reports, and managing performance.<br>' +
        'The process infrastructure is a combination of Reporting<br>' +
        'Services functions like shared schedules, and<br>' +
        'process-oriented metadata connections with the rest of the<br>' +
        'DW/BI system. You need to create a mechanism for<br>' +
        'initiating the execution of a set of standard reports when a<br>' +
        'particular ETL process completes. The process should<br>' +
        'implement simple logic like, “When the nightly orders<br>' +
        'update is finished, start these reports.”<br>' +
        'Security is also part of a solid, reliable reporting system.<br>' +
        'After you’ve designed the report, you need to control who<br>' +
        'has access to the report, when they can view it, and how<br>' +
        'they can interact with it. This can be accomplished using<br>' +
        'role-based security. These roles can be managed through<br>' +
        'SQL Server Management Studio and assigned to users in<br>' +
        'Report Manager.<br>' +
        'Performance is part of the system reliability job. If the<br>' +
        'report server is too slow, users will get frustrated.<br>' +
        'Reporting Services has several options for managing<br>' +
        'performance, including scaling out to multiple report<br>' +
        'servers, and scheduling reports to be executed during low<br>' +
        'demand windows like early in the morning. You can set up<br>' +
        'a large report to execute on a regular schedule, and save its<br>' +
        '633<br>' +
        'results in an intermediate snapshot structure. Users have<br>' +
        'fast access to the snapshot rather than re-executing the<br>' +
        'report itself. This option works well for the daily load<br>' +
        'cycle of the standard DW/BI system. In Chapter 17, we<br>' +
        'discuss ways to monitor performance over time to find<br>' +
        'opportunities for improvement.<br>' +
        'Reporting Services Assessment<br>' +
        'Overall, Reporting Services provides the basic<br>' +
        'functionality needed to create and deliver standard reports<br>' +
        'that will meet a majority of the business requirements.<br>' +
        'Because it’s oriented toward developers, it’s more difficult<br>' +
        'to use than other reporting tools. Creating reports will take<br>' +
        'a bit longer. This is balanced by the flexibility of the<br>' +
        'programming paradigm. You can generally create a<br>' +
        'work-around to solve most any problem.<br>' +
        'In a way, Reporting Services is a “nobody ever got fired”<br>' +
        'choice. The incremental cost and reasonable functionality<br>' +
        'make it an easy decision. Reporting Services’ Report<br>' +
        'Builder component offers acceptable ad hoc query<br>' +
        'functionality to those advanced business users who are<br>' +
        'capable of and interested in developing their own queries.<br>' +
        'When you add in the reporting and analysis capabilities of<br>' +
        'Office/Excel, and the collaboration and structure of<br>' +
        'SharePoint, you should be able to solve most of the<br>' +
        'significant reporting and analysis related business<br>' +
        'problems.<br>' +
        'Of course, there are third party tools for query and<br>' +
        'reporting, and many organizations already have licenses<br>' +
        'for one or more of these tools. You will need to decide<br>' +
        '634<br>' +
        'how these tools will fit into your overall DW/BI system<br>' +
        'strategy. Remember, selecting end-user tools is more than<br>' +
        'just a technical process. It is also highly political and<br>' +
        'personal. You need to get the end users deeply involved in<br>' +
        'making these decisions.<br>' +
        'The Reporting System Design and Development Process<br>' +
        'With an understanding of the Reporting Services<br>' +
        'architecture in place, you can now start working on your<br>' +
        'standard reports. This section uses the Lifecycle approach<br>' +
        'to outline a process for defining and building your standard<br>' +
        'reports. This same process can be easily adapted to other<br>' +
        'BI applications types as well.<br>' +
        'The BI application track highlighted in Figure 10-6<br>' +
        'includes two major steps: design and development. The<br>' +
        'design step begins soon after the business requirements are<br>' +
        'complete. Most of the design effort is about identifying<br>' +
        'and documenting the set of reports and analyses you will<br>' +
        'deliver as part of the current Lifecycle iteration.<br>' +
        'Figure 10-6: The BI application track in the Kimball<br>' +
        'Lifecycle<br>' +
        '635<br>' +
        'The development step is about building the target set of<br>' +
        'reports and analyses and the portal environment where<br>' +
        'they can be found. Of course, you can’t really get started<br>' +
        'on the development step until you have data available in<br>' +
        'the target dimensional model and the BI tools are installed.<br>' +
        'Let’s examine these two steps in more detail.<br>' +
        'Reporting System Design<br>' +
        'The goal of the design step is to capture what you learned<br>' +
        'about reporting needs during the requirements definition<br>' +
        'process in a way that can be quickly turned into real<br>' +
        'applications once the pieces are in place. Create these<br>' +
        'specifications as soon after gathering requirements as<br>' +
        'possible. The longer you wait, the harder it will be to<br>' +
        'remember the details. It’s a good idea to include some of<br>' +
        'your key end users in this process of defining the<br>' +
        'applications, assigning priorities, and generally making<br>' +
        'sure you get it right. Report specification typically includes<br>' +
        'the following tasks:<br>' +
        '• Determining the initial report set<br>' +
        '636<br>' +
        '• Creating a standard look-and-feel template<br>' +
        '• Creating a mock-up and documentation for each target report<br>' +
        '• Designing the navigation framework<br>' +
        '• Conducting the user review<br>' +
        'Let’s go through each of these tasks in a bit more detail.<br>' +
        'RESOURCES<br>' +
        'The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition (Wiley, 2008) pp. 505–521<br>' +
        'offers additional details and tools on the BI<br>' +
        'application design process.<br>' +
        'Determining the Initial Report Set<br>' +
        'The first step in creating the target BI application or report<br>' +
        'set is to go back through the user interview documentation<br>' +
        'and pull out every reporting/analysis request, desire,<br>' +
        'fantasy, or hope that anyone expressed. As you make this<br>' +
        'list of candidate reports, give each report its own name and<br>' +
        'description. Capture your best sense of its business value<br>' +
        'and the effort it will take to build. Group related reports<br>' +
        'that draw on the same data sources to speed the<br>' +
        'prioritization process. It also helps to make note of who<br>' +
        'asked for it, (there may be several people), and any other<br>' +
        'parties you think might benefit from it. These are the folks<br>' +
        'who will help you further define the report should it make<br>' +
        'it on to the target list.<br>' +
        'It’s easiest to capture this list in a spreadsheet, like the<br>' +
        'example shown in Figure 10-7, or perhaps in SharePoint.<br>' +
        '637<br>' +
        'The list shown here is a bit simplified in order to fit on the<br>' +
        'page. Your list will likely have additional descriptive<br>' +
        'information.<br>' +
        'Figure 10-7: Example candidate report list<br>' +
        'Work with some of the key business users to make sure<br>' +
        'you have a complete list of candidate reports and that they<br>' +
        'are ranked in priority order. Review the list and the<br>' +
        'business value scores with the group, making sure<br>' +
        'everyone understands the reports. Some reports, like actual<br>' +
        'orders versus quota, might be particularly interesting to the<br>' +
        'VP of Sales, but they may get a lower priority than the<br>' +
        'reports that include only orders data because quota<br>' +
        'information may not be in the DW/BI system yet. If<br>' +
        'everyone agrees on the business value score, the rest is<br>' +
        'relatively easy.<br>' +
        '638<br>' +
        'NOTE<br>' +
        'You may find yourself in a scenario where<br>' +
        'you have a requirement to replace an<br>' +
        'existing reporting system so it can be<br>' +
        'phased out. This is unfortunate because it<br>' +
        'means you will do a lot of work and end up<br>' +
        'with the same set of reports everyone<br>' +
        'already had. You may be saving some<br>' +
        'money when you turn off the old system,<br>' +
        'but the perceived value of your efforts will<br>' +
        'be close to zero.<br>' +
        'In this case, see if you can improve the<br>' +
        'reports in the migration process. Perhaps<br>' +
        'combine several similar reports by using<br>' +
        'parameters or sub-reports. You may even<br>' +
        'get agreement on removing some reports<br>' +
        'that are no longer used, and adding some<br>' +
        'new reports that people would like to have.<br>' +
        'After the whole list has been reviewed, re-sort the rows<br>' +
        'and set a cutoff point at about 10 to 15 reports down the<br>' +
        'list. This is your initial target list. Eyeball the results with<br>' +
        'the group to make sure all the reports on the target list<br>' +
        'deserve their success. Also, make sure all the reports that<br>' +
        'didn’t make the target list deserved to be cut. Remember,<br>' +
        'this decision is often as political as it is logical, and just<br>' +
        'because a report doesn’t make the initial list doesn’t mean<br>' +
        'it won’t get done. Many of these may be handed off to the<br>' +
        '639<br>' +
        'experts from the departments (the power users) who were<br>' +
        'most interested in the reports in the first place.<br>' +
        'Creating the Report Template<br>' +
        'People can find information more quickly if it’s presented<br>' +
        'to them in a consistent fashion. If you read the newspaper<br>' +
        'at all, you are well aware of this (or at least you benefit<br>' +
        'from it). The information is grouped in categories: sports,<br>' +
        'business, lifestyle, and world news. Even though different<br>' +
        'newspapers generally carry much of the same information,<br>' +
        'each has its own standard structures and formats.<br>' +
        'The BI applications team is in the publishing business.<br>' +
        'You need to have your own format and content standards<br>' +
        'and use them consistently. You’ll need standards at the<br>' +
        'portal level and at the individual document level. (We deal<br>' +
        'with the portal level in the section on navigation structure.)<br>' +
        'Create a template to identify the standard elements that<br>' +
        'will appear on every report, including their locations and<br>' +
        'styles.<br>' +
        'It’s helpful to define the standard report template before<br>' +
        'you begin documenting the individual reports because the<br>' +
        'template will give you some context for defining the<br>' +
        'reports. The following standard elements need to be<br>' +
        'defined and in most cases included on every report that<br>' +
        'comes out of the DW/BI system:<br>' +
        '• Report name: Create a clear, descriptive name for each<br>' +
        'report that communicates the contents of the report to the<br>' +
        'viewer.<br>' +
        '• Report title: Develop standards for the information that’s<br>' +
        'included in the title and how it’s displayed.<br>' +
        '640<br>' +
        '• Report description: Every report should have a comment or<br>' +
        'description. This is what we often have appear when the<br>' +
        'users hits the help button.<br>' +
        '• Report body: Establish standards for each report component,<br>' +
        'including the column and row layout of data, including data<br>' +
        'justification, data precision, column and row heading<br>' +
        'formats, background fills and colors, and formatting of totals<br>' +
        'or subtotal breakout rows.<br>' +
        '• Header and footer: Create a standard layout, font, and<br>' +
        'justification scheme, and stick to it. The header and footer<br>' +
        'typically include the report name, parameters used, report<br>' +
        'location information, report notes, page numbering, report<br>' +
        'execution date and time, data sources, confidentiality<br>' +
        'statement, and of course, the DW/BI logo.<br>' +
        '• Report file name: Create the report definition file name that<br>' +
        'is based on your standard file-naming convention. The file<br>' +
        'itself and any associated code should be under source<br>' +
        'control.<br>' +
        'Figure 10-8 shows one way to lay these elements out on a<br>' +
        'page. The angle bracket signs (<>) and curly brackets ({})<br>' +
        'indicate elements that are context sensitive. That is, they<br>' +
        'may be system variables, or parameters specific to the<br>' +
        'report that is being run.<br>' +
        'Figure 10-8: Example standard template<br>' +
        '641<br>' +
        'Not all report information is displayed on the report itself.<br>' +
        'You will also need to identify and document the following<br>' +
        'information for each report:<br>' +
        '• All parameter values used in the execution of the report.<br>' +
        'This is often set up as an addendum to the report that prints<br>' +
        'as a separate page. It’s especially useful for identifying why<br>' +
        'two reports are different.<br>' +
        '• User entered parameters and other user interactions like drill<br>' +
        'downs and report links.<br>' +
        '• Report metadata, including descriptions, calculations,<br>' +
        'derivations, author, the date created, and so on.<br>' +
        '• Security requirements, including a list or description of the<br>' +
        'security groups that can see the report.<br>' +
        '• Execution cycle, if the report is to run automatically on a<br>' +
        'periodic basis.<br>' +
        '• Execution trigger event, if the report is to be executed in<br>' +
        'response to a system event like the completion of the nightly<br>' +
        'HR data load.<br>' +
        '• Delivery mechanisms, like email, web site, file directory, or<br>' +
        'printer.<br>' +
        '• Delivery list, which is generally the name of an email<br>' +
        'distribution list.<br>' +
        '642<br>' +
        '• Standard output format, like text, html, PDF, Excel, or<br>' +
        'Word.<br>' +
        '• Page orientation, size, and margin settings.<br>' +
        'Observe that all of these elements are essentially report<br>' +
        'metadata. Therefore, this information should be kept in a<br>' +
        'metadata repository at some point, so it can be used during<br>' +
        'report creation or accessed by the users on demand when<br>' +
        'they need to understand more about a given report.<br>' +
        'Meanwhile, you can use a spreadsheet or text document<br>' +
        'while you are creating the specs. Metadata structures for<br>' +
        'Reporting Services are discussed in Chapter 14.<br>' +
        'Creating Report Specifications and Documentation<br>' +
        'It may sound obvious, but during the application<br>' +
        'specification step in the Lifecycle, you should create a<br>' +
        'specification for each report. The report specification<br>' +
        'consists of the template information outlined previously<br>' +
        'plus a report mock-up and supporting detail. The report<br>' +
        'mock-ups are a great way to communicate the content and<br>' +
        'purpose of the reports. Beyond the mock-up itself, create<br>' +
        'two additional items to complete the spec: a user<br>' +
        'interaction list and detailed documentation. We’ll go<br>' +
        'through the report mock-up first, and then discuss the user<br>' +
        'interaction list and the detailed documentation.<br>' +
        'Report Mock-Ups<br>' +
        'The example report mock-up shown in Figure 10-9 is<br>' +
        'based on the standard template we created earlier. The<br>' +
        'difference is we’ve filled in the report structure for one of<br>' +
        'the reports on our target list (you can see another report<br>' +
        'mock-up example later in this chapter).<br>' +
        '643<br>' +
        'It’s helpful to indicate several user interaction functions on<br>' +
        'the mock-up. For example, the double angle bracket signs<br>' +
        '(<<>>) indicate drill-down capabilities — that is, a user<br>' +
        'can click on an entry in this column or row header and drill<br>' +
        'down to the next level of detail. We’ve found it useful to<br>' +
        'indicate the functions shown in the following table on the<br>' +
        'mock-up. You may have additional needs, or prefer to use<br>' +
        'other indicators.<br>' +
        '< > User-entered variable<br>' +
        '<< >> Drillable field<br>' +
        '{ } Application-entered variable (either from the system or metadata)<br>' +
        '\\\\ \\\\ Link/URL — link to another report or documentation source<br>' +
        '( ) Page or section break field<br>' +
        '[ ] Report template comments<br>' +
        'Figure 10-9: Example Product Performance Report<br>' +
        'mock-up<br>' +
        '644<br>' +
        'User Interaction List<br>' +
        'Although the function indicators on the report template tell<br>' +
        'you what kind of interaction is possible, they don’t tell you<br>' +
        'what that interaction is like. The user interaction list<br>' +
        'identifies the nature and degree of interaction a user may<br>' +
        'have with a given report. This can range from None for a<br>' +
        'completely static report to an extensive list of fields and<br>' +
        'behaviors for a fully interactive report. Capture the basic<br>' +
        'interactions: variable specification (and its sub-types: user<br>' +
        'entry or user selection), drill down, and field addition/<br>' +
        'replacement. Figure 10-10 shows an example user<br>' +
        'interaction list for the Product Performance report shown<br>' +
        'in the mock-up in Figure 10-9.<br>' +
        '645<br>' +
        'Figure 10-10 shows a row on the user interaction list for<br>' +
        'each function indicator on the report mock-up. Include<br>' +
        'enough information so that someone who’s building this<br>' +
        'report can use the mock-up and the user interaction list to<br>' +
        'do the job.<br>' +
        'Detailed Documentation<br>' +
        'Create detailed documentation to collect the information<br>' +
        'you haven’t captured elsewhere. Note the report category,<br>' +
        'the sources of the data in the report, the calculations for<br>' +
        'each column and row, and any exceptions or exclusions to<br>' +
        'build into the query. Additional information about the<br>' +
        'report might include creation and modification tracking,<br>' +
        'and an expiration date, if the report has a limited useful<br>' +
        'life. A good place to keep this is at the end of the user<br>' +
        'interaction list.<br>' +
        'Figure 10-10: Example user interaction list<br>' +
        '646<br>' +
        'Designing the Navigation Framework<br>' +
        'Once you know which reports to build, you need to<br>' +
        'categorize them so the users can find the information<br>' +
        'they’re looking for as quickly as possible. We call this<br>' +
        'organizing structure the navigation framework, or<br>' +
        'navigation hierarchy. Ideally, this structure is<br>' +
        'self-explanatory. That is, anyone who knows something<br>' +
        'about the business can generally find what they want fairly<br>' +
        'quickly.<br>' +
        'The best approach we’ve found, and this may sound<br>' +
        'obvious, is to organize the reports by business process. If<br>' +
        'someone knows your business, even at a cursory level,<br>' +
        'they will be able to find what they need. There are a lot of<br>' +
        'additional design principles that come into play here, but<br>' +
        'we will leave them to the SharePoint chapter when we<br>' +
        '647<br>' +
        'describe a simple navigation framework for Adventure<br>' +
        'Works Cycles.<br>' +
        'Conducting the User Review<br>' +
        'Once you have a solid set of application specs in place, it’s<br>' +
        'extremely helpful to go over them with the user<br>' +
        'community. This design review covers a lot of ground.<br>' +
        'Validate your choice of high priority applications and test<br>' +
        'the clarity of the specifications — do they make sense to<br>' +
        'the business folks? The review involves users in the<br>' +
        'process, emphasizing their central role and developing<br>' +
        'their commitment. It also can keep people engaged in the<br>' +
        'project by giving them a sense for what will be possible in<br>' +
        'just a few short months. Leave time in your project plan to<br>' +
        'make any modifications to the specs that come out of this<br>' +
        'design review.<br>' +
        'Once the specs are complete and have been reviewed, you<br>' +
        'can put them on the shelf, so to speak. Unless you’re<br>' +
        'planning to do a BI tool evaluation, in which case these<br>' +
        'specs can be invaluable, there isn’t much you can do with<br>' +
        'them until you’re ready to begin the report development<br>' +
        'process.<br>' +
        'Reporting System Development<br>' +
        'It’s difficult to start the reporting system development<br>' +
        'process before a lot of the DW/BI system infrastructure is<br>' +
        'in place. You need the final dimensional model<br>' +
        'implemented in the presentation database and populated<br>' +
        'with a reasonably representative subset of the data. The BI<br>' +
        'tools must be selected and installed. And of course, you<br>' +
        '648<br>' +
        'must have completed the report specifications. Typically,<br>' +
        'all of these events don’t occur until sometime close to the<br>' +
        'system test process we describe in Chapter 16. As a result,<br>' +
        'we usually develop the reporting application at the same<br>' +
        'time as the system testing phase just prior to deployment.<br>' +
        'This makes sense because these reports are excellent test<br>' +
        'cases for several reasons: They represent a range of<br>' +
        'analyses, they are typically more complex than fake test<br>' +
        'reports, and they are real-world in that they are based on<br>' +
        'how users want to see the data.<br>' +
        'The reporting system development process itself is a fairly<br>' +
        'typical development effort. We usually cover the steps<br>' +
        'listed in Table 10-2 within the Prepare-Build-Test-Rollout<br>' +
        'framework.<br>' +
        'Table 10-2: Reporting System Development Process<br>' +
        'Let’s examine each of these tasks in more detail.<br>' +
        'Prepare<br>' +
        '• Install software: Installing Reporting Services by itself isn’t<br>' +
        'difficult, but integrating with SharePoint can be. We<br>' +
        'described some of these options in Chapter 4.<br>' +
        '• Set up security: As we describe in Chapter 14, most standard<br>' +
        'reports and other BI applications rely on security in the BI<br>' +
        'application layer (Reporting Services and/or SharePoint). A<br>' +
        'relatively small number of reports rely on row level security<br>' +
        'implemented in the database.<br>' +
        '• Create business metadata: As you begin report<br>' +
        'development, you should plan ahead for the business<br>' +
        '649<br>' +
        'metadata that describes the contents of the standard reports.<br>' +
        'See Chapter 15 for more information on business metadata.<br>' +
        '• Create process metadata: Process metadata for the BI<br>' +
        'applications is information about the execution of reports,<br>' +
        'and usage of other applications. As we describe in Chapter<br>' +
        '15, Reporting Services naturally captures information about<br>' +
        'every connection and report execution. Other BI<br>' +
        'applications, such as a data mining application, may need to<br>' +
        'be explicitly designed to collect process metadata.<br>' +
        'Build<br>' +
        '• Build reports: Finally, you get to have some fun! Actually<br>' +
        'building the reports takes relatively little time compared to<br>' +
        'the rest of the process. As we mentioned earlier, it also<br>' +
        'provides a great opportunity to build relationships with your<br>' +
        'user community. Set up a temporary lab (it can be the<br>' +
        'training room) and dedicate it to report development for as<br>' +
        'long as necessary. Bring in a group of power users to help<br>' +
        'build out the initial target list of reports. Encourage lots of<br>' +
        'interaction. Keep at least two lists of issues and difficulties:<br>' +
        'one for the data and one for Reporting Services. If this is<br>' +
        'your first experience developing reports with Reporting<br>' +
        'Services, bring in an expert as a consultant toward the<br>' +
        'mid-point of the development process. Go through your list<br>' +
        'of issues and have the person show you how to solve or<br>' +
        'work around the problems you’ve encountered. You may<br>' +
        'have to actually pay for the consulting unless you negotiated<br>' +
        'it as part of the software purchase. Either way, get help from<br>' +
        'an expert. It’s worth it.<br>' +
        '• Create the navigation portal: At the same time you’re<br>' +
        'building the initial set of reports, you need to be building<br>' +
        'them a home as well. Although Reports Services ships with<br>' +
        'a default website, called Report Manager, it doesn’t provide<br>' +
        'all the functionality needed to help your users successfully<br>' +
        'navigate the DW/BI system. As we describe in Chapter 12,<br>' +
        'most Microsoft shops will implement a navigation portal in<br>' +
        'SharePoint.<br>' +
        '650<br>' +
        '• Build other BI applications: Building other BI applications<br>' +
        'follows a similar process, although with different tools. Even<br>' +
        'if you are building a .NET application, it’s a great idea to get<br>' +
        'business users involved in a similar agile-style rapid iteration<br>' +
        'approach.<br>' +
        'Test<br>' +
        '• Unit test: Test each report. Have someone other than the<br>' +
        'developer go through the report piece by piece, testing the<br>' +
        'calculations, report layout, user inputs, and results. If<br>' +
        'possible, compare the results to those from another,<br>' +
        'independent source from outside the DW/BI system. We<br>' +
        'cover testing in Chapter 16 when we describe the<br>' +
        'deployment process.<br>' +
        '• System test: Once the report is deemed to work on a<br>' +
        'stand-alone basis, see how it works as part of the production<br>' +
        'process. Depending on your front-end tool and the<br>' +
        'complexity of your warehouse environment, there can be a<br>' +
        'large number of elements that need to be tested, including<br>' +
        'time- or event-based scheduling, distribution, and failure<br>' +
        'notification processes. If you’re supporting a large user<br>' +
        'community with these reports, plan for some stress testing as<br>' +
        'well. Include ample time in your project plan to tune these<br>' +
        'applications for large user communities.<br>' +
        '• User test: If the users have not yet seen the reports, or you’d<br>' +
        'like to get reactions from non-technical users, include a task<br>' +
        'in your process to give them a chance to inspect and approve<br>' +
        'them. This may be a hands-on session, or it may take the<br>' +
        'form of a demo to a group of users with time for questions<br>' +
        'and answers. Or, it can be something users do from their<br>' +
        'desks with a simple web survey form or email response.<br>' +
        'Deploy<br>' +
        '• Publish: The initial release of a report set is primarily a<br>' +
        'public relations process. You need to notify users that the<br>' +
        'new reports are now available in the BI portal and give them<br>' +
        'a link to check it out. The users will immediately begin to<br>' +
        '651<br>' +
        'think of many other reports they would like to see, so you<br>' +
        'should plan for a period of additional report development<br>' +
        'and deployment. Report deployment also includes creating a<br>' +
        'system for users to get the help they need. These issues are<br>' +
        'discussed in Chapter 16.<br>' +
        '• Maintain: Ensure the existing report set is available and<br>' +
        'continues to perform as expected. Make sure that the<br>' +
        'existing reports are being used; prune or replace them if their<br>' +
        'usage falls below the expected level. Chapter 17 describes<br>' +
        'the activities associated with maintaining a healthy DW/BI<br>' +
        'system.<br>' +
        'Building and Delivering Reports<br>' +
        'At last, it’s time for you to put together your standard<br>' +
        'reporting environment. This section concentrates on the<br>' +
        'process of building the environment and the reports<br>' +
        'themselves. As always, our process starts with a bit of<br>' +
        'planning and preparation. We then proceed to the creation<br>' +
        'of the initial set of standard reports based on the prioritized<br>' +
        'list created in the BI application design phase. We leave<br>' +
        'the topics of creating the BI portal, and report operations<br>' +
        'and maintenance for subsequent chapters. Although this<br>' +
        'section is not intended to be a step-by-step tutorial, we will<br>' +
        'build an example report based on the Adventure Works<br>' +
        'DW 2008 R2 Analysis Services database that is included<br>' +
        'in the SQL Server samples. You should be able to follow<br>' +
        'this walk through and create the example report once you<br>' +
        'get a little Reporting Services experience.<br>' +
        'Planning and Preparation<br>' +
        'The temptation to dive in and start building reports is<br>' +
        'almost irresistible at this point. Be strong. Even if you’re a<br>' +
        'Reporting Services pro, it’s worth taking a few days to set<br>' +
        '652<br>' +
        'up your reporting environment and figure out the overall<br>' +
        'reporting process before you start creating reports. You<br>' +
        'should also have done all the application specification<br>' +
        'work described earlier in this chapter. The major setup<br>' +
        'items you need to address are setting up the development<br>' +
        'environment, creating standard templates and styles,<br>' +
        'setting up the delivery and notification metadata and<br>' +
        'processes, and setting up a usage tracking system.<br>' +
        'Setting Up the Development Environment<br>' +
        'We discussed setting up the development environment for<br>' +
        'Reporting Services in Chapter 4. The main challenge lies<br>' +
        'in finding the optimal combination of services and<br>' +
        'machines. There are two major components to consider:<br>' +
        'the Reporting Services server and the Reporting Services<br>' +
        'metadata database called ReportServer. While it is possible<br>' +
        'to have a single SQL Server machine with Reporting<br>' +
        'Services, the ReportServer database, and the data<br>' +
        'warehouse database all together, it’s generally not a good<br>' +
        'idea for all but the smallest implementations. Moving<br>' +
        'Reporting Services and its metadata database to its own<br>' +
        'SQL Server instance on a separate machine is the easiest<br>' +
        'way to improve Reporting Services performance. Of<br>' +
        'course, this has licensing implications.<br>' +
        'Depending on your reporting workload, it is common to<br>' +
        'develop new reports and run production reports on the<br>' +
        'same server. You can set up folders in Report Manager<br>' +
        'that are only accessible to the developers. Once a report is<br>' +
        'finished, tested, and ready for public use, you simply move<br>' +
        'it into a publicly accessible folder.<br>' +
        '653<br>' +
        'Creating Standard Templates<br>' +
        'Once the development environment is in place, you need<br>' +
        'to set up the standard report layout templates we described<br>' +
        'earlier. The easiest way to create a report template is to<br>' +
        'create a blank report with the standard elements you and<br>' +
        'your business partners defined, and lay it out according to<br>' +
        'your design specifications. We won’t go through the<br>' +
        'step-by-step process of creating the template because it’s<br>' +
        'the same process as creating a report, which we do in the<br>' +
        'next section. Figure 10-11 shows what a simple report<br>' +
        'template might look like in the Report Designer. It<br>' +
        'includes both a Table and Matrix pre-formatted with the<br>' +
        'appropriate styles (fonts, background colors, and the like)<br>' +
        'predefined, but there are no data fields in these items.<br>' +
        'Usually, the developer uses the appropriate controls for the<br>' +
        'report and deletes the controls that are not needed.<br>' +
        'After you’ve defined your layout template, use your source<br>' +
        'control system to make it available to the development<br>' +
        'team. If you’d like to have it appear as a template in the<br>' +
        'dialog box when you choose Add New Item … from the<br>' +
        'Project menu in BIDS, you need to save the template to a<br>' +
        'special directory. Save the completed layout template<br>' +
        'report to its project directory and copy the resulting .rdl<br>' +
        'file to the template directory. To make the template<br>' +
        'available to Report Builder, put it on the Report Server, or<br>' +
        'put it in the file system.<br>' +
        'There are other ways to impart a basic look and feel to all<br>' +
        'your reports. If you are handy with cascading style sheets,<br>' +
        'you might consider creating a report style sheet and<br>' +
        'referencing it in the RSReportServer.config file.<br>' +
        '654<br>' +
        'Books Online doesn’t provide much help with this, but<br>' +
        'search for the string “customizing style sheets” to get<br>' +
        'started.<br>' +
        'NOTE<br>' +
        'The location of the template directory for<br>' +
        'BIDS depends on how your system is<br>' +
        'configured. It’s usually located under the<br>' +
        'Visual Studio folder inside Program Files.<br>' +
        'Search for the ReportProject directory in<br>' +
        'the Program Files directory.<br>' +
        'Figure 10-11: Example Adventure Works Cycles report<br>' +
        'template in the Report Designer<br>' +
        '655<br>' +
        'In most cases, experienced report designers will start each<br>' +
        'report from the standard layout template by selecting the<br>' +
        'Add New Item … choice in the Project menu, or by<br>' +
        'right-clicking on the project in the Solution Explorer pane.<br>' +
        'One of the report item choices will be your standard layout<br>' +
        'template, if you put it in the right directory.<br>' +
        'NOTE<br>' +
        'The Report Wizard is a great place to start<br>' +
        'if you are just learning Reporting Services<br>' +
        'because it provides a structured<br>' +
        'step-by-step framework for creating a<br>' +
        'report. However, you will soon move<br>' +
        'beyond its capabilities as you begin to<br>' +
        'create real-world reports. Also, the wizard<br>' +
        'does not provide the opportunity to select a<br>' +
        'layout template.<br>' +
        'One drawback of the standard layout template in Reporting<br>' +
        'Services is that it does not include a master style sheet.<br>' +
        'The developer can define the style of each individual<br>' +
        'element in the template — bold type, font, color, and so on<br>' +
        '— but he or she cannot set default styles for any new<br>' +
        'elements that are added once the template is being used to<br>' +
        'create a specific report. New controls added to the report<br>' +
        'take on the bland styles of the generic report. This is where<br>' +
        'a cascading style sheet would be particularly useful.<br>' +
        'Interestingly enough, while the Report Wizard does not<br>' +
        'access the layout template, it does allow the selection of a<br>' +
        '656<br>' +
        'style template. You can choose from at least six predefined<br>' +
        'styles in the Report Wizard and in the wizards for the<br>' +
        'various controls: Slate, Forest, Corporate, Bold, Ocean,<br>' +
        'and Generic. If these don’t work for you, you can add to<br>' +
        'the XML document that defines the available styles. This<br>' +
        'XML document is called StyleTemplates.xml.<br>' +
        'RESOURCES<br>' +
        'Search Books Online for the “Creating a<br>' +
        'Report Using Report Wizard” topic for<br>' +
        'more information on finding and editing<br>' +
        'the StyleTemplates.xml document.<br>' +
        'Creating Reports<br>' +
        'Now that you’re ready to build some reports, revisit the<br>' +
        'report specifications you created in the BI Application<br>' +
        'Design step of the Lifecycle described earlier to figure out<br>' +
        'where to start. These specifications list the standard reports<br>' +
        'in priority order along with mock-ups and documentation<br>' +
        'on the definitions and contents of the reports. After<br>' +
        'reviewing the specification document, work through the<br>' +
        'report creation process to build out your report set. When<br>' +
        'the set is complete, deploy them to the test server for<br>' +
        'testing. Finally, deploy them to the production server.<br>' +
        'We’ll go through each of these steps in this section using<br>' +
        'the Report Designer in BIDS. You can use Report Builder<br>' +
        '3.0 just as easily, because they share most design<br>' +
        'components.<br>' +
        '657<br>' +
        'Revisit the Report Specifications<br>' +
        'The BI Application Design step of the Lifecycle involved<br>' +
        'creating a list of candidate reports based on the<br>' +
        'requirements interviews and then prioritizing the list with<br>' +
        'the business folks. At a smaller product-oriented company<br>' +
        'such as Adventure Works, orders data is almost always the<br>' +
        'top priority. During the requirements gathering process, it<br>' +
        'becomes clear that everyone wants to see orders data, but<br>' +
        'they all want to see it sliced differently. Because you did<br>' +
        'such a great job capturing the standard reporting needs in<br>' +
        'the design step, all you need to do at this point is pull out<br>' +
        'the prioritized list and the associated mock-ups and start at<br>' +
        'the top.<br>' +
        'The specifications from Figure 10-7 list the Sales Rep<br>' +
        'Performance Ranking report as the top priority report. The<br>' +
        'specifications include a report mock-up for this report,<br>' +
        'illustrated in Figure 10-12. At first glance, this is a simple<br>' +
        'report, but as you’ll see as you work through this section,<br>' +
        'even a straightforward report like this presents some<br>' +
        'development challenges.<br>' +
        'Reporting Services Workarounds<br>' +
        'Your standard template should include the layout<br>' +
        'of standard elements on the page (headers, titles,<br>' +
        'footers) and the standard formatting styles (fonts,<br>' +
        'sizes, colors). Reporting Services can use standard<br>' +
        'layout templates or custom report styles, but not<br>' +
        '658<br>' +
        'both at the same time. If you’re willing to go the<br>' +
        'extra mile, you can have both a layout template and<br>' +
        'a style template. Create a metadata table with the<br>' +
        'standard style entries in your database, and then<br>' +
        'create a dataset to retrieve the style information<br>' +
        'into your report. You can then use this to set style<br>' +
        'properties based on the query results. If the dataset<br>' +
        'is named StyleDataSet, and it has a column called<br>' +
        'FontFamily, then an expression to assign the<br>' +
        'FontFamily property of a Textbox might look like<br>' +
        'the following:<br>' +
        '=First(Fields!FontFamily.Value,"StyleDataSet"))<br>' +
        'This option is probably overkill for most DW/BI<br>' +
        'projects, but it does give you the best of both<br>' +
        'worlds. You can create a standard layout template<br>' +
        'and include a dataset that reads the style<br>' +
        'information along with the code that applies it. You<br>' +
        'need to create this only once and save it out to the<br>' +
        'template directory. From then on, all reports built<br>' +
        'from this template will have the standard layout<br>' +
        'and style. An extra benefit comes from the fact that<br>' +
        'the styles are applied dynamically, every time the<br>' +
        'report is run. When you change the standard styles<br>' +
        'in the metadata table, all the standard reports will<br>' +
        'automatically change to the new style the next time<br>' +
        'they are run.<br>' +
        'The Report Creation Process<br>' +
        '659<br>' +
        'The process of creating a report in the Reporting Services<br>' +
        'world goes through the following five basic steps, whether<br>' +
        'you are working with Report Designer or Report Builder.<br>' +
        'This list defaults to Report Designer, but we note Report<br>' +
        'Builder differences.<br>' +
        '1. Create or select the data source(s): In Report<br>' +
        'Designer you can create a data source connection that<br>' +
        'will be embedded in the report, or choose from a list of<br>' +
        'existing shared data sources in your BIDS report project,<br>' +
        'or from the report server in the case of Report Builder.<br>' +
        '2. Create or select the dataset(s): You can create a<br>' +
        'dataset based on a data source from step 1 using a query<br>' +
        'designer. The resulting dataset is a flattened table of<br>' +
        'rows and columns with repeating values as needed to fill<br>' +
        'each row. Or, in Report Builder, you may choose to use<br>' +
        'an existing, shared dataset which you select from the<br>' +
        'Report Server. In this case, if the original dataset is<br>' +
        'updated, it will update your version automatically. Note<br>' +
        'that you can create and deploy a shared dataset in BIDS,<br>' +
        'but you cannot use it in the Report Designer in SQL<br>' +
        '2008 R2. If you are creating your own query, you will<br>' +
        'work with one of several query designers depending on<br>' +
        'the type of data source you select.<br>' +
        'RESOURCES<br>' +
        'For a descriptions and screen captures of<br>' +
        'the major query designers, search Books<br>' +
        '660<br>' +
        'Online for “Query Design Tools in<br>' +
        'Reporting Services.”<br>' +
        '3. Define report layout on the Design tab: You can<br>' +
        'build the structure of your report from scratch by<br>' +
        'dragging toolbox components onto the design pane and<br>' +
        'populating them with fields from your datasets. The<br>' +
        'design pane is under the Home Ribbon in Report<br>' +
        'Builder. Report Builder also allows you to drag in report<br>' +
        'parts from the Report Part Gallery. A report part is a<br>' +
        'predefined report component that will bring any needed<br>' +
        'data sources and datasets along with it. Once the report<br>' +
        'part is in place, you can change it as needed. If the<br>' +
        'original is updated, Reporting Services will ask if you<br>' +
        'want to update your copy the next time you edit the<br>' +
        'report.<br>' +
        '4. Preview report by selecting the Preview tab: This<br>' +
        'is usually an iterative process. You make changes in the<br>' +
        'design pane and preview the report to see how it looks,<br>' +
        'repeating until it looks the way you want it. To preview<br>' +
        'a report in Report Builder, select Run in the Home<br>' +
        'Ribbon.<br>' +
        '5. Deploy report to the server: You can deploy a<br>' +
        'single report to the report server in BIDS by<br>' +
        'right-clicking on the report name and selecting Deploy.<br>' +
        'In Report Builder, you deploy a report by simply saving<br>' +
        'it to the report server. You can also save the report<br>' +
        'locally.<br>' +
        '661<br>' +
        'This outline should help keep you from getting lost as you<br>' +
        'work your way through the creation of a standard report in<br>' +
        'BIDS or Report Builder.<br>' +
        'TIP<br>' +
        'To get the most out of this section, you<br>' +
        'should be at your computer with SQL<br>' +
        'Server and the Adventure Works relational<br>' +
        'and Analysis Services sample databases<br>' +
        'installed. This section uses a walk-through<br>' +
        'format to describe the process of creating a<br>' +
        'report. We don’t describe every mouse<br>' +
        'click, but if you’ve at least worked through<br>' +
        'the SQL Server Reporting Services<br>' +
        'tutorials, there should be enough<br>' +
        'information in each step for you to follow<br>' +
        'along.<br>' +
        'Creating Your First Standard Report<br>' +
        'Begin by creating a new Report Server Project in BIDS.<br>' +
        'Check the box labeled “Create directory for solution,”<br>' +
        'rename the solution to Sales Reports, and rename the<br>' +
        'project to Sales Rep Performance. Close the Report<br>' +
        'Wizard and add a new report to the project by<br>' +
        'right-clicking the Reports directory in the Solution<br>' +
        'Explorer pane and select Add ⇒ New Item from the popup<br>' +
        'menu. Be careful not to use the Add New Report choice<br>' +
        'because it will bring up the Report Wizard, and you won’t<br>' +
        'be able to select your standard template.<br>' +
        '662<br>' +
        'Figure 10-12: Sales Rep Performance Ranking report<br>' +
        'mock-up<br>' +
        'Select your standard template in the Add New Item dialog<br>' +
        'window and rename it Sales Rep Performance Ranking.<br>' +
        'Click the Add button, and the new report should open up<br>' +
        'in the Report Designer design surface with the Design tab<br>' +
        'selected and all the standard report elements in place. At<br>' +
        'this point, it should look much like your version of the<br>' +
        'standard report template shown back in Figure 10-11. If<br>' +
        'you haven’t defined a standard template, just add a new<br>' +
        'blank report item.<br>' +
        'Creating the Data Source and Dataset Query<br>' +
        'Reporting Services uses the same kind of data sources that<br>' +
        'Integration Services and Analysis Services use. Share data<br>' +
        '663<br>' +
        'sources across the reports in your projects to make it easy<br>' +
        'to change connection information. Create a shared data<br>' +
        'source called AdventureWorksAS that references the<br>' +
        'Adventure Works DW 2008 R2 Analysis Services<br>' +
        'database, which you can download from the CodePlex<br>' +
        'website. Be sure to test the connection before you proceed.<br>' +
        'While you’re at it, create another shared data source called<br>' +
        'AdventureWorksDW that references the<br>' +
        'AdventureWorksDW2008 R2 SQL Server relational<br>' +
        'database.<br>' +
        'TIP<br>' +
        'In general, it makes sense to use the<br>' +
        'production data warehouse databases as the<br>' +
        'sources, even for report development<br>' +
        '(assuming the data has already been loaded<br>' +
        'into production). Report development is<br>' +
        'essentially the same as ad hoc querying and<br>' +
        'poses no significant additional risks to the<br>' +
        'database server. Using the production data<br>' +
        'warehouse server makes the development<br>' +
        'job easier because the data used for<br>' +
        'creating the report is the final data. In<br>' +
        'addition, you don’t need to change the data<br>' +
        'sources when you move from development<br>' +
        'to test to production. Of course, you will<br>' +
        'need to work against dev or test databases<br>' +
        'for new data sources that are not yet in<br>' +
        'production.<br>' +
        '664<br>' +
        'Next you need to make your shared data source from the<br>' +
        'project available to the new report. You should see a<br>' +
        'Report Data pane on the far left of your BIDS window.<br>' +
        'Select Data Source… from the New dropdown menu in the<br>' +
        'Report Data pane and create a data source named AWAS<br>' +
        'using a shared data source reference to the<br>' +
        'AdventureWorksAS data source.<br>' +
        'Now select Dataset… from the New dropdown menu in<br>' +
        'the Report Data pane and create a dataset called<br>' +
        'SalesRankData based on the AWAS data source. You<br>' +
        'typically define the dataset query by clicking the Query<br>' +
        'Designer… button in the Dataset Properties window. This<br>' +
        'will bring up the appropriate designer tool depending on<br>' +
        'the nature of the data source you are using.<br>' +
        'If your data source was a SQL Server relational database,<br>' +
        'the query designer would allow you to enter SQL into the<br>' +
        'upper pane and view the results in the lower pane. The<br>' +
        'relational query design also has a more graphical interface<br>' +
        'mode called the query builder. It looks suspiciously like<br>' +
        'the Microsoft Access query builder. If the data source was<br>' +
        'a report model, you’d use the report model query designer<br>' +
        'to define datasets.<br>' +
        'In this case, when you click the Query Designer… button,<br>' +
        'BIDS displays the Analysis Services query designer in a<br>' +
        'separate window. The Analysis Services query designer<br>' +
        'lets you build simple MDX queries with a drag-and-drop<br>' +
        'interface. You can view the MDX or enter it directly, by<br>' +
        'switching out of design mode in the upper-right corner of<br>' +
        'the designer.<br>' +
        '665<br>' +
        'The query design tools in the Report Designer and Report<br>' +
        'Builder are getting better as Reporting Services matures.<br>' +
        'However, in many cases, developers will end up creating<br>' +
        'the SQL or MDX in some other tool and pasting it in. Even<br>' +
        'in our simple example, the fields for the Sales Rep<br>' +
        'Performance Ranking report require the creation of<br>' +
        'calculated members in MDX.<br>' +
        'WARNING<br>' +
        'If you enter your SQL or MDX directly, do<br>' +
        'not try to switch over to the query builder<br>' +
        'or into design mode. The designer will try<br>' +
        'to rewrite your query and may fail because<br>' +
        'of its complexity, ruining the SQL and the<br>' +
        'formatting in the process. In the MDX<br>' +
        'case, it doesn’t even pay to rewrite the<br>' +
        'query. You have to start over.<br>' +
        'According to the mock-up for the Sales Rep Performance<br>' +
        'Ranking report in Figure 10-12, the VP of Sales wants to<br>' +
        'compare current year and prior year sales by sales rep.<br>' +
        'Like many business requests, this is conceptually simple,<br>' +
        'but it turns out to be not so easy to do in MDX, SQL, or<br>' +
        'the Report Designer. For purposes of the example, this<br>' +
        'report will use an MDX query against the Analysis<br>' +
        'Services database. The Query Designer window in Figure<br>' +
        '10-13 shows the completed dataset.<br>' +
        'When you create a report, you need to break it down into<br>' +
        'subsets to make it easier to build. You usually need a<br>' +
        '666<br>' +
        'minimum of two datasets: one for the report contents and<br>' +
        'at least one for the user input parameter prompt lists. The<br>' +
        'report in Figure 10-12 is more complex than it first appears<br>' +
        'because it has two subsets of data, current year sales and<br>' +
        'prior year sales by employee. To make matters worse, you<br>' +
        'need to rank each of those subsets independently and<br>' +
        'calculate the change in ranking from the prior year to the<br>' +
        'current year. In MDX, the easiest way to accomplish the<br>' +
        'creation of subsets within the Query Designer is to create<br>' +
        'calculated members for each of the two sales subsets (in<br>' +
        'the Calculated Members pane in the lower-left corner),<br>' +
        'then create calculated members that rank those subsets,<br>' +
        'and finally, calculate the difference of the two rank<br>' +
        'members.<br>' +
        'NOTE<br>' +
        'By far the easiest way for the report<br>' +
        'developer to get Prior Year Sales is to<br>' +
        'have the Analysis Services cube developer<br>' +
        'create the measure inside the database.<br>' +
        'Then all you’d have to do is drag and drop<br>' +
        'it into the query designer. Measures that are<br>' +
        'likely to be used in many reports and<br>' +
        'analyses should be defined within the cube.<br>' +
        'Nonetheless, report developers inevitably<br>' +
        'will need to create some complex MDX<br>' +
        'snippets.<br>' +
        'First, drag the Employee attribute from the Employee<br>' +
        'Department hierarchy in the Employee dimension onto<br>' +
        '667<br>' +
        'the results pane. Next, the current year sales calculated<br>' +
        'member is easy because it is just the Reseller Sales<br>' +
        'Amount measure. To create this calculated member,<br>' +
        'right-click in the Calculated Members box and select New<br>' +
        'Calculated Member. Drag the appropriate measure from<br>' +
        'the Measures list in the Metadata box in the lower-left<br>' +
        'corner of the Calculated Member Builder window into the<br>' +
        'Expression box. For this report, the expression would be:<br>' +
        '[Measures].[Reseller Sales Amount]<br>' +
        'Technically, you don’t need a calculated member for this<br>' +
        'measure, but it helps clarify the contents of the report and<br>' +
        'subsequent calculations. Next, recall that the mock-up<br>' +
        'identified the target year as a user-entered parameter. If<br>' +
        'you limit the calendar year to CY 2008 in the filter pane at<br>' +
        'the upper part of the designer, you will see a check box to<br>' +
        'make this limit a parameter. When you check the box, the<br>' +
        'Analysis Services report designer creates a Reporting<br>' +
        'Services parameter along with the query needed to<br>' +
        'populate the choice list.<br>' +
        'Figure 10-13: The completed data tab for the Sales Rep<br>' +
        'Performance Ranking report<br>' +
        '668<br>' +
        'The next calculated member is the prior year sales field.<br>' +
        'This is a bit more complex because it relies on the<br>' +
        'ParallelPeriod function:<br>' +
        'SUM({ParallelPeriod([Date].[Calendar Year].LEVEL,1)},<br>' +
        '[Measures].[Current Year Sales])<br>' +
        'This says to sum the measure for one year lagged behind<br>' +
        'the current year. Notice that it refers to the Current Year<br>' +
        'Sales measure you defined first.<br>' +
        'The next calculated measure, Current Year Rank, does<br>' +
        'a rank on current year sales. Like so much of the MDX<br>' +
        'language, the RANK function in MDX is not like the RANK<br>' +
        'function in SQL, although it has the same name. The MDX<br>' +
        'RANK tells you the location of a member in its current set.<br>' +
        'For employees, the current set might be in alphabetical<br>' +
        'order. Therefore, a straight ranking of sales reps would<br>' +
        '669<br>' +
        'rank Amy Alberts as number one, even though she’s<br>' +
        'sixteenth on the list as measured by sales. When you use<br>' +
        'the RANK, you need to tell it what set it is part of, and what<br>' +
        'order the set is in, as follows:<br>' +
        'IIF (ISEMPTY( [Measures].[Current Year Sales] ), NULL,<br>' +
        'RANK (<br>' +
        '[Employee].[Employee Department].CurrentMember,<br>' +
        'ORDER (<br>' +
        '[Employee].[Employee Department].[Employee].Members<br>' +
        ', ([Measures].[Current Year Sales])<br>' +
        ', BDESC<br>' +
        ')<br>' +
        ')<br>' +
        ')<br>' +
        'This MDX uses the ISEMPTY function to make sure the<br>' +
        'employee has Current Year Sales before it does the<br>' +
        'RANK. This is because only a handful of employees are<br>' +
        'sales reps, and you don’t want the report to list all 296<br>' +
        'employees. Next, the expression ranks the current<br>' +
        'employee according to where it is in a set of employees<br>' +
        'ordered by Current Year Sales, in descending order.<br>' +
        'The Prior Year Rank is the same expression ordered by<br>' +
        'Prior Year Sales.<br>' +
        'The final column, Rank Change, is simply the difference<br>' +
        'in the two ranking calculated members:<br>' +
        '670<br>' +
        '[Measures].[Prior Year Rank] - [Measures].[Current Year Rank]<br>' +
        'While this may seem complicated, there are two aspects to<br>' +
        'consider that might make it seem relatively easy. First, you<br>' +
        'can define commonly used calculated members, like<br>' +
        'Current Year Sales or Prior Year Sales, in the<br>' +
        'Analysis Services OLAP database once; then they become<br>' +
        'drag-and-drop fields in the query designer, just like any<br>' +
        'other measure. Second, the SQL alternative to this query is<br>' +
        'even less attractive; it goes on for almost a page to<br>' +
        'accomplish the same results. (You can get a script for the<br>' +
        'equivalent SQL query at the book’s Web site:<br>' +
        'www.KimballGroup.com/html/booksMDWTtools.html.)<br>' +
        'Even though the primary dataset is complete, you still have<br>' +
        'at least one more dataset to consider. When you select the<br>' +
        'parameter check box in the Date.Calendar Year limit,<br>' +
        'the MDX query designer automatically creates a second<br>' +
        'dataset, called DateCalendarYear, and links that dataset<br>' +
        'to a parameter also called DateCalendarYear. The<br>' +
        'dataset retrieves the distinct list of choices for the attribute<br>' +
        'selected and includes a value field and a caption field. The<br>' +
        'caption field is used to populate the choice list of a<br>' +
        'pulldown menu in the user interface, and the<br>' +
        'corresponding value field is passed to the<br>' +
        'DateCalendarYear parameter in the MDX query. If you<br>' +
        'use the SQL query designer, you have to create the dataset<br>' +
        'to populate the parameter choice list yourself.<br>' +
        'NOTE<br>' +
        '671<br>' +
        'To see the DateCalendarYear dataset,<br>' +
        'right-click the Datasets folder in the<br>' +
        'ReportData tab and toggle the Show<br>' +
        'Hidden Datasets choice. To change the<br>' +
        'parameter so the user cannot choose more<br>' +
        'than one target year, right-click the<br>' +
        'DateCalendarYear parameter in the<br>' +
        'Parameters folder and uncheck the Allow<br>' +
        'multiple values check box. See if you can<br>' +
        'figure out how to omit the All Periods<br>' +
        'choice. Even better, see if you can omit the<br>' +
        'earliest year from the list because there<br>' +
        'won’t be a prior year for the earliest year.<br>' +
        'Design the Report Layout<br>' +
        'Once your datasets are defined, switch over to the Design<br>' +
        'tab in the Report Designer design surface and start laying<br>' +
        'out the actual report. Fortunately, the Sales Rep<br>' +
        'Performance Ranking report has a simple layout. We’ll<br>' +
        'start with the core report layout, and then add a few subtle<br>' +
        'items like the DateCalendarYear parameter and some<br>' +
        'conditional formatting to highlight potential problem areas.<br>' +
        'The design surface in the Design tab should look familiar<br>' +
        'to anyone who has created forms or reports with Access,<br>' +
        'Visual Basic, VBA, or Visual Studio. The basic process<br>' +
        'involves dragging various Report Items from the Toolbox<br>' +
        '(a tab in the same pane as the Report Data tab on the left)<br>' +
        'onto the design surface and then specifying their properties<br>' +
        'as appropriate.<br>' +
        '672<br>' +
        'The standard template shown back in Figure 10-11 already<br>' +
        'has predefined Table and Matrix items. The report<br>' +
        'mock-up looks like a good candidate for the Table control,<br>' +
        'so delete the Matrix control to clear up a little space. (If<br>' +
        'you’re not using a standard template, just drag a Table<br>' +
        'from the Toolbox into the report area.) Fill in the Table<br>' +
        'control by dragging columns from the dataset in the Report<br>' +
        'Data pane onto the columns of the Table control. Once you<br>' +
        'have the columns in place, select the Preview tab to see<br>' +
        'how the initial report looks.<br>' +
        'NOTE<br>' +
        'If the template doesn’t have enough<br>' +
        'columns in the table, you can add more by<br>' +
        'right-clicking in the gray header for any<br>' +
        'column. You can also drop a data field in<br>' +
        'between two columns. Either way, the<br>' +
        'report designer will clone any pre-set<br>' +
        'formatting into the new column.<br>' +
        'WARNING<br>' +
        'Be careful if you drop-insert a column<br>' +
        'because it may create an item with the<br>' +
        'same name as an existing item except with<br>' +
        'a difference in case. Thus, you may end up<br>' +
        'with TextBox1 and Textbox1. If you have<br>' +
        '673<br>' +
        'expressions in the text boxes, the parser<br>' +
        'won’t like the similar names and will give<br>' +
        'you a fairly unhelpful error message.<br>' +
        'When you select Preview, the report should run and<br>' +
        'generate results for the default calendar year of 2008. Try<br>' +
        'changing the Date.Calendar Year parameter in the<br>' +
        'pulldown menu at the top of the report. You need to hit the<br>' +
        'View Report button to re-execute the query. If you haven’t<br>' +
        'already fixed it, the pulldown menu allows you to select<br>' +
        'more than one year by default. This will break the prior<br>' +
        'year calculations, so go back and change this.<br>' +
        'To improve the default settings, return to the Design tab<br>' +
        'and select the Parameters folder in the Report Data pane<br>' +
        'on the left. Double-click the<br>' +
        'DateCalendarYear parameter to display the Report<br>' +
        'Parameter Properties dialog box, as illustrated in Figure<br>' +
        '10-14.<br>' +
        'Change the prompt string to make sense to your business<br>' +
        'users, and uncheck the multi-value box in the Properties<br>' +
        'section. Now, if you select the Preview tab, you should be<br>' +
        'able to select only a single year in the pulldown menu.<br>' +
        'Figure 10-14: Report Parameter Properties dialog box<br>' +
        '674<br>' +
        'Creating reports is a classic 80/20 process, or even 90/10,<br>' +
        'in that it takes only 10 percent of the time to create 90<br>' +
        'percent of the report. Most of the final 10 percent of<br>' +
        'creating a report is formatting, and it usually takes much<br>' +
        'longer than you would expect. In this case, the Sales Rep<br>' +
        'Performance report now has the key elements it will have<br>' +
        'when it’s finished, but it doesn’t look very professional.<br>' +
        'Go back to the Layout view and take a few minutes to<br>' +
        'clean up some of the following appearance problems:<br>' +
        '• Change the column widths to make better use of the available<br>' +
        'space.<br>' +
        '• Format the sales columns to get rid of all those decimal values<br>' +
        'and add some commas. Try selecting the field in the Detail row<br>' +
        'and putting an “N0” in its Format property, or try a custom<br>' +
        'format string, like “#,##0.” (Search for the “Predefined Numeric<br>' +
        '675<br>' +
        'Formats” topic in the Visual Basic language reference at<br>' +
        'http://msdn.microsoft.com for more options.)<br>' +
        'NOTE<br>' +
        'You might think that number formatting<br>' +
        'would be easier if the data source is<br>' +
        'Analysis Services rather than SQL. One of<br>' +
        'the advantages of Analysis Services is that<br>' +
        'it contains metadata about such things as<br>' +
        'formatting for measures. Reporting<br>' +
        'Services will show the correct formatting,<br>' +
        'but it is not the default. You have to change<br>' +
        'the text box expression to =Fields!<your<br>' +
        'field name>.FormattedValue.<br>' +
        '• Verify the line spacing and borders.<br>' +
        '• Add and format appropriate totals.<br>' +
        'Figure 10-15 shows the finished report in the Preview tab<br>' +
        'of the Report Designer environment.<br>' +
        'The guiding philosophy for creating and formatting reports<br>' +
        'is that they should be as clear, consistent, and<br>' +
        'self-explanatory as possible. Users will not take the time to<br>' +
        'look elsewhere for report documentation, nor should they<br>' +
        'be expected to. Clarity is one of the major challenges the<br>' +
        'DW/BI team takes on when it includes standard reports as<br>' +
        'part of its responsibilities. Involve someone who has solid<br>' +
        'graphic design expertise in the design process of the layout<br>' +
        'template and the initial report set. Experiment with<br>' +
        'alternatives and get feedback from users as to which one<br>' +
        '676<br>' +
        'works best. A bit of extra work at this point will pay off<br>' +
        'massively in the long run.<br>' +
        'Figure 10-15: Final report layout for the Sales Rep<br>' +
        'Performance Ranking report<br>' +
        'Tweaking the Report Layout<br>' +
        'To create a clear, understandable,<br>' +
        'professional-looking report, you need to go beyond<br>' +
        '677<br>' +
        'these basic formatting items. Some additional<br>' +
        'improvements to consider include:<br>' +
        '• Add a report description to the report’s metadata.<br>' +
        'This description lives in the properties of the report<br>' +
        'itself. To access these properties, select the area<br>' +
        'outside the design surface in the Design tab and look<br>' +
        'for the description property in the properties window.<br>' +
        '• Add conditional formatting. Reporting Services<br>' +
        'supports expressions written in Visual Basic .NET.<br>' +
        'You can use this powerful capability to define most<br>' +
        'of the properties in the report programmatically.<br>' +
        'Conditional formatting is a good example of this<br>' +
        'capability. (Search for the “Expression Examples”<br>' +
        'topic in Books Online for more examples.)<br>' +
        'For example, in the Sales Rep Performance<br>' +
        'Ranking report, the VP of Sales might want to<br>' +
        'highlight those sales reps that have risen or slipped<br>' +
        'more than two places in the rankings. You need a<br>' +
        'soothing green color for the rising stars and an<br>' +
        'ominous red color for the slackers. To accomplish<br>' +
        'this, apply conditional formatting to the entire<br>' +
        'detail group row. Select the row, and then select<br>' +
        'the pulldown menu for the BackgroundColor<br>' +
        'property in the Properties window. The last choice<br>' +
        'in the menu is <Expression … >. Select it and the<br>' +
        'Edit Expression dialog window opens. Delete the<br>' +
        'contents of the Expression box and enter the<br>' +
        'following expression:<br>' +
        '=iif( Fields!Rank_Change.Value > 2, "LightGreen",<br>' +
        'iif(Fields!Rank_Change.Value < -2, "MistyRose", "White"))<br>' +
        '678<br>' +
        'Select OK, and then select the Preview tab. You<br>' +
        'should see some nice highlighting at this point. Can<br>' +
        'you tell who is going to be in trouble when you<br>' +
        'publish this report?<br>' +
        '• Add parameterized labeling. This is another useful<br>' +
        'application of expressions. Rather than have generic<br>' +
        'column labels that don’t tell the user exactly what is<br>' +
        'in the column, you can use expressions to incorporate<br>' +
        'parameters right into the report text. This is<br>' +
        'particularly helpful for when reports are printed.<br>' +
        'Once you separate the report from the Reporting<br>' +
        'Service, it is unclear which parameters you used to<br>' +
        'generate the report. Labels can be made more<br>' +
        'descriptive by using expressions as follows:<br>' +
        '• Right-click the SalesAmnt header field and select<br>' +
        'Expression. In the Expression box, delete the text title<br>' +
        'and enter the following expression:<br>' +
        '=Left(Right(Parameters!DateCalendarYear.Value,5),4) & " Sales"<br>' +
        '• Select OK, and then select the Preview tab. If the<br>' +
        'TargetYear is still set to 2008, the Sales column<br>' +
        'header should now be 2008 Sales.<br>' +
        '• Make appropriate, similar changes to the<br>' +
        'CurrentRank, PriorSalesAmnt, and PriorRank<br>' +
        'header fields. Remember, the parameter is a string<br>' +
        'value, so you may need to use the Val() and Str()<br>' +
        'functions to get the prior year into the titles.<br>' +
        '• Add an interactive sort. Enable the Interactive Sorting<br>' +
        'in the Text Box Properties on the Current Ranking<br>' +
        'column header text box. This will allow your users to<br>' +
        'sort out the top or bottom performers.<br>' +
        '• Verify print layout. It is rare that a report you have<br>' +
        'created to fit on the screen will also fit on the printed<br>' +
        'page the first time. You may need to change the<br>' +
        'column widths and font sizes to squeeze things in a<br>' +
        'bit. You can change the page setup as well, adjusting<br>' +
        'margins and switching from portrait to landscape. If<br>' +
        '679<br>' +
        'you can’t keep all the columns on a single page, you<br>' +
        'may need to change some of the properties of certain<br>' +
        'fields and groups to cause them to repeat on<br>' +
        'subsequent printed pages.<br>' +
        'Unit Test<br>' +
        'Report developers should do the first round of testing right<br>' +
        'in the development environment. At the very least, the<br>' +
        'developer should test various combinations of the<br>' +
        'parameters and validate the results with existing reports if<br>' +
        'possible.<br>' +
        'Test different parameters. For example, try the Sales Rep<br>' +
        'Performance Report with the year 2007. What happens? It<br>' +
        'looks like some of the rows are missing a few values. This<br>' +
        'makes sense in this case because you would expect some<br>' +
        'sales reps to have data in one year and not the other.<br>' +
        'Fortunately, Analysis Services does an outer join for you<br>' +
        'to make sure everyone in the target year is represented in<br>' +
        'the report. Try it again with the year 2005.<br>' +
        'Validate the numbers. Check the numbers as carefully as<br>' +
        'possible. Compare them to any known alternative sources<br>' +
        'for the same information. If the numbers should be the<br>' +
        'same and they are not, figure out why. Research the<br>' +
        'business rules in the alternative source and compare them<br>' +
        'to your own. The problem may be in your query, or all the<br>' +
        'way back somewhere in the ETL process. If there’s a<br>' +
        'problem, resolving it is serious detective work and can<br>' +
        'take a lot of time and energy.<br>' +
        '680<br>' +
        'If the numbers are supposed to be different because they<br>' +
        'have been improved or corrected in the ETL process,<br>' +
        'carefully document the reasons for the differences. If<br>' +
        'possible, show how you can get from the data warehouse<br>' +
        'numbers back to the alternative source numbers. This<br>' +
        'documentation should be available in the BI portal, and the<br>' +
        'report description should refer to it.<br>' +
        'Before you actually deploy the report, you may want to<br>' +
        'create a few more reports and deploy an entire project all<br>' +
        'at once. In this case, you can add a Sales Rep Detail report<br>' +
        'or a report that includes quota information and set up the<br>' +
        'ranking report to drill-through to the detail.<br>' +
        'Deploy to the Test Report Server and Test Some More<br>' +
        'In large environments with hundreds or thousands of users<br>' +
        'pounding on the standard report set, it makes sense to<br>' +
        'deploy the reports to a test server environment that is as<br>' +
        'similar to the production environment as possible. This<br>' +
        'step allows the reporting team to stress test the new reports<br>' +
        'to ensure they perform and they don’t reduce the<br>' +
        'performance of other reports before moving them into<br>' +
        'production. In medium-sized or smaller organizations<br>' +
        'where the user population is smaller, it may not be<br>' +
        'necessary for a full test server environment. The reporting<br>' +
        'team can deploy the reports to the production Report<br>' +
        'Server and test them there. You can minimize the risk of<br>' +
        'this move by limiting access to the new report directories,<br>' +
        'and by not publishing the new reports in the BI portal until<br>' +
        'you have completed testing.<br>' +
        '681<br>' +
        'This test phase typically involves several steps. The<br>' +
        'process begins with deploying the project to the target<br>' +
        'Report Server (either test or production). Once there, the<br>' +
        'reports need to be retested to ensure proper performance,<br>' +
        'display, and printing. If they are not working well enough,<br>' +
        'there are a number of tuning techniques available. These<br>' +
        'range from tuning the query to creating report snapshots to<br>' +
        'actually changing the server configurations.<br>' +
        'Deploying the project or report to a report server is<br>' +
        'straightforward. Each project has its own configuration<br>' +
        'properties, so if you have multiple projects in a solution,<br>' +
        'you will need to set up the properties for each project.<br>' +
        'Within each project, there are several configurations and<br>' +
        'each can have its own target Report Server. The default<br>' +
        'configurations are DebugLocal, Debug, and Production.<br>' +
        'To set up the target server in the project properties, in the<br>' +
        'Project menu, select Properties. This opens a<br>' +
        'project-specific Properties pages window. To deploy the<br>' +
        'project, you need to provide a target server URL for the<br>' +
        'active configuration. In the simplest case, where the web<br>' +
        'server is on the same machine as the development<br>' +
        'environment, the target server URL can be<br>' +
        'http://localhost eportServer.<br>' +
        'After you test the basics of appearance and performance,<br>' +
        'the next step is to integrate the new reports into the<br>' +
        'production process. If there are standard schedules these<br>' +
        'reports depend on, the reports should be linked to the<br>' +
        'appropriate schedules in the Report Manager. If there are<br>' +
        'standard distribution lists that should receive these reports,<br>' +
        'they should be set up at this point. The DW/BI team<br>' +
        'should also validate the subscription process to make sure<br>' +
        '682<br>' +
        'the report is available for users to subscribe to and receive<br>' +
        'on a regular basis.<br>' +
        'Deploy to Production<br>' +
        'When there is an actual deployment to the production<br>' +
        'server, you will need to repeat many of the steps you went<br>' +
        'through to move the reports into test. These include<br>' +
        'schedules, snapshots, subscriptions, and email distribution<br>' +
        'lists. However, in most cases, the deployment to<br>' +
        'production has already taken place in the test step, so this<br>' +
        'step is more of an unveiling than anything else. This is<br>' +
        'especially true when the primary user interface with the<br>' +
        'reporting environment is through a web site or portal. In<br>' +
        'this case, the reports are not visible to the users until you<br>' +
        'make them available in the portal.<br>' +
        'The BI portal is such a powerful tool for the DW/BI<br>' +
        'system that we explore it in detail in Chapter 12 when we<br>' +
        'discuss SharePoint. At this point, it is enough to say that if<br>' +
        'you are providing reports through a portal, you need to<br>' +
        'integrate this new set of reports into that portal as part of<br>' +
        'the production deployment.<br>' +
        'An important part of deploying a report to production is<br>' +
        'setting the security for the report. Reporting Services has<br>' +
        'several options for managing security, as do the relational<br>' +
        'engine and Analysis Services. We discuss security in<br>' +
        'Chapter 14.<br>' +
        'Reporting Operations<br>' +
        '683<br>' +
        'The report deployment process included steps to tie the<br>' +
        'new reports in with existing reporting operations processes<br>' +
        'like schedules and distribution. These operations processes<br>' +
        'will also need to be maintained on an ongoing basis,<br>' +
        'independent of the introduction of new reports.<br>' +
        'In addition to deploying the reports to the report server,<br>' +
        'you may need to cause a set of reports to run automatically<br>' +
        'when the data load for a business process dimensional<br>' +
        'model is finished. Reporting Services isn’t directly tied to<br>' +
        'the ETL system, but it does have a control structure called<br>' +
        'a subscription that can help. Create a set of subscriptions<br>' +
        'that the main load processes will kick off as part of the<br>' +
        'regular update process.<br>' +
        'One way to accomplish this is to write a script that invokes<br>' +
        'the subscription from within the data load Integration<br>' +
        'Services package. The problem with this approach is it<br>' +
        'hard-codes the link between Integration Services and<br>' +
        'Reporting Services, making it difficult to maintain and<br>' +
        'enhance.<br>' +
        'An alternative solution is to create a metadata layer<br>' +
        'between the two components. You can use a simple<br>' +
        'metadata table to drive the relationships between your ETL<br>' +
        'and reporting systems. The table-driven approach is easy<br>' +
        'for an administrator to manage once it’s set up.<br>' +
        'REFERENCE<br>' +
        '684<br>' +
        'We uploaded detailed instructions on how<br>' +
        'to build this metadata-driven system,<br>' +
        'including scripts and an example<br>' +
        'Integration Services solution to the book’s<br>' +
        'website: www.kimballgroup.com/html/<br>' +
        'booksMDWTtools.html.<br>' +
        'As you deploy new standard reports to the Report Server,<br>' +
        'add them to the appropriate schedules and create any<br>' +
        'system subscriptions to distribute the reports as needed.<br>' +
        'The DW/BI team will also need to maintain any<br>' +
        'data-driven subscriptions that involve individual users and<br>' +
        'email lists. The team may need to add and delete users<br>' +
        'from these lists as they come and go from the organization.<br>' +
        'The same goes for other distribution mechanisms, like file<br>' +
        'shares. Computers and networks have a tendency to<br>' +
        'change. For example, the accounting department may have<br>' +
        'requested a set of reports distributed to their file server.<br>' +
        'Then they get a new file server and turn off the old one<br>' +
        'without telling you. At this point, you have a subscription<br>' +
        'failing on a regular basis and a set of users not receiving<br>' +
        'scheduled reports.<br>' +
        'Ad Hoc Reporting Options<br>' +
        'There is a class of users who actually want to get their<br>' +
        'hands directly on the data. This group of power users is<br>' +
        'relatively small, but very important, so you need to make<br>' +
        'sure they have the tools they need. There are many options<br>' +
        'for providing ad hoc access to the DW/BI system.<br>' +
        '685<br>' +
        'Reporting Services has Report Builder 3.0, as we’ve<br>' +
        'mentioned, and Office has Access and Excel. As we<br>' +
        'describe in Chapter 11, Pivot tables and PowerPivot are<br>' +
        'particularly enticing data access tools for many of the<br>' +
        'power users because these users are very comfortable with<br>' +
        'Excel. Beyond Microsoft, there are dozens of products<br>' +
        'designed to support this user community, some of which<br>' +
        'are big, multi-platform products, and some of which are<br>' +
        'targeted specifically at one platform, such as Analysis<br>' +
        'Services.<br>' +
        'Given that Report Builder is part of Reporting Services,<br>' +
        'we will describe some of its unique capabilities. There are<br>' +
        'previous versions of Report Builder, but beginning with<br>' +
        'Report Builder 3.0, the product has progressed far enough<br>' +
        'to be considered a broadly useful query and reporting tool.<br>' +
        'As you can see in Figure 10-16, Report Builder shares<br>' +
        'most of its report controls and design panes with the<br>' +
        'Report Designer, so much of what you’ve seen in this<br>' +
        'chapter applies to Report Builder. Report Builder has made<br>' +
        'several efforts to simplify the report creation process.<br>' +
        'These efforts began with the metadata layer called the<br>' +
        'report model, that Report Builder can use to access<br>' +
        'relational and Analysis Services data. They also include<br>' +
        'the concepts of shared datasets and report parts.<br>' +
        'Figure 10-16: The Report Builder design interface<br>' +
        '686<br>' +
        'The Report Model<br>' +
        'The report model is a remnant of the original reporting tool<br>' +
        'Microsoft bought in 2004 that became Report Builder 1.0.<br>' +
        'It is a metadata layer between the tool and the data that<br>' +
        'defines the tables and columns and the relationships<br>' +
        'among tables. It also defines aggregation options, such as<br>' +
        'sum and count, and allows the user to choose the<br>' +
        'appropriate option for a given query. You only work with a<br>' +
        'report model when you define a data source on a model<br>' +
        'and then build a dataset based on that data source. You can<br>' +
        'auto-generate a model for both Analysis Services and<br>' +
        'relational databases by navigating to the appropriate data<br>' +
        'source in the Report Manager and selecting Generate<br>' +
        'Model in the header bar. Relational models work best if<br>' +
        'you create them via a report model project in BIDS. You<br>' +
        'are probably better off using the Analysis Services query<br>' +
        'designer rather than creating a report model for an<br>' +
        'Analysis Services cube. The report model was originally<br>' +
        '687<br>' +
        'created for relational access, and doesn’t do its best work<br>' +
        'against Analysis Services.<br>' +
        'Interacting with the model takes a bit of getting used to.<br>' +
        'The model follows the join paths, so if you have a model<br>' +
        'of the Adventure Works DW Analysis Services database,<br>' +
        'and select the Customer table in the Entities list as a<br>' +
        'starting point, and then select the Location folder, you<br>' +
        'see Customer location columns in the Fields list below. If<br>' +
        'you select the Country field and drag it to the column list,<br>' +
        'your choice of entities now shrinks to those tables that are<br>' +
        'joined to Customer in the model. In this case, Customer<br>' +
        'only joins to the Internet Sales fact table, so you can<br>' +
        'continue to select Customer columns, or select the fact<br>' +
        'table. If you select the fact table, you can now select fields<br>' +
        'from the fact table, or select any of the tables the fact table<br>' +
        'is joined to, which of course is all of the dimensions.<br>' +
        'Shared Datasets<br>' +
        'Rather than having to define a dataset from scratch, a<br>' +
        'Report Builder user can select a pre-existing dataset from a<br>' +
        'directory on the Report Server. This can simplify the<br>' +
        'process of defining the dataset, but it introduces its own<br>' +
        'issues. The user only sees the name of the dataset and its<br>' +
        'directory path. The description is not shown. This means<br>' +
        'the naming convention for datasets must be carefully<br>' +
        'thought out, documented, and followed. You can ease the<br>' +
        'navigation problem by keeping the number of shared<br>' +
        'datasets as low as possible. This means each shared dataset<br>' +
        'will likely have a fairly large set of data that can be filtered<br>' +
        'within the report itself. This can result in everyone<br>' +
        'building reports that run large queries to answer smaller<br>' +
        '688<br>' +
        'questions. In other words, it may lead to an unnecessary<br>' +
        'increase in query workload.<br>' +
        'Once the shared dataset is linked to Report Builder, it<br>' +
        'appears in the Datasets tree on the left of Figure 10-16.<br>' +
        'The arrow in the icon, similar to a shortcut icon,<br>' +
        'distinguishes a shared dataset from an embedded dataset.<br>' +
        'This link is fixed, which means changes to the master<br>' +
        'dataset will be reflected in all reports that use that shared<br>' +
        'dataset.<br>' +
        'Shared datasets might be a good way to define common<br>' +
        'parameter pick lists across your standard report set.<br>' +
        'Report Parts<br>' +
        'Report parts are a similar attempt to reuse existing<br>' +
        'components rather than build from scratch for each report,<br>' +
        'but rather than stop at the dataset, report parts go all the<br>' +
        'way out to the finished and formatted data region or other<br>' +
        'report element. This means a user can build a report by<br>' +
        'simply dragging in report parts as needed. Report parts are<br>' +
        'on the Report Server; you can specify a default directory<br>' +
        'by clicking the Options button under the Report button at<br>' +
        'the top left of the Report Builder window. The report parts<br>' +
        'available to the user are displayed in a pane called the<br>' +
        'Report Part Gallery, seen on the right side of Figure 10-16.<br>' +
        'The Report Part Gallery is more advanced in terms of how<br>' +
        'it displays its contents than the basic dataset selection<br>' +
        'window. The gallery allows a user to search for report<br>' +
        'parts based on the name and description. The gallery<br>' +
        'displays the results with a thumbnail of each report and<br>' +
        '689<br>' +
        'displays the selected report’s metadata at the foot of the<br>' +
        'gallery. It also displays the report description when you<br>' +
        'mouse-over the report.<br>' +
        'The Report Part Gallery is a good start at providing a<br>' +
        'means to organize and access a set of report parts. Its<br>' +
        'success will be determined by how well you manage your<br>' +
        'business metadata, especially the names and descriptions<br>' +
        'of each report part.<br>' +
        'It will be interesting to see how effective report parts are at<br>' +
        'easing the report creation task. It may be the case that<br>' +
        'report parts are too specific and users always need to make<br>' +
        'changes to someone else’s work, which may be harder<br>' +
        'than starting from scratch. Or, it may be a big timesaver.<br>' +
        'Summary<br>' +
        'BI applications are the vehicle for delivering value to the<br>' +
        'business users. There are different kinds of BI applications<br>' +
        'you might need to create depending on the business<br>' +
        'requirements for your DW/BI system, including ad hoc<br>' +
        'access tools, standard reports, dashboards and scorecards,<br>' +
        'analytic applications, and data mining tools.<br>' +
        'Delivering a set of high value standard reports is usually a<br>' +
        'major DW/BI team responsibility. These reports are the<br>' +
        'primary interface to the DW/BI system for a large majority<br>' +
        'of the user community. Since Reporting Services is SQL<br>' +
        'Server’s standard reporting platform, we examined the<br>' +
        'Reporting Services architecture to see what functions it<br>' +
        'provides to meet the organization’s reporting needs. This<br>' +
        'gave you a good sense for the components of the tool and<br>' +
        '690<br>' +
        'how those components fit together. Comparing the<br>' +
        'Reporting Services architecture to the business<br>' +
        'requirements for reporting, we concluded that Reporting<br>' +
        'Services has a reasonable feature-cost ratio, especially<br>' +
        'considering its incremental cost.<br>' +
        'With this understanding in place, the next section offered a<br>' +
        'detailed process for creating a set of standard reports. Start<br>' +
        'with some preparation steps to get the development<br>' +
        'environment set up and to create a standard template that<br>' +
        'serves as a starting point for every new standard report.<br>' +
        'Next, revisit the standard report prioritization list,<br>' +
        'specifications, and mock-ups that you captured as part of<br>' +
        'the requirements gathering process early on in the<br>' +
        'Lifecycle.<br>' +
        'The last major section of this chapter walked through a<br>' +
        'case study that picked the top-priority report from<br>' +
        'Adventure Work’s list and dove into the development<br>' +
        'process.<br>' +
        'We ended up with a brief look at ad hoc reporting options<br>' +
        'including the Report Builder and its report model, shared<br>' +
        'datasets, and report parts.<br>' +
        'Delivering the reports is only half of delivering the<br>' +
        'information. The DW/BI team needs to provide a<br>' +
        'navigation framework to help users find the reports they<br>' +
        'need. You can build this framework using standard web<br>' +
        'tools, a web portal system like SharePoint, or using the<br>' +
        'portal capabilities of a third-party BI tool. We explore this<br>' +
        'more in Chapter 12.<br>' +
        '691<br>' +
        'The standard reports are a critical part of the DW/BI<br>' +
        'system, but they take a fair amount of work to build and<br>' +
        'maintain. The DW/BI team must plan for that work on an<br>' +
        'ongoing basis, as we describe in Chapter 15.<br>' +
        '692<br>';
    document.getElementById('chapter9').innerHTML = 'Chapter 9<br>' +
        'Design Requirements for Real-Time BI<br>' +
        '“The only reason for time is so that everything doesn’t<br>' +
        'happen at once.”<br>' +
        '— Albert Einstein<br>' +
        'What does real time mean in the context of data<br>' +
        'warehousing and business intelligence? If you ask your<br>' +
        'business users what they mean when they ask for real-time<br>' +
        'data, you’ll get such a range of answers that you may<br>' +
        'decide it simply means “faster than they get data today.”<br>' +
        'Throughout this book we’ve been assuming the DW/BI<br>' +
        'system is refreshed periodically, typically daily. All the<br>' +
        'techniques we’ve discussed are perfectly appropriate for a<br>' +
        'daily load cycle. In this chapter, we turn our attention to<br>' +
        'the problem of delivering data to business users throughout<br>' +
        'the day. This can be every 12 hours, hourly, or possibly<br>' +
        'even with a very low latency of seconds.<br>' +
        'We’ll begin the chapter by confessing that we’re not huge<br>' +
        'fans of integrating real-time data into the data warehouse.<br>' +
        'This isn’t to say we don’t think real-time data is interesting<br>' +
        '— just that putting it in the data warehouse database can<br>' +
        'be very expensive and may be requested impulsively by<br>' +
        'end users who haven’t made a solid case for real-time data.<br>' +
        'The best use cases we’ve heard for very low latency data<br>' +
        'come from mixed workload operational applications where<br>' +
        'a customer is on the phone or online.<br>' +
        '579<br>' +
        'Putting aside our doubts, and assuming your business users<br>' +
        'truly require intraday data, we turn our attention to the<br>' +
        'hard problem: getting low latency data to the business<br>' +
        'users. Depending on users’ requirements, as well as the<br>' +
        'technologies you’re sourcing data from, there are several<br>' +
        'ways to deliver real-time data. The first and easiest<br>' +
        'approach is to skip the data warehouse database entirely<br>' +
        'and write reports directly on the source systems.<br>' +
        'Next, we talk about several approaches for bringing the<br>' +
        'real-time data into the DW/BI system. These techniques<br>' +
        'are most valuable for solving the data transformation and<br>' +
        'integration problems inherent in complex reporting. We<br>' +
        'recommend that you segregate the real-time data in its own<br>' +
        'relational database: the real-time partition. Set up a nightly<br>' +
        'process to consolidate the real-time partition into the<br>' +
        'historical data warehouse. Because most of the data<br>' +
        'cleaning and conforming tasks have taken place as the data<br>' +
        'flows into the partition during the day, this nightly<br>' +
        'processing is usually quick.<br>' +
        'If your business users need to perform ad hoc analysis on<br>' +
        'the real-time data, you should set up Analysis Services to<br>' +
        'process the incoming data stream. There are several<br>' +
        'techniques for processing real-time data into the Analysis<br>' +
        'Services cube.<br>' +
        'Real-Time Triage<br>' +
        'If you ask your business users if they want “real-time”<br>' +
        'delivery of data, they’re almost certain to answer yes. So<br>' +
        'don’t ask that question. During business requirements<br>' +
        'interviews, ask the users if “having yesterday’s data today”<br>' +
        '580<br>' +
        'meets their analytic needs. In the cases where the answer is<br>' +
        'no, you may ask a few additional questions to determine if<br>' +
        'the problem can be solved by improvements to the<br>' +
        'transaction system (as it is a majority of the time). If the<br>' +
        'real-time need is complex, we suggest you make a note of<br>' +
        'it during your initial interviews, and plan to return later for<br>' +
        'a discussion exclusively around real time.<br>' +
        'You need to understand several elements of the real-time<br>' +
        'requirements in order to craft an effective architecture with<br>' +
        'the resources you have available.<br>' +
        'What Does Real-Time Mean?<br>' +
        'What is meant by real-time delivery of data? Is it<br>' +
        'instantaneous, frequent, or daily? Many times we’ve<br>' +
        'spoken with users who’ve asked for real-time data, only to<br>' +
        'learn that they mean daily. As we’ve already discussed,<br>' +
        'daily loads are the most common business practice, and in<br>' +
        'most cases nothing to get alarmed about. When<br>' +
        'requirements call for a latency of less than 24 hours, there<br>' +
        'is a significant boundary that drives changes in your ETL<br>' +
        'architecture and data structures. This is the boundary<br>' +
        'between frequent and instantaneous updates.<br>' +
        'Daily is the norm for business today. Whether your DW/BI<br>' +
        'system supports daily, weekly, or monthly loads, the<br>' +
        'design for that system is fundamentally the same.<br>' +
        'NOTE<br>' +
        '581<br>' +
        'A fairly common scenario related to timely<br>' +
        'data updates affects global companies,<br>' +
        'whose customers — and DW/BI users —<br>' +
        'span the globe. It’s common for a global<br>' +
        'company to process data in a handful of<br>' +
        'regions, after local midnight. This isn’t a<br>' +
        'true real-time scenario because each<br>' +
        'region’s data is processed daily.<br>' +
        'Frequent means that the data visible in a report or analysis<br>' +
        'is updated many times a day but is not guaranteed to be the<br>' +
        'absolute current truth. Most of us are familiar with stock<br>' +
        'market quote data that’s current to within 15 minutes.<br>' +
        'Frequently delivered data is usually processed as<br>' +
        'micro-batches using a conventional ETL architecture: SQL<br>' +
        'Server Integration Services (SSIS). The data undergoes the<br>' +
        'full gamut of change data capture, extraction, staging,<br>' +
        'cleaning, error checking, surrogate key assignment, and<br>' +
        'conforming.<br>' +
        'Very low latency means the data must be delivered to the<br>' +
        'business users faster than we can load it into the DW/BI<br>' +
        'system, but not truly instantaneously. The lower bound for<br>' +
        'very low latency is measured in seconds. The upper bound<br>' +
        'is measured in minutes, usually less than 5 minutes. A very<br>' +
        'low latency BI solution is usually implemented as an<br>' +
        'enterprise information integration (EII) application. Such a<br>' +
        'system must limit the complexity of query requests.<br>' +
        'BizTalk is a key Microsoft technology for EII solutions.<br>' +
        'BizTalk offers lightweight data cleaning and<br>' +
        'transformation services, but the underlying ETL cannot<br>' +
        '582<br>' +
        'approach the complexity that SSIS can handle. Usually,<br>' +
        'the very low latency application has its own data store, and<br>' +
        'is an application that is related to the data warehouse, but<br>' +
        'not actually in the data warehouse database.<br>' +
        'Instantaneous means that the data visible on the screen<br>' +
        'represents the true state of the transaction system(s) at<br>' +
        'every instant. When the source system status changes, the<br>' +
        'screen responds instantly and synchronously. An<br>' +
        'instantaneous BI solution is outside the scope of this<br>' +
        'chapter, and this book. It’s a requirement that the<br>' +
        'transaction system itself must meet.<br>' +
        'This chapter focuses on the “frequent” scenario: Latency<br>' +
        'of less than a day but not instantaneous. We touch on the<br>' +
        '“very low latency” scenario, as we discuss techniques to<br>' +
        'reduce the latency of “frequent” delivery of information.<br>' +
        'Who Needs Real Time?<br>' +
        'The requirements for your real-time BI system should<br>' +
        'include a specification of which business users need<br>' +
        'real-time access, and for what. Most often, the consumers<br>' +
        'of real-time information are those in operational roles. This<br>' +
        'makes perfect sense: your executives ought not to be<br>' +
        'worrying about how many orders were placed in the past<br>' +
        '15 minutes. The operational focus of the information<br>' +
        'consumers is good news for the real-time system designer,<br>' +
        'because you can focus on delivering the required<br>' +
        'information without having to support ad hoc use of the<br>' +
        'current data.<br>' +
        '583<br>' +
        'NOTE<br>' +
        'We have seldom seen ad hoc requirements<br>' +
        'for real-time data. In the vast majority of<br>' +
        'cases, real-time needs are met by a handful<br>' +
        'of predefined reports and displays, usually<br>' +
        'parameterized along a very small set of<br>' +
        'attributes such as customer account<br>' +
        'number. You may have to do some work to<br>' +
        'extract crisp requirements from the users,<br>' +
        'because a demand for ad hoc access may be<br>' +
        'covering up uncertainty about what<br>' +
        'information is important. But delivering<br>' +
        'predefined reports on real-time data is an<br>' +
        'order of magnitude easier than supporting a<br>' +
        'wide range of random ad hoc queries.<br>' +
        'As you consider the tradeoffs that must inevitably be made<br>' +
        'in order to accommodate real-time information, you need<br>' +
        'to keep the usage scenarios in mind. Don’t weaken a<br>' +
        'powerful analytic system to deliver real-time data unless<br>' +
        'the rewards are great and the alternatives poor.<br>' +
        'Real-Time Scenarios<br>' +
        'My favorite example of a silly real-time data<br>' +
        'warehouse was a chain of dental clinics whose<br>' +
        'CEO wanted to know how many teeth have been<br>' +
        '584<br>' +
        'filled as of right now. I can just picture the CEO<br>' +
        'pushing the refresh button on his BI dashboard<br>' +
        'while his ship is sailing onto the rocks. On the<br>' +
        'other hand, the IT guy who related this anecdote<br>' +
        'actually did deliver this report, though not<br>' +
        'integrated with the entire DW/BI system. The CEO<br>' +
        'was so impressed that he provided enough funds to<br>' +
        'build a real data warehouse.<br>' +
        'The best examples we’ve seen of a strong need for<br>' +
        'real-time information is in the new economy.<br>' +
        'Consider an online services company that has a<br>' +
        'team of analysts who adjust the placement and<br>' +
        'pricing of advertisements based on usage not just<br>' +
        'for the last 30 days but also up to the moment. In<br>' +
        'the long term this task would have to be automated<br>' +
        'and hence not at all ad hoc. But in the short to<br>' +
        'medium term there’s a clear need for analytic<br>' +
        'access to low latency data.<br>' +
        'The most common scenario for real-time<br>' +
        'information is a call center support application.<br>' +
        'The help desk operator needs to know the caller’s<br>' +
        'historical usage, including the current activity.<br>' +
        'Often this information is scattered across multiple<br>' +
        'transaction systems, so we can’t solve the problem<br>' +
        'by improving the operational systems. Note,<br>' +
        'however, that this scenario does not require the call<br>' +
        'center operator to perform any ad hoc analysis.<br>' +
        'Rather, we must simply provide a report or screen<br>' +
        '585<br>' +
        'that contains very low latency data. This is a<br>' +
        'requirement that we can fill in a variety of ways,<br>' +
        'without necessarily hosting the real-time data in the<br>' +
        'data warehouse database.<br>' +
        'Real-Time Tradeoffs<br>' +
        'Responding to real-time requirements means you’ll need to<br>' +
        'change your DW/BI architecture to get data to the business<br>' +
        'users faster. The architectural choices you make will<br>' +
        'involve tradeoffs that affect data quality and<br>' +
        'administration.<br>' +
        'We assume that your overall goals for your DW/BI system<br>' +
        'are unchanged by the move to real-time delivery. You<br>' +
        'remain just as committed to data quality, integration,<br>' +
        'conformance, usability, and security as you were before<br>' +
        'you started designing a real-time system. If you agree with<br>' +
        'this statement, you will have to juggle a number of<br>' +
        'tradeoffs as you implement a real-time architecture:<br>' +
        '• Delivering real-time data to people who want it, while<br>' +
        'insulating those who don’t. One of the biggest issues for the<br>' +
        'DW/BI team is to meet both the demands of those who want<br>' +
        'real-time data, and those who most decidedly do not. You<br>' +
        'may think that pushing latency closer to real time would be a<br>' +
        'win for everyone. Surely, in the absence of any cost of<br>' +
        'delivering data, we’d all prefer to have real-time data?<br>' +
        'Actually, no. Anyone who’s trying to develop a non-trivial<br>' +
        'analysis knows that you need to work on a static dataset,<br>' +
        'where the numbers aren’t changing from moment to<br>' +
        '586<br>' +
        'moment and query to query. If you try to make these<br>' +
        'analysts work against a dynamic database, they’ll copy a<br>' +
        'bunch of data to a personal computer — exactly the kind<br>' +
        'of behavior you’re probably hoping to stem with your DW/<br>' +
        'BI project.<br>' +
        '• Managing a DW/BI team that has an operational focus. If you<br>' +
        'add operational duties to your team’s charter, you risk having<br>' +
        'the urgent overwhelm the important. In other words, the<br>' +
        'real-time operations will co-opt the strategic nature of the DW/<br>' +
        'BI system. Think about it: This is why strategic groups<br>' +
        'generally have no operational responsibility.<br>' +
        '• Replacing a batch file extract with reading from a message<br>' +
        'queue or transaction log file. A batch file delivered from the<br>' +
        'source system should represent a clean and consistent view of<br>' +
        'the source data. The extract contains only those records<br>' +
        'resulting from completed transactions. Message queue data and<br>' +
        'frequent pulls of transaction log (change data capture or CDC)<br>' +
        'data, on the other hand, is raw instantaneous data that may not<br>' +
        'be subject to any corrective process or business rule<br>' +
        'enforcement in the source system. In the worst case, the<br>' +
        'incoming data is incorrect or incomplete. In this case we<br>' +
        'recommend a hybrid strategy where today’s data residing in the<br>' +
        '“hot partition” is replaced during the quiet period at night with<br>' +
        'data that is extracted from the conventional batch source.<br>' +
        '• Restricting data quality screening only to column screens and<br>' +
        'simple decode lookups. As the time available to process data<br>' +
        'moving through the ETL pipeline is reduced, it may be<br>' +
        'necessary to eliminate more costly data quality screening.<br>' +
        'Recall that column screens involve single field tests or lookups<br>' +
        'to replace or expand known values. Even in the most aggressive<br>' +
        'real-time applications, including instantaneous, most column<br>' +
        'screens should survive. But the more complex structural and<br>' +
        'business rule screens, which by definition require more fields,<br>' +
        'rows, and tables, may not be feasible to execute within the time<br>' +
        'and resources allowed. For example, you may not be able to<br>' +
        'perform a remote credit check through a web service. You may<br>' +
        '587<br>' +
        'need to educate users about the provisional and potentially<br>' +
        'unreliable state of the real-time data.<br>' +
        '• Allowing current facts to be posted with old copies of<br>' +
        'dimensions. In the real-time world, it’s common to receive<br>' +
        'transaction events before the context (such as the attributes of<br>' +
        'the customer) of those transactions. In other words, you’re very<br>' +
        'likely to get facts arriving before their dimension members. If<br>' +
        'the real-time system can’t wait for the dimensions to be<br>' +
        'resolved, you can use the technique described in Chapter 7 of<br>' +
        'posting generic placeholder versions of the dimension members<br>' +
        'in order to maintain referential integrity between facts and<br>' +
        'dimensions. You are much more likely to need to design this<br>' +
        'logic in the real-time ETL than for a daily batch.<br>' +
        '• Eliminating data staging. Some real-time architectures,<br>' +
        'especially very low latency EII systems, stream data directly<br>' +
        'from the production source system to the users’ screens without<br>' +
        'writing the data to permanent storage in the ETL pipeline. If this<br>' +
        'kind of system is part of the DW/BI team’s responsibility, you<br>' +
        'should have a serious talk with senior management about<br>' +
        'whether backup, recovery, archiving, and compliance<br>' +
        'responsibilities can be met, or whether those responsibilities are<br>' +
        'now the sole concern of the production source system. At the<br>' +
        'very least, the data stream going through the DW/BI system<br>' +
        'should be captured in its entirety.<br>' +
        '• Tracking misleadingly separate dimension attribute changes.<br>' +
        'Depending on your source system architecture and your latency<br>' +
        'requirements, you run a real risk of tracking an unreasonable<br>' +
        'number of dimension attribute changes during the day when<br>' +
        'actually there is only one administrative event taking place that<br>' +
        'needs to change several attributes. Recall that a type 2<br>' +
        'dimension adds a new row to the dimension table when a<br>' +
        'tracked attribute is updated. If you don’t have a good way of<br>' +
        'pulling only fully committed changes from the source system,<br>' +
        'you can find yourself adding multiple rows to your customer<br>' +
        'dimension, dated seconds or minutes apart, for what is really<br>' +
        'one address change. Remember that this conformed dimension<br>' +
        'table is used throughout the DW/BI system, not just the portion<br>' +
        'of it being updated in real time.<br>' +
        '588<br>' +
        '• Recalculating performance aggregations. Perhaps the greatest<br>' +
        'design challenge in integrating real-time data into your DW/BI<br>' +
        'system has to do with type 1 updates to dimension attributes.<br>' +
        'The update to the dimension table itself isn’t so problematic.<br>' +
        'The challenging operation is to keep all performance<br>' +
        'aggregations or summary tables updated. Let’s say you’re<br>' +
        'managing customer address as type 1, and some of your fact<br>' +
        'tables have performance aggregations on levels of the<br>' +
        'geography hierarchy (region, country, state, and city). When<br>' +
        'you update a customer’s address as type 1, updating the attribute<br>' +
        'in place, you also need to adjust the pre-computed aggregations.<br>' +
        'You need to move that customer’s historical fact rows from the<br>' +
        'old geography to the new one. This problem ripples to every<br>' +
        'fact table that subscribes to the dimension, whether or not that<br>' +
        'fact table itself is being updated in real time.<br>' +
        'WARNING<br>' +
        'If you’re updating a dimension in real time,<br>' +
        'and an important attribute is managed as<br>' +
        'type 1 (update in place), you are placing a<br>' +
        'huge constraint on all fact tables that<br>' +
        'subscribe to that dimension. Every<br>' +
        'performance aggregation that’s built on the<br>' +
        'type 1 attribute, for each fact table in your<br>' +
        'DW/BI system, for all of history, must be<br>' +
        'adjusted. As your latency drops and you get<br>' +
        'closer to instantaneous, there are only a few<br>' +
        'practical alternatives:<br>' +
        '• Forbid the processing of type 1 attributes<br>' +
        'during the day. You can add new<br>' +
        'dimension members and process type 2<br>' +
        'attributes, but type 1 updates must be<br>' +
        'deferred to nightly processing. Depending<br>' +
        '589<br>' +
        'on your requirements, this approach may<br>' +
        'be unacceptable to the business users.<br>' +
        '• Forbid the use of performance aggregations<br>' +
        'on type 1 attributes that will be updated in<br>' +
        'real time. This proscription applies to all<br>' +
        'fact tables whether or not that fact table is<br>' +
        'updated in real time. You may see a large<br>' +
        'negative impact on query performance if<br>' +
        'the performance aggregation was<br>' +
        'frequently used.<br>' +
        '• Use a different version of the customer<br>' +
        'dimension for the real-time data than for<br>' +
        'the rest of the data warehouse. You’ll need<br>' +
        'to educate users about the potential data<br>' +
        'inconsistencies. As mentioned above, it’s<br>' +
        'common to reprocess the real-time data on<br>' +
        'a nightly basis, at which point the<br>' +
        'inconsistencies would be resolved.<br>' +
        'If you’re using Analysis Services as your<br>' +
        'presentation server, as described in Chapter<br>' +
        '8, it will identify these type 1 attribute<br>' +
        'changes during incremental processing, and<br>' +
        'will rebuild the affected performance<br>' +
        'aggregations for you. The problem is that<br>' +
        'as you get closer to real time, SSAS will be<br>' +
        'spending all of its time dropping and<br>' +
        're-creating the aggregations. It will not be<br>' +
        'pretty.<br>' +
        'In general, we’ve found that real-time DW/BI systems are<br>' +
        'harder, more costly, and more time consuming to build.<br>' +
        'Adding a real-time element to your project will greatly<br>' +
        'increase its risk of failure. We certainly don’t recommend<br>' +
        'pushing toward zero latency data delivery in a phase 1<br>' +
        'project.<br>' +
        '590<br>' +
        'Scenarios and Solutions<br>' +
        'The business has a problem: It needs better, more flexible<br>' +
        'access to real-time data. Business users often look at the<br>' +
        'DW/BI tools, and ask for that same level of functionality<br>' +
        'and flexibility on operational systems.<br>' +
        'Often, the best solution is to improve the operational<br>' +
        'systems. If your operational system is purchased — as<br>' +
        'most are — your company can extend the set of reports<br>' +
        'that are shipped with the product. If those reports can’t be<br>' +
        'modified, you should think about whether you made a<br>' +
        'great product selection. But in the meantime, you can<br>' +
        'certainly replace or extend the packaged reports with a<br>' +
        'Reporting Services portal. That portal may even be<br>' +
        'integrated with the DW/BI portal.<br>' +
        'The problem is more interesting if the need for real-time<br>' +
        'data spans several operational data sources. In this case,<br>' +
        'you must perform some data transformation and<br>' +
        'integration steps. For really simple scenarios, where the<br>' +
        'data is perfectly clean (what alternate universe would that<br>' +
        'be?), both Analysis Services and Reporting Services can<br>' +
        'span multiple data sources. But realistically, any<br>' +
        'integration that’s not trivial will require that the data flow<br>' +
        'through Integration Services. From there it can populate a<br>' +
        'relational database, or even flow directly into a cube. The<br>' +
        'most common and effective scenario is to create a<br>' +
        'real-time partition rather than to flow real-time data<br>' +
        'directly into the historical data warehouse tables.<br>' +
        'The remainder of this chapter describes the technical<br>' +
        'features in the SQL Server toolset that you can use to<br>' +
        '591<br>' +
        'create an architecture that supports access to real-time<br>' +
        'data. Always strive to keep your solution as simple as<br>' +
        'possible. The different technological approaches usually<br>' +
        'require a compromise between latency and integration.<br>' +
        'The closer to instantaneous you need the data, the less<br>' +
        'opportunity and ability you have to integrate and transform<br>' +
        'that data. Table 9-1 below summarizes the options<br>' +
        'discussed in this section.<br>' +
        'Table 9-1: Alternative architectural solutions to delivering<br>' +
        'real-time business intelligence<br>' +
        'Solution Latency Integration<br>' +
        'Execute reports in real<br>' +
        'time<br>' +
        'Very low latency; instantaneous at<br>' +
        'the moment the report is executed None<br>' +
        'Serve reports from a<br>' +
        'cache Frequent None<br>' +
        'Create an ODS using<br>' +
        'database mirror and<br>' +
        'snapshot<br>' +
        'Frequent None<br>' +
        'Create an ODS using<br>' +
        'replication Very low latency Very little<br>' +
        'Build a BizTalk<br>' +
        'application Very low latency Moderate<br>' +
        'Build a real-time partition Frequent Substantial<br>' +
        'Executing Reports in Real Time<br>' +
        'The most common way to use SQL Server technology to<br>' +
        'access real-time data is to use Reporting Services. A report<br>' +
        'written against the transaction system will, by default, be<br>' +
        'executed on demand using live data. If your system is<br>' +
        'small, your usage is light, you have few cross-system<br>' +
        'integration requirements, and no one needs ad hoc access<br>' +
        '592<br>' +
        'to the real-time data, you can serve real-time data from<br>' +
        'standard reports.<br>' +
        'The main drawback of this approach is that it stresses the<br>' +
        'transaction system. Many companies decided to build a<br>' +
        'DW/BI system in part to move reporting off of the<br>' +
        'transaction systems. A popular report that queries a large<br>' +
        'section of the relational tables is going to be very<br>' +
        'expensive to run in real time. You shouldn’t abandon<br>' +
        'Reporting Services immediately, however. As we describe<br>' +
        'in the next section, Reporting Services provides several<br>' +
        'caching features that will help you address this<br>' +
        'performance problem.<br>' +
        'Executing reports directly from the transaction systems<br>' +
        'provides very low latency information. It delivers<br>' +
        'instantaneous access at the moment the report is executed,<br>' +
        'but changes in the data are not automatically pushed into<br>' +
        'the report. The closest you can get to instantaneous is to<br>' +
        'have the user hit the refresh button repeatedly (which we<br>' +
        'are not recommending!).<br>' +
        'Reporting directly from the transaction database provides<br>' +
        'no opportunity to integrate or improve the data, beyond<br>' +
        'that which can be done in the report definition.<br>' +
        'Serving Reports from a Cache<br>' +
        'Reporting Services offers two ways to cache reports to<br>' +
        'improve user query performance, but at the cost of an<br>' +
        'increase in latency. The first technique is to cache reports<br>' +
        'on a schedule. A user can’t tell the difference between a<br>' +
        'cached report and a normal on-demand report, except for<br>' +
        '593<br>' +
        'the date and time the report ran. The first user to run the<br>' +
        'report has to wait for the query to execute and the report to<br>' +
        'render. Subsequent users simply pull the report from the<br>' +
        'cache. You specify a schedule for each report, detailing<br>' +
        'how long the report can be cached before it expires. Once<br>' +
        'the cached report expires, the next query will result in a<br>' +
        'new, refreshed report being created and cached. Users may<br>' +
        'be surprised to see uneven query performance from one<br>' +
        'time the report is run to the next.<br>' +
        'The second very easy technique for improving<br>' +
        'performance of reports against live data is to create a<br>' +
        'snapshot report. A snapshot report is a feature of Reporting<br>' +
        'Services Enterprise and Data Center Editions. A snapshot<br>' +
        'report saves the report’s body, including the dataset, in the<br>' +
        'report catalog database. A snapshot report addresses the<br>' +
        'problem of cached reports’ uneven performance. Someone<br>' +
        '— we can’t predict who — is going to pay the price of<br>' +
        'executing a cached report after the old cache has expired.<br>' +
        'With a snapshot report you can instead schedule the<br>' +
        'execution of the report to run on a schedule and store its<br>' +
        'results. You’d probably choose this approach if you’re<br>' +
        'worried that the CEO would be the one who might execute<br>' +
        'the expired cached report and have to wait for the refresh.<br>' +
        'Cached and snapshot reports can be parameterized, but you<br>' +
        'need to define and cache the parameter values that will be<br>' +
        'used. Realistically, most parameterized reports are not<br>' +
        'served from a cache but instead execute directly against<br>' +
        'the underlying database. Cached and snapshot reports are<br>' +
        'very easy to implement. You should certainly consider<br>' +
        'whether they meet your needs, before turning to more<br>' +
        'complex architectures.<br>' +
        '594<br>' +
        'Creating an ODS with Mirrors and Snapshots<br>' +
        'The phrase Operational Data Store (ODS) has been used to<br>' +
        'mean so many things that we hesitate to use it. By ODS,<br>' +
        'we mean a low latency copy of the transaction systems.<br>' +
        'The ODS may be part of the overall DW/BI system, but is<br>' +
        'definitely not the data warehouse database itself.<br>' +
        'The simplest ODS is a copy of a transaction database with<br>' +
        'little or no integration. If the transaction database is in<br>' +
        'SQL Server 2005 (or later), the easiest way to set up the<br>' +
        'ODS is to create a database snapshot of a mirror of the<br>' +
        'transaction system.<br>' +
        'Mirroring is a feature of SQL Server that’s used primarily<br>' +
        'to increase the availability of a transaction database.<br>' +
        'Typically, the transaction database is mirrored to a<br>' +
        'separate server in a different location. Depending on how<br>' +
        'you configure and run the mirror, it can act as either a hot<br>' +
        'or warm standby.<br>' +
        'Mirroring by itself doesn’t help the ODS, because a<br>' +
        'characteristic of a mirror is that it cannot be queried.<br>' +
        'Create a database snapshot on top of the mirror, and serve<br>' +
        'low latency reports from the snapshot. A database snapshot<br>' +
        'is a read-only, static view of the source database.<br>' +
        'Snapshots always reside on the same server instance as the<br>' +
        'source database, which is why it’s better to snapshot a<br>' +
        'mirror rather than the transaction database directly.<br>' +
        'Mirroring and Snapshots are enterprise features of SQL<br>' +
        'Server.<br>' +
        '595<br>' +
        'REFERENCE<br>' +
        'Books Online has a lot of information on<br>' +
        'mirroring and snapshots, and more is<br>' +
        'available on MDSN. To get started, read<br>' +
        'these Books Online topics:<br>' +
        '• How Database Snapshots Work<br>' +
        '• Database Mirroring and Database<br>' +
        'Snapshots<br>' +
        '• Database Mirroring Overview<br>' +
        'Creating an ODS with Replication<br>' +
        'An alternative approach for creating an ODS is to use SQL<br>' +
        'Server replication. Most people use database mirrors and<br>' +
        'snapshots to create an ODS, but replication continues to be<br>' +
        'a popular technique. It has several advantages over the<br>' +
        'mirror + snapshot approach, including:<br>' +
        '• Replication is available on older versions of SQL Server,<br>' +
        'prior to SQL Server 2005.<br>' +
        '• Replication can be defined on a subset of the source<br>' +
        'database. If the real-time requirements are focused in a<br>' +
        'specific area of the database, replication may be faster and<br>' +
        'use fewer resources.<br>' +
        '• Slight transformations are possible as the data is moved into<br>' +
        'the replication target. This feature doesn’t come close to<br>' +
        'providing the functionality available in Integration Services,<br>' +
        'and should be used with caution. Any significant<br>' +
        'transformations can affect performance on the source<br>' +
        'system. This will not make you popular with the DBAs.<br>' +
        'Building a BizTalk Application<br>' +
        '596<br>' +
        'BizTalk Server is a Microsoft development environment<br>' +
        'for enterprise application integration, business process<br>' +
        'automation, and inter-business communication. BizTalk is<br>' +
        'a rich and robust environment for building integration<br>' +
        'applications, but it’s not a general-purpose replacement for<br>' +
        'Integration Services. SSIS can do far more complex<br>' +
        'transformations, on a much higher volume of data, than<br>' +
        'BizTalk.<br>' +
        'However, you may want to consider using BizTalk to build<br>' +
        'an application to meet your real-time information<br>' +
        'requirements. The scenarios in which BizTalk is<br>' +
        'particularly appealing to include in the DW/BI system<br>' +
        'architecture are:<br>' +
        '• Well-defined, such as a report or dashboard.<br>' +
        '• Integrating data from multiple sources. Otherwise, the<br>' +
        'reporting solutions are more appealing.<br>' +
        '• Very low latency. Otherwise, the DW/BI team will use the<br>' +
        'more familiar Integration Services.<br>' +
        'We view a BizTalk application as ancillary to the core<br>' +
        'DW/BI system. We would consider using it to solve a<br>' +
        'specific problem, to populate a table that feeds a specific<br>' +
        'report or dashboard object, or feed data into a data mining<br>' +
        'application. Depending on the circumstances, its target<br>' +
        'data structures may be nearby, but not exactly inside, the<br>' +
        'main data warehouse database.<br>' +
        'Building a Real-Time Relational Partition<br>' +
        'It’s time to discuss how to load the DW/BI system in real<br>' +
        'time. The problems with loading the relational data<br>' +
        'warehouse database are design issues, not technology<br>' +
        '597<br>' +
        'issues. There’s no technical reason that you could not<br>' +
        'trickle-feed your relational data warehouse database every<br>' +
        'hour, even every minute. The problem, as we’ve already<br>' +
        'discussed, is meeting a diverse set of requirements with a<br>' +
        'single integrated system.<br>' +
        'The design solution for responding to the business users’<br>' +
        'demands is to build a real-time partition as an extension of<br>' +
        'the conventional daily data warehouse. To achieve<br>' +
        'real-time reporting, build a special partition that is<br>' +
        'physically and administratively separated from the<br>' +
        'conventional data warehouse tables.<br>' +
        'The real-time partition should meet the following tough set<br>' +
        'of requirements. It must:<br>' +
        '• Contain all the activity that has occurred since the last<br>' +
        'update of the historical data warehouse, for the fact tables<br>' +
        'that have a low latency requirement.<br>' +
        '• Link as seamlessly as possible to the grain and content of the<br>' +
        'historical data warehouse fact tables.<br>' +
        '• Be indexed so lightly that incoming data can continuously be<br>' +
        'trickled in.<br>' +
        '• Support highly responsive queries.<br>' +
        'The real-time partition is usually not a true table partition.<br>' +
        'Instead, it’s a separate physical table, usually in a different<br>' +
        'database that uses transactional database options. Most<br>' +
        'implementations will use Integration Services to populate<br>' +
        'this real-time partition intraday. If you need very low<br>' +
        'latency, and have (or can obtain) BizTalk expertise<br>' +
        'in-house, you may consider using BizTalk instead of SSIS.<br>' +
        'The real-time partition should store data for the current day<br>' +
        'only.<br>' +
        '598<br>' +
        'NOTE<br>' +
        'It’s usually best to put the real-time<br>' +
        'partition database on the same server as the<br>' +
        'historical data warehouse database.<br>' +
        'Presumably, your users will need to<br>' +
        'combine current and historical data.<br>' +
        'Although distributed queries are feasible,<br>' +
        'it’s easier and works better if all the data is<br>' +
        'on the same server.<br>' +
        'The real-time partition should contain tables that are<br>' +
        'structured similarly to the relational data warehouse<br>' +
        'database. Start with the fact tables that you plan to track in<br>' +
        'real time. These fact tables should look just like the<br>' +
        'corresponding fact table in the data warehouse database,<br>' +
        'with the same columns in the same order. There is one<br>' +
        'vital difference: these fact tables contain the transaction<br>' +
        'system keys, not the surrogate keys that are managed by<br>' +
        'the data warehouse. You will want to add the date and time<br>' +
        'of the transaction, and any other information necessary to<br>' +
        'identify the transaction, like a transaction number. We<br>' +
        'generally recommend that you include such information in<br>' +
        'the main fact table, but be sure to include it here.<br>' +
        'Dimension tables are a little trickier, because conformed<br>' +
        'dimensions are used throughout the enterprise data<br>' +
        'warehouse, not just in the portion being updated in real<br>' +
        'time. Most often, you’d copy dimensions to the real-time<br>' +
        'partition each morning, and make inserts and updates into<br>' +
        'those versions of the dimension. Create a nightly process<br>' +
        '599<br>' +
        'to consolidate those changes into the permanent<br>' +
        'dimensions at the same time the real-time fact data is<br>' +
        'moved over.<br>' +
        'There are several reasons for implementing the real-time<br>' +
        'partition with transaction system keys rather than surrogate<br>' +
        'keys.<br>' +
        '• The surrogate key pipeline is usually the most expensive<br>' +
        'piece of processing in the ETL system, by a substantial<br>' +
        'margin.<br>' +
        '• All of the type 2 (track history) dimension attribute changes<br>' +
        'for a dimension member can be compressed to the<br>' +
        'end-of-day state. Most DW/BI environments choose to track<br>' +
        'attribute changes daily.<br>' +
        '• The main data warehouse database is insulated from the type<br>' +
        '1 (restate history) dimension attribute changes. Recall from<br>' +
        'earlier in this chapter that any performance aggregations in<br>' +
        'the entire data warehouse which are built on a type 1<br>' +
        'attribute will have to be adjusted whenever that attribute is<br>' +
        'updated.<br>' +
        'If you have decided not to process dimension attribute<br>' +
        'changes during the day, then evaluate whether you can<br>' +
        'support the fact table surrogate key pipeline in real time. If<br>' +
        'so, you can integrate the real-time partition into the main<br>' +
        'data warehouse as part of the normal fact table.<br>' +
        'NOTE<br>' +
        'Even if you don’t have a compelling need<br>' +
        'to query integrated data in real time, you<br>' +
        'may still develop a real-time partition. By<br>' +
        '600<br>' +
        'populating it in real time, you can spread<br>' +
        'much of the ETL burden over 24 hours,<br>' +
        'and reduce the time required to perform the<br>' +
        'DW/BI system’s daily update.<br>' +
        'Querying Real-Time Data in the Relational Database<br>' +
        'A purely relational implementation of real-time data is<br>' +
        'appealing, because it minimizes the processing steps.<br>' +
        'However, the drawback of a pure relational<br>' +
        'implementation is that we don’t have a way to break apart<br>' +
        'a query into the portion that needs to be sent to the<br>' +
        'historical data warehouse tables, and the portion to be<br>' +
        'resolved from the real-time partition.<br>' +
        'In a scenario where there is no ad hoc use of the real-time<br>' +
        'partition, this isn’t a deal-breaker. Report builders in IT<br>' +
        'should not find it difficult to construct a report that<br>' +
        'combines identically structured data from the historical<br>' +
        'data warehouse with the real-time partition. The data can<br>' +
        'be combined into a single consolidated chart or report.<br>' +
        'This may be as simple as a query that UNIONs today with<br>' +
        'history, or you may need to join the result sets in<br>' +
        'Reporting Services. Today’s data is always queried with<br>' +
        'the current set of attributes. Depending on the report’s<br>' +
        'business requirements, the historical data will include<br>' +
        'either the current attributes from the real-time partition or<br>' +
        'the historical attributes from the data warehouse. Given the<br>' +
        'nature of operational information needs, it’s more likely<br>' +
        'that you’ll use the current image of the dimension for the<br>' +
        'entire report.<br>' +
        '601<br>' +
        'Alternatively, a very useful display might show the most<br>' +
        'recent activity as a heartbeat-style graph, with a different<br>' +
        'format such as a table for older data. This scenario can be<br>' +
        'implemented as two separate but similar queries presented<br>' +
        'together in a linked report or dashboard.<br>' +
        'Using Analysis Services to Query Real-Time Data<br>' +
        'If your users absolutely, positively, have to have ad hoc<br>' +
        'access to real-time data, the best way to provide this access<br>' +
        'is to use Analysis Services. SSAS can provide the service<br>' +
        'to redirect the historical part of a query to one storage<br>' +
        'location, and the low latency part of the same query to a<br>' +
        'different data store.<br>' +
        'WARNING<br>' +
        'Build a separate Analysis Services database<br>' +
        'exclusively for the business users who need<br>' +
        'real-time information. Don’t attempt to<br>' +
        'modify your core SSAS database to be<br>' +
        'updated in real time. Strip down this<br>' +
        'real-time cube to be as small as possible<br>' +
        'while still meeting users’ needs.<br>' +
        'Build a real-time cube much the same as described in<br>' +
        'Chapter 8, in an SSAS database devoted to delivering<br>' +
        'real-time data. You should be using Enterprise Edition,<br>' +
        'because you’ll need multiple partitions.<br>' +
        '602<br>' +
        'The dimensions should be structured identically to the<br>' +
        'corresponding dimension in the core SSAS database, but<br>' +
        'they will be built from the real-time versions of the<br>' +
        'dimension tables located in the relational real-time<br>' +
        'partition.<br>' +
        'The cube’s measure groups must be partitioned by date,<br>' +
        'with the majority of the data coming from the historical<br>' +
        'fact table in the main data warehouse, and the current day<br>' +
        'from the real-time partition. Make sure your aggregation<br>' +
        'plan for this real-time cube does not contain any<br>' +
        'pre-computed performance aggregations for dimension<br>' +
        'attributes managed as type 1, as we described earlier in this<br>' +
        'chapter.<br>' +
        'Often, you can get perfectly acceptable query performance<br>' +
        'by leaving the real-time partition in the relational database<br>' +
        '(using ROLAP storage mode). For best processing<br>' +
        'performance, the real-time partition should have no<br>' +
        'performance aggregations defined on it. The dimensions<br>' +
        'and the historical fact data should use MOLAP storage<br>' +
        'mode as usual, with plenty of performance aggregations. If<br>' +
        'query performance is not good enough with this approach,<br>' +
        'use MOLAP storage on the real-time partition.<br>' +
        'There are several ways to get the real-time data into<br>' +
        'Analysis Services. All of these methods allow background<br>' +
        'processing while users are connected to, and actively<br>' +
        'querying, the cube.<br>' +
        '• Schedule frequent processing. You can set up scheduled<br>' +
        'processing of the real-time dimensions and partition. Have<br>' +
        'SSIS perform its ETL into the relational database at the<br>' +
        '603<br>' +
        'latency you need, such as hourly. When the ETL is done,<br>' +
        'kick off dimension processing and then incremental<br>' +
        'processing of the real-time partition. If the real-time partition<br>' +
        'is stored as ROLAP with no aggregations, processing will be<br>' +
        'extremely fast. The design of the ETL is fundamentally the<br>' +
        'same as for the common daily processing scenario described<br>' +
        'in Chapter 7. As always, the Analysis Services cube remains<br>' +
        'available for user queries while processing is under way.<br>' +
        '• Use SSIS to process data directly. Similar to the processing<br>' +
        'scenario just described, you can use the SSAS destination<br>' +
        'adapter of the SSIS data flow to reduce latency a little bit.<br>' +
        'The difference between this scenario and the scheduled<br>' +
        'processing just described is that SSIS flows the data directly<br>' +
        'into SSAS instead of into a relational table first, and from<br>' +
        'there, into SSAS. If you implement this approach, you<br>' +
        'should multicast the flow just before the insert, and flow it<br>' +
        'into the relational real-time partition at the same time it’s<br>' +
        'processed into Analysis Services. This approach is unlikely<br>' +
        'to shave more than a few seconds from the data delivery<br>' +
        'time, as SSAS processing is quite fast, especially if the<br>' +
        'Analysis Services partition is in ROLAP with no<br>' +
        'aggregations.<br>' +
        '• Use proactive caching. Proactive caching is a feature of<br>' +
        'SSAS that transparently maintains a dimension or partition.<br>' +
        'Proactive caching consists of two components:<br>' +
        '• A mechanism for watching the relational database<br>' +
        'from which the cube is sourced, to identify new<br>' +
        'data.<br>' +
        '• Sophisticated caching that enables uninterrupted<br>' +
        'high-performance querying while the new data is<br>' +
        'being processed and added to the cube.<br>' +
        'Proactive caching itself does not provide real-time<br>' +
        'capability; it’s a feature that helps you manage your<br>' +
        'real-time business needs.<br>' +
        'Summary<br>' +
        '604<br>' +
        'We’ve spent a lot of time in this chapter talking about the<br>' +
        'challenges of real-time BI. These challenges are even<br>' +
        'greater if you’re trying to deliver real-time data that’s<br>' +
        'consistent across the enterprise with the well managed data<br>' +
        'warehouse database.<br>' +
        'Our goal in this chapter is to present you with a realistic<br>' +
        'description of the challenges, the alternatives and their<br>' +
        'pros and cons, and practical advice for implementing each<br>' +
        'significant alternative.<br>' +
        'These alternatives start with encouraging you to keep the<br>' +
        'real-time data out of the data warehouse database, and<br>' +
        'away from the DW/BI team. We’ve seen strategic-thinking<br>' +
        'DW/BI teams get sucked into delivering real-time data,<br>' +
        'never to be heard from again. A lot of the business<br>' +
        'requirements for real-time data can be met by the<br>' +
        'transaction system owners using the very nice functionality<br>' +
        'in SQL Server, especially Reporting Services, directly<br>' +
        'against the transaction systems.<br>' +
        'If you need to present data in real time that integrates<br>' +
        'information from multiple sources, you’ll need to use<br>' +
        'Integration Services. Store the results of these expensive<br>' +
        'integration and transformation operations in the real-time<br>' +
        'partition of the relational data warehouse. We described<br>' +
        'several designs and techniques for populating the DW/BI<br>' +
        'system in real time.<br>' +
        'We believe that the greatest benefit to the real-time<br>' +
        'functionality offered in SQL Server 2008 will be to<br>' +
        'software vendors who are building and improving<br>' +
        'operational systems, and the future customers of those<br>' +
        '605<br>' +
        'systems. Ideally, the operational systems will present<br>' +
        'information in a useful, timely, flexible way. We hope<br>' +
        'software developers will use these features to deliver<br>' +
        'products that delight rather than frustrate their users.<br>' +
        '606<br>';
    document.getElementById('chapter8').innerHTML = 'Chapter 8<br>' +
        'The Core Analysis Services OLAP Database<br>' +
        'The tip of the iceberg.<br>' +
        'Once you’ve designed, built, and loaded the relational data<br>' +
        'warehouse, it’s time for the fun part: delivering<br>' +
        'information to the business user community. In order for<br>' +
        'users to be able to consume that information — especially<br>' +
        'for ad-hoc use — you need to make sure they can<br>' +
        'formulate queries and get responses quickly. The Analysis<br>' +
        'Services OLAP database is a popular and effective tool for<br>' +
        'meeting these goals, and should be a core part of your<br>' +
        'strategy to enable self-service business intelligence. OLAP<br>' +
        'stands for On Line Analytic Processing, to distinguish this<br>' +
        'analytic database from the more familiar OLTP transaction<br>' +
        'processing database.<br>' +
        'Designing and developing the Analysis Services database<br>' +
        'straddles the data track of the Kimball Lifecycle, as<br>' +
        'illustrated in Figure 8-1. The OLAP design process begins<br>' +
        'with modeling. If you’ve followed the Kimball Method to<br>' +
        'implement the dimensional model in the relational<br>' +
        'database, the OLAP design step is a straightforward<br>' +
        'translation from that existing design. There are some<br>' +
        'physical design decisions to be made later in the<br>' +
        'development process, and a modest extension of the ETL<br>' +
        'system for populating the OLAP database.<br>' +
        'Start the process of building the OLAP database by<br>' +
        'sourcing it from a robust, cleanly populated, relational<br>' +
        '468<br>' +
        'dimensional model, as described in the previous chapters.<br>' +
        'From a solid starting point, you can develop a decent<br>' +
        'prototype in a few days.<br>' +
        'It’ll take more than a few days to polish that prototype,<br>' +
        'adding complex calculations and other decorations,<br>' +
        'making the physical design decisions, and setting up and<br>' +
        'testing the process to keep the Analysis Services database<br>' +
        'up-to-date. But you’re starting from a clean and conformed<br>' +
        'dimensional model, so it’s really not that hard. The<br>' +
        'investment in building and populating the OLAP database<br>' +
        'is typically measured in person-weeks, not person-months.<br>' +
        'Figure 8-1: Business Dimensional Lifecycle: The data<br>' +
        'track<br>' +
        'This chapter starts with an overview of SQL Server<br>' +
        'Analysis Services (SSAS) OLAP. We discuss why you<br>' +
        'should include an SSAS database in your DW/BI system.<br>' +
        'What it comes down to is this: It’s substantially easier, and<br>' +
        'lots more fun, to deliver fast query performance and<br>' +
        'complex analytics in Analysis Services than in a relational<br>' +
        '469<br>' +
        'database. Analysis Services works well. Especially when<br>' +
        'you consider its price, it’s the obvious choice of OLAP<br>' +
        'technology on the Microsoft platform.<br>' +
        'We spend most of the chapter discussing how to develop<br>' +
        'your SSAS dimensions and measures. We end with a<br>' +
        'discussion of physical design considerations, including<br>' +
        'precomputed aggregations and partitions, along with an<br>' +
        'overview of the approaches for keeping your Analysis<br>' +
        'Services database up-to-date.<br>' +
        'This chapter describes how to use Analysis Services as a<br>' +
        'core presentation server in your DW/BI system. The core<br>' +
        'OLAP database is typically managed centrally, and is<br>' +
        'closely tied to the relational data warehouse and ETL<br>' +
        'systems. In Chapter 11 we describe the Power Pivot<br>' +
        'functionality of SSAS, new in SQL Server 2008 Release 2.<br>' +
        'Power Pivot is a standalone Excel add-in version of<br>' +
        'Analysis Services designed for business users. It can query<br>' +
        'data from a central OLAP cube for some or all of its data.<br>' +
        'In Chapter 13, we look at the other part of Analysis<br>' +
        'Services’ functionality: data mining.<br>' +
        'Overview of Analysis Services OLAP<br>' +
        'The relational database is a great place to store and<br>' +
        'manage the data in your DW/BI system. But the relational<br>' +
        'database doesn’t, by itself, have enough intelligence. There<br>' +
        'are several things missing from the relational data store:<br>' +
        '• Rich metadata to help users navigate the data and create<br>' +
        'queries.<br>' +
        '470<br>' +
        '• Powerful analytic calculations and functions defined in a<br>' +
        'context-sensitive query language<br>' +
        '• Excellent and consistent query performance for a broad<br>' +
        'range of ad hoc queries.<br>' +
        'For non-Microsoft DW/BI systems, the most common<br>' +
        'approach to providing these missing elements is to use a<br>' +
        'full-featured reporting tool on the relational data<br>' +
        'warehouse. The reporting tool contains the metadata to<br>' +
        'help users navigate. And the reporting tools work closely<br>' +
        'with the database engine to construct well-formed SQL<br>' +
        'queries. Some reporting tools can also provide transparent<br>' +
        'aggregate navigation for performance and more advanced<br>' +
        'analytics.<br>' +
        'This is a common architecture in Microsoft DW/BI<br>' +
        'systems as well. In the SQL Server environment, you can<br>' +
        'use Reporting Services Report Builder as the ad hoc query<br>' +
        'and reporting tool, accessing the relational data warehouse<br>' +
        'directly. This scenario is described in Chapter 10.<br>' +
        'The preferred architecture in the Microsoft platform is to<br>' +
        'use SSAS as the primary presentation database. As you’ll<br>' +
        'see in this chapter, when you define an Analysis Services<br>' +
        'database on top of your relational data warehouse, you’re<br>' +
        'creating that rich metadata layer. At the same time, you<br>' +
        'can create a physical storage layer that includes<br>' +
        'aggregations and indexes to deliver excellent query<br>' +
        'performance. Analysis Services also brings a powerful,<br>' +
        'although complex, language that supports advanced,<br>' +
        'context sensitive analytics.<br>' +
        'In the Microsoft SQL Server DW/BI system, we<br>' +
        'recommend building an Analysis Services database for ad<br>' +
        '471<br>' +
        'hoc users, and possibly to serve as the database for most<br>' +
        'predefined reports as well. This chapter describes how to<br>' +
        'use SSAS as the presentation database — what we call the<br>' +
        'core OLAP database.<br>' +
        'Why Use Analysis Services?<br>' +
        'The obvious reason for using Analysis Services is that it<br>' +
        'plugs several gaps in the relational database’s analytic<br>' +
        'functionality. We’ll begin with a summary of why we<br>' +
        'generally recommend this architecture for SQL Server.<br>' +
        '• User-oriented metadata: The definition of an OLAP cube<br>' +
        'highlights elements of the dimensional model that improve<br>' +
        'the user experience, especially for ad hoc queries. These<br>' +
        'metadata elements include the distinction between facts and<br>' +
        'dimensions, hierarchies and drilldown paths, groupings of<br>' +
        'attributes and facts, and the ability to easily combine facts<br>' +
        'from multiple business processes through conformed<br>' +
        'dimensions. These elements are defined once on the SSAS<br>' +
        'server, and are available to any client tool without further<br>' +
        'configuration.<br>' +
        '• Calculations: You can define business calculations like<br>' +
        'profit, sales year to date, and sales same period last year.<br>' +
        'You can define sets like top 10 customers and a host of other<br>' +
        'calculations. Once these calculations are defined, all<br>' +
        'business users — no matter what tool they use to access<br>' +
        'Analysis Services — will use the same formula. Complex<br>' +
        'calculations can be challenging to define correctly, but the<br>' +
        'work is done once by the development team and shared by<br>' +
        'all. The cube calculations that you define using the MDX<br>' +
        'language can be orders of magnitude more complex than<br>' +
        'those supported by SQL.<br>' +
        '• Complex security rules: One of the challenges of providing<br>' +
        'ad hoc access to the relational data warehouse is to secure<br>' +
        'detailed data but provide open access to summarized data.<br>' +
        '472<br>' +
        'This is particularly true for ad hoc use. SSAS security<br>' +
        'enables complex security rules, as we discuss in Chapter 14.<br>' +
        '• Query performance: Query performance is the first reason<br>' +
        'most people are attracted to OLAP. In general, Analysis<br>' +
        'Services offers excellent dimensional query performance, in<br>' +
        'a way that’s usually cheaper and easier to manage than is<br>' +
        'possible from the relational database alone. You can get<br>' +
        'good query performance from a pure relational system, but it<br>' +
        'requires a lot of work that SSAS does for you. Best of all,<br>' +
        'achieving this performance is transparent to the user.<br>' +
        'NOTE<br>' +
        'The SQL Server relational database has an<br>' +
        'indexed views feature, which performs<br>' +
        'some aggregate navigation. However,<br>' +
        'we’ve not found it to be particularly useful<br>' +
        'for dimensional schemas. Analysis<br>' +
        'Services works so well that almost no one<br>' +
        'uses SQL Server indexed views directly for<br>' +
        'aggregate navigation on dimensional<br>' +
        'schemas. There are some storage<br>' +
        'configurations, known as relational OLAP,<br>' +
        'in which SSAS uses indexed views to build<br>' +
        'performance aggregations in the relational<br>' +
        'database. In this scenario, Analysis<br>' +
        'Services manages the indexed views for<br>' +
        'you.<br>' +
        '• Aggregation management: The single most important thing you<br>' +
        'can do to improve the query performance of any DW/BI system,<br>' +
        'regardless of platform, is to define aggregations. Aggregations<br>' +
        'are precomputed and stored summarizations of the detailed data<br>' +
        'in the fact table. They’re nothing mysterious: They’re simply<br>' +
        '473<br>' +
        'summary tables at different grains, for example monthly, or by<br>' +
        'geographic region, or both. We called these aggregate tables in<br>' +
        'the relational database, but Analysis Services calls them<br>' +
        'aggregations, so that’s what we call them here. Defining a good<br>' +
        'set of aggregations is more valuable to query performance than<br>' +
        'indexing and cheaper than upgrading your hardware. SSAS<br>' +
        'helps you define and maintain these aggregations. And queries<br>' +
        'into your OLAP cube will seamlessly use the appropriate<br>' +
        'aggregations.<br>' +
        'What’s New in Analysis Services 2008 R2<br>' +
        'OLAP?<br>' +
        'Not much. The core OLAP database tools and<br>' +
        'technology improved slightly between SQL Server<br>' +
        '2005 and 2008, and even more slightly between<br>' +
        'SQL Server 2008 and 2008 Release 2. Look for the<br>' +
        'following improvements made in SQL Server<br>' +
        '2008:<br>' +
        '• Improved query performance.<br>' +
        '• Improved aggregation design wizards. You can now<br>' +
        'manually specify which aggregations you want SSAS<br>' +
        'to build. Aggregations are discussed in greater detail<br>' +
        'later in this chapter.<br>' +
        '• Attribute relationship designer. Attribute<br>' +
        'relationships are, unsurprisingly, how you specify<br>' +
        'that two attributes are related. Correctly setting<br>' +
        'attribute relationships is vital for query performance<br>' +
        'for medium and large cubes, as we discuss later in<br>' +
        'this chapter.<br>' +
        '• Better design guidance. The wizards and editors do a<br>' +
        'better job of encouraging good dimension and cube<br>' +
        'design.<br>' +
        '• Improved backup and restore.<br>' +
        'Why Not Analysis Services?<br>' +
        '474<br>' +
        'We recommend that most DW/BI systems use Analysis<br>' +
        'Services as the primary query server. The relational<br>' +
        'version of the dimensional schema serves as the permanent<br>' +
        'store of the cleaned and conformed data, and feeds data to<br>' +
        'the OLAP database.<br>' +
        'Some systems will close the relational data warehouse<br>' +
        'database to business users. This is an appealing<br>' +
        'architecture, for in this case the relational database can be<br>' +
        'lightly indexed. The Analysis Services database, including<br>' +
        'data, indexes, and a reasonable set of precomputed<br>' +
        'aggregations, is smaller than the relational indexes it<br>' +
        'replaces. Standard reports, KPIs, analytic reports, and ad<br>' +
        'hoc analyses can all be sourced from SSAS, using<br>' +
        'Reporting Services, Excel, and SharePoint.<br>' +
        'Other systems will permit user access to both Analysis<br>' +
        'Services and the corresponding relational database. And<br>' +
        'others don’t use Analysis Services at all. What are some of<br>' +
        'the common reasons to commit less than fully to SSAS?<br>' +
        '• The Analysis Services development market is immature.<br>' +
        'There are fewer tools, experts, and informational material<br>' +
        'about how to work with SSAS than there are for relational<br>' +
        'data warehouses.<br>' +
        '• The query and reporting tool market is confusing and<br>' +
        'immature. Companies have a large investment in existing<br>' +
        'client tools and developer and user skills.<br>' +
        '• Some kinds of analyses are intrinsically difficult in OLAP.<br>' +
        'This is particularly true for analyses that align data not by<br>' +
        'dimension attributes but by an event that’s buried in the<br>' +
        'facts.<br>' +
        '475<br>' +
        'NOTE<br>' +
        'Most ad hoc analyses are easier to<br>' +
        'construct with MDX and OLAP than with<br>' +
        'SQL. A counter-example from a<br>' +
        'clickstream business process is to analyze<br>' +
        'how users behave after they first visit a<br>' +
        'certain page. First you need to go into the<br>' +
        'facts to find who visited the page (and<br>' +
        'when). Then you may want to align all the<br>' +
        'users’ page clicks by time to see patterns in<br>' +
        'behavior. This is moderately difficult in<br>' +
        'SQL but extremely challenging in OLAP<br>' +
        'and MDX. In truth, this kind of analysis is<br>' +
        'best handled by data mining. But<br>' +
        'sometimes we just want to poke around,<br>' +
        'and for this kind of problem we prefer<br>' +
        'SQL.<br>' +
        'Designing the OLAP Structure<br>' +
        'If you’ve followed our instructions to build the relational<br>' +
        'layer of your DW/BI system with surrogate keys,<br>' +
        'conformed dimensions, and well-managed dimension<br>' +
        'changes, then building an Analysis Services OLAP<br>' +
        'database is straightforward. There are several major steps:<br>' +
        '1. Develop your plan.<br>' +
        '2. Set up the design and development environment.<br>' +
        '476<br>' +
        '3. Create a Data Source View.<br>' +
        '4. Create and fine tune your dimensions.<br>' +
        '5. Run the Cube Wizard and edit the resulting cube.<br>' +
        '6. Create calculations and other decorations.<br>' +
        '7. Iterate, iterate, iterate.<br>' +
        'Later in the chapter, we discuss physical storage issues,<br>' +
        'which are largely independent of the database’s logical<br>' +
        'design.<br>' +
        'NOTE<br>' +
        'Analysis Services contains features to help<br>' +
        'you overcome flaws in the design of the<br>' +
        'source database, like referential integrity<br>' +
        'violations, but we’re not going to talk about<br>' +
        'those features. Instead, build your DW/BI<br>' +
        'system correctly, as described in this book.<br>' +
        'Planning<br>' +
        'The first step in developing your SSAS database is to<br>' +
        'document its scope. If you’re following the Kimball<br>' +
        'Method, your phase 1 project includes a single business<br>' +
        'process dimensional model. This is a reasonable sized<br>' +
        'chunk for the first phase of an Analysis Services cube:<br>' +
        'usually 3–5 related fact tables and their associated<br>' +
        '477<br>' +
        'dimensions. You will need to consider the contents of each<br>' +
        'cube, the grain of the fact tables and their dimensions, and<br>' +
        'the nature of cube usage in your plan.<br>' +
        'RESOURCES<br>' +
        'We reviewed the Kimball Method very<br>' +
        'briefly in the introduction to this book. For<br>' +
        'much more detail, see The Data Warehouse<br>' +
        'Lifecycle Toolkit — an entire book devoted<br>' +
        'to the subject of the Kimball Method.<br>' +
        'Cube Content<br>' +
        'A single cube can — and should — contain multiple fact<br>' +
        'tables. The most challenging aspect of cube planning is to<br>' +
        'decide how many fact tables to include in a single cube.<br>' +
        'There’s no physical upper limit on the number of fact<br>' +
        'tables to include, and the best design for you depends on<br>' +
        'your data and usage patterns. When two fact tables are in<br>' +
        'the same cube, it’s trivial for users to develop queries and<br>' +
        'analyses that include information from both. This is hugely<br>' +
        'valuable, and is the reason we create conformed<br>' +
        'dimensions in the first place. The payoff for building<br>' +
        'conformed dimensions is the ability to drill across separate<br>' +
        'business processes, assembling an integrated final result.<br>' +
        'How Many Fact Tables in a Cube?<br>' +
        '478<br>' +
        'There is a hierarchy of sorts inherent in an Analysis<br>' +
        'Services installation. At the top is the server<br>' +
        'instance. Usually there is one server instance on a<br>' +
        'machine, just as with the relational database<br>' +
        'engine.<br>' +
        'A server can have multiple databases, and a<br>' +
        'database can contain multiple cubes. Finally, each<br>' +
        'cube can contain multiple fact tables, which<br>' +
        'Analysis Services somewhat confusingly calls<br>' +
        'Measure Groups. All of the measure groups and<br>' +
        'cubes in a database share conformed dimensions.<br>' +
        'Most organizations will have a handful of<br>' +
        'databases in production, each of which has 1–10<br>' +
        'cubes. There’s no technical limit on how many<br>' +
        'databases, cubes, or fact tables you have.<br>' +
        'Rules of thumb: Five fact tables in a cube should<br>' +
        'be entirely comfortable in most situations. A cube<br>' +
        'that includes fifteen fact tables is often reasonable,<br>' +
        'though your experience will depend on your data<br>' +
        'volumes, the complexity of your calculations, and<br>' +
        'usage patterns. Fifty fact tables in a cube is almost<br>' +
        'certainly too many for broad use.<br>' +
        'If you have resources to spare, consider creating an<br>' +
        'exploratory cube that contains many or all fact<br>' +
        'tables, but is open only to the DW team and a very<br>' +
        'small number of power users. The exploratory cube<br>' +
        '479<br>' +
        'is used to inform the development of smaller subset<br>' +
        'cubes.<br>' +
        'The fact table and dimensions in a single business process<br>' +
        'dimensional model equates comfortably to a single cube. It<br>' +
        'is certainly the place to begin for your first SSAS core<br>' +
        'cube development effort. But as you iterate through the<br>' +
        'Lifecycle and add new business process dimensional<br>' +
        'models, don’t blindly spin up a new cube for each. There is<br>' +
        'so much analytic richness to be gained from including<br>' +
        'additional fact tables in a single cube. The problem is that<br>' +
        'as the cube grows in complexity, its performance degrades.<br>' +
        'Cube Granularity<br>' +
        'As you’re deciding which fact tables are included in the<br>' +
        'cube, you should also determine which dimensions, and<br>' +
        'which grain of the dimensions, are included. Analysis<br>' +
        'Services scales very well, and in many cases it is<br>' +
        'reasonable — and best practice — to build cubes with the<br>' +
        'same grain as the underlying fact tables. Cubes that are<br>' +
        'built on and include a terabyte of relational data are not<br>' +
        'uncommon. Remember, SSAS manages and uses<br>' +
        'precomputed performance aggregations, so most user<br>' +
        'queries will not actually use the detailed data.<br>' +
        'Our initial approach is to build the cubes at the same grain<br>' +
        'as the dimensional data in the relational database. The<br>' +
        'factors that might lead us to deviate from that approach<br>' +
        'include:<br>' +
        '480<br>' +
        '• Severe resource constraints.<br>' +
        '• Potential usability problems, usually because of poorly<br>' +
        'structured natural hierarchies that lead to a huge number of<br>' +
        'children in a drilldown path. It’s not a good idea to have<br>' +
        '100,000 nodes open up when a user clicks a + sign in an<br>' +
        'Excel spreadsheet, as she will wait a long time for the data to<br>' +
        'return or Excel to crash.<br>' +
        '• Clearly defined business cases for accessing the lowest grain<br>' +
        'of detail. In this case, it can be quite effective to reach back<br>' +
        'into the relational database for that lowest level of detail.<br>' +
        'We usually exclude the audit dimension described in<br>' +
        'Chapter 7.<br>' +
        'Cube Usage<br>' +
        'The cube scope document should also detail how much of<br>' +
        'the overall ad hoc access will go through Analysis<br>' +
        'Services. Typically, the answer is between 90 percent and<br>' +
        '100 percent. Document any scenarios where business users<br>' +
        'need to access the relational data warehouse directly for ad<br>' +
        'hoc use. Also, document how much of standard reporting<br>' +
        'will go through SSAS. This is a much broader range,<br>' +
        'typically between 50 percent and 100 percent.<br>' +
        'Getting Started<br>' +
        'There are several steps in the setup process. First, you need<br>' +
        'to install and configure one or more Analysis Services<br>' +
        'development servers. You need to ensure the correct<br>' +
        'software is installed on developers’ desktops. You should<br>' +
        'have some clean data loaded into the data warehouse<br>' +
        'database, and you should have created a set of views on<br>' +
        'those database tables.<br>' +
        '481<br>' +
        'Setup<br>' +
        'As the Analysis Services database developer, the only SQL<br>' +
        'Server components you must have on your desktop PC are<br>' +
        'the development tools, notably BI Development Studio<br>' +
        '(BIDS) and Management Studio. Many developers run the<br>' +
        'server components on their desktop PCs, but it’s not<br>' +
        'required. You can point to a shared Analysis Services<br>' +
        'development server, just as you would share a<br>' +
        'development relational database server. Use BI Studio to<br>' +
        'design and develop the Analysis Services database, and<br>' +
        'Management Studio to operate and maintain that database.<br>' +
        'Work through the tutorial that ships with SQL Server<br>' +
        'before trying to design and build your first OLAP<br>' +
        'database. It’s a good tutorial, and not only teaches you<br>' +
        'which buttons to push but also provides information about<br>' +
        'why. This chapter is not a replacement for the tutorial.<br>' +
        'Instead we assume you’ll learn the basics from the tutorial.<br>' +
        'Here, we focus more on process and design issues.<br>' +
        'Create Relational Views<br>' +
        'In Chapter 5, we recommended that you create a database<br>' +
        'view for each table in the dimensional model. End users’<br>' +
        'and Analysis Services’ access to the relational database<br>' +
        'should come through these views. The views provide a<br>' +
        'layer of insulation, and can greatly simplify the process of<br>' +
        'making future changes to the DW/BI system. Give the<br>' +
        'views and columns user-friendly names: These names<br>' +
        'become SSAS object names like dimensions and attributes.<br>' +
        '482<br>' +
        'NOTE<br>' +
        'Analysis Services does a pretty good job of<br>' +
        'turning the common styles of database<br>' +
        'object names into friendly names. It will<br>' +
        'parse CamelCase and Underscore_Case<br>' +
        'names, stripping underscores and inserting<br>' +
        'spaces as appropriate. It’s not perfect; you<br>' +
        'do need to review the friendly names in the<br>' +
        'Data Source View.<br>' +
        'The downside of using views is that you obscure any<br>' +
        'foreign key relationships defined in the data. Many tools<br>' +
        'automatically create join paths based on these<br>' +
        'relationships; these join paths must be added by hand when<br>' +
        'you use views.<br>' +
        'NOTE<br>' +
        'Creating the view layer sounds like<br>' +
        'make-work. But we’ve always regretted it<br>' +
        'when we’ve skipped this step.<br>' +
        'Populate the Data Warehouse Database<br>' +
        'You don’t need to wait until the ETL system is finished,<br>' +
        'and the relational data warehouse is fully populated with<br>' +
        'historical and incremental data, before you start working<br>' +
        'on the Analysis Services database. Technically, you don’t<br>' +
        '483<br>' +
        'have to populate the database at all. But that’s just a<br>' +
        'theoretical point: In practice, you want to look at the data<br>' +
        'as you’re designing the OLAP database.<br>' +
        'NOTE<br>' +
        'There is nothing like designing, processing,<br>' +
        'and exploring an SSAS dimension to<br>' +
        'highlight data inconsistencies. We<br>' +
        'recommend that you experiment with<br>' +
        'building SSAS dimensions as soon as you<br>' +
        'have data available. You will be amazed by<br>' +
        'what you’ll turn up in what you perhaps<br>' +
        'imagined was very solid and clean data.<br>' +
        'We hate to see projects that finish the ETL work before<br>' +
        'anyone begins poking around the data in SSAS. Some<br>' +
        'organizations run projects in a very compartmentalized<br>' +
        'way and declare that once ETL is accepted, it cannot be<br>' +
        'changed. If your organization is that way, you will be well<br>' +
        'served to mock up your dimensions as soon as possible.<br>' +
        'It’s helpful during the design process to work from a static<br>' +
        'copy of the warehouse database. It’s much easier to design<br>' +
        'and debug the cube if the underlying data isn’t changing<br>' +
        'from day to day. Most people fully populate the<br>' +
        'dimensions and several months of data in the large fact<br>' +
        'tables. Small fact tables, like the Exchange Rates fact<br>' +
        'table in the Adventure Works Cycles case study, can be<br>' +
        'fully populated.<br>' +
        '484<br>' +
        'Fully populate most of the dimensions because that’s<br>' +
        'where most of the design work occurs. Dimensions are<br>' +
        'usually small enough that you can restructure and rebuild<br>' +
        'them many times, without an intolerable wait to see how<br>' +
        'the modified dimension looks. If you have a larger<br>' +
        'dimension, say 100,000 to 1 million members, you may<br>' +
        'want to work on a dimension subset for the early phases of<br>' +
        'the design cycle. Define your dimension table view with a<br>' +
        'WHERE clause so that it subsets the dimension to a<br>' +
        'reasonable size, iterate on the design until you’re satisfied,<br>' +
        'and then redefine the view and rebuild the dimension at its<br>' +
        'full size. We can almost guarantee you’ll still tweak the<br>' +
        'design a few more times, but you should be past the worst<br>' +
        'of the iterations.<br>' +
        'We mentioned that most people use a few months of fact<br>' +
        'data. This works great for structural design tasks like<br>' +
        'setting the grain, dimension usage, and base measures for<br>' +
        'that fact data. But defining complex calculations is often<br>' +
        'easier to do with a longer time series. As a simple<br>' +
        'example, consider a measure that compares sales this<br>' +
        'month to the same month last year.<br>' +
        'If you have really big dimensions and facts, you should<br>' +
        'take the time to build a physical subset of the relational<br>' +
        'database. Keep all small dimensions intact, but subset<br>' +
        'large dimensions to fewer than 100,000 members.<br>' +
        'Randomly choose leaf nodes rather than choose a specific<br>' +
        'branch of the dimension. In other words, if you have 5<br>' +
        'million customers, randomly choose 100,000 of them<br>' +
        'rather than choose all of the customers from California<br>' +
        '(who may not be representative of your full customer<br>' +
        'base). Choose the subset of facts from the intersection of<br>' +
        '485<br>' +
        'your dimensions. Be very careful to avoid grabbing facts<br>' +
        'for the dimension members you’ve excluded from the test<br>' +
        'database. In other words, load facts for the 100,000<br>' +
        'selected customers only.<br>' +
        'Create a Project and a Data Source View<br>' +
        'Finally, you’re ready to use the tools to get started on the<br>' +
        'design process. Create an Analysis Services project in BI<br>' +
        'Studio. By default, BI Studio points to the default instance<br>' +
        'on the local server — the developer’s desktop. To specify<br>' +
        'a different Analysis Services instance or server, right-click<br>' +
        'the project name in the Solution Explorer, and choose<br>' +
        'Properties. As illustrated in Figure 8-2, you can specify<br>' +
        'several properties of the development and deployment<br>' +
        'servers.<br>' +
        'NOTE<br>' +
        'Most DW/BI teams will use SQL Server<br>' +
        'Developer Edition, which contains all the<br>' +
        'functionality of the Data Center and<br>' +
        'Enterprise Editions. If you’re using<br>' +
        'Standard Edition in production, change the<br>' +
        'Deployment Server Edition property of the<br>' +
        'project to Standard. That way, you’ll get<br>' +
        'warnings if you attempt to use functionality<br>' +
        'that’s not available in Standard Edition.<br>' +
        'Figure 8-2: Choose the Deployment Server Edition<br>' +
        '486<br>' +
        'The next step in designing the Analysis Services database<br>' +
        'is to create a Data Source View (DSV) on the relational<br>' +
        'data warehouse database. The Analysis Services database<br>' +
        'is built from the Data Source View, so you must create a<br>' +
        'DSV before designing the database.<br>' +
        'The DSV contains the tables that will be included in your<br>' +
        'cube. As its name implies, it is a view of your data source<br>' +
        '— in this case, the relational data warehouse database. The<br>' +
        'main activities that you will perform in the DSV are:<br>' +
        '• Identify the subset of dimension and fact tables that will be<br>' +
        'included in the cube.<br>' +
        '• Identify the primary key in dimension and fact tables.<br>' +
        '• Specify foreign key relationships between fact and<br>' +
        'dimension tables, and between multiple dimension tables.<br>' +
        'These usually show up correctly if you build the DSV<br>' +
        'directly from tables, but you have to add them by hand if<br>' +
        'you build the DSV from views. This is a task that takes 20<br>' +
        'seconds, so it’s not a good enough reason to avoid using<br>' +
        'relational views.<br>' +
        '487<br>' +
        '• Rename the tables and columns, although the relational view<br>' +
        'definition is a better place to do this. Note that dimension<br>' +
        'and attribute naming should take place during the<br>' +
        'dimensional modeling step with the active involvement of<br>' +
        'key users. In general, those names should be used in the<br>' +
        'relational database and here in SSAS.<br>' +
        '• Add computed columns, although the relational view<br>' +
        'definition is a better place to do this. The kinds of computed<br>' +
        'columns you can add in the DSV are exactly the same kinds<br>' +
        'of things you can do in the relational view definition. This is<br>' +
        'not the same thing as an Analysis Services computed<br>' +
        'measure, which we discuss later in this chapter.<br>' +
        'NOTE<br>' +
        'There’s an option on the first screen of the<br>' +
        'Data Source Wizard to create a data source<br>' +
        'based on another object. Use this option to<br>' +
        'pick up the data source from another<br>' +
        'project rather than creating a new one.<br>' +
        'Figure 8-3 illustrates the DSV for the MDWT_2008R2<br>' +
        'database in the DSV Designer. Use the list of tables on the<br>' +
        'left-hand side to quickly find a table.<br>' +
        'You must construct the relationships correctly in the DSV<br>' +
        'because these relationships help define the Analysis<br>' +
        'Services database. A relationship between a fact and<br>' +
        'dimension table appears as an arrow, as you can see in<br>' +
        'Figure 8-3. Make sure you set the relationship in the<br>' +
        'correct direction.<br>' +
        'Figure 8-3: A Data Source View<br>' +
        '488<br>' +
        'WARNING<br>' +
        'Avoid using the DSV to cobble together an<br>' +
        'Analysis Services database from a variety<br>' +
        'of heterogeneous sources. First, unless<br>' +
        'you’ve scrubbed all the data, it’s not going<br>' +
        'to join well at all. Second, there’s a<br>' +
        'considerable performance cost to joining<br>' +
        'remotely. You’re almost certainly better off<br>' +
        'cleaning and aligning the data and hosting<br>' +
        'it on a single physical server before<br>' +
        'building your cube. Yes: You’re better off<br>' +
        'building a data warehouse.<br>' +
        '489<br>' +
        'However, if you’re careful and judicious,<br>' +
        'you can use this distributed sources feature<br>' +
        'to solve some knotty problems, especially<br>' +
        'for a prototype. A distributed DSV uses the<br>' +
        'SQL Server engine under the covers to<br>' +
        'resolve the distributed query. At least one<br>' +
        'SQL Server data source must be defined in<br>' +
        'order to create a distributed DSV.<br>' +
        'Dimension Designs<br>' +
        'The database that you design and build in Analysis<br>' +
        'Services will be very similar to the relational data<br>' +
        'warehouse that you’ve built using the Kimball Method.<br>' +
        'This section describes how Analysis Services handles<br>' +
        'dimensions in general, and how it handles several of the<br>' +
        'dimension design issues we discussed in Chapter 2.<br>' +
        'After we’ve talked about how OLAP dimensions<br>' +
        'correspond to the dimension tables in your relational data<br>' +
        'warehouse, we describe the actual process of creating and<br>' +
        'editing those dimensions.<br>' +
        'Standard Dimensions<br>' +
        'A standard dimension contains a surrogate key, one or<br>' +
        'more attributes (columns), and usually one or more<br>' +
        'multilevel hierarchies. Analysis Services can build<br>' +
        'dimensions from dimension tables that:<br>' +
        '490<br>' +
        '• Are denormalized into a flat table structure, as the Kimball<br>' +
        'Method recommends<br>' +
        '• Are normalized into a snowflake structure, with separate<br>' +
        'tables for each hierarchical level<br>' +
        '• Include a mixture of normalized and denormalized structures<br>' +
        '• Include a parent-child hierarchy<br>' +
        '• Have all type 1 (update history) attributes, type 2 (track<br>' +
        'history) attributes, or a combination<br>' +
        'Even though Analysis Services doesn’t require a surrogate<br>' +
        'key, recall that surrogate keys are a cornerstone of the<br>' +
        'Kimball Method — build a permanent, enterprise Analysis<br>' +
        'Services database only from a dimensional relational<br>' +
        'source with surrogate keys.<br>' +
        'Your business users will tell you whether dimension<br>' +
        'attributes should be tracked as type 1 or type 2. We’ve<br>' +
        'found that most dimension attributes are tracked as type 2.<br>' +
        'In Chapter 6 we talked extensively about how to set up<br>' +
        'your ETL process to manage the propagation and<br>' +
        'assignment of surrogate keys for type 2 dimension<br>' +
        'attributes. Although type 2 dimensions seem harder to<br>' +
        'manage in the ETL system, the payoff comes with your<br>' +
        'SSAS database. Analysis Services handles type 2 attribute<br>' +
        'changes gracefully. The larger your data volumes, the<br>' +
        'more you should be relying on type 2 changes, where they<br>' +
        'make sense for the business user.<br>' +
        'Type 1 attributes can cause some problems for OLAP<br>' +
        'databases for the same reason they’re troubling in the<br>' +
        'relational world: precomputed aggregations. When a<br>' +
        'dimension attribute is updated in place, any aggregations<br>' +
        'built on that attribute are invalidated. If you were<br>' +
        'managing aggregations in the relational database, you’d<br>' +
        '491<br>' +
        'have to write logic to fix the historical summary tables<br>' +
        'when an attribute changes. Analysis Services faces the<br>' +
        'same problem. You don’t have to perform any action to fix<br>' +
        'up the historical attributes — SSAS does that for you. But<br>' +
        'this is an expensive operation. If you have a lot of type 1<br>' +
        'changes in a very large database, you’re going to be<br>' +
        'unpleasantly surprised by the incremental processing<br>' +
        'performance, as the system responds to your changes and<br>' +
        'rebuilds its aggregations.<br>' +
        'Variable Depth or Parent-Child Hierarchies<br>' +
        'Variable depth or parent-child hierarchies are a useful way<br>' +
        'of expressing organization hierarchies and bills of<br>' +
        'materials. However, these dimension structures are<br>' +
        'difficult to maintain in the relational data warehouse. This<br>' +
        'is especially true if the dimension has type 2 attributes.<br>' +
        'If you can maintain your variable depth hierarchy in the<br>' +
        'relational database as either a type 1 or type 2 dimension,<br>' +
        'then Analysis Services can consume it. It’s straightforward<br>' +
        'to set up a Parent-Child dimension in Analysis Services.<br>' +
        'It’s straightforward to query that dimension — far more so<br>' +
        'in Analysis Services, by the way, than using standard SQL.<br>' +
        'NOTE<br>' +
        'Parent-Child hierarchies are a valuable<br>' +
        'feature in Analysis Services. However, it’s<br>' +
        'not a trivial exercise to get them to perform<br>' +
        '492<br>' +
        'well in a very large database or with many<br>' +
        'members in the Parent-Child dimension. If<br>' +
        'your system is large, you may want to<br>' +
        'bring in an expert or launch phase 1 using a<br>' +
        'standard dimension. Add the variable depth<br>' +
        'relationship when your team has developed<br>' +
        'more expertise.<br>' +
        'We usually try to avoid implementing a<br>' +
        'variable depth hierarchy in our dimensional<br>' +
        'models. Sometimes it’s possible to<br>' +
        'shoehorn the variable depth hierarchy into<br>' +
        'a standard hierarchy of fixed levels, and<br>' +
        'that’s our preferred approach. If that’s just<br>' +
        'not possible, the SSAS Parent-Child<br>' +
        'hierarchy is the best way to store the<br>' +
        'information for consumption by the users.<br>' +
        'When you build a Parent-Child hierarchy in Analysis<br>' +
        'Services, you don’t need a bridge table, although it can<br>' +
        'improve performance for large hierarchies. You may have<br>' +
        'built a bridge table to facilitate relational queries, as<br>' +
        'described in Chapter 2. If so, you should eliminate that<br>' +
        'bridge table from the DSV for the SSAS database.<br>' +
        'Multivalued or Many-to-Many Dimensions<br>' +
        'A multivalued dimension has a many-to-many relationship<br>' +
        'between a dimension and the fact table, or between two<br>' +
        'dimension tables. Each row in the fact table connects to<br>' +
        'potentially many rows in the dimension table. Common<br>' +
        '493<br>' +
        'examples are patient hospital visits and diagnoses (one<br>' +
        'patient visit can have multiple diagnoses), and purchase<br>' +
        'reasons (a customer can have multiple reasons for<br>' +
        'purchasing your product).<br>' +
        'The relational design for a multivalued dimension includes<br>' +
        'a bridge table between the fact and dimension tables. This<br>' +
        'bridge table identifies a group of dimension values that<br>' +
        'occur together. The example in the Adventure Works<br>' +
        'Cycles case study is the sales reason. Adventure Works<br>' +
        'Cycles collects multiple possible reasons for a customer<br>' +
        'purchase, so each sale is associated with potentially many<br>' +
        'reasons. The bridge table lets you keep one row in the fact<br>' +
        'table for each sale, and relates that fact event to the<br>' +
        'multiple reasons.<br>' +
        'This same structure serves to populate the Analysis<br>' +
        'Services database. The only nuance is that the bridge table<br>' +
        'is used as both a dimension table and a fact table, or as just<br>' +
        'a fact table, within Analysis Services. This seems odd at<br>' +
        'first, but it really is correct. The fact the bridge table tracks<br>' +
        'is the relationship between sales and sales reasons — you<br>' +
        'can think of it as the Reasons Selected fact table.<br>' +
        'Keep the bridge table from your relational data warehouse<br>' +
        'in your DSV. The designer wizards correctly identify the<br>' +
        'structure as a multivalued dimension, which Analysis<br>' +
        'Services calls a many-to-many dimension.<br>' +
        'Role-Playing Dimensions<br>' +
        'Often a dimension plays multiple roles within the context<br>' +
        'of a fact table. For example, you may track several roles of<br>' +
        '494<br>' +
        'an employee dimension, such as the sales executive and<br>' +
        'the salesperson for an account. It’s quite common for the<br>' +
        'date dimension to play multiple roles in a fact table.<br>' +
        'There are two ways to implement role-playing dimensions<br>' +
        'in Analysis Services, and neither is entirely satisfactory:<br>' +
        '• Create one relational view corresponding to the physical<br>' +
        'dimension table, include that one view in the DSV, and<br>' +
        'create one physical dimension in Analysis Services. Use the<br>' +
        'Analysis Services role-playing dimension feature to use that<br>' +
        'single dimension many times.<br>' +
        '• Advantage: The resulting cube is more efficient,<br>' +
        'because only one physical dimension is created.<br>' +
        '• Disadvantage: Analysis Services does not let you<br>' +
        'rename columns for each role. You rename the<br>' +
        'entire dimension, but not each attribute. This may<br>' +
        'be acceptable when a user is constructing an<br>' +
        'analysis, because that user knows which<br>' +
        'dimension she’s working with. But it presents<br>' +
        'serious challenges to other people looking at the<br>' +
        'same analysis or report, because the context of the<br>' +
        'dimension role is not always obvious. If you take<br>' +
        'this approach, you will need to train users to label<br>' +
        'their reports thoroughly (good luck with that).<br>' +
        '• Create one relational view for each role, include those<br>' +
        'multiple views in the DSV, and create multiple Analysis<br>' +
        'Services dimensions.<br>' +
        '• Advantage: You can uniquely name each attribute<br>' +
        'in each role.<br>' +
        '• Disadvantages: This approach uses more system<br>' +
        'resources, which can be a significant performance<br>' +
        'drain in a complex system. Also, it substantially<br>' +
        'increases development complexity as there’s no<br>' +
        'good way to keep the dimension definitions in<br>' +
        'sync.<br>' +
        '495<br>' +
        'Despite the disadvantages of the second approach, we are<br>' +
        'inclined to recommend it as the default strategy. Having<br>' +
        'two objects with the same name (such as currency name<br>' +
        'rather than local currency name and standard currency<br>' +
        'name) violates a fundamental principle of good<br>' +
        'dimensional design.<br>' +
        'However, the Analysis Services role-playing feature<br>' +
        'should be used for the dimension that most often plays<br>' +
        'multiple roles: date. That’s because you should define one<br>' +
        'and only one dimension of type Time. Having an officially<br>' +
        'designated Time dimension lets you use functions like<br>' +
        'YearToDate in calculations, and create measures that are<br>' +
        'semi-additive across time. Later in this chapter, we<br>' +
        'describe how to designate the date dimension as type<br>' +
        'Time.<br>' +
        'It’s a judgment call whether to treat the date dimension<br>' +
        'one way and other role-playing dimensions another, or to<br>' +
        'handle all dimensions consistently by using the SSAS<br>' +
        'role-playing feature. We dislike both alternatives.<br>' +
        'Creating and Editing Dimensions<br>' +
        'The process of creating a new dimension consists of two<br>' +
        'steps: run the Dimension Wizard and then edit the results.<br>' +
        'Create one dimension at a time, refining and improving it<br>' +
        'until you’re satisfied. Remember, dimensions are the<br>' +
        'windows into the data. They are a key element of the<br>' +
        'user’s experience, so you need to make them as clean and<br>' +
        'well formed as possible.<br>' +
        '496<br>' +
        'As you run through the Dimension Wizard, you’re asked<br>' +
        'to identify the key of the dimension. You should identify<br>' +
        'the surrogate key as the key column, which SSAS does for<br>' +
        'you automatically. However, by definition the surrogate<br>' +
        'key is anonymous and is not at all useful to business users.<br>' +
        'In the Dimension Wizard, always change the surrogate key<br>' +
        'name column to something meaningful to business users,<br>' +
        'like the date as illustrated in Figure 8-4, or an account<br>' +
        'number (perhaps combined with customer name) for a<br>' +
        'customer dimension. The data warehouse surrogate key<br>' +
        'becomes the physical key in the SSAS dimension, but the<br>' +
        'user never sees it.<br>' +
        'Running the Cube Wizard: The One-Click<br>' +
        'Cube<br>' +
        'In most materials about Analysis Services,<br>' +
        'including Books Online and the tutorials, you’re<br>' +
        'encouraged to start building your OLAP database<br>' +
        'by running the Cube Wizard after you’ve defined<br>' +
        'the DSV. The Cube Wizard reads the relationships<br>' +
        'you’ve defined in your Data Source View, and<br>' +
        'automatically generates the definition for the entire<br>' +
        'OLAP database, including all dimensions.<br>' +
        'We’ve found that most people will take this route<br>' +
        'two or three times. It certainly makes a fun demo.<br>' +
        'But if you’re trying to get work done, you’ll<br>' +
        'probably find this alternative process more<br>' +
        'productive. Create dimensions one at a time, as we<br>' +
        '497<br>' +
        'describe in the next section. Work on each<br>' +
        'dimension, setting its attributes and properties<br>' +
        'correctly, processing and browsing it until you like<br>' +
        'the way it looks. Move down the list of dimensions<br>' +
        'until you’ve defined all the dimensions associated<br>' +
        'with a fact table. Only then should you run the<br>' +
        'Cube Wizard.<br>' +
        'Figure 8-4: Defining the key in the Dimension Wizard<br>' +
        '498<br>' +
        'The next step of the Dimension Wizard is to identify the<br>' +
        'attributes that you want to include in the SSAS dimension.<br>' +
        'It’s tempting to choose to include all attributes, but there<br>' +
        'are costs:<br>' +
        '• Users have to locate the attributes they really want.<br>' +
        '• Each attribute — especially those attributes that are available<br>' +
        'for users to slice and dice during analysis — uses valuable<br>' +
        'system resources.<br>' +
        '499<br>' +
        'WARNING<br>' +
        'Small databases, say less than 100 GB, can<br>' +
        'often be casual in their definition of the<br>' +
        'cube’s dimensions and measures. The<br>' +
        'defaults are good enough for a wide range<br>' +
        'of scenarios. However, as the demands of<br>' +
        'your system increase — including more<br>' +
        'data, more complexity, and more users —<br>' +
        'you need to be increasingly careful of<br>' +
        'decisions that can impact performance.<br>' +
        'Pruning the list of browsable attributes is<br>' +
        'one of those decision points.<br>' +
        'As you can see in Figure 8-5, for each column in the<br>' +
        'dimension table you have three choices in the Dimension<br>' +
        'Wizard:<br>' +
        '• Make the column into a completely flexible attribute, which<br>' +
        'can be included in multilevel hierarchies, dragged, dropped,<br>' +
        'used in filters, and so on. Activate this option by checking<br>' +
        '“Enable browsing” for the column in the Dimension Wizard,<br>' +
        'as shown in Figure 8-5.<br>' +
        '• Include the column as a non-drillable attribute. Users can<br>' +
        'display this non-browsable attribute in analyses, but it’s not<br>' +
        'something they can use to pivot, slice, or dice. There are<br>' +
        'additional options for this type of attribute, which we discuss<br>' +
        'shortly.<br>' +
        '• Exclude the column from the SSAS dimension.<br>' +
        'Any of the choices you make in the Dimension Wizard can<br>' +
        'be changed later in the Dimension Designer.<br>' +
        '500<br>' +
        'Figure 8-5: Choosing attributes in the Dimension Wizard<br>' +
        'As you exit the Dimension Wizard, BI Studio generates<br>' +
        'the metadata for the dimension and leaves you looking at<br>' +
        'the basic dimension structure in the Dimension Designer.<br>' +
        'Next, edit your dimensions in the Dimension Designer,<br>' +
        'getting each dimension the way you like it before moving<br>' +
        'on to the next one. At this point in the process, you have<br>' +
        'only metadata — the definition of the dimension. Later we<br>' +
        '501<br>' +
        'describe how you build, deploy, and process the dimension<br>' +
        'so you can actually look at the dimension’s data.<br>' +
        'Figure 8-6 shows the final version of the date dimension.<br>' +
        'In the rightmost pane of the Dimension Designer is a<br>' +
        'representation of the tables or views that underlie the<br>' +
        'dimension. This pane is linked to the Data Source View; if<br>' +
        'you try to edit the entities in this pane, you’re flipped over<br>' +
        'to the DSV Editor. When you edit the DSV and return to<br>' +
        'the Dimension Designer, the changes follow.<br>' +
        'Figure 8-6: The Dimension Designer<br>' +
        'The left-hand pane of the Dimension Designer is the<br>' +
        'Attributes pane, and shows the list of attributes within the<br>' +
        'dimension. Within the Attributes pane, you can rename<br>' +
        'and delete attributes, and change the properties of those<br>' +
        '502<br>' +
        'attributes. To create a new attribute, drag a column from<br>' +
        'the data source in the DSV. You may need to create a<br>' +
        'calculated column in the DSV.<br>' +
        'In the central pane of Figure 8-6 is the view of Hierarchies.<br>' +
        'Create a new hierarchy by dragging an attribute into the<br>' +
        'background area. Add levels to hierarchies by dragging<br>' +
        'and dropping.<br>' +
        'The final pane in Figure 8-6 is the Properties pane, which<br>' +
        'is sometimes docked below the Solution Explorer. We set<br>' +
        'it to float in order to maximize screen real estate. Some of<br>' +
        'these properties are very important; others you might never<br>' +
        'change.<br>' +
        'When you’re editing a new dimension, take the following<br>' +
        'steps:<br>' +
        '• Edit the name and other properties of the dimension.<br>' +
        '• Edit the names and other properties of each attribute in the<br>' +
        'dimension.<br>' +
        '• Create hierarchies.<br>' +
        '• Create attribute relationships.<br>' +
        '• If necessary, define dimension translations.<br>' +
        '• Build, deploy, and process the dimension so you can look at<br>' +
        'the dimension’s data.<br>' +
        '• Iterate, iterate, iterate.<br>' +
        'The order of these steps isn’t very important, but it helps to<br>' +
        'have a checklist. As you can see by reviewing the<br>' +
        'preceding list, the process of editing the dimension is<br>' +
        'largely a process of editing object properties. There are<br>' +
        'only a few other design tasks, like creating attribute<br>' +
        'relationships and creating hierarchies.<br>' +
        '503<br>' +
        'In the next few sections, we’ll run through the properties<br>' +
        'of dimensions, attributes, hierarchies, and levels. We’ll<br>' +
        'talk only about the most important properties that you can<br>' +
        'change; see Books Online for the details. In all cases, the<br>' +
        'Properties pane shows the properties for the highlighted<br>' +
        'item. Properties that you’ve changed from the default<br>' +
        'values are highlighted in boldface.<br>' +
        'Editing Dimension Properties<br>' +
        'A dimension has several editable properties. The most<br>' +
        'important during your initial development phase are:<br>' +
        '• Name: You’ve had many opportunities to get the dimension<br>' +
        'name right. This is your last chance.<br>' +
        '• Description: Most query tools for Analysis Services have a<br>' +
        'way of showing the description to the user, usually as a tool<br>' +
        'tip. Populate the Description metadata from the information<br>' +
        'you captured as part of the dimensional modeling process<br>' +
        'step in Chapter 2.<br>' +
        'DOWNLOADS<br>' +
        'There is no way to directly connect<br>' +
        'descriptions held in the relational database<br>' +
        'to the Description property in SSAS<br>' +
        'dimensions and attributes. SSAS orphans<br>' +
        'the descriptions collected in the Excel<br>' +
        'spreadsheet that we introduced in Chapter<br>' +
        '2. However, there is an extremely useful<br>' +
        'tool called BIDS Helper than you can<br>' +
        'download from Codeplex<br>' +
        '504<br>' +
        '(http://bidshelper.codeplex.com). BIDS Helper<br>' +
        'extends the functionality of BI Studio<br>' +
        '(BIDS) with several dozen features. One of<br>' +
        'those features lets you copy descriptions<br>' +
        'held in SQL Server table and column<br>' +
        'extended properties, into dimension and<br>' +
        'attribute descriptions.<br>' +
        '• AttributeAllMemberName: This label will show up to users<br>' +
        'when they start to drill down in the dimension and look at the<br>' +
        'highest level member of the dimension. The default name is<br>' +
        '“All.”<br>' +
        '• ErrorConfiguration: Analysis Services provides a lot of<br>' +
        'different options for handling problems with the dimension data,<br>' +
        'such as duplicate keys and referential integrity violations. You<br>' +
        'can see the options if you set ErrorConfiguration to (custom). If<br>' +
        'you’re sourcing your dimension from a solid Kimball Method<br>' +
        'relational dimension table, as we’ve described in great detail in<br>' +
        'this book, you shouldn’t have to worry about this property. You<br>' +
        'shouldn’t have bad dimension data; shame on you if you do.<br>' +
        'NOTE<br>' +
        'Though you shouldn’t have duplicate<br>' +
        'surrogate keys — or bad data anywhere in<br>' +
        'your dimension — it’s not unheard of<br>' +
        'between levels of a dimension hierarchy.<br>' +
        'For example, Postal Code should roll up to<br>' +
        'State, but in the source data there may be a<br>' +
        'handful of violations. If your ETL process<br>' +
        'doesn’t catch and fix these violations, it<br>' +
        '505<br>' +
        'should be modified to do so. If you change<br>' +
        'the ErrorConfiguration from its default to<br>' +
        'custom, and change the KeyDuplicate<br>' +
        'property from IgnoreError to<br>' +
        'ReportAndContinue or ReportAndStop,<br>' +
        'you’ll enlist Analysis Services’ help in<br>' +
        'figuring out where your data problems are.<br>' +
        'We usually turn on at least this level of<br>' +
        'error checking on dimensions.<br>' +
        'RESOURCES<br>' +
        'See the Books Online topic “Database<br>' +
        'Dimension Properties” for a complete list<br>' +
        'and extremely brief descriptions of all the<br>' +
        'dimension properties.<br>' +
        'Editing Attribute Properties<br>' +
        'As with the dimension properties, most of the important<br>' +
        'properties of an attribute are set correctly by the<br>' +
        'Dimension Wizard. A few useful properties aren’t set by<br>' +
        'the wizards. And if you add attributes within the<br>' +
        'Dimension Designer, you need to be aware of how to<br>' +
        'define these important attributes.<br>' +
        '• Name and Description: Ensure these properties are set<br>' +
        'correctly, as we discussed earlier for the dimension.<br>' +
        '506<br>' +
        '• Usage: This property is usually set correctly by the<br>' +
        'Dimension Wizard. One attribute for the dimension can and<br>' +
        'should have its usage set as Key. This is, obviously, the<br>' +
        'surrogate key for the dimension. Almost always, the other<br>' +
        'attributes are correctly set as Regular. The exception is the<br>' +
        'parent key for a Parent-Child dimension (set to Parent).<br>' +
        '• NameColumn: You have an opportunity to set the<br>' +
        'NameColumn for the Key attribute in the Dimension<br>' +
        'Wizard. If you forgot to set it there, set it here. Sometimes,<br>' +
        'you set the NameColumn for other attributes of a dimension,<br>' +
        'especially levels that you will make into a hierarchy. The<br>' +
        'Key for each attribute must be unique, but the NameColumn<br>' +
        'doesn’t have to be. If you don’t explicitly supply a different<br>' +
        'NameColumn, SSAS uses the Key as the name.<br>' +
        '• OrderBy: You can set the default sort order for each attribute<br>' +
        'to its Key or its NameColumn. You can even define one<br>' +
        'attribute to be sorted by another attribute, as long as you’ve<br>' +
        'already defined an attribute relationship between the two<br>' +
        'attributes. Later in this chapter, we describe how to define<br>' +
        'relationships between attributes.<br>' +
        '• AttributeHierarchyEnabled: This property can be set to True<br>' +
        'or False. If it’s set to False, the user can include the attribute<br>' +
        'in a query or report, but usually cannot pivot, slice, or dice<br>' +
        'on it. You can substantially improve the performance of your<br>' +
        'cube by setting many attributes to False. Consider setting<br>' +
        'AttributeHierarchyEnabled to False for any attribute, such as<br>' +
        'an address line or product description, that’s not constrained<br>' +
        'to a domain.<br>' +
        '• AttributeHierarchyDisplayFolder: You can add a lot of<br>' +
        'value to your Analysis Services database by adding display<br>' +
        'folders for the Attribute Hierarchies. If you have more than<br>' +
        '10 to 12 attributes in your dimension, you should create<br>' +
        'multiple display folders and assign attributes to them.<br>' +
        'Creating a display folder is as simple as typing a name into<br>' +
        'the attribute’s AttributeHierarchyDisplayFolder property.<br>' +
        '507<br>' +
        'NOTE<br>' +
        'The usefulness of the display folder<br>' +
        'depends on the client tool you’re using to<br>' +
        'access the Analysis Services database. The<br>' +
        'display folder shows up in a Report Builder<br>' +
        'Report Model that you build atop the SSAS<br>' +
        'database, and in Excel. Using display<br>' +
        'folders substantially improves the user<br>' +
        'experience.<br>' +
        'RESOURCES<br>' +
        'See the Books Online topic “Defining<br>' +
        'Dimensional Attributes” for a complete list<br>' +
        'of attribute properties, and a terse<br>' +
        'description of each property. There are<br>' +
        'many more attribute properties than we’ve<br>' +
        'listed here.<br>' +
        'Time and Account Dimensions<br>' +
        'You may have noticed the Type property of a dimension or<br>' +
        'attribute. It’s a frustratingly vague name for a property, but<br>' +
        'it’s hard to think of a better name. Type is a classification<br>' +
        'of the dimension that’s used by Analysis Services only in a<br>' +
        'few special cases (Time and Account). Most of the time, if<br>' +
        'you set the dimension type to anything other than Time or<br>' +
        'Account, nothing happens.<br>' +
        '508<br>' +
        'The best way to set the types correctly is to run the Add<br>' +
        'Business Intelligence Wizard. To set up Time intelligence,<br>' +
        'the wizard has you identify the core date dimension, and<br>' +
        'then specify which attribute refers to year, quarter, month,<br>' +
        'and so on. The advantage of going through this step is that<br>' +
        'you’ll be able to use built-in MDX functions like<br>' +
        'YearToDate and PriorPeriod. If you have multiple<br>' +
        'physical date dimensions, run through the wizard for only<br>' +
        'one of them.<br>' +
        'If your cube includes financial data, such as in a chart of<br>' +
        'accounts, the Add Business Intelligence Wizard will let<br>' +
        'you identify accounts as assets, liabilities, and so on.<br>' +
        'There are about a dozen other kinds of dimension types<br>' +
        'that you can add, but Time and Account are the only ones<br>' +
        'that do anything within Analysis Services.<br>' +
        'NOTE<br>' +
        'If you’re a software developer building a<br>' +
        'packaged application on Analysis Services,<br>' +
        'you can use the dimension Type property<br>' +
        'to identify dimensions that are important to<br>' +
        'your application. You can’t rely on the<br>' +
        'name because users can name the<br>' +
        'dimension whatever they want.<br>' +
        'Creating Hierarchies<br>' +
        '509<br>' +
        'A hierarchy is a metadata relationship that defines standard<br>' +
        'drill paths in the data. Classic examples of hierarchies<br>' +
        'include calendar (day, month, year); geography (city, state,<br>' +
        'country, region); and product rollups. The dimension<br>' +
        'design should include hierarchies where appropriate,<br>' +
        'because a hierarchy will provide:<br>' +
        '• User navigation of the dimension. A hierarchy is essentially<br>' +
        'a paved road for user navigation to drill up and down the<br>' +
        'data.<br>' +
        '• Complex calculations. A hierarchy provides a framework for<br>' +
        'many interesting calculations and analyses, such as sales<br>' +
        'contribution to parent.<br>' +
        '• Improved query performance. As we discuss in this section,<br>' +
        'SSAS leverages hierarchies to provide substantial query<br>' +
        'performance gains, by storing precomputed aggregations.<br>' +
        '• A framework for security. It’s common to define a security<br>' +
        'role that limits a user group to one branch of a hierarchy,<br>' +
        'such as the western region. Security is discussed in Chapter<br>' +
        '14.<br>' +
        'Creating multilevel hierarchies is incredibly easy. Just drag<br>' +
        'and drop attributes from the Attribute pane to the<br>' +
        'Hierarchies pane. You can create many hierarchies, or<br>' +
        'none at all.<br>' +
        'When you first create a new hierarchy such as the standard<br>' +
        'Date-Month-Quarter-Year calendar hierarchy as shown<br>' +
        'back in Figure 8-6, you’ll see a warning sign on the<br>' +
        'hierarchy. What this exclamation point is telling you is that<br>' +
        'you have not defined attribute relationships between the<br>' +
        'levels of this hierarchy. We’ll describe how to do that in<br>' +
        'the next section. Here we’ll discuss when and why you<br>' +
        'need to worry about attribute relationships for hierarchies.<br>' +
        '510<br>' +
        'The hierarchy just defined, even with the exclamation<br>' +
        'point warning in place, is a valid hierarchy. Users can start<br>' +
        'at year and drill down to quarter, month, and day. Whether<br>' +
        'or not you define attribute relationships on the hierarchy<br>' +
        'doesn’t affect the user experience of browsing the<br>' +
        'dimension.<br>' +
        'What’s missing is an assurance to Analysis Services that a<br>' +
        'month rolls up to one and only one quarter, and a quarter<br>' +
        'to one and only one year. That sounds kind of stupid when<br>' +
        'we’re talking about the date dimension, but it’s a very<br>' +
        'good example.<br>' +
        'What if your quarter attribute took the values Q1, Q2, Q3<br>' +
        'and month took the values Jan, Feb, Mar. This won’t<br>' +
        'work! A month (Jan) always rolls up to Q1, but it rolls up<br>' +
        'to many years. You can’t define the levels that way. Our<br>' +
        'sample date dimension includes attributes YearQtr<br>' +
        '(2010Q1) and YearMonth (2010-01). You have two<br>' +
        'options:<br>' +
        '• Build key columns for hierarchical levels into your relational<br>' +
        'date dimension table or view definition. This is our<br>' +
        'recommended solution.<br>' +
        '• Use concatenated keys for the hierarchical levels. In other<br>' +
        'words, the key of the quarter attribute is actually year and<br>' +
        'quarter. Set this up in the Key Columns property of the<br>' +
        'attribute.<br>' +
        'If a hierarchy really does have referential integrity in<br>' +
        'place, you should declare the attribute relationships. Once<br>' +
        'you do so, that warning will go away. There are two<br>' +
        'reasons why it’s important to declare attribute relationships<br>' +
        '(where they exist) between levels of a hierarchy:<br>' +
        '511<br>' +
        '• Analysis Services will build intermediate performance<br>' +
        'aggregations only within hierarchies with attribute<br>' +
        'relationships defined. Remember that performance<br>' +
        'aggregations are a cornerstone to good query performance.<br>' +
        '• As your system grows to include fact tables at different<br>' +
        'levels of granularity, they connect correctly only if the<br>' +
        'attribute relationships are in place. For example, you have<br>' +
        'detailed transaction data at the daily level and want to hook<br>' +
        'it to planning data at the quarterly level. Analysis Services<br>' +
        'needs your guarantee that daily data rolls cleanly to monthly<br>' +
        'and quarterly.<br>' +
        'The warning about not having attribute relationships in<br>' +
        'place is just that: a warning. You can process, browse, and<br>' +
        'deploy a hierarchy without attribute relationships defined.<br>' +
        'In some cases it makes sense to do so: if users for<br>' +
        'whatever reason want to drill down in a product dimension<br>' +
        'from color to size to manufacturing location, you can make<br>' +
        'that easy. Analysis Services still calls this a hierarchy, and<br>' +
        'no analytic tool’s UI will distinguish between it and a<br>' +
        'clean, structured hierarchy.<br>' +
        'There are a handful of little details to clean up in the new<br>' +
        'hierarchy:<br>' +
        '• Fix the name: The default name generated by SSAS is quite<br>' +
        'long.<br>' +
        '• Add a description: As usual.<br>' +
        '• Hide the non-hierarchical attributes: When you create a<br>' +
        'multilevel hierarchy, the hierarchy’s levels are still available<br>' +
        'as standalone attributes. In most cases you want to hide the<br>' +
        'standalone attribute, and force users to navigate through the<br>' +
        'hierarchy. Do this by setting the attribute’s<br>' +
        'AttributeHierarchyVisible property to False.<br>' +
        '• Set the display order for multiple hierarchies: A subtlety of<br>' +
        'the Dimension Designer interface is that the order in which<br>' +
        'the hierarchies are displayed in the hierarchy pane is the<br>' +
        '512<br>' +
        'same order in which they’ll appear in most query tools’ user<br>' +
        'interfaces. You can drag and drop the hierarchies to re-order<br>' +
        'them. This isn’t an official property that you can edit, except<br>' +
        'by re-ordering the hierarchies.<br>' +
        'RESOURCES<br>' +
        'See the Books Online topic “User<br>' +
        'Hierarchy Properties” for a complete list of<br>' +
        'the properties of a multilevel hierarchy.<br>' +
        'See the Books Online topic “Level<br>' +
        'Properties” for a complete list of the<br>' +
        'properties of hierarchy levels.<br>' +
        'Set Up Attribute Relationships<br>' +
        'We’ve already described the most important scenario for<br>' +
        'which you must set up attribute relationships: declaring<br>' +
        'referential integrity between levels of a multilevel<br>' +
        'hierarchy. Attribute relationships are also used to sort one<br>' +
        'column by another, for example month name by month<br>' +
        'number.<br>' +
        'A careful dimension designer will think about attribute<br>' +
        'relationships for all attributes, not just those that define the<br>' +
        'level in a hierarchy or need a sort order. Think about<br>' +
        'shrunken subset dimensions — the piece of a dimension<br>' +
        'that’s associated with fact tables at a higher grain, such as<br>' +
        'quarterly forecasts by brand. In Analysis Services, you<br>' +
        'don’t need to create separate shrunken subset dimensions.<br>' +
        'They come “for free” when you bring in a fact table and<br>' +
        '513<br>' +
        'associate it with a dimension at the appropriate grain. But<br>' +
        'you do need to tell SSAS which dimension attributes are<br>' +
        'available at a higher grain. For example, a product<br>' +
        'dimension has some attributes that are rightfully associated<br>' +
        'with the brand, such as brand manager. If you accept the<br>' +
        'default, which is to associate all attributes with the<br>' +
        'dimension key (the product SKU), then queries of<br>' +
        'quarterly sales forecasts by brand won’t have access to the<br>' +
        'name of the brand manager.<br>' +
        'All of these relationships should have been documented in<br>' +
        'your initial dimensional model design. Dig back through<br>' +
        'your documentation and spreadsheets, and take a few<br>' +
        'minutes to set up the attribute relationships correctly and<br>' +
        'completely.<br>' +
        'It’s easy to set up attribute relationships in Analysis<br>' +
        'Services 2008. Go to the Attribute Relationships tab of the<br>' +
        'Dimension Designer, as illustrated in Figure 8-7. The first<br>' +
        'time you go to the Attribute Relationships tab, Analysis<br>' +
        'Services provides a hint of where you need to fix<br>' +
        'relationships. All attributes that are used in multilevel<br>' +
        'hierarchies have been dragged up to the yellow design<br>' +
        'surface. Your job is to drag and drop (from left to right or<br>' +
        'detailed to summary), to fix up the relationships.<br>' +
        'Figure 8-7: Create attribute relationships<br>' +
        '514<br>' +
        'Looking closely at 8-7, you can see that there are six<br>' +
        'attributes correctly associated with the date: if you know<br>' +
        'the date, you know the day of the month. There are three<br>' +
        'multilevel hierarchies: calendar year, quarter, month, day;<br>' +
        'calendar year, week, day; and fiscal year, quarter, month,<br>' +
        'day. The layout is a little messy, and there’s nothing you<br>' +
        'can do to change it, but Figure 8-7 illustrates correctly<br>' +
        'defined attribute relationships for these three hierarchies.<br>' +
        'Several of the levels contain additional attribute<br>' +
        'relationships. The Calendar Year Month level is expanded<br>' +
        'to show the two attributes that apply to this level.<br>' +
        'NOTE<br>' +
        '515<br>' +
        'An observant reader may be surprised by<br>' +
        'the path of attribute relationships that goes<br>' +
        'day, month, fiscal month, fiscal quarter,<br>' +
        'fiscal year. Why did we put the calendar<br>' +
        'month between day and fiscal month? For<br>' +
        'Adventure Works Cycles, fiscal months<br>' +
        'directly map to calendar months; they’re<br>' +
        'simply offset. If you know the calendar<br>' +
        'month, you know the fiscal month. The<br>' +
        'advantage of setting up the attribute<br>' +
        'relationships this way is that Analysis<br>' +
        'Services can create aggregations at the<br>' +
        'calendar month level, and use those same<br>' +
        'aggregations for queries along the fiscal<br>' +
        'calendar. Monthly aggregations are very<br>' +
        'common, and this one little trick<br>' +
        'substantially cuts down the number of<br>' +
        'aggregations that Analysis Services will<br>' +
        'need to build and maintain. Adding<br>' +
        'calendar month to the attribute<br>' +
        'relationships doesn’t change the way the<br>' +
        'hierarchy works for the users.<br>' +
        'By declaring attribute relationships correctly, you’re<br>' +
        'informing SSAS that it can index and aggregate the data<br>' +
        'and rely on that relationship. Query performance will<br>' +
        'improve relative to a dimension whose attribute<br>' +
        'relationships are not set correctly. For medium and large<br>' +
        'data volumes, especially with very large dimensions, query<br>' +
        'performance improves substantially, largely because of<br>' +
        '516<br>' +
        'improved design of aggregations that are based on the<br>' +
        'attribute relationships.<br>' +
        'NOTE<br>' +
        'It can be tricky to define the attribute<br>' +
        'relationships correctly, but it’s vital that<br>' +
        'you do so. If you’ve defined a relationship<br>' +
        'between two attributes, but the relationship<br>' +
        'is not truly many-to-one, problems will<br>' +
        'crop up when you process the dimension.<br>' +
        'You’ll see error messages complaining<br>' +
        'about duplicate keys.<br>' +
        'Consider a U.S. geography hierarchy that<br>' +
        'rolls from zip code to city to state. If the<br>' +
        'city attribute is built on the city name,<br>' +
        'you’ll get many duplicate key errors for the<br>' +
        'city named Riverside (46 U.S. states have a<br>' +
        'location named Riverside). You can solve<br>' +
        'this problem in several ways:<br>' +
        '• Redesign the relational dimension to<br>' +
        'include a surrogate key for the city. The<br>' +
        'surrogate key would distinguish Riverside,<br>' +
        'CA from all the other Riversides. In SSAS,<br>' +
        'define the attribute so that the Key uses the<br>' +
        'surrogate key and the NameColumn<br>' +
        'property uses the city name. This approach<br>' +
        'is recommended if the hierarchy is defined<br>' +
        'as type 2 (tracking history).<br>' +
        '• Redesign the dimension in Analysis<br>' +
        'Services to use state plus city as the key for<br>' +
        'the city attribute.<br>' +
        '517<br>' +
        '• Remove the attribute relationship from the<br>' +
        'hierarchy.<br>' +
        'There is one vital property associated with a Related<br>' +
        'Attribute: RelationshipType, which can be Rigid or<br>' +
        'Flexible. RelationshipType affects how Analysis Services<br>' +
        'manages dimension changes. Rigid means the historical<br>' +
        'relationship is fixed. Usually this means that you’re<br>' +
        'managing those attributes as type 2, and not updating<br>' +
        'history. Flexible means the historical relationship can<br>' +
        'change over time: Your ETL can perform updates on the<br>' +
        'hierarchical attributes. As you might expect, Flexible is the<br>' +
        'default, and this setting works well and is easy to manage<br>' +
        'for small- to medium-sized cubes.<br>' +
        'The date dimension is an unusual example, because of<br>' +
        'course the relationship between a day, month, and year<br>' +
        'will not change over time. For the date dimension, define<br>' +
        'all attribute relationships as rigid.<br>' +
        'If your cube is built on hundreds of gigabytes or terabytes<br>' +
        'of data, you need to pay careful attention to the settings for<br>' +
        'RelationshipType, and the aggregations that are built on<br>' +
        'attributes with flexible relationships. The issue, as we<br>' +
        'discuss later in the chapter, is about whether aggregations<br>' +
        'are dropped during incremental processing. Aggregations<br>' +
        'build really quickly, so this isn’t a big deal for small cubes.<br>' +
        'But it’s an important tuning consideration for large cubes.<br>' +
        'RESOURCES<br>' +
        '518<br>' +
        'See the Books Online topic “Attribute<br>' +
        'Relationships” for a description of attribute<br>' +
        'relationships.<br>' +
        'See the Books Online topic “Attribute<br>' +
        'Relationship Properties” for a complete list<br>' +
        'of the properties of attribute relationships.<br>' +
        'Browsing Dimension Data<br>' +
        'The fourth tab across the top of the Dimension Designer is<br>' +
        'labeled Browser. This is where you can look at the<br>' +
        'dimension’s data and hierarchies, as illustrated in Figure<br>' +
        '8-8.<br>' +
        'Before you browse the dimension’s data, you need to<br>' +
        'process the dimension. Up to now we’ve been dealing at<br>' +
        'the logical level, refining the dimension’s structure without<br>' +
        'reference to its data.<br>' +
        'Later in this chapter, we talk more about building and<br>' +
        'deploying the Analysis Services database. But for the<br>' +
        'purposes of looking at a dimension whose attributes<br>' +
        'you’ve been editing, it’s sufficient to know that you don’t<br>' +
        'need to build, deploy, and process the entire database in<br>' +
        'order to look at the dimension. You can right-click the<br>' +
        'dimension in the Solution Explorer, and choose to build<br>' +
        'and deploy the changes to the project, then process the<br>' +
        'changed dimension.<br>' +
        '519<br>' +
        'Take time to look at the dimension in the browser. Look at<br>' +
        'the different hierarchies, available from the dropdown list<br>' +
        'as highlighted in Figure 8-8. Drill around the different<br>' +
        'levels and attributes in the dimension to ensure everything<br>' +
        'looks the way you want it to. It’s a good bet that you’ll<br>' +
        'need to adjust something. On the first time through, you’re<br>' +
        'fairly likely to have to go back to your ETL team and talk<br>' +
        'about improving the cleaning of the dimension’s data. You<br>' +
        'should get detailed feedback from a representative of the<br>' +
        'business user community before you decide that a<br>' +
        'dimension is correctly defined.<br>' +
        'Keep working on your dimensions, hierarchies, and<br>' +
        'dimension data until you’re pleased with them. At the very<br>' +
        'least, be sure to define the key level of each dimension<br>' +
        'correctly before moving on to the Cube Designer.<br>' +
        'NOTE<br>' +
        'It’s hard to overemphasize the importance<br>' +
        'of ensuring that the dimension’s data and<br>' +
        'structure are clean and make sense to the<br>' +
        'business users. The dimension attributes<br>' +
        'are used for drilling down in queries; they<br>' +
        'also show up as report column and row<br>' +
        'headers. Spend time now to get them right,<br>' +
        'or you’ll be stuck with ugliness and<br>' +
        'confusion for years to come.<br>' +
        'Figure 8-8: Browsing dimension data<br>' +
        '520<br>' +
        'Creating and Editing the Cube<br>' +
        'After you’ve created and perfected your dimensions, it’s<br>' +
        'time to run the Cube Wizard to bring in the facts and<br>' +
        'create the cube structure. As illustrated in Figure 8-9, the<br>' +
        'Cube Wizard asks you to identify the fact tables (which it,<br>' +
        'somewhat perplexingly, calls measure group tables). You<br>' +
        'can have it suggest fact tables for you, though you really<br>' +
        'should know which tables in your DSV are fact tables.<br>' +
        'The Cube Wizard automatically creates a measure for each<br>' +
        'numeric non-key column in the fact table, plus a count<br>' +
        'measure. This is usually what you want. Next, the Cube<br>' +
        'Wizard identifies the existing dimensions that will be<br>' +
        'included in the new cube. Finally, it may suggest creating<br>' +
        'some new dimensions; as we discussed previously, you<br>' +
        'should have built out all your dimensions already.<br>' +
        '521<br>' +
        'Figure 8-9: Identifying fact tables in the Cube Wizard<br>' +
        'When you finish with the Cube Wizard, you’re dropped<br>' +
        'into the Cube Designer interface, illustrated in Figure 8-10.<br>' +
        'Anything you did in the Cube Wizard, you can do in the<br>' +
        'Cube Designer. You can — and will! — modify the<br>' +
        'objects that were created by the Cube Wizard. Use the<br>' +
        'Cube Designer to improve the cube, add new dimensions<br>' +
        'to a measure group, and add new measure groups.<br>' +
        '522<br>' +
        'Figure 8-10: The Cube Designer<br>' +
        'There’s a lot going on in the Cube Designer, but it’s<br>' +
        'organized logically. Note the tabs going across the top of<br>' +
        'the window of Figure 8-10, titled Cube Structure,<br>' +
        'Dimension Usage, and so on. We’ll examine these tabs in<br>' +
        'greater depth, but first we’ll quickly describe what each is<br>' +
        'for.<br>' +
        '• Cube Structure: Use this tab to examine the physical layout<br>' +
        'of the cube’s sources, and to modify the properties of<br>' +
        'measures. This is the active pane in Figure 8-10.<br>' +
        '• Dimension Usage: Use this tab to refine the dimensionality<br>' +
        'of each measure group.<br>' +
        '• Calculations: Use this tab to define, edit, and debug<br>' +
        'calculations for the cube.<br>' +
        '• KPIs: Use this tab to create and edit the Key Performance<br>' +
        'Indicators (KPIs) in a cube.<br>' +
        '523<br>' +
        '• Actions: Use this tab to create and edit Drillthrough and<br>' +
        'other actions for the cube. Actions are a powerful<br>' +
        'mechanism for building an actionable business intelligence<br>' +
        'application that does more than just view data.<br>' +
        '• Partitions: Use this tab to set up the physical storage of the<br>' +
        'measure groups. We usually accept defaults during initial<br>' +
        'development, and turn our attention to these issues later in<br>' +
        'the development cycle. Physical storage decisions are<br>' +
        'discussed later in this chapter.<br>' +
        '• Aggregations: Do not use this tab during initial<br>' +
        'development. Defining and building performance<br>' +
        'aggregations are tasks for later in the development cycle.<br>' +
        '• Perspectives: Use this Enterprise Edition tab to create and<br>' +
        'edit the perspectives in a cube, to provide a simplified<br>' +
        'logical view of the cube for a set of business users.<br>' +
        '• Translations: Use this Enterprise Edition tab to create and<br>' +
        'edit the translated names for objects in a cube, like measures.<br>' +
        '• Browser: Use this tab to look at the data in the cube, once<br>' +
        'the cube has been deployed and processed.<br>' +
        'Edit the Cube Structure<br>' +
        'Figure 8-10 illustrates the Cube Structure tab of the Cube<br>' +
        'Designer. In the center pane is a representation of the Data<br>' +
        'Source View of the relational tables or views on which the<br>' +
        'cube is built. This is similar to the source data pane in the<br>' +
        'Dimension Designer, but a lot more complicated because it<br>' +
        'includes all the tables used in the cube. As with the<br>' +
        'Dimension Designer, if you want to edit the Data Source<br>' +
        'View you’re flipped into the DSV Editor. Changes in the<br>' +
        'underlying DSV are reflected here immediately. In this<br>' +
        'view, fact tables are coded in yellow and dimensions in<br>' +
        'blue.<br>' +
        'As with dimensions, the main activity in editing the cube<br>' +
        'structure consists of editing the properties of objects. In the<br>' +
        '524<br>' +
        'case of the cube, edit the properties of measure groups,<br>' +
        'measures, and cube dimensions.<br>' +
        'Edit Measure Groups and Measures<br>' +
        'Measure group is the Analysis Services term for fact table.<br>' +
        'The small pane in the top left is where the cube’s measure<br>' +
        'groups are listed. Open a measure group to see its<br>' +
        'measures. The most important properties for a measure<br>' +
        'group are the name and description. We discuss the other<br>' +
        'properties of the measure group later, in the discussion of<br>' +
        'physical design considerations. For the initial iterations of<br>' +
        'the design and development, these other settings are not<br>' +
        'very important.<br>' +
        'The most important properties for a measure are:<br>' +
        '• Name, Description, and FormatString: Ensure these are set<br>' +
        'correctly. Most Analysis Services client query tools<br>' +
        'automatically format data correctly, according to the<br>' +
        'FormatString (search Books Online for<br>' +
        '“FORMAT_STRING Contents” for more on format strings).<br>' +
        '• DisplayFolder: Measures are grouped by measure group.<br>' +
        'Since each measure group can have many measures,<br>' +
        'DisplayFolders give you an extra grouping layer within a<br>' +
        'measure group. They don’t exist anywhere else. You make<br>' +
        'them up by typing them in the property window.<br>' +
        '• AggregateFunction: Most measures are defined to<br>' +
        'summarize by summing or counting; less frequently by<br>' +
        'distinct count, min, or max. You can also define options for<br>' +
        'semi-additive or non-additive measures: averaging,<br>' +
        'beginning of period, end of period, and so on.<br>' +
        '525<br>' +
        'NOTE<br>' +
        'Perhaps you noticed that one obvious<br>' +
        'aggregate function is missing from the list:<br>' +
        'a simple average. There’s an average of<br>' +
        'children, but not a simple average. The<br>' +
        'reason is that Analysis Services requires<br>' +
        'you to explicitly define how the average is<br>' +
        'computed: the numerator is a simple sum,<br>' +
        'and the denominator is some kind of count.<br>' +
        'You need to create a calculated measure to<br>' +
        'present a measure that aggregates by<br>' +
        'averaging. We usually change the name of<br>' +
        'the measure we want to average and set its<br>' +
        'visibility property to False. Then we create<br>' +
        'a new calculated measure with the correct<br>' +
        'name, as you’ll see later in this chapter in<br>' +
        'the section “The Calculations Tab,” with<br>' +
        'Unit Price USD.<br>' +
        '• Visible: It’s surprisingly common to hide a measure. Many<br>' +
        'measures are used for calculations but aren’t very interesting on<br>' +
        'their own. Hiding them reduces clutter for the business users.<br>' +
        'Edit Dimensions in the Cube<br>' +
        'The small pane in the lower left is where the cube’s<br>' +
        'dimensions are listed. You can launch the Dimension<br>' +
        'Editor from here or from the Solution Explorer as we<br>' +
        'discussed earlier.<br>' +
        '526<br>' +
        'The primary editing activity in the Dimensions pane of the<br>' +
        'Cube Structure tab is to add a new dimension or to re-order<br>' +
        'dimensions. The order of the dimensions in the cube, as<br>' +
        'they appear in this pane, is the same as they’ll show up to<br>' +
        'the users. Put the obscure dimensions at the bottom!<br>' +
        'NOTE<br>' +
        'There is a second implication of the order<br>' +
        'of the dimensions in this list, which is vital<br>' +
        'if you have complex calculations. The<br>' +
        'order of dimensions in this list affects the<br>' +
        'application of dimension calculations, like<br>' +
        'unary operators. This is particularly<br>' +
        'important for a financial model, where you<br>' +
        'might allocate along one dimension before<br>' +
        'calculating along a second dimension<br>' +
        '(usually the account dimension). You can<br>' +
        'see this in the Adventure Works DW 2008<br>' +
        'Analysis Services database. The<br>' +
        'organizations dimension in that database<br>' +
        'must be listed before the accounts<br>' +
        'dimension for the calculations to work<br>' +
        'properly. Try reordering the dimensions<br>' +
        'and recalculating the measure group to see<br>' +
        'how important the ordering is.<br>' +
        'Dimensions are defined first at the database level, then<br>' +
        'optionally added to the cube. As we discussed earlier in<br>' +
        'this chapter, most cubes correspond roughly to a business<br>' +
        'process dimensional model, and contain between 5 and 15<br>' +
        '527<br>' +
        'fact tables or measure groups. If you add new dimensions<br>' +
        'to your database after your initial run through the Cube<br>' +
        'Wizard, you may need to add them to your cube as well.<br>' +
        'Do so here.<br>' +
        'The properties of a cube dimension that you might edit are:<br>' +
        '• Name: The dimension name as created in the database is<br>' +
        'usually the correct name. The only time you’re likely to<br>' +
        'want to edit the name of the dimension in the cube is if the<br>' +
        'dimension has multiple roles. This is very common with the<br>' +
        'Date dimension: Order Date, Due Date, Ship Date, and so on<br>' +
        'are different roles of the Date dimension.<br>' +
        '• Description: Similarly, adjust the Description property for<br>' +
        'dimension roles.<br>' +
        'You can also edit the properties of the hierarchy of a cube<br>' +
        'dimension, and attributes of a cube dimension. It’s<br>' +
        'unlikely that you’d want to do this.<br>' +
        'Cube Properties<br>' +
        'There are a few properties of the cube that you might want<br>' +
        'to adjust during development. You can see the list of these<br>' +
        'properties by clicking the cube in either the Measures or<br>' +
        'Dimensions panes of the Cube Structure tab.<br>' +
        '• Name and Description: Edit the cube’s name and description<br>' +
        'as appropriate.<br>' +
        '• DefaultMeasure: One measure will come up by default, if<br>' +
        'users don’t specify a measure in a query. You should choose<br>' +
        'which measure that is, rather than letting Analysis Services<br>' +
        'choose it for you.<br>' +
        'Most of the other options have to do with physical storage<br>' +
        'and processing, and are discussed later in the chapter.<br>' +
        '528<br>' +
        'During the first part of the development cycle, the defaults<br>' +
        'are usually fine.<br>' +
        'Edit Dimension Usage<br>' +
        'You specify exactly how dimensions participate in<br>' +
        'measure groups on the next tab of the Cube Designer.<br>' +
        'Figure 8-11 illustrates the Dimension Usage tab for the<br>' +
        'MDWT_2008R2 sample cube. Our simplification of the<br>' +
        'Adventure Works schema presents a simple display. To<br>' +
        'see a more realistic display of dimension usage, check out<br>' +
        'the Adventure Works DW 2008 sample Analysis Services<br>' +
        'database that you can download from CodePlex.<br>' +
        'The Dimension Usage tab’s summarization is very similar<br>' +
        'to the Kimball Method bus matrix with the rows and<br>' +
        'columns flipped. At a glance you can identify which<br>' +
        'dimensions participate in which measure groups, and at<br>' +
        'which level of granularity.<br>' +
        'Figure 8-11: Dimension usage<br>' +
        '529<br>' +
        'The dimensionality of the Orders measure group displayed<br>' +
        'in Figure 8-11 has been modified from the defaults<br>' +
        'generated by the Cube Wizard. There are three date keys in<br>' +
        'the two fact tables, each with a different name. Originally,<br>' +
        'Analysis Services defined three roles for the date<br>' +
        'dimension: date from the Exchange Rates measure group,<br>' +
        'and order date and due date from the Orders measure<br>' +
        'group. However, the business users think of the order date<br>' +
        'as the main date key. Therefore, we deleted the order date<br>' +
        'role, and reassigned the OrderDateKey to the main date<br>' +
        'role, using the Define Relationship window shown in<br>' +
        'Figure 8-12.<br>' +
        'Figure 8-12: Define the relationship between a fact table<br>' +
        'and a dimension<br>' +
        '530<br>' +
        'Measure Groups at Different Granularity<br>' +
        'Measure groups, and their underlying relational fact tables,<br>' +
        'may hook into a conformed dimension at a summary level.<br>' +
        'The most common scenario is for forecasts and quotas.<br>' +
        'Most businesses develop forecasts quarterly or monthly,<br>' +
        'even if they track sales on a daily basis. The fundamentals<br>' +
        'of dimensional modeling tell you to conform the date<br>' +
        'dimension across uses, even if those uses are at different<br>' +
        'grains. By re-using the dimension, you’re making it easy to<br>' +
        'compare quarterly data from multiple measure groups.<br>' +
        'Analysis Services makes this conformation easy for you.<br>' +
        'All you have to do is correctly define the granularity<br>' +
        'attribute in the Define Relationship dialog box. Make sure<br>' +
        'you set the attribute relationships properly in any<br>' +
        'dimension for which some measure groups join in at a<br>' +
        'coarser grain.<br>' +
        '531<br>' +
        'WARNING<br>' +
        'It’s absolutely critical to set the attribute<br>' +
        'relationships properly in any dimension<br>' +
        'that’s used at multiple granularities. You<br>' +
        'need to inform Analysis Services that it can<br>' +
        'expect referential integrity between various<br>' +
        'attributes in the dimension. If you aren’t<br>' +
        'careful here, you can see inconsistent query<br>' +
        'results because aggregations are computed<br>' +
        'and stored incorrectly.<br>' +
        'Complex Relationships Between Dimensions and<br>' +
        'Measure Groups<br>' +
        'There are several kinds of relationships between<br>' +
        'dimensions and measure groups, which you can set in the<br>' +
        'Dimension Usage tab. Most dimension relationships are<br>' +
        'Regular, but you can also specify Fact, Reference, Data<br>' +
        'Mining, and Many-to-Many relationships here.<br>' +
        'The Cube Wizard does a good job of setting up these<br>' +
        'relationships correctly, but you should always check them.<br>' +
        'Build, Deploy, and Process the Project<br>' +
        'At this point in the development cycle, you’ve worked on<br>' +
        'your dimensions, and you’ve built and processed them<br>' +
        'several times. You’ve defined the basic structure of your<br>' +
        'cube, including the relationships between measure groups<br>' +
        'and dimensions.<br>' +
        '532<br>' +
        'Now is a good time to perform the first processing of the<br>' +
        'OLAP database. The easiest way to do this is to right-click<br>' +
        'the project name in the Solution Explorer and choose<br>' +
        'Process. By choosing Process, you’re doing three things:<br>' +
        '• Building the project, looking for structural errors.<br>' +
        '• Deploying that project to the target Analysis Services server,<br>' +
        'creating a new database structure (or updating an existing<br>' +
        'database structure).<br>' +
        '• Processing that database, to fully or incrementally add data.<br>' +
        'The last tab of the Cube Designer is where you browse the<br>' +
        'data in the SSAS database. You don’t need to work<br>' +
        'through all the other tabs before you take a look at the<br>' +
        'data. After you’ve defined the basic cube structure and<br>' +
        'verified the relationships with the dimensions, you<br>' +
        'generally want to check your work.<br>' +
        'As we described earlier when talking about browsing the<br>' +
        'dimensions, you need to build, deploy, and process the<br>' +
        'cube before you can browse the data. The browser tab will<br>' +
        'remind you to do that if necessary.<br>' +
        'Earlier in this chapter, we talked about starting your<br>' +
        'Analysis Services development against a much reduced<br>' +
        'data set. As you process the cube for the umpteenth time to<br>' +
        'check the implications of an edit, you’ll recognize why this<br>' +
        'was a good idea.<br>' +
        'Create Calculations<br>' +
        'The third tab of the Cube Designer is where you’ll create<br>' +
        'calculations using the Multidimensional Expression<br>' +
        'language, or MDX. Most BI teams we’ve worked with<br>' +
        '533<br>' +
        'have resisted learning MDX as long as they can. No one<br>' +
        'wants to learn a new language, but all eventually agree that<br>' +
        'at least one person on the team needs to step up to this<br>' +
        'challenge. The good news is that the BI Studio tools<br>' +
        'include wizards to help you define common calculations.<br>' +
        'And you can learn a lot about MDX by looking at how<br>' +
        'these calculations are defined for you.<br>' +
        'You have opportunities to sprinkle MDX throughout the<br>' +
        'definition of the cube. The most obvious place is here, on<br>' +
        'the Calculations tab, where you can create calculated<br>' +
        'measures, other calculated members, sets, and calculated<br>' +
        'sub-cubes. You’ll greatly improve the usefulness and<br>' +
        'user-friendliness of your Analysis Services database by<br>' +
        'defining common calculations that benefit all business<br>' +
        'users regardless of how they’re accessing the cube.<br>' +
        '• Calculated measures will show up under the Measures<br>' +
        'dimension. Many calculated measures are quite simple, like<br>' +
        'the sum or ratio of two other measures. To a business user<br>' +
        'browsing the cube, calculated measures are largely<br>' +
        'indistinguishable from physical measures. Some query tools,<br>' +
        'including the Cube Browser integrated with BI Studio, will<br>' +
        'show a different icon for calculated measures and physical<br>' +
        'measures. A calculated measure is really just a calculated<br>' +
        'member, assigned to the Measures dimension. A very<br>' +
        'complex calculated measure may perform less well than a<br>' +
        'physical measure because calculations are performed at<br>' +
        'runtime.<br>' +
        '• Calculated members can be created on any dimension.<br>' +
        'Non-measure calculated members can be very powerful.<br>' +
        'They’re a way to create a kind of calculation that applies for<br>' +
        'some or all measures. For example, you can create a<br>' +
        'calculated member Year To Date on the Date dimension, to<br>' +
        'automatically calculate multiple measures year-to-date.<br>' +
        '534<br>' +
        'NOTE<br>' +
        'The Add Business Intelligence Wizard will<br>' +
        'create a wide variety of calculations for<br>' +
        'you, including the Year To Date calculated<br>' +
        'member.<br>' +
        '• Named sets are a set of dimension members. A really simple<br>' +
        'named set would specify the set explicitly, perhaps as a list of<br>' +
        'important products. More complicated set definitions locate the<br>' +
        'set of products with a high price, products that sold well this<br>' +
        'year, and so on.<br>' +
        '• Calculated sub-cubes are a way to calculate an arbitrary portion<br>' +
        'of the cube’s data and summarizations. A common use of<br>' +
        'calculated sub-cubes is to allocate summary data (like monthly<br>' +
        'quotas) down to more fine-grained data. Or, calculated<br>' +
        'sub-cubes can provide a complex summarization method, if<br>' +
        'your business rules are more complicated than can be supported<br>' +
        'by the standard aggregate functions.<br>' +
        'NOTE<br>' +
        'How do you know whether you need a<br>' +
        'calculated member or a calculated<br>' +
        'sub-cube? A calculated member changes<br>' +
        'the visible structure of the cube; you can<br>' +
        'see the calculated member in the list of<br>' +
        'dimension members. A calculated sub-cube<br>' +
        'doesn’t change the list of members or<br>' +
        'attributes in a dimension. Instead, it<br>' +
        '535<br>' +
        'changes the way the numbers inside the<br>' +
        'cube are calculated.<br>' +
        'The Calculations Tab<br>' +
        'The Calculations tab of the Cube Designer is illustrated in<br>' +
        'Figure 8-13. As you can see, this is another complicated<br>' +
        'screen. But let’s face it: Calculations are complicated.<br>' +
        'In the upper left is the Script Organizer. This pane lists the<br>' +
        'calculations that have been defined for this cube. The<br>' +
        'selected calculation is a calculated measure called [Unit<br>' +
        'Price USD], which calculates unit price as an average.<br>' +
        'Recall from earlier in this chapter that there is no standard<br>' +
        'aggregation method for a simple average: if you want to<br>' +
        'display an average, you need to create a calculation. The<br>' +
        'main area of the Calculations tab shows a form where this<br>' +
        'calculation is defined. The lower-left pane contains three<br>' +
        'tabs that show a list of objects in the cube; a list of<br>' +
        'available functions, which you can drag into your<br>' +
        'calculation; or a set of templates, which can provide a<br>' +
        'starting point for some kinds of calculations.<br>' +
        'Figure 8-13: The Calculations tab<br>' +
        '536<br>' +
        'This calculated member is defined on the Measures<br>' +
        'dimension; it’s a calculated measure. The next area<br>' +
        'contains the MDX for the calculation. Even if you don’t<br>' +
        'know anything about MDX, you can probably parse this<br>' +
        'statement:<br>' +
        '• If the denominator — [Orders Count] — is not zero, return<br>' +
        '[Unit Price USD HIDE], which aggregates as a sum, divided<br>' +
        'by [Orders Count], which aggregates as a count.<br>' +
        '• Otherwise, the denominator — [Orders Count] — is zero, so<br>' +
        'return zero.<br>' +
        'There are several important properties of a calculated<br>' +
        'member:<br>' +
        '• Description: It’s possible to set the description of a<br>' +
        'calculated member, but they sure don’t make it easy to find.<br>' +
        'It’s on a different screen entirely, called Calculation<br>' +
        'Properties, that you can launch from a toolbar icon.<br>' +
        '537<br>' +
        '• Format string and Visible: The same as for physical<br>' +
        'measures.<br>' +
        '• Non-empty behavior: You should always set this to one or a<br>' +
        'few items in the calculation. What you’re telling Analysis<br>' +
        'Services by setting this property is that if the base measure<br>' +
        '([Unit Price USD HIDE] in this case) is empty, don’t spend<br>' +
        'any time calculating the measure.<br>' +
        'WARNING<br>' +
        'Always set the non-empty behavior<br>' +
        'property. It can make a huge difference in<br>' +
        'the query performance of your calculated<br>' +
        'measures. In one case, we saw query<br>' +
        'performance drop from hours to seconds,<br>' +
        'simply by filling in the non-empty behavior<br>' +
        'property.<br>' +
        '• Associated measure group and Display folder: These settings<br>' +
        'are largely cosmetic, but can make it much easier for users to<br>' +
        'find the measures you’ve so painstakingly created.<br>' +
        'The calculation form view shown in the Calculation tab is<br>' +
        'a good way to create new measures and named sets. It is an<br>' +
        'item-by-item form that builds an underlying script. If you<br>' +
        'want to look at the entire script, switch to the script view<br>' +
        'by choosing Cube ⇒ Show Calculations in Script (or click<br>' +
        'the Script toolbar icon).<br>' +
        'Let’s take a look at the MDX for a second calculation:<br>' +
        '[Contribution to Parent Product]. Wherever you are in the<br>' +
        'product hierarchy, whether at the category, subcategory, or<br>' +
        'product level, calculate the current member’s contribution<br>' +
        '538<br>' +
        'to the sales of the parent. If you’re at the product category<br>' +
        'level it’s your sales divided by all sales, at the subcategory<br>' +
        'level it’s those sales divided by all sales for this category,<br>' +
        'and so on. The MDX for this calculation is:<br>' +
        'Case<br>' +
        'When [Product].[Product Categories].CurrentMember IS<br>' +
        '[Product].[Product Categories].[All Products]<br>' +
        'Then 1<br>' +
        'Else [Measures].[Sales Amt USD] /<br>' +
        '( [Product].[Product Categories].CurrentMember.Parent,<br>' +
        '[Measures].[Sales Amt USD] )<br>' +
        'End<br>' +
        'This may not be clear to someone new to MDX, but it’s<br>' +
        'fairly straightforward:<br>' +
        '• If you’re at the “all” level of the product dimension, return 1.<br>' +
        '• Otherwise, wherever you are in the product category<br>' +
        'hierarchy, return the ratio of your sales divided by your<br>' +
        'parent’s sales.<br>' +
        'You can get an idea of how the calculation works by<br>' +
        'viewing the browser output in Figure 8-14.<br>' +
        'Figure 8-14: Browsing a cube to examine calculated<br>' +
        'members<br>' +
        '539<br>' +
        'NOTE<br>' +
        'If you find yourself defining lots of similar<br>' +
        'calculations, like [Product Contribution to<br>' +
        'Subcategory] and [Subcategory<br>' +
        'Contribution to Category], you should step<br>' +
        'back and figure out how to generalize.<br>' +
        'Probably you really need a single<br>' +
        'calculation that computes relative to the<br>' +
        'current position in the cube — a far more<br>' +
        'elegant solution.<br>' +
        'RESOURCES<br>' +
        '540<br>' +
        'Like so much we’ve introduced,<br>' +
        'Calculations and MDX are rich and<br>' +
        'complex topics about which entire books<br>' +
        'are written. We’ve barely scratched the<br>' +
        'surface here.<br>' +
        'The first place to go for help is the<br>' +
        'excellent section on Calculations in the<br>' +
        'SQL Server Analysis Services tutorial. This<br>' +
        'tutorial walks you through the process of<br>' +
        'creating several calculated members,<br>' +
        'named sets, and calculated sub-cubes. The<br>' +
        'tutorial shows you how to debug your<br>' +
        'Calculation Script by setting breakpoints,<br>' +
        'stepping through calculations, and<br>' +
        'watching how each step modifies the<br>' +
        'cube’s structure and data.<br>' +
        'The debugger is a great feature. Learn how<br>' +
        'to use it, especially if you have any MDX<br>' +
        'scripts. The debugger is absolutely vital if<br>' +
        'you have multiple calculations that overlap<br>' +
        'one another.<br>' +
        'The MDWT_2008R2 database is available<br>' +
        'on the book’s website<br>' +
        '(http://kimballgroup.com/html/<br>' +
        'booksMDWTtools.html). It contains a handful<br>' +
        'of calculated members and sets, for the<br>' +
        'same simplified version of the Adventure<br>' +
        '541<br>' +
        'Works case study that we’ve used<br>' +
        'throughout this book.<br>' +
        'The Adventure Works DW 2008 SSAS<br>' +
        'database that’s available on CodePlex is a<br>' +
        'good learning tool for MDX and<br>' +
        'calculating. It contains a lot of fairly<br>' +
        'complex calculations, especially for the<br>' +
        'finance module.<br>' +
        'There are several books that the MDX<br>' +
        'expert on your team should own:<br>' +
        '• SQL Server 2008 MDX Step by Step by<br>' +
        'Smith and Clay (Microsoft, 2009). This is<br>' +
        'probably the best place to start if you need<br>' +
        'to learn MDX. Like other books in the Step<br>' +
        'by Step series, it’s practically an extended<br>' +
        'class or tutorial.<br>' +
        '• MDX Solutions by Spofford, Harinath,<br>' +
        'Webb, Huang, and Civardi (Wiley, 2006).<br>' +
        'Make sure you get the edition for Analysis<br>' +
        'Services 2005 and Hyperion Essbase.<br>' +
        'MDX has not changed substantially since<br>' +
        'SQL Server 2005, and the reference is still<br>' +
        'up-to-date.<br>' +
        '• Professional SQL Server Analysis Services<br>' +
        '2008 with MDX by Harinath, Zare,<br>' +
        'Meenakshisundaram, and Quinn (Wrox,<br>' +
        '2009). This book covers a lot more than<br>' +
        'MDX.<br>' +
        'Adding Business Intelligence<br>' +
        'Unless you’re already an MDX expert, one of the best<br>' +
        'ways to get started in adding calculations to your cube is<br>' +
        '542<br>' +
        'by using the Business Intelligence Wizard. This wizard<br>' +
        'will build for you the most common kinds of calculations.<br>' +
        'You can launch the Business Intelligence Wizard in<br>' +
        'several ways, including from the leftmost icon on the<br>' +
        'toolbar in the Calculations tab.<br>' +
        'Most application developers will use the Business<br>' +
        'Intelligence Wizard to add “time intelligence” to their<br>' +
        'cube. This wizard option will automatically create for you<br>' +
        'calculations like [Year to Date], [12 Month Moving<br>' +
        'Average], and so on. These calculations appear on the Date<br>' +
        'dimension. They’re non-measure calculated members. As<br>' +
        'such, they can apply to multiple underlying measures in<br>' +
        'one fell swoop.<br>' +
        'A second advantage of the Business Intelligence Wizard is<br>' +
        'that you can pick apart the calculations to learn how they<br>' +
        'were done.<br>' +
        'Define Key Performance Indicators<br>' +
        'Key Performance Indicators (KPIs) are numbers or<br>' +
        'displays that are intended to measure the health of the<br>' +
        'organization. When KPIs are based on a clear<br>' +
        'understanding of the factors that drive the business, and a<br>' +
        'robust DW/BI platform, they can be an extremely powerful<br>' +
        'organizational management tool. Unfortunately, most KPIs<br>' +
        'are either brain-dead simple, divorced from the underlying<br>' +
        'data, or both. Really, who cares that sales are down 0.1<br>' +
        'percent today unless we know that today is a Friday and<br>' +
        'Friday’s sales are usually up 0.1 percent? And even if we<br>' +
        'did know the context, simply telling the executive that<br>' +
        '543<br>' +
        'sales are down doesn’t provide any understanding about<br>' +
        'why.<br>' +
        'We hope that the definition and display of KPIs with<br>' +
        'Analysis Services will help address these shortcomings.<br>' +
        'The KPI implementation is well thought out. Because KPIs<br>' +
        'are defined in the cube, the supporting data can be made<br>' +
        'available in useful ways.<br>' +
        'Any KPIs you create should be defined as part of the BI<br>' +
        'Applications track based on the business requirements and<br>' +
        'with business user involvement. Do not try and make them<br>' +
        'up here on the fly.<br>' +
        'Analysis Services defines a KPI on the KPIs tab of the<br>' +
        'Cube Designer:<br>' +
        '• Name of the KPI: And the measure group with which the<br>' +
        'KPI is associated.<br>' +
        '• Value to be measured: The underlying measure for the KPI.<br>' +
        'Most often you’ll use a calculated measure that you’ve<br>' +
        'already defined. You can enter an MDX expression here, but<br>' +
        'it usually makes more sense to create a calculated measure<br>' +
        'and use that. In the very simple example we used in this<br>' +
        'section, the value to be measured might be [Sales Revenue].<br>' +
        'NOTE<br>' +
        'Whenever you specify an MDX expression<br>' +
        'when you create a KPI, the server will<br>' +
        'create a hidden calculated measure.<br>' +
        '544<br>' +
        '• Goal for the value: An MDX expression that defines the target<br>' +
        'for the measure. A trivial goal would be a number: The sales<br>' +
        'target is $100,000. That would give a stupid KPI. More<br>' +
        'interesting would be a goal to increase sales 0.1 percent from<br>' +
        'the same day the previous week, or a goal that’s based on sales<br>' +
        'quotas. Often, targets are defined and stored as their own<br>' +
        'measure group within the cube. The KPI goal is then a simple<br>' +
        'reference to the appropriate measure within the sales target<br>' +
        'measure group.<br>' +
        '• Status: A graphic and an MDX expression that describe how the<br>' +
        'value to be measured is doing relative to its goal. Analysis<br>' +
        'Services provides several built-in graphic gauges, including<br>' +
        'thermometers, traffic lights, and happy faces. The MDX<br>' +
        'expression needs to evaluate to numbers between –1 (very bad)<br>' +
        'to +1 (very good). If you’re clever about defining the status<br>' +
        'expression, you can do a good job of conveying important<br>' +
        'information about wide deviations from the goal, and keep<br>' +
        'minor deviations quiet.<br>' +
        '• Trend: A graphic and an MDX expression that describe whether<br>' +
        'the value to be measured is moving toward or away from its<br>' +
        'goal. Is the situation getting better or worse? As with the status<br>' +
        'graphic, Analysis Services provides a few built-in trend<br>' +
        'graphics, notably a trend arrow. The MDX expression must<br>' +
        'evaluate to numbers between –1 (going south in a hurry) and +1<br>' +
        '(getting better fast).<br>' +
        'To display your KPIs, you need some client software that<br>' +
        'understands KPIs. Microsoft’s current versions of Excel,<br>' +
        'Reporting Services, and SharePoint are able to work with<br>' +
        'Analysis Services KPIs. Software vendors who sell<br>' +
        'packaged analytics on Analysis Services often use KPIs<br>' +
        'extensively in their front ends.<br>' +
        'NOTE<br>' +
        '545<br>' +
        'Excel, Reporting Services, and SharePoint<br>' +
        'all do a very nice job of displaying KPIs<br>' +
        'that are defined in the cube. There is<br>' +
        'SharePoint functionality that lets power<br>' +
        'users define and modify KPIs while<br>' +
        'working in the BI portal. The functionality<br>' +
        'to play “what if” scenarios with SharePoint<br>' +
        'KPIs does not write those KPIs back to the<br>' +
        'Analysis Services database.<br>' +
        'Create Actions<br>' +
        'The next tab in the Cube Designer is the Action tab. An<br>' +
        'Analysis Services Action is a command that’s stored on the<br>' +
        'server. Actions are defined using MDX, which means that<br>' +
        'the command is context-sensitive. In other words, you can<br>' +
        'define an Action that executes when the user right-clicks a<br>' +
        'cell in a report. The Action knows the values of each of the<br>' +
        'dimensions that make up the address of the cell and can<br>' +
        'execute a customized command for that address.<br>' +
        'Like KPIs, the server-side definition of an Action is only<br>' +
        'half of the solution. You need a client tool that implements<br>' +
        'Actions. It’s the job of the query tool to intercept the<br>' +
        '“right-click” on a report cell, and present (and execute) the<br>' +
        'appropriate list of Actions. Luckily the most common<br>' +
        'query tool, Excel, does implement Actions.<br>' +
        'The most obvious use of an Action is to execute a<br>' +
        'relational query. As we’ve already described, Analysis<br>' +
        'Services databases are often built at the same grain as the<br>' +
        '546<br>' +
        'underlying dimensional model. An interesting BI<br>' +
        'application can include an Action that drills back to<br>' +
        'enterprise data, perhaps even back to the original<br>' +
        'transaction system. This query Action can easily be<br>' +
        'implemented as an Action that launches a Reporting<br>' +
        'Services report.<br>' +
        'Analysis Services provides for several kinds of Actions<br>' +
        'including:<br>' +
        '• Execute a Reporting Services report.<br>' +
        '• Execute a generic Action. Actions can bring up a URL,<br>' +
        'return a rowset or data set, or execute a command. An<br>' +
        'Action is defined as an MDX statement, and can be attached<br>' +
        'to specific parts of the cube. For example, you can have one<br>' +
        'Action that executes when someone right-clicks the Month<br>' +
        'label in the Date dimension, and a second Action that<br>' +
        'launches when someone right-clicks a data cell.<br>' +
        'WARNING<br>' +
        'Two kinds of Actions — HTML scripts<br>' +
        'and Command Line Actions — pose<br>' +
        'security risks and should be avoided.<br>' +
        'Microsoft has moved these Action types<br>' +
        'out of the BI Studio interface. You’d have<br>' +
        'to create the Action by writing a script<br>' +
        'without the benefit of the UI.<br>' +
        'Under the covers, the Drillthrough Action is simply a kind<br>' +
        'of rowset Action that’s common enough that Microsoft<br>' +
        'built a simple user interface for defining it. Similarly, the<br>' +
        'Report Action is just an instance of a URL Action.<br>' +
        '547<br>' +
        'Drillthrough and Reporting Services Actions are very easy<br>' +
        'to set up. Generic Actions, like launching a parameterized<br>' +
        'web page, are pretty tricky. Start with something really<br>' +
        'simple, like launching a static web page (like Google or<br>' +
        'MSN). Add complexity a bit at a time. Process the cube<br>' +
        'and try out the Action in the browser window before<br>' +
        'adding another layer of complexity. If you change only an<br>' +
        'Action definition between processing, the processing step<br>' +
        'refreshes only metadata (not data), so processing occurs<br>' +
        'quickly.<br>' +
        'Partitions and Aggregations<br>' +
        'Partitioning allows you to break up a large cube into<br>' +
        'smaller subsets for easier management, much like<br>' +
        'relational partitioning. You can use the Partitions and<br>' +
        'Aggregations tabs of the Cube Designer to define the<br>' +
        'physical storage characteristics of your cube. During the<br>' +
        'first part of your development cycle, don’t bother with<br>' +
        'these tabs. Use the default settings so you can focus on<br>' +
        'getting the cube structure and calculations correct.<br>' +
        'In the next section of this chapter, we discuss physical<br>' +
        'design considerations. We talk at some length about what’s<br>' +
        'going on under the covers, so you can make informed<br>' +
        'decisions about how to deploy your Analysis Services<br>' +
        'database in production.<br>' +
        'Maintain Perspectives<br>' +
        'An Analysis Services Perspective limits a user’s view to a<br>' +
        'set of related measure groups and dimensions. A<br>' +
        'Perspective is analogous to a business process dimensional<br>' +
        '548<br>' +
        'model in the Kimball Method vocabulary. It’s a very nice<br>' +
        'approach because you’re not replicating data into multiple<br>' +
        'cubes. You’re simply providing different users with<br>' +
        'different views of the same data.<br>' +
        'You’ll almost certainly define a few Perspectives if you<br>' +
        'follow the best practice recommendation of creating a<br>' +
        'large Analysis Services cube that contains multiple<br>' +
        'measure groups. Unless your implementation is really<br>' +
        'simple, this single-cube approach will lead to a structure<br>' +
        'that’s challenging for business users to navigate.<br>' +
        'Remember, though, the advantage of many measure<br>' +
        'groups (fact tables) in a cube is that it’s easy for users to<br>' +
        'construct analyses that combine data from those fact tables<br>' +
        '— which is the whole point of conformed dimensions and<br>' +
        'the bus architecture.<br>' +
        'Use the Perspectives tab of the Cube Designer to define<br>' +
        'your cube’s Perspectives. You can create as many<br>' +
        'Perspectives as you like; it’s a simple matter of choosing<br>' +
        'which portions of the overall cube to hide in each<br>' +
        'Perspective. You can also specify which measure is shown<br>' +
        'by default in each Perspective.<br>' +
        'Most users will use only a few Perspectives, rather than the<br>' +
        'entire cube containing all measure groups. You can hide<br>' +
        'the cube itself by setting its Visible property to False,<br>' +
        'and reveal only the Perspectives that contain the<br>' +
        'information your user communities are interested in.<br>' +
        'It may feel as though Perspectives are a security<br>' +
        'mechanism. Not so! As we discuss in Chapter 14, Analysis<br>' +
        'Services security is applied at the database object level.<br>' +
        '549<br>' +
        'NOTE<br>' +
        'Unfortunately, Perspectives are not<br>' +
        'available in SQL Server Standard Edition.<br>' +
        'Many organizations don’t have the users or<br>' +
        'data volumes needed to justify Enterprise<br>' +
        'Edition or Data Center Edition, but almost<br>' +
        'all organizations can benefit from<br>' +
        'Perspectives.<br>' +
        'Translations<br>' +
        'Translations are a very nice feature of Analysis Services<br>' +
        'Enterprise and Data Center Editions. If your company is<br>' +
        'multinational, business users will probably prefer to view<br>' +
        'the cube in their native languages. A fully translated cube<br>' +
        'will have translations for its metadata (names and<br>' +
        'descriptions of dimensions, attributes, hierarchies, levels,<br>' +
        'and measures) as well as the dimension data itself<br>' +
        '(member names and attribute values). In other words, the<br>' +
        'dimension name Date needs to be translated, the attribute<br>' +
        'name Month, and the attribute values January, February,<br>' +
        'and so on.<br>' +
        'The translations for the metadata occur in two places: in<br>' +
        'the Translations tab of the Dimension Designer and the<br>' +
        'Translations tab of the Cube Designer. You can create<br>' +
        'translations in multiple languages.<br>' +
        'If you want to translate the dimension attribute values,<br>' +
        'your relational dimension table or view needs to have<br>' +
        '550<br>' +
        'additional columns for the new languages. These don’t<br>' +
        'become additional attributes in the Analysis Services<br>' +
        'dimension; instead you can set them up as the translation<br>' +
        'content in the dimension Translations tab.<br>' +
        'NOTE<br>' +
        'Be very careful when you are setting up the<br>' +
        'translations, especially translations of the<br>' +
        'dimension attribute values. If you make a<br>' +
        'mistake in the translation, users of different<br>' +
        'languages will find it very difficult to<br>' +
        'construct comparable reports.<br>' +
        'When you use the Browser tab to browse the data, you can<br>' +
        'view the data in different languages. This is fun to do, and<br>' +
        'if your organization is multilingual, it is certainly worth<br>' +
        'including in any demos you create for senior management.<br>' +
        'The Adventure Works DW 2008 sample database includes<br>' +
        'a rich set of translations.<br>' +
        'The Translation feature works really well. It leverages the<br>' +
        'localization technology in all Microsoft products. It picks<br>' +
        'up the locale ID from the user’s desktop and automatically<br>' +
        'displays either the appropriate translation, if it exists, or<br>' +
        'the default language. Most client tools don’t need to do<br>' +
        'anything special.<br>' +
        'Designing the Analysis Services cube is generally<br>' +
        'straightforward if you’re starting from a clean,<br>' +
        'dimensionally structured relational data warehouse. In<br>' +
        '551<br>' +
        'most cases, the only hard part is defining the calculations.<br>' +
        'Someone on the DW/BI team is going to have to learn<br>' +
        'MDX for that. But most concepts translate very smoothly<br>' +
        'from the relational dimensional database into Analysis<br>' +
        'Services. When we’ve seen people struggle with designing<br>' +
        'their cubes, it’s usually because they’re trying to build the<br>' +
        'cube from a faulty dimensional design.<br>' +
        'Physical Design Considerations<br>' +
        'Up to this point in this chapter, we have discussed the<br>' +
        'logical design process for the Analysis Services OLAP<br>' +
        'database. We recommend that you work with a small<br>' +
        'subset of data so that you can concentrate on the structure,<br>' +
        'calculations, and other cube decorations, without worrying<br>' +
        'about the physical design.<br>' +
        'Now it’s time to address the physical issues. Most of the<br>' +
        'time, physical implementation decisions are made<br>' +
        'independently of the logical design. That’s not completely<br>' +
        'true, and in this section, we discuss how some design<br>' +
        'decisions can have a large impact on the manageability and<br>' +
        'performance of your cube.<br>' +
        'Cube Physical Storage Terminology<br>' +
        'Readers who are familiar with older versions of<br>' +
        'Analysis Services will already be familiar with the<br>' +
        'terminology for the physical storage and<br>' +
        'processing of the Analysis Services database. This<br>' +
        '552<br>' +
        'sidebar merely defines these concepts.<br>' +
        'Recommendations and implications are discussed<br>' +
        'in detail elsewhere in the chapter.<br>' +
        '• Leaf data: Leaf data is the finest grain of data that’s<br>' +
        'defined in the cube’s measure group. Usually, the leaf<br>' +
        'data corresponds exactly to the fact table from which<br>' +
        'a cube’s measure group is sourced. Occasionally<br>' +
        'you’ll define a measure group at a higher grain than<br>' +
        'the underlying fact table, for example by eliminating<br>' +
        'a dimension from the measure group.<br>' +
        '• Aggregations: Precomputed aggregations are<br>' +
        'analogous to summary tables in the relational<br>' +
        'database. You can think of them as a big SELECT<br>' +
        '… GROUP BY statement whose result set is stored<br>' +
        'for rapid access.<br>' +
        '• Data storage mode: Analysis Services supports three<br>' +
        'kinds of storage for data:<br>' +
        '• MOLAP (Multidimensional OLAP): Leaf data and<br>' +
        'aggregations are stored in Analysis Services’<br>' +
        'MOLAP format.<br>' +
        '• ROLAP (Relational OLAP): Leaf data and<br>' +
        'aggregations are stored in the source relational<br>' +
        'database.<br>' +
        '• HOLAP (Hybrid OLAP): Leaf data is stored in the<br>' +
        'relational database, and aggregations are stored in<br>' +
        'MOLAP format.<br>' +
        '• Dimension storage mode: Dimension data,<br>' +
        'corresponding to the relational dimension tables, can<br>' +
        'be stored in MOLAP format or left in the relational<br>' +
        'database (ROLAP mode).<br>' +
        '• Partition: Fact data can be divided into partitions.<br>' +
        'Most systems that partition their data do so along the<br>' +
        'Date dimension; for example, one partition for each<br>' +
        'month or year. You can partition along any<br>' +
        'dimension, or along multiple dimensions. The<br>' +
        'partition is the unit of work for fact processing.<br>' +
        'Partitions are a feature of SQL Server Enterprise and<br>' +
        'Data Center Editions.<br>' +
        '553<br>' +
        '• Fact processing: Analysis Services supports several<br>' +
        'kinds of fact processing for a partition:<br>' +
        '• Full processing: All the data for the partition is pulled<br>' +
        'from the source system into the Analysis Services<br>' +
        'engine, and written in MOLAP format if requested.<br>' +
        'Aggregations are computed and stored in MOLAP<br>' +
        'format if requested, or back in the RDBMS (ROLAP<br>' +
        'mode).<br>' +
        '• Incremental processing: New data for the partition is<br>' +
        'pulled from the source system and stored in MOLAP<br>' +
        'if requested. Aggregations — either MOLAP or<br>' +
        'ROLAP — are updated. It’s your job to tell Analysis<br>' +
        'Services how to identify new data.<br>' +
        '• Proactive caching: Proactive caching is a mechanism<br>' +
        'for automatically pushing data into a cube partition.<br>' +
        'When you set up proactive caching, you’re asking<br>' +
        'SSAS to monitor the relational source for the measure<br>' +
        'group’s partition and to automatically perform<br>' +
        'incremental processing when it sees changes.<br>' +
        'Understanding Storage Modes<br>' +
        'The first question to resolve in your physical design is the<br>' +
        'easiest one. Should you use MOLAP, HOLAP, or ROLAP<br>' +
        'mode to store your Analysis Services data? The answer is<br>' +
        'MOLAP.<br>' +
        'We’re not even being particularly facetious with this terse<br>' +
        'answer, but we will justify it a bit.<br>' +
        'Analysis Services MOLAP format has been designed to<br>' +
        'hold dimensional data. It uses sophisticated compression<br>' +
        'and indexing technologies to deliver excellent query<br>' +
        'performance. All else being equal, query performance<br>' +
        'against a MOLAP store is significantly faster than against<br>' +
        'a HOLAP or ROLAP store.<br>' +
        '554<br>' +
        'Some people argue that MOLAP storage wastes disk<br>' +
        'space. You’ve already copied data at least once to store it<br>' +
        'in the relational data warehouse. Now you’re talking about<br>' +
        'copying it again for the cube storage. To this argument we<br>' +
        'reply that a relational index also copies data. No one<br>' +
        'asserts you shouldn’t index your relational database.<br>' +
        'MOLAP storage is highly efficient. The 20 percent rule<br>' +
        'says that leaf data in MOLAP mode (data and indexes)<br>' +
        'tends to require about 20 percent of the storage of the<br>' +
        'uncompressed relational source (data only, no indexes).<br>' +
        'The MOLAP store of the leaf data is probably smaller than<br>' +
        'a single relational index on the fact table — and buys a lot<br>' +
        'more for you than any single relational index can possibly<br>' +
        'do.<br>' +
        'Another common argument against MOLAP is that it<br>' +
        'slows processing. ROLAP is always the slowest to process<br>' +
        'because writing the aggregations to the relational database<br>' +
        'is expensive. HOLAP is slightly faster to process than<br>' +
        'MOLAP, but the difference is surprisingly small.<br>' +
        'Dimensions also can be stored in MOLAP or ROLAP<br>' +
        'mode. Use MOLAP.<br>' +
        'NOTE<br>' +
        'Why does Microsoft offer these alternative<br>' +
        'storage modes, if they’re inferior to<br>' +
        'MOLAP storage? The main answer is<br>' +
        '555<br>' +
        'marketing: It’s been an effective strategy to<br>' +
        'get Analysis Services into companies that<br>' +
        'would never countenance an instance of the<br>' +
        'SQL Server relational database in their data<br>' +
        'center. The multiple storage modes make<br>' +
        'the initial sale. Often, the DW/BI team<br>' +
        'figures out how much faster MOLAP is,<br>' +
        'and convinces management that it’s okay.<br>' +
        'It’s also a hedge against the future. It’s<br>' +
        'possible that improvements to relational<br>' +
        'technology can make HOLAP or ROLAP<br>' +
        'storage more appealing. Certainly we’d<br>' +
        'love to see Analysis Services’ indexing,<br>' +
        'aggregation, compression, and processing<br>' +
        'abilities integrated into the SQL Server<br>' +
        'RDBMS, as does appear to be happening.<br>' +
        'In Chapter 9 we back off slightly from the strong<br>' +
        'recommendation to use MOLAP in situations where you<br>' +
        'have a compelling business need for near-zero data<br>' +
        'latency. In this case, you may set up a ROLAP partition for<br>' +
        'data for the current hour or day.<br>' +
        'Developing the Partitioning Plan<br>' +
        'You can define multiple partitions for a measure group.<br>' +
        'Multiple partitions are a feature of SQL Server Enterprise<br>' +
        'Edition, and are very important for good query and<br>' +
        'processing performance for large Analysis Services<br>' +
        'measure groups. If you have a measure group with more<br>' +
        '556<br>' +
        'than 50 million rows, you probably should be using<br>' +
        'partitioning.<br>' +
        'Partitioning is vital for large measure groups because<br>' +
        'partitions can greatly help query performance. The<br>' +
        'Analysis Services query engine can selectively query<br>' +
        'partitions: It’s smart enough to access only the partitions<br>' +
        'that contain the data requested in a query. This difference<br>' +
        'can be substantial for a cube built on, say, a billion rows of<br>' +
        'data.<br>' +
        'The second reason partitioning is valuable is for<br>' +
        'management of a large cube. It’s faster to add a day’s<br>' +
        'worth of fact data to a small partition than to incrementally<br>' +
        'process that same day of data into a huge partition that<br>' +
        'contains all of history. With a small partition for current<br>' +
        'data, you have many more options to easily support<br>' +
        'real-time data delivery.<br>' +
        'Partitioning also makes it possible — even easy! — to set<br>' +
        'up your measure group to support a rolling window of<br>' +
        'data. For example, your users may want to keep only 90<br>' +
        'days of detailed data live in the cube. With a partitioned<br>' +
        'measure group, you simply drop the dated partitions. With<br>' +
        'a single partition, you have no option for deleting data<br>' +
        'other than reprocessing the entire measure group.<br>' +
        'If your ETL performs updates or deletes on fact table rows,<br>' +
        'partitioning is important for good processing performance.<br>' +
        'Recall from Chapter 7 that the only way to process an<br>' +
        'update or delete to a fact row is to fully process the<br>' +
        'partition that contains that row. This means that even if<br>' +
        'only one fact row in a measure group is updated, the entire<br>' +
        '557<br>' +
        'partition that contains that fact must be fully processed. If<br>' +
        'you have a large fact table that requires many updates,<br>' +
        'SSAS partitions are a necessity.<br>' +
        'Partitioning improves processing performance, especially<br>' +
        'for full processing of the measure group. Analysis Services<br>' +
        'automatically processes partitions in parallel.<br>' +
        'The largest measure groups, containing hundreds of<br>' +
        'millions or even billions of rows, should be partitioned<br>' +
        'along multiple dimensions — say by month and product<br>' +
        'category. More partitions are better for both query and<br>' +
        'processing performance, but at the cost of making your<br>' +
        'maintenance application more complicated.<br>' +
        'NOTE<br>' +
        'If you’ve defined a lot of partitions, say a<br>' +
        'thousand, Management Studio will be<br>' +
        'slow. It’ll take a few minutes to populate<br>' +
        'the lists of database objects.<br>' +
        'NOTE<br>' +
        'You can always set up partitioning on your<br>' +
        'development server because SQL Server<br>' +
        'Developer Edition contains all the<br>' +
        'functionality in Data Center Edition. But<br>' +
        '558<br>' +
        'those partitions won’t work on your<br>' +
        'production server if you use Standard<br>' +
        'Edition in production. As we described<br>' +
        'previously in this chapter, be sure to set the<br>' +
        'project’s Deployment Server Edition<br>' +
        'property, so Analysis Services can help you<br>' +
        'avoid features that won’t work in<br>' +
        'production.<br>' +
        'There are two ways to define multiple partitions:<br>' +
        '• Create multiple relational views, one for each partition. You<br>' +
        'can use multiple physical fact tables, but most people want<br>' +
        'an integrated relational fact table.<br>' +
        '• Define a WHERE clause in the SSAS source query for each<br>' +
        'partition, limiting each partition to the appropriate range in<br>' +
        'the single fact table. This is our recommended approach, as<br>' +
        'the alternative leads to a large number of otherwise useless<br>' +
        'relational views.<br>' +
        'Every measure group initially has one partition. To create<br>' +
        'additional partitions, you first need to explicitly change the<br>' +
        'scope of the original partition. Whether in BIDS or<br>' +
        'Management Studio, change the binding type on the<br>' +
        'original partition from Table Binding to Query Binding.<br>' +
        'Then you can add the WHERE clause. For a cube that’s<br>' +
        'partitioned by date, we usually change the WHERE clause on<br>' +
        'the first partition to span the period before the data in the<br>' +
        'fact table begins. In effect, it becomes an empty partition,<br>' +
        'there as a safeguard in case you unexpectedly get old data.<br>' +
        '559<br>' +
        'WARNING<br>' +
        'When you convert the original partition<br>' +
        'from Table Binding to Query Binding, you<br>' +
        'break its connection to the table in the Data<br>' +
        'Source View. If you modify the fact table<br>' +
        'structure in the DSV, you will need to<br>' +
        'remember to come into each partition and<br>' +
        'adjust the source query appropriately. This<br>' +
        'is yet another reason you should wait until<br>' +
        'the logical cube development is very nearly<br>' +
        'complete, before implementing<br>' +
        'partitioning.<br>' +
        'Once you’ve added a WHERE clause to the original<br>' +
        'partition, you can add new partitions. Although most<br>' +
        'partitioning plans are simple — the majority of them are<br>' +
        'monthly — partitioning plans can be complex. If you<br>' +
        'partition by multiple dimensions, say by product category<br>' +
        'and month, you can put each large category in its own<br>' +
        'monthly partition, and lump all the small product<br>' +
        'categories in a single monthly partition.<br>' +
        'WARNING<br>' +
        'You need to be very careful to define the<br>' +
        'WHERE clauses correctly. Analysis Services<br>' +
        'isn’t smart enough to see whether you’ve<br>' +
        '560<br>' +
        'skipped data or, even worse,<br>' +
        'double-counted it.<br>' +
        'When you’re working with your database in development,<br>' +
        'you set up the partitions by hand in BIDS or Management<br>' +
        'Studio. In test and production, you need to automate the<br>' +
        'process of creating a new partition and setting its source.<br>' +
        'These issues are discussed in Chapter 17.<br>' +
        'Designing Performance Aggregations<br>' +
        'The design goal for aggregations is to minimize the<br>' +
        'number of aggregations while maximizing their<br>' +
        'effectiveness. Effective aggregations will greatly improve<br>' +
        'query performance, but they’re not free. Each aggregation<br>' +
        'adds to the time it takes to process the cube, and the<br>' +
        'storage required. You don’t have to define every possible<br>' +
        'aggregation. At query time, Analysis Services will<br>' +
        'automatically choose the most appropriate (smallest)<br>' +
        'aggregation that can be used to answer the query.<br>' +
        'Aggregation design is a difficult problem, and Analysis<br>' +
        'Services provides several tools to help you out.<br>' +
        'Before we talk about designing aggregations, however, it’s<br>' +
        'worth pointing out that small cubes don’t really need<br>' +
        'aggregations at all. During your development cycle, you’re<br>' +
        'probably working with a small enough data set that you<br>' +
        'don’t need to worry about aggregations. Systems with<br>' +
        'small data volumes may not need to create any<br>' +
        'aggregations even in production. If you have only a<br>' +
        'hundred thousand rows in your measure group’s partition,<br>' +
        '561<br>' +
        'you don’t need to worry about aggregations. Even large<br>' +
        'enterprises can find that many of their measure groups —<br>' +
        'for example for quotas and financial data — are so small<br>' +
        'that aggregations are unnecessary.<br>' +
        'Large data volumes, of course, do require thoughtful<br>' +
        'aggregation design. You should be experimenting with<br>' +
        'different aggregation plans in the later part of your<br>' +
        'development cycle, when you start to work with realistic<br>' +
        'data volumes. During development, you can design<br>' +
        'aggregations within BIDS, by launching the Aggregation<br>' +
        'Design Wizard from the Partitions tab of the Cube<br>' +
        'Designer. Often, this task is deferred until the database has<br>' +
        'been deployed to the larger test server. Exactly the same<br>' +
        'functionality is available from SQL Server Management<br>' +
        'Studio.<br>' +
        'The Aggregation Design Wizard will design aggregations<br>' +
        'based on the cardinality of your data. It looks at your<br>' +
        'dimensions and figures out where aggregations are going<br>' +
        'to do the most good. As a simplification of what it’s doing,<br>' +
        'consider that aggregating data from a daily grain up to<br>' +
        'monthly creates an aggregation that’s one-thirtieth the size<br>' +
        'of the original data. (This isn’t strictly true, but suffices for<br>' +
        'explanatory purposes.) Summarizing from monthly to<br>' +
        'quarterly gives an aggregation that’s only one-third the<br>' +
        'size. The Aggregation Design Wizard has sophisticated<br>' +
        'algorithms to look at the intersection of hierarchical levels<br>' +
        'to figure out where aggregations are best built.<br>' +
        '562<br>' +
        'NOTE<br>' +
        'One of the most common physical design<br>' +
        'mistakes is to build too many aggregations.<br>' +
        'The first several aggregations provide a lot<br>' +
        'of benefit. After that, the incremental<br>' +
        'benefit at query time is slight; and the cost<br>' +
        'during processing time can grow<br>' +
        'substantially.<br>' +
        'Run the Aggregation Design Wizard only once you’ve<br>' +
        'loaded the full dimensions, usually in the test environment.<br>' +
        'The first time you run the Aggregation Wizard, it will<br>' +
        'connect to the relational database to count dimension<br>' +
        'attribute objects. Those counts are cached as metadata. If<br>' +
        'you ran the wizard against subset dimensions in the<br>' +
        'development database, you will need to flush the stored<br>' +
        'counts in order to gather new counts. Or, you can enter<br>' +
        'estimated counts manually. This is all very easy to do in<br>' +
        'the wizard.<br>' +
        'We recommend running the Aggregation Design Wizard at<br>' +
        '5–10% “performance improvement,” rather than the<br>' +
        'default 30% that shows up in the wizard. We think 30% is<br>' +
        'too high, and the SQL CAT team agrees with us, as you<br>' +
        'can see in their technical note (search www.sqlcat.com for<br>' +
        '“aggregation design strategy”).<br>' +
        'As clever as the Aggregation Design Wizard is, and as<br>' +
        'good as its recommendations are, it’s missing the most<br>' +
        'important information in aggregation design: usage. The<br>' +
        '563<br>' +
        'aggregations that you really want are those based on the<br>' +
        'queries that business users issue. Once the system is in<br>' +
        'production you can monitor what queries are issued<br>' +
        'against the system. Even during the test phase, you can<br>' +
        'build aggregations for the queries underlying standard<br>' +
        'reports and ad hoc usage of the system. Rely primarily on<br>' +
        'the Usage-Based Optimization Wizard for the ongoing<br>' +
        'design of SSAS aggregations. The Usage-Based<br>' +
        'Optimization Wizard is available from Management<br>' +
        'Studio.<br>' +
        'You have to perform a little bit of setup before you can use<br>' +
        'the Usage-Based Optimization Wizard. First, you need to<br>' +
        'turn on the query log, to collect information about which<br>' +
        'queries are being issued. Note that the query log used by<br>' +
        'the wizard is not the SQL Profiler query log that we will<br>' +
        'discuss in Chapter 17. Instead, this usage log is designed<br>' +
        'exclusively for use by this wizard. You can find the<br>' +
        'properties needed to turn on the usage log by right-clicking<br>' +
        'on the server name in Management Studio and choosing<br>' +
        'Properties. The TechNet article titled “Configuring the<br>' +
        'Analysis Services Query Log” provides detailed<br>' +
        'instructions.<br>' +
        'Once you’ve turned on the query log, you should let it run<br>' +
        'for a few days. The log is stored in a relational table,<br>' +
        'configured when you turn on the query log. Maintain a<br>' +
        'script that contains all the known queries from predefined<br>' +
        'reports, so that it’s easy to automatically run the known<br>' +
        'set. Delete from the query log any very fast queries, for<br>' +
        'example any query that takes less than a tenth of a second.<br>' +
        'There’s no point in designing aggregations for those<br>' +
        'queries.<br>' +
        '564<br>' +
        'Finally, run the Usage-Based Optimization Wizard from<br>' +
        'Management Studio, to design an effective set of<br>' +
        'aggregations for your system. Continue re-running the<br>' +
        'wizard weekly during the test phase, and into the first<br>' +
        'month or two of production. Ongoing, plan to evaluate<br>' +
        'aggregation design monthly or even quarterly.<br>' +
        'Planning for Deployment<br>' +
        'After you’ve developed the logical structure of your<br>' +
        'Analysis Services database using a subset of data, you<br>' +
        'need to process the full historical data set. This is unlikely<br>' +
        'to take place on the development server, unless your data<br>' +
        'volumes are quite small. Instead, you’ll probably perform<br>' +
        'full cube processing only on the test or production servers.<br>' +
        'In Chapter 4, we discussed some of the issues around<br>' +
        'where the Analysis Services data should be placed. We<br>' +
        'discussed using RAID and SAN arrays; you should have<br>' +
        'your hardware vendor assist with the detailed<br>' +
        'configuration of these technologies. Note that Solid State<br>' +
        'Drives are a strong candidate for Analysis Services<br>' +
        'database storage.<br>' +
        'One of the biggest questions, of course, is how to size the<br>' +
        'system. Earlier in this chapter, we mentioned the 20<br>' +
        'percent rule: The leaf level MOLAP data tends to take 20<br>' +
        'percent of the space required by the same data in the<br>' +
        'uncompressed relational database (data only, no indexes).<br>' +
        'Another equally broad rule of thumb is that aggregations<br>' +
        'double that figure, up to a total of 40 percent of the<br>' +
        'uncompressed relational data. This is still amazingly small.<br>' +
        '565<br>' +
        'NOTE<br>' +
        'We’ve seen only one or two cubes whose<br>' +
        'leaf data plus aggregations take up more<br>' +
        'space than the uncompressed relational<br>' +
        'data (data only, no indexes). The<br>' +
        'uncompressed relational data size is a<br>' +
        'reasonable upper bound to consider during<br>' +
        'the early stages of planning for Analysis<br>' +
        'Services.<br>' +
        'Your system administrators, reasonably enough, want a<br>' +
        'more accurate number than this 40 percent figure pulled<br>' +
        'from the air. We wish we could provide you with a sizing<br>' +
        'tool, but we don’t know how to solve that problem. Exact<br>' +
        'cube size depends on your logical design, data volumes,<br>' +
        'and aggregation plan. In practice, the way people perform<br>' +
        'system sizing for large Analysis Services databases is to<br>' +
        'process the dimensions and one or two partitions using the<br>' +
        'initial aggregation plan. The full system data requirements<br>' +
        'scale up the partitions linearly (ignoring the dimension<br>' +
        'storage, which usually rounds to zero in comparison to the<br>' +
        'partitions).<br>' +
        'When you’re planning storage requirements, it’s very<br>' +
        'important to understand that during processing, you’ll need<br>' +
        'temporary access to significant extra storage. Analysis<br>' +
        'Services is designed to remain available for queries while<br>' +
        'processing is under way. It does so by keeping a shadow<br>' +
        'copy of the portion of the SSAS database being processed.<br>' +
        'If you plan to process the entire database as one<br>' +
        '566<br>' +
        'transaction, you’ll need double the disk space during<br>' +
        'processing. Most very large SSAS databases are processed<br>' +
        'in pieces (each dimension, each measure group), in order<br>' +
        'to save disk space during processing.<br>' +
        'Processing the Full Cube<br>' +
        'In an ideal world, you’ll fully process a measure group<br>' +
        'only once, when you first move it into test and then<br>' +
        'production. But change is inevitable, and it’s pretty likely<br>' +
        'that one or more necessary changes to the database’s<br>' +
        'structure will require full reprocessing.<br>' +
        'You can fully process a measure group from within<br>' +
        'Management Studio: right-click the measure group and<br>' +
        'choose Process. Once you’re in production, this is not the<br>' +
        'best strategy for full processing. Instead, you should write<br>' +
        'a script or Integration Services package to perform the<br>' +
        'processing.<br>' +
        'NOTE<br>' +
        'Don’t get in the habit of using Management<br>' +
        'Studio to launch processing. This isn’t<br>' +
        'because of performance — exactly the<br>' +
        'same thing happens whether you use<br>' +
        'Management Studio, a script, or Integration<br>' +
        'Services. The difference is in a<br>' +
        'commitment to an automated and hands-off<br>' +
        '567<br>' +
        'production environment. These issues are<br>' +
        'discussed in Chapter 17.<br>' +
        'We recommend that you use Integration Services to<br>' +
        'perform measure group processing. This is especially true<br>' +
        'if you’re using Integration Services for your ETL system,<br>' +
        'as you’ll have a logging and auditing infrastructure in<br>' +
        'place. Integration Services includes a task to perform<br>' +
        'Analysis Services processing.<br>' +
        'If you’re not using SSIS as your ETL tool, you can still<br>' +
        'automate cube processing by writing a script. There are<br>' +
        'two approaches for writing a script: XMLA (XML for<br>' +
        'Analysis) or AMO (Analysis Management Objects). We<br>' +
        'find AMO easier to work with than XMLA, but exactly the<br>' +
        'same thing happens under the covers, no matter which<br>' +
        'interface you use.<br>' +
        'Developing the Incremental Processing Plan<br>' +
        'Long before you put your Analysis Services database into<br>' +
        'production, you need to develop a plan for keeping it<br>' +
        'up-to-date. There are several ways to do this. Scheduled<br>' +
        'processing will continue to be the most common method of<br>' +
        'processing.<br>' +
        'NOTE<br>' +
        '568<br>' +
        'The alternative to scheduled processing is<br>' +
        'to use the SSAS feature called Proactive<br>' +
        'Caching. Proactive Caching is a feature<br>' +
        'that lets your cube monitor the source<br>' +
        'database — the relational data warehouse<br>' +
        'in this case — and automatically kick off<br>' +
        'processing when new data is detected. In<br>' +
        'the vast majority of cases, you know<br>' +
        'exactly when you added data to the data<br>' +
        'warehouse. It’s more straightforward<br>' +
        'simply to tack on cube processing at the<br>' +
        'end of the ETL job, rather than use<br>' +
        'Proactive Caching. We discuss Proactive<br>' +
        'Caching in Chapter 9 when we describe<br>' +
        'low latency scenarios.<br>' +
        'Scheduled Processing<br>' +
        'Scheduled processing of Analysis Services objects is<br>' +
        'basically a pull method of getting data into the cube. On a<br>' +
        'schedule, or upon successful completion of an event like a<br>' +
        'successful relational data warehouse load, launch a job to<br>' +
        'pull the data into the cube. In this section we provide an<br>' +
        'overview of techniques. We go into greater detail in<br>' +
        'Chapter 17.<br>' +
        'Full Reprocessing<br>' +
        'The simplest strategy is to perform full processing every<br>' +
        'time you want to add data. We’re surprised by how many<br>' +
        'people choose this strategy, which is akin to fully<br>' +
        '569<br>' +
        'reloading the data warehouse on every load cycle. It’s<br>' +
        'certainly the easiest strategy to implement. Analysis<br>' +
        'Services performs processing efficiently, so this approach<br>' +
        'can perform tolerably well for monthly or weekly load<br>' +
        'cycles — or even daily for small measure groups.<br>' +
        'The same script or Integration Services package that you<br>' +
        'used for the initial population of the cube can be<br>' +
        're-executed as needed. If full processing completes within<br>' +
        'the ETL load window, it’s the simplest approach and<br>' +
        'there’s no need to develop an incremental processing plan.<br>' +
        'Incremental Processing<br>' +
        'If your data volumes are large enough that full processing<br>' +
        'is not desirable, the next obvious choice is to schedule<br>' +
        'incremental processing.<br>' +
        'Incremental dimension processing is straightforward and<br>' +
        'can be scripted in the same way as database, cube, or<br>' +
        'measure group full processing. In Chapter 7, we<br>' +
        'recommend that you create an Integration Services<br>' +
        'package for each dimension table. You can add the<br>' +
        'Analysis Services processing task to each dimension’s<br>' +
        'package, to automatically start dimension processing when<br>' +
        'the corresponding relational dimension has successfully<br>' +
        'loaded. Alternatively, wait until the relational work is done<br>' +
        'and process all Analysis Services objects together in a<br>' +
        'single transaction.<br>' +
        'Incremental measure group processing is more<br>' +
        'complicated than dimension processing because you must<br>' +
        'design your system so you process only new data. Analysis<br>' +
        '570<br>' +
        'Services doesn’t check to make sure you’re not<br>' +
        'inadvertently adding data twice or skipping a set of data.<br>' +
        'The best way to identify the incremental rows to be<br>' +
        'processed is to tag all fact table rows with an audit key, as<br>' +
        'we describe in Chapter 7. All rows that were added today<br>' +
        '(or this hour, or this month) are tied together by the audit<br>' +
        'key. Now, you just need to tell Analysis Services which<br>' +
        'batch to load. You can write a simple program that would<br>' +
        'redefine a view of the fact table that filters to the current<br>' +
        'batch. Or, define a static metadata-driven view definition<br>' +
        'that points Analysis Services to the fact rows that haven’t<br>' +
        'been loaded yet.<br>' +
        'NOTE<br>' +
        'It’s tempting to use the transaction date as<br>' +
        'the filter condition for the view or query<br>' +
        'that finds the current rows. In the real<br>' +
        'world, we usually see data flowing in for<br>' +
        'multiple days, so we tend to prefer the<br>' +
        'audit key method. If you’re sure that can’t<br>' +
        'happen in your environment, you can use<br>' +
        'the transaction date.<br>' +
        'As before, once your view of the fact table has been<br>' +
        'redefined (if necessary) to filter only the new rows, it’s<br>' +
        'simple to launch measure group processing from a script or<br>' +
        'package.<br>' +
        '571<br>' +
        'NOTE<br>' +
        'Every time you incrementally process a<br>' +
        'partition, it gets a bit fragmented. If you<br>' +
        'rely primarily on incremental processing,<br>' +
        'you should fully process occasionally. For<br>' +
        'daily processing, monthly full reprocessing<br>' +
        'should be fine.<br>' +
        'Incremental Processing with Multiple Partitions<br>' +
        'If you’re using SQL Server Data Center or Enterprise<br>' +
        'Edition and are partitioning your measure groups, your<br>' +
        'incremental processing job is even more challenging. First,<br>' +
        'you need to be sure that you create a new partition or<br>' +
        'partition set before you need it. In other words, if you’re<br>' +
        'partitioning your measure group by month, then each<br>' +
        'month you need to create a new partition designed to hold<br>' +
        'the new month’s data.<br>' +
        'NOTE<br>' +
        'There’s no harm in creating twelve<br>' +
        'monthly partitions in advance. But you still<br>' +
        'need to add some logic to your Integration<br>' +
        'Services package, to be sure you march<br>' +
        'through the partitions as each new month<br>' +
        'begins.<br>' +
        '572<br>' +
        'Make sure that the source query for the measure group’s<br>' +
        'incremental processing has two filters:<br>' +
        '• Grab only the rows that are new.<br>' +
        '• Grab only the rows that belong in this partition.<br>' +
        'This is particularly tedious if you have late-arriving facts<br>' +
        '— in other words, if today’s load can include data for<br>' +
        'transactions that occurred a long time ago. If this is the<br>' +
        'case, you’ll need to set up a more complicated Integration<br>' +
        'Services package. Query the data loaded today to find out<br>' +
        'which partitions you’ll need to incrementally process;<br>' +
        'define a loop over those time periods.<br>' +
        'REFERENCE<br>' +
        'SQL Server includes a sample Integration<br>' +
        'Services package that manages Analysis<br>' +
        'Services partitions. Explore and leverage<br>' +
        'this excellent sample. It’s installed by<br>' +
        'default at C:\\Program<br>' +
        'Files\\Microsoft SQL<br>' +
        'Server\\100\\Samples\\Integration<br>' +
        'Services\\Package<br>' +
        'Samples\\SyncAdvWorksPartitions<br>' +
        'Sample.<br>' +
        'Planning for Updates to Dimensions<br>' +
        'Updates to dimensions generally happen gracefully and<br>' +
        'automatically in the dimension incremental processing.<br>' +
        '573<br>' +
        'The easiest kind of dimension update is a type 2 dimension<br>' +
        'change. Analysis Services treats a type 2 change as a new<br>' +
        'dimension member: It has its own key, which the database<br>' +
        'has never seen before. From the point of view of<br>' +
        'dimension processing, there’s no way to distinguish<br>' +
        'between a new set of attributes for an existing customer<br>' +
        'and a new row for a new customer.<br>' +
        'Type 1 changes are picked up during dimension<br>' +
        'processing. Type 1 changes are potentially expensive<br>' +
        'because if any aggregation is defined on the type 1<br>' +
        'attribute, that aggregation must be rebuilt. Analysis<br>' +
        'Services drops the entire affected aggregation, but it can<br>' +
        're-compute it as a background process.<br>' +
        'Specify whether or not to compute aggregations in the<br>' +
        'background by setting the ProcessingMode property of the<br>' +
        'dimension. The ProcessingMode can be:<br>' +
        '• Regular: During processing, the leaf-level data plus any<br>' +
        'aggregations and indexes are computed before the processed<br>' +
        'cube is published to users.<br>' +
        '• LazyAggregations: Aggregations and indexes are computed<br>' +
        'in the background, and new data is made available to the<br>' +
        'users as soon as the leaf-level data is processed. This sounds<br>' +
        'great, but it can be problematic for query performance,<br>' +
        'depending on the timing of the processing. You want to<br>' +
        'avoid a situation where many users are querying a large cube<br>' +
        'at a time when that cube has no indexes or aggregations in<br>' +
        'place because it’s performing background processing.<br>' +
        'Most dimensions should be defined to use lazy<br>' +
        'aggregations, combined with the processing option to<br>' +
        'Process Affected Objects. This processing option ensures<br>' +
        '574<br>' +
        'that indexes and aggregations are rebuilt as part of the<br>' +
        'incremental processing transaction.<br>' +
        'The place to worry about a type 1 change is if you have<br>' +
        'declared the attribute to have a rigid relationship with<br>' +
        'another attribute: in other words, if you have declared<br>' +
        'there will never be a type 1 change on the attribute. You do<br>' +
        'want to use rigid attribute relationships because they<br>' +
        'provide substantial processing performance benefits. But if<br>' +
        'you try to change the attribute, Analysis Services will raise<br>' +
        'an error.<br>' +
        'Deleting a dimension member is impossible, short of fully<br>' +
        'reprocessing the dimension. Fully reprocessing a<br>' +
        'dimension requires that any cubes using this dimension<br>' +
        'also be fully reprocessed. If you must delete dimension<br>' +
        'members, the best approach is to create a type 1 attribute<br>' +
        'to flag whether the dimension member is currently active,<br>' +
        'and to filter those dimension members out of most reports<br>' +
        'and queries. Monthly or annually, fully reprocess the<br>' +
        'database.<br>' +
        'Planning for Fact Updates and Deletes<br>' +
        'The best source for an Analysis Services cube is a ledgered<br>' +
        'fact table. A ledgered fact table handles updates to facts by<br>' +
        'creating an offsetting transaction to zero out the original<br>' +
        'fact, then inserting a corrected fact row. This ledgering<br>' +
        'works smoothly for the associated Analysis Services<br>' +
        'measure group, because the ledger entries are treated as<br>' +
        'new facts.<br>' +
        '575<br>' +
        'Sometimes it’s not that easy. How do you plan for the<br>' +
        'situation where you mess up and somehow incorrectly<br>' +
        'assign a bunch of facts to the wrong dimension member?<br>' +
        'The only solution — unless you want to ledger out the<br>' +
        'affected rows — is to fully reprocess the affected partition.<br>' +
        'NOTE<br>' +
        'Multiple partitions are starting to sound<br>' +
        'like a really good idea.<br>' +
        'There are several kinds of deleted data. The simplest,<br>' +
        'where you roll off the oldest month or year of fact data, is<br>' +
        'easily handled with a partitioned measure group. Just<br>' +
        'delete the partition by right-clicking it and choosing Delete<br>' +
        'in Management Studio or, more professionally, by<br>' +
        'scripting that action.<br>' +
        'The only way to delete a fact row is to fully process the<br>' +
        'partition that contains that row. The scenario is fairly<br>' +
        'common: You inadvertently load the same data twice into<br>' +
        'the relational database. It’s unpleasant but certainly<br>' +
        'possible to back out that load from the relational tables,<br>' +
        'especially if you use the auditing system described in<br>' +
        'Chapter 7. But within Analysis Services, you must fully<br>' +
        'reprocess the affected partition.<br>' +
        'NOTE<br>' +
        '576<br>' +
        'As we describe in Chapter 7, your ETL<br>' +
        'system should perform reasonableness<br>' +
        'checks to ensure you’re not double-loading.<br>' +
        'If you have late-arriving facts, where<br>' +
        'you’re writing data to multiple partitions<br>' +
        'during each day’s load, you’ll be especially<br>' +
        'motivated to develop a solid ETL system.<br>' +
        'Summary<br>' +
        'Analysis Services is one of the key components of the<br>' +
        'Microsoft Business Intelligence technologies. It’s a solid,<br>' +
        'scalable OLAP server that you can use as the primary or<br>' +
        'only query engine for even the largest DW/BI system. In<br>' +
        'Chapter 11, we discuss the new PowerPivot functionality<br>' +
        'of Analysis Services, which enables power users to define<br>' +
        'their own cubes and powerful analyses. In Chapter 13, we<br>' +
        'discuss how to use Analysis Services to build a data<br>' +
        'mining application.<br>' +
        'In this chapter, you learned:<br>' +
        '• After you build a conformed dimensional relational data<br>' +
        'warehouse database, building the Analysis Services database<br>' +
        'is relatively easy.<br>' +
        '• The tools and wizards in the BI Studio give you a good head<br>' +
        'start on the logical design of your cube database.<br>' +
        '• There’s still lots of editing to do when you’ve finished the<br>' +
        'wizards. The more challenging your application — in data<br>' +
        'volumes or complexity — the more careful you need to be in<br>' +
        'your logical and physical design choices.<br>' +
        '577<br>' +
        '• There’s still more work to do to integrate cube processing<br>' +
        'with the ETL system.<br>' +
        '• You must set attribute relationships correctly to get good<br>' +
        'query performance. The single most important Analysis<br>' +
        'Services feature of SQL Server Enterprise and Data Center<br>' +
        'Editions is measure group partitioning. Partitioning greatly<br>' +
        'improves the query and processing performance of your<br>' +
        'database, and provides greater flexibility in system<br>' +
        'management. These benefits come with the fairly substantial<br>' +
        'cost of increased system complexity.<br>' +
        '• Some of the interesting features of Analysis Services, such<br>' +
        'as KPIs and Actions, require support from client software in<br>' +
        'order to be useful additions to your system. The Microsoft<br>' +
        'tools (Excel, ReportBuilder, and SharePoint) generally<br>' +
        'support these SSAS features.<br>' +
        '• Most features, like calculations, storage mode, advanced<br>' +
        'processing techniques, and even translations, are available to<br>' +
        'even the simplest Analysis Services client software.<br>' +
        'Analysis Services is a complex piece of software. In this<br>' +
        'chapter, we presented only the bare essentials of the<br>' +
        'information necessary for you to be successful. The<br>' +
        'Analysis Services expert on your team should plan to<br>' +
        'purchase a few additional books devoted to the subject of<br>' +
        'Analysis Services and MDX.<br>' +
        '578<br>';
    document.getElementById('chapter7').innerHTML =
        'Chapter 7<br>' +
        'Designing and Developing the ETL System<br>' +
        'Measure twice; cut once.<br>' +
        'Some people like to plan, specify, and document systems;<br>' +
        'most don’t. We’ve observed that Extract, Transformation,<br>' +
        'and Load (ETL) system development draws folks in the<br>' +
        'latter category. We’ve found so few people who write<br>' +
        'adequate design specifications for their ETL systems that<br>' +
        'we’ve practically stopped asking to see our clients’<br>' +
        'planning documents. Either they’re unaware of the<br>' +
        'impending complexity, or they don’t have the planning<br>' +
        'tools.<br>' +
        'In this chapter, we begin by discussing the steps you need<br>' +
        'to take before you start development of your ETL system:<br>' +
        'Round up your requirements and write an ETL plan. Next,<br>' +
        'we briefly introduce SQL Server Integration Services<br>' +
        '(SSIS) and present some of the basic concepts and<br>' +
        'vocabulary of that product.<br>' +
        'Most of this chapter is structured around the 34 subsystems<br>' +
        'of a well designed ETL system. We describe each of the<br>' +
        '34 subsystems and recommend alternative approaches for<br>' +
        'implementing them within SSIS. Throughout, we will refer<br>' +
        'to the Adventure Works Cycles case study.<br>' +
        'As Figure 7-1 illustrates and common sense dictates, the<br>' +
        'ETL portion of the project is part of the data track.<br>' +
        'Remember, however, that these boxes aren’t to scale. The<br>' +
        '367<br>' +
        'ETL effort is the most time-consuming step in the data<br>' +
        'track and often in the entire project.<br>' +
        'In this chapter, you learn:<br>' +
        '• How to plan for the ETL system, including the components<br>' +
        'of a solid ETL design specification, and how to create this<br>' +
        'document.<br>' +
        '• What SQL Server Integration Services is and its role in the<br>' +
        'DW/BI system. You’ll receive an overview of its most<br>' +
        'important features for ETL system design.<br>' +
        '• The components and functionality of a well designed ETL<br>' +
        'system, grouped into 34 subsystems.<br>' +
        '• How to implement those subsystems in the SSIS<br>' +
        'environment.<br>' +
        'Figure 7-1: The Business Dimensional Lifecycle<br>' +
        'RESOURCES<br>' +
        '368<br>' +
        'This chapter describes the basic<br>' +
        'components of an ETL system and how to<br>' +
        'go about implementing a production<br>' +
        'quality system in SSIS. For additional<br>' +
        'detailed guidance about the ETL<br>' +
        'subystems, we refer you to The Data<br>' +
        'Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition. We will provide page references<br>' +
        'for more information on specific concepts<br>' +
        'and techniques throughout this chapter.<br>' +
        'Round Up the Requirements<br>' +
        'Establishing the architecture of your ETL system begins<br>' +
        'with one of the toughest challenges: rounding up the<br>' +
        'requirements. By this we mean gathering and<br>' +
        'understanding all the known requirements, realities, and<br>' +
        'constraints affecting the ETL system. The list of<br>' +
        'requirements can be overwhelming, but it’s essential to lay<br>' +
        'them on the table before launching into the development of<br>' +
        'your system.<br>' +
        'The ETL system requirements are mostly constraints you<br>' +
        'must live with and adapt to. ETL system requirements<br>' +
        'should not drive or even affect the target dimensional<br>' +
        'model. And the ETL developers seldom have any ability to<br>' +
        'substantially affect the design or operation of the source<br>' +
        'transaction systems. The job of the ETL system is to<br>' +
        'bridge those two immovable objects. Within the<br>' +
        'framework of these requirements, there are opportunities<br>' +
        'to exercise your judgment and leverage your creativity, but<br>' +
        '369<br>' +
        'the requirements dictate the core elements that your ETL<br>' +
        'system must deliver. These elements include:<br>' +
        '• Business needs: Wise ETL designers maintain a dialogue<br>' +
        'with the business community. Often, we learn during<br>' +
        'development that the source data doesn’t match our<br>' +
        'expectations. The business requirements, rather than the<br>' +
        'whim of the developer, should dictate how these issues are<br>' +
        'resolved. Make sure the ETL designers and developers have<br>' +
        'good lines of communication with the team members who<br>' +
        'best understand the users’ requirements.<br>' +
        '• Compliance: Changing legal and reporting requirements<br>' +
        'have forced many organizations to tighten their reporting<br>' +
        'and provide proof of report accuracy. Of course, DW/BI<br>' +
        'systems in regulated industries have complied with<br>' +
        'regulatory requirements for years. But the tenor of financial<br>' +
        'reporting has become more rigorous for everyone. Typical<br>' +
        'due diligence requirements for the ETL system include:<br>' +
        '• Saving archived copies of source data.<br>' +
        '• Providing proof of the complete ETL flow.<br>' +
        '• Fully documenting algorithms for allocations,<br>' +
        'adjustments, and derivations.<br>' +
        '• Supplying proof of security of the data copies over<br>' +
        'time.<br>' +
        '• Data quality: Business users demand quality data. They are<br>' +
        'increasingly using data rather than intuition to run their<br>' +
        'businesses. Regulation and compliance issues also heighten<br>' +
        'the demand for good data. Unfortunately most business users<br>' +
        'have no idea what data problems exist, where they originate,<br>' +
        'or how hard it is to identify and fix them. The ETL team<br>' +
        'needs to be agile and proactive:<br>' +
        '• Push back hard on the transaction systems to clean<br>' +
        'data at the source.<br>' +
        '• Implement a master data management system to<br>' +
        'fix data before it enters the ETL stream.<br>' +
        '370<br>' +
        '• Decide what can and should be fixed in ETL based<br>' +
        'on business requirements, not on developer<br>' +
        'convenience.<br>' +
        '• Enlist the user community as partners to identify<br>' +
        'and prioritize data faults. Ideally, this would be<br>' +
        'part of an overall data governance/data<br>' +
        'stewardship program.<br>' +
        '• Data latency: Data latency describes how quickly the source<br>' +
        'system data must be delivered to the business users via the<br>' +
        'DW/BI system. As we discuss in Chapter 9, most DW/BI<br>' +
        'systems process data on a daily basis; we expect this to<br>' +
        'continue to be true for the near future. As far as ETL is<br>' +
        'concerned, the basic system architecture can remain<br>' +
        'unchanged for populating data several times a day. If you are<br>' +
        'planning to deliver DW/BI data with very low latency<br>' +
        '(seconds or minutes), your ETL system and data warehouse<br>' +
        'database architectures may profoundly change.<br>' +
        '• Archiving: Archiving requirements are driven by auditing<br>' +
        'and compliance demands. Depending on your architecture,<br>' +
        'other reasons to store copies of extracted and staged data<br>' +
        'include:<br>' +
        '• Maintain extracted data for restarting the ETL<br>' +
        'process after a failure.<br>' +
        '• Determine historical changed rows in the absence<br>' +
        'of a reliable source of changed data from the<br>' +
        'transaction system.<br>' +
        '• Lineage: Lineage requirements, like archiving requirements,<br>' +
        'are driven by auditing and compliance demands. But<br>' +
        'business users are also interested in lineage. They’d like to<br>' +
        'look at a number in a report and be able to learn exactly how<br>' +
        'and when it entered the data warehouse, and what<br>' +
        'transformations occurred to it along the way. Delivering<br>' +
        'lineage information to business users in the context of a<br>' +
        'report is a bit impractical today because Microsoft doesn’t<br>' +
        'provide a tool to make it easy. Nonetheless, the ETL system<br>' +
        'design should strive to make lineage as transparent as<br>' +
        'possible.<br>' +
        '371<br>' +
        '• Cube processing: If your DW/BI system architecture<br>' +
        'includes Analysis Services cubes, the ETL job flow includes<br>' +
        'cube processing. If you’re not using Analysis Services, your<br>' +
        'architecture probably includes summary tables (also called<br>' +
        'aggregates) in the relational database. In this case, the ETL<br>' +
        'job flow includes updating the summary tables. In sum, the<br>' +
        'ETL job flow includes all the processing steps necessary to<br>' +
        'present a complete, consistent set of data to the business user<br>' +
        'community.<br>' +
        '• Available skills: The most important experience for the ETL<br>' +
        'team on the SQL Server platform is to have good system<br>' +
        'development skills, knowing how to develop as part of a<br>' +
        'team, being able to write and execute comprehensive unit<br>' +
        'tests, and to build resiliency and redundancy into the ETL<br>' +
        'system. Of course, good SQL skills are required. And every<br>' +
        'production quality SSIS ETL system will need a little bit of<br>' +
        'scripting in a scripting language (VB or C#). Most ETL<br>' +
        'systems do not use any coding beyond SQL and some simple<br>' +
        'scripting; the rest is provided by SSIS. It is helpful to have at<br>' +
        'least one team member who already has experience with<br>' +
        'SSIS 2005 or later, but many teams learn as they go.<br>' +
        'Develop the ETL Plan<br>' +
        'Before you begin the ETL system design for populating a<br>' +
        'dimensional model, you should have completed the logical<br>' +
        'dimensional model, drafted your high-level architecture<br>' +
        'plan, and drafted the source to target mapping for all data<br>' +
        'elements. The physical design and implementation work<br>' +
        'described in Chapters 4 and 5 should be well under way.<br>' +
        'The ETL system design process is critical. You should<br>' +
        'make some key architectural design decisions up front, and<br>' +
        'design all the components of your system in a consistent<br>' +
        'way. Any deviations from the standard pattern should be<br>' +
        'briefly justified and fully documented.<br>' +
        '372<br>' +
        'Start the design process with a simple schematic of the<br>' +
        'pieces of the plan that you know: sources and targets. Keep<br>' +
        'it high level, highlighting in one or two pages the data<br>' +
        'sources and annotating the major challenges that you<br>' +
        'already know about.<br>' +
        'Next, develop a schematic for each table in the target<br>' +
        'dimensional model, graphically diagramming the complex<br>' +
        'restructurings. Where the high level plan graphics can fit<br>' +
        'all the target tables in a business process onto a single page<br>' +
        '(more or less), the detailed plan graphics may devote a<br>' +
        'page or more to each complex target table. This detailed<br>' +
        'schematic is backed up with a few pages of discussion and<br>' +
        'pseudo-code for any truly complex derivations.<br>' +
        'Figure 7-2 illustrates what we mean. This schematic<br>' +
        'describes the ETL flow for part of the customer dimension<br>' +
        'of the Adventure Works data warehouse. The SSIS<br>' +
        'package for this part of the customer dimension will follow<br>' +
        'the flow of this schematic very closely.<br>' +
        'NOTE<br>' +
        'We use some shorthand notation in our<br>' +
        'detailed schematics. These include:<br>' +
        '• (+) indicates we can’t rely on the source<br>' +
        'system for referential integrity, and must<br>' +
        'use some kind of outer join technique with<br>' +
        'defaults for the missing values.<br>' +
        '• SCD(2) indicates the business users have<br>' +
        'told us these attributes should be managed<br>' +
        'as type 2.<br>' +
        '373<br>' +
        '• Hist indicates the question or issue is<br>' +
        'related to the one-time historical load.<br>' +
        '• Incr indicates the question or issue is<br>' +
        'related to the ongoing incremental load.<br>' +
        'RESOURCES<br>' +
        'See The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, pages 428–436 for more<br>' +
        'information about developing the ETL<br>' +
        'plan.<br>' +
        'Figure 7-2: Example draft of detailed load schematic for<br>' +
        'the customer dimension<br>' +
        '374<br>' +
        'The ETL specification document should include:<br>' +
        '• Default strategies for the major subsystems, including data<br>' +
        'extract, archiving, data quality tracking, and dimension<br>' +
        'attribute change management<br>' +
        '• High level schematics<br>' +
        '• Table design, detailed source to target mappings, and data<br>' +
        'profile reports<br>' +
        '• Detailed table-level schematics<br>' +
        'DOWNLOAD<br>' +
        'The book’s web site at<br>' +
        'http://kimballgroup.com/html/<br>' +
        'booksMDWTtools.html contains a sample<br>' +
        'outline ETL specification document.<br>' +
        'Introducing SQL Server Integration Services<br>' +
        'Before we dive into the details of ETL system design on<br>' +
        'the Microsoft platform, we present an overview of SQL<br>' +
        'Server 2008 R2 Integration Services. The goal of the<br>' +
        'introduction is to familiarize you with SSIS so that you can<br>' +
        'understand its features and grow comfortable with its<br>' +
        'vocabulary. This overview is not a tutorial on Integration<br>' +
        'Services; we’re focusing more on the “what and why” of<br>' +
        'the tool than on the “how.”<br>' +
        'SQL Server 2008 Release 2 Changes<br>' +
        '375<br>' +
        'SQL Server 7.0 and SQL Server 2000 included a<br>' +
        'product called Data Transformation Services, DTS<br>' +
        'for short. SQL Server 2005 introduced Integration<br>' +
        'Services (SSIS), which was effectively a new<br>' +
        'product. When you move from the old DTS to<br>' +
        'SSIS you should completely redesign your ETL<br>' +
        'system. Moving from SSIS 2005 to SSIS 2008<br>' +
        'requires a simple, relatively painless upgrade<br>' +
        'process. Moving from SSIS 2008 to SSIS 2008 R2<br>' +
        'requires no change.<br>' +
        'Your ETL team will develop SSIS packages to populate<br>' +
        'your DW/BI system. A package is analogous to a<br>' +
        'computer program or a script. You can execute a package<br>' +
        'to perform a database maintenance task, to load a table, or<br>' +
        'to populate an entire business process dimensional model.<br>' +
        'An Integration Services package contains a single control<br>' +
        'flow which contains one or more tasks. In the ETL<br>' +
        'application you’ll use a handful of control flow tasks, like<br>' +
        'manipulating files, sending email to an operator, or<br>' +
        'executing a SQL statement. By far the most interesting<br>' +
        'control flow task is called the data flow task, in which<br>' +
        'most of the real ETL work is done. We’ll talk a lot more<br>' +
        'about the data flow task later in this chapter.<br>' +
        'Prepare the Development Environment<br>' +
        '376<br>' +
        'There are a few characteristics of the development<br>' +
        'environment, discussed in Chapter 4, which we’ll<br>' +
        'briefly repeat here.<br>' +
        '• Install SSIS and SQL Server BI Development Studio<br>' +
        'on a shared machine. Plan for developers to remote<br>' +
        'desktop into that shared server for their ETL<br>' +
        'development. If developers use BI Studio locally,<br>' +
        'then during their development and testing the<br>' +
        'packages will run on their local machines. If your<br>' +
        'development database is large, you may be moving a<br>' +
        'lot of data to the developer’s desktop.<br>' +
        '• Install the BIDSHelper add on, which is available for<br>' +
        'free download from CodePlex (www.codeplex.com).<br>' +
        'It includes many features to help you develop SSIS<br>' +
        'packages. BIDSHelper affects only the development<br>' +
        'environment; it contains no code that runs in<br>' +
        'production.<br>' +
        '• Most development teams store packages in the file<br>' +
        'system during development, and use source control to<br>' +
        'check packages in and out.<br>' +
        '• For initial package development, especially for the<br>' +
        'packages for the one-time historic load, it’s useful to<br>' +
        'work from a static copy of the source database(s).<br>' +
        'Control Flow and Data Flow<br>' +
        'There are two major design surfaces in SSIS: control flow<br>' +
        'and data flow. They look similar — rectangles connected<br>' +
        'by arrows on a pale yellow background — but they are<br>' +
        'quite different. Control flow is where the basic logic of the<br>' +
        'package is defined, and a package has only one control<br>' +
        'flow. Most ETL packages consist of the following<br>' +
        'components, as illustrated in Figure 7-3.<br>' +
        '377<br>' +
        '• A few control flow tasks to define variables, setup auditing<br>' +
        'metadata, and so on. Usually these tasks consist of Execute<br>' +
        'SQL statements and scripts.<br>' +
        '• One or more data flow tasks to perform the heavy lifting of<br>' +
        'the ETL.<br>' +
        '• A few control flow tasks to clean up the package.<br>' +
        'Figure 7-3: Viewing a control flow<br>' +
        'There are dozens of control flow tasks available. Of these,<br>' +
        'you will heavily use:<br>' +
        '• Execute SQL<br>' +
        '• Execute package<br>' +
        '• Data flow<br>' +
        '• Script<br>' +
        'You will undoubtedly use some of the other types of tasks<br>' +
        'as well.<br>' +
        '378<br>' +
        'NOTE<br>' +
        'The Execute SQL task, like many other<br>' +
        'tasks and objects in Integration Services,<br>' +
        'uses an interface such as OLE DB or<br>' +
        'ADO.NET to execute SQL even against<br>' +
        'SQL Server. You may find that a statement<br>' +
        'that you develop and test in Management<br>' +
        'Studio will generate an error inside the<br>' +
        'Execute SQL task. Errors are most<br>' +
        'common if your T-SQL scripts include<br>' +
        'parameters.<br>' +
        'Data Flow<br>' +
        'The data flow task is a pipeline in which data is picked up,<br>' +
        'processed, and written to a destination. The key<br>' +
        'characteristic of the pipeline is defined by the task’s name:<br>' +
        'The data flows through the pipeline in memory. An<br>' +
        'implication of the data flow pipeline architecture is that<br>' +
        'avoiding I/O provides excellent performance, subject to<br>' +
        'the memory characteristics of the physical system.<br>' +
        'In the control flow design surface, the data flow task looks<br>' +
        'like any other task. It’s unique in that if you double-click<br>' +
        'on the data flow task, you switch tabs to the data flow<br>' +
        'design surface where you can view and edit the many steps<br>' +
        'of the data flow, as illustrated in Figure 7-4.<br>' +
        'Figure 7-4: Viewing a data flow<br>' +
        '379<br>' +
        'The appearance and content of data flow tasks varies<br>' +
        'widely, depending on the work that they need to perform.<br>' +
        'Most data flow tasks will include:<br>' +
        '• One or more data sources: Think of a data source as a query<br>' +
        'that brings a set of data into the data flow. Data flows out of<br>' +
        'a data flow source.<br>' +
        '• Multiple transformation steps: Common transformations<br>' +
        'derive new columns, perform lookups to database tables,<br>' +
        'combine multiple data flow sources, and split one data flow<br>' +
        'into multiple streams. Data flows into and out of a data flow<br>' +
        'transform.<br>' +
        '• One or more data destinations: The data destination is<br>' +
        'typically the data warehouse relational table or staging table<br>' +
        'that the data is being written to. Data flows into a data flow<br>' +
        'destination; it’s the end of the line.<br>' +
        'Error Flows<br>' +
        '380<br>' +
        'Most of the data flow sources, targets, and transformations<br>' +
        'support two kinds of output flows: normal flows (which<br>' +
        'appear in green on the design surface) and error flows (in<br>' +
        'red). The error flow from a flat file source would include<br>' +
        'any rows with a type mismatch, for example string data in<br>' +
        'a numeric field. The error flow from an OLE DB<br>' +
        'destination would include any rows that violate table<br>' +
        'constraints. The error flow is a data flow just like the<br>' +
        'normal flow. You can transform the data to fix the error,<br>' +
        'and hook it back up with the normal flow. You can write<br>' +
        'the error flow to a table or file.<br>' +
        'The most important characteristic of the error flow<br>' +
        'construct is that an error in some data rows does not halt<br>' +
        'the entire data flow step unless you design the flow to do<br>' +
        'so. And, you can perform arbitrarily complex<br>' +
        'transformations to the error flow, to correct the data or to<br>' +
        'log the bad rows for a person to examine and fix.<br>' +
        'Error flows are arguably the single most valuable feature<br>' +
        'of SSIS.<br>' +
        'WARNING<br>' +
        'By default, all steps in the data flow are set<br>' +
        'up to fail (and therefore halt the process) if<br>' +
        'an error is encountered. To make use of<br>' +
        'error flows, you’ll need to modify this<br>' +
        'default behavior by setting up an error flow<br>' +
        '381<br>' +
        'and changing the error flow characteristics<br>' +
        'of the transform.<br>' +
        'Database Engine versus Data Flow Pipeline<br>' +
        'The data flow pipeline is a distinct execution<br>' +
        'engine from the relational database engine. When a<br>' +
        'data flow task executes, SSIS picks up a batch of<br>' +
        'data, buffers it, and operates on that batch. If you<br>' +
        'watch a package execute in BI Development<br>' +
        'Studio, you may notice SSIS picking up a second<br>' +
        'batch at the top while the bottom of the flow is still<br>' +
        'working on the first batch.<br>' +
        'At design time it feels like you’re designing<br>' +
        'something that operates row by row, which sets off<br>' +
        'warning bells to experienced ETL developers.<br>' +
        'Experienced ETL developers know that for good<br>' +
        'processing performance, you need to operate on<br>' +
        'data in batch. The data flow pipeline does operate<br>' +
        'on batches of data, and is designed to be intelligent<br>' +
        'and perform well.<br>' +
        'We’ve reviewed many SSIS implementations that<br>' +
        'do most of the ETL in the query embedded in the<br>' +
        'data flow source. At its extreme, this approach<br>' +
        'performs all the work in SQL, and uses SSIS only<br>' +
        'as a framework to glue scripts together. At this<br>' +
        '382<br>' +
        'extreme, the data flow tasks consist only of a<br>' +
        'source and a target, with no transformation steps<br>' +
        'between. Some vendors call this architecture<br>' +
        '“ELT,” meaning that the data is extracted (E),<br>' +
        'immediately loaded (L) into a relational table, and<br>' +
        'then all the heavy transforming (T) is done by<br>' +
        'complex SQL commands.<br>' +
        'We prefer to use SSIS as it has been designed. We<br>' +
        'recommend keeping source queries simple, and<br>' +
        'using data flow transforms to do most of the<br>' +
        'transformation work. The advantages of this<br>' +
        'approach include:<br>' +
        '• Error flows: Leverage the power of the error flow<br>' +
        'architecture to handle errors elegantly, within a single<br>' +
        'pass over the data.<br>' +
        '• Readability and maintainability: A recent client had<br>' +
        'source queries that combined 30 tables. Pity the<br>' +
        'future developer who is charged with maintaining<br>' +
        'that SQL query!<br>' +
        '• Transparency of lineage: Recall our desire for a user<br>' +
        'to click on a number in Excel and see a report<br>' +
        'describing exactly how that data got into the data<br>' +
        'warehouse. If all the transformation logic is<br>' +
        'embedded in a SQL statement, we’ll never be able to<br>' +
        'track the detailed lineage in any useful or readable<br>' +
        'way.<br>' +
        '• Comparable performance: It’s impossible to make a<br>' +
        'blanket statement that SSIS data flow pipeline is<br>' +
        'faster or that SQL is faster. It depends on too many<br>' +
        'variables. We have observed roughly comparable<br>' +
        'performance from well designed packages using the<br>' +
        'two approaches.<br>' +
        '383<br>' +
        'This preference for transforming within SSIS<br>' +
        'rather than SQL does not mean blind adherence.<br>' +
        'The SQL Server relational engine is a powerful<br>' +
        'tool. Throughout this chapter we’ll highlight<br>' +
        'several places where you can save yourself a lot of<br>' +
        'pain — and greatly improve performance — by<br>' +
        'staging data to a table and using the power of the<br>' +
        'relational engine.<br>' +
        'SSIS Package Architecture<br>' +
        'The standard ETL system design approach in SSIS is to<br>' +
        'develop a separate package to load each table, and a master<br>' +
        'package that calls each of these table-specific packages.<br>' +
        'The master package contains one Execute Package task for<br>' +
        'each child package. Many of the dimension packages can<br>' +
        'execute in parallel, but some dimension tables may have a<br>' +
        'dependency on another table. Use control flow precedence<br>' +
        'arrows to define these dependencies.<br>' +
        'A master package can use package configurations to pass<br>' +
        'variables to the child package. Variables you might want<br>' +
        'to coordinate between all packages include auditing<br>' +
        'information and the date range for which the current set of<br>' +
        'packages is to be run.<br>' +
        'When the child package finishes, it returns control to the<br>' +
        'master package, along with an indicator of whether the<br>' +
        'package succeeded or failed. You must design the logic for<br>' +
        'whether the master package should continue loading other<br>' +
        '384<br>' +
        'tables or stop all work when it receives notification of<br>' +
        'child package failure.<br>' +
        'WARNING<br>' +
        'Other than success or failure, there is no<br>' +
        'direct mechanism for a child package to<br>' +
        'communicate to the master package. It can<br>' +
        'be useful for the child package to<br>' +
        'communicate an error reason (if any),<br>' +
        'count of rows loaded, and so on. The only<br>' +
        'way to do this is to have the child package<br>' +
        'write the information — usually to a table<br>' +
        '— and have the master package read that<br>' +
        'same information.<br>' +
        'The Major Subsystems of ETL<br>' +
        'Now that you have an understanding of the existing<br>' +
        'requirements, realities, and constraints; a commitment to<br>' +
        'design before you develop; and a basic understanding of<br>' +
        'SSIS; it’s time to introduce the critical subsystems that<br>' +
        'form the architecture for every ETL system. Although<br>' +
        'we’ve adopted the industry ETL acronym to describe these<br>' +
        'steps, the process really has four major components:<br>' +
        '• Extracting: Gathering raw data from the source systems and<br>' +
        'usually writing it to disk in the ETL environment before any<br>' +
        'significant restructuring of the data takes place. Subsystems<br>' +
        '1 through 3 support the extracting process.<br>' +
        '• Cleaning and conforming: Sending source data through a<br>' +
        'series of processing steps in the ETL system to improve the<br>' +
        '385<br>' +
        'quality of the data received from the source, and merging<br>' +
        'data from two or more sources to create and enforce<br>' +
        'conformed dimensions and conformed metrics. Subsystems<br>' +
        '4 through 8 describe the architecture required to support the<br>' +
        'cleaning and conforming processes.<br>' +
        '• Delivering: Applying the standard dimensional constructs<br>' +
        'such as surrogate keys, attribute change tracking, and fact<br>' +
        'table key substitution. Physically structuring and loading the<br>' +
        'data into the presentation server’s target dimensional<br>' +
        'models. Subsystems 9 through 21 provide the capabilities for<br>' +
        'delivering the data to the presentation server.<br>' +
        '• Managing: Managing the related systems and processes of<br>' +
        'the ETL environment in a coherent manner. Subsystems 22<br>' +
        'through 34 describe the components needed to support the<br>' +
        'ongoing management of the ETL system.<br>' +
        'Our descriptions of these four major components, and the<br>' +
        '34 subsystems they encompass, may be familiar to readers<br>' +
        'of other Kimball Group books such as The Data<br>' +
        'Warehouse Lifecycle Toolkit and The Kimball Group<br>' +
        'Reader. In this chapter, we’ve provided just enough<br>' +
        'description of the subsystems to provide context for the<br>' +
        'descriptions of how to implement those subsystems in the<br>' +
        'Microsoft toolset.<br>' +
        'Extracting Data<br>' +
        'To no surprise, the initial subsystems of the ETL<br>' +
        'architecture address the issues of understanding your<br>' +
        'source data, extracting the data, and transferring it to the<br>' +
        'data warehouse environment where SSIS can work on it<br>' +
        'independent of the operational systems.<br>' +
        'Subsystem 1: Data Profiling<br>' +
        '386<br>' +
        'Data profiling is the technical analysis of data to describe<br>' +
        'its content, consistency, and structure. In some sense, any<br>' +
        'time you perform a SELECT DISTINCT investigative query<br>' +
        'on a database field, you’re doing data profiling.<br>' +
        'As we’ve already discussed in Chapter 2, data profiling<br>' +
        'must begin as soon as you identify a possible data source.<br>' +
        'Serious profiling occurs during the dimensional model<br>' +
        'design process, as you develop the source to target map<br>' +
        'that describes exactly how each data warehouse column is<br>' +
        'populated. Any holes in your early data profiling must be<br>' +
        'plugged now.<br>' +
        'SSIS contains a Data Profiling task and viewer that you<br>' +
        'can run against your source databases. That feature,<br>' +
        'combined with the interactive reports described in Chapter<br>' +
        '2, inexpensively meets most data profiling requirements.<br>' +
        'In addition, many teams are finding the PowerPivot feature<br>' +
        'of Excel described in Chapter 11 to be an effective ad-hoc<br>' +
        'data profiling tool. This certainly isn’t the main use for<br>' +
        'which PowerPivot was developed, but it’s a great way to<br>' +
        'poke around and get a feel for the structure of the data in<br>' +
        'the source systems.<br>' +
        'WARNING<br>' +
        'ETL development is too late to start<br>' +
        'profiling! If you’ve waited to begin<br>' +
        'profiling your source system data until<br>' +
        'you’re starting ETL development, you are<br>' +
        '387<br>' +
        'virtually guaranteed to uncover roadblocks<br>' +
        'that you cannot maneuver around. If you’re<br>' +
        'not already deeply familiar with the source<br>' +
        'systems and data, drop everything else until<br>' +
        'you are comfortable that you actually can<br>' +
        'populate your dimensional database with<br>' +
        'the data available today.<br>' +
        'Subsystem 2: Change Data Capture System<br>' +
        'During the data warehouse’s initial historic load, capturing<br>' +
        'incremental source data content changes is not important<br>' +
        'because you’re loading all data from a point in time<br>' +
        'forward. However, many data warehouse tables are so<br>' +
        'large they cannot be refreshed during every ETL cycle.<br>' +
        'You must have a capability to transfer only the relevant<br>' +
        'changes to the source data since the last update. Isolating<br>' +
        'the latest source data is called change data capture.<br>' +
        'There is no one-size-fits-all solution to change data<br>' +
        'capture: the best solution varies from source to source. The<br>' +
        'most common approaches include:<br>' +
        '• Audit columns: Many source systems include audit columns<br>' +
        'that store the date and time a record was added or most<br>' +
        'recently modified. For daily processing, a new row is<br>' +
        'identified as where the date part of the inserted timestamp<br>' +
        'equals the date part of the modified timestamp.<br>' +
        'WARNING<br>' +
        '388<br>' +
        'If you’re using source system audit<br>' +
        'columns, make sure they are reliable. Often<br>' +
        'these columns are maintained by<br>' +
        'application logic, and a batch operation by<br>' +
        'a database administrator might not<br>' +
        'maintain the audit columns. Even if the<br>' +
        'columns are correctly maintained by<br>' +
        'database constraints or triggers, we’ve seen<br>' +
        'the DBAs turn off those constraints before<br>' +
        'performing a batch operation. There’s no<br>' +
        'perfect solution; the best is to have detailed<br>' +
        'written procedures for DBAs to follow<br>' +
        'whenever they touch the transaction<br>' +
        'database.<br>' +
        '• Triggers: A trigger is an obvious solution to the problem of<br>' +
        'identifying changed rows in the source database. It’s not a<br>' +
        'perfect solution, however — it suffers the same fragility as the<br>' +
        'audit column technique. We’ve seen triggers used most often to<br>' +
        'capture deletes from transaction systems that perform hard<br>' +
        'deletes, but use audit columns for inserts and updates. A hard<br>' +
        'delete is when a data row is physically deleted, as opposed to<br>' +
        'being marked as inactive. In most systems, deletes are relatively<br>' +
        'unusual, so you can overcome DBAs’ resistance to triggers by<br>' +
        'confining them to deletes.<br>' +
        '• Replication: Replication is a time honored technique for<br>' +
        'capturing the change data stream. Set up replication on the<br>' +
        'source database to see changes flow to the replicated database.<br>' +
        'Replication alone doesn’t solve the problem, as it creates a copy<br>' +
        'of the replicated elements. But sometimes you can create<br>' +
        'triggers on the replica, when those same triggers are forbidden<br>' +
        'on the transaction system.<br>' +
        '389<br>' +
        '• Change Data Capture: If your source transaction system is<br>' +
        'implemented in SQL Server 2008, the new change data capture<br>' +
        '(CDC) feature is the obvious solution. Like replication, CDC<br>' +
        'works from the SQL Server logs. When you set up CDC on a<br>' +
        'source system table, it will accumulate an image of all new or<br>' +
        'deleted rows, and the before and after image of any changed<br>' +
        'rows. You can track a subset of the table’s columns. In most<br>' +
        'cases with daily processing you’ll want only the end of day<br>' +
        'image of the updates, which is easy to find. Be aware that you<br>' +
        'will need to prune the CDC tables periodically, and that pruning<br>' +
        'should be synchronized with your ETL job stream.<br>' +
        '• Change Tracking: The Change Tracking feature is very similar<br>' +
        'to CDC, but it identifies only the keys for changed rows, not the<br>' +
        'full text of the changes. For that reason, it uses fewer system<br>' +
        'resources at the time the transaction is entered, at the cost of a<br>' +
        'more expensive query during the ETL process.<br>' +
        'RESOURCES<br>' +
        'There is a Books Online topic that clearly<br>' +
        'describes how to access CDC data from<br>' +
        'SSIS. Look for the topic “Improving<br>' +
        'Incremental Loads with Change Data<br>' +
        'Capture.”<br>' +
        'Change Data Capture versus Replication<br>' +
        'If you’re trying to decide whether to use SQL<br>' +
        'Server Replication or Change Data Capture to<br>' +
        'identify changed rows in your source system, the<br>' +
        'answer is CDC. Replication is a great feature with<br>' +
        '390<br>' +
        'many uses, and it may still have a place in your<br>' +
        'overall architecture. But if CDC is available to you,<br>' +
        'it’s the right choice for capturing changed data.<br>' +
        'Using replication to identify changed rows is a<br>' +
        'trick to overcome poorly designed transaction<br>' +
        'systems that don’t have modified date audit<br>' +
        'columns. You add that column to the replication<br>' +
        'target and add a trigger to maintain it. Replication<br>' +
        'lets you circumvent a proscription against<br>' +
        'modifying the transaction database itself.<br>' +
        'CDC by contrast delivers exactly what you want:<br>' +
        'an image of new, deleted, and changed rows;<br>' +
        'accurate timestamps of the event; and an indicator<br>' +
        'of what the event was (insert, update, delete).<br>' +
        'Both replication and CDC work from the SQL<br>' +
        'Server logs. You can have replication and CDC<br>' +
        'active on the same table.<br>' +
        '• Non SQL Server techniques: If your source database is not<br>' +
        'SQL Server, there may still be non-Microsoft tools available<br>' +
        'to help you identify the change data stream. Other relational<br>' +
        'database engines have their own versions of replication and<br>' +
        'CDC. And there are third party “log scraper” tools that may<br>' +
        'meet your requirements.<br>' +
        '• Full “diff compare”: A full diff compare keeps a snapshot of<br>' +
        'yesterday’s data and compares it, record by record, against<br>' +
        'today’s data to find what changed. This technique is very<br>' +
        'resource intensive. Investigate using cyclic redundancy<br>' +
        'checksum (CRC) or hash algorithms to quickly tell if a<br>' +
        '391<br>' +
        'complex record has changed. Full diff compare is the<br>' +
        'solution of last resort, especially for fact tables and very<br>' +
        'large dimension tables. The only sensible place to identify<br>' +
        'new and changed rows is in the source system itself.<br>' +
        'Subsystem 3: Extract System<br>' +
        'Extracting data from the source systems is a fundamental<br>' +
        'component of the ETL architecture. If you’re lucky, all<br>' +
        'your source data is in a single system that can be readily<br>' +
        'extracted using SSIS. More commonly, each source is in a<br>' +
        'different system, environment, and/or DBMS.<br>' +
        'For each source system, identify the best way to<br>' +
        'implement the extract. Except in the simplest cases, you’re<br>' +
        'unlikely to come up with a single solution for all sources.<br>' +
        'Organizations that need to extract data from mainframe<br>' +
        'environments often run into issues involving COBOL<br>' +
        'copybooks, EBCDIC to ASCII conversions, packed<br>' +
        'decimals, and multiple and variable record types. Older<br>' +
        'legacy systems may require the use of different procedural<br>' +
        'languages. Although you might be able to write or<br>' +
        'purchase an SSIS task or transform to read an idiosyncratic<br>' +
        'source, it’s usually easiest to have older source systems<br>' +
        'push the data for you into a flat file.<br>' +
        'Most source data is stored in databases, usually relational<br>' +
        'databases. Integration Services can access a wide variety<br>' +
        'of databases through its OLE DB and .NET providers,<br>' +
        'including the .NET provider for ODBC. SQL Server ships<br>' +
        'with providers for all sorts of Microsoft formats, including<br>' +
        'Excel, Access, and even Analysis Services. It includes<br>' +
        'providers for Oracle. Other providers are available from<br>' +
        'database vendors or third party vendors.<br>' +
        '392<br>' +
        'NOTE<br>' +
        'Providers are not created equal. If you’re<br>' +
        'accessing data from a non-Microsoft<br>' +
        'database, you should evaluate a variety of<br>' +
        'providers. If you do a web search on<br>' +
        '“oracle ole db providers” you’ll see<br>' +
        'conflicting anecdotes about which provider<br>' +
        'is faster. As far as we can tell, there is no<br>' +
        'clear winner. As ever, “it depends.”<br>' +
        'If you’re having trouble getting good performance between<br>' +
        'SSIS and a non-Microsoft data source, consider pushing<br>' +
        'the data from the source system into flat files. Let SSIS<br>' +
        'begin its work with the flat files rather than touching the<br>' +
        'source database directly.<br>' +
        'The extract system should be separate from the<br>' +
        'transformation and delivery systems. In other words, you<br>' +
        'should create separate SSIS packages for the extract that<br>' +
        'simply pull the appropriate data from the source systems<br>' +
        'and stage it, untransformed, in a staging database or flat<br>' +
        'files. If you’re using a push technique, you already have<br>' +
        'those extracts as flat files.<br>' +
        'WARNING<br>' +
        '393<br>' +
        'Many SSIS demos and examples work<br>' +
        'directly from transaction databases rather<br>' +
        'than use the separate extract-to-stage<br>' +
        'design pattern that we advocate. That’s fine<br>' +
        'for demos, but there are excellent reasons<br>' +
        'to decouple the extract from the<br>' +
        'transformation and load:<br>' +
        '• Reduce the connection time into the source database. This can<br>' +
        'be a very important issue for large data volumes, creating<br>' +
        'significant pressure on the source database logs while the<br>' +
        'package is running.<br>' +
        '• Provide a consistent restart point in the event of a failure. Not<br>' +
        'only do you avoid touching the source database again, but also<br>' +
        'you have the consistent image as of the extract date and time.<br>' +
        '• Maintain the untransformed data for auditing purposes. Your<br>' +
        'internal auditors will love you.<br>' +
        'It may seem that decoupling extract from transformation<br>' +
        'and load is a lot of busy work. As with much of the<br>' +
        'Kimball Group’s advice, it’s a recommendation based on<br>' +
        'experience: any time we’ve skipped this step, we’ve come<br>' +
        'to regret it.<br>' +
        'Figure 7-5 illustrates the data flow for the extract of a<br>' +
        'single table. The source query is trivial, performing no<br>' +
        'joins, transformations, or even column name changes. The<br>' +
        'staging table is structured the same as in the source<br>' +
        'database.<br>' +
        'The untransformed extracts should be archived for at least<br>' +
        'a week. Many organizations archive the extracts for a<br>' +
        'month or even forever.<br>' +
        '394<br>' +
        'Figure 7-5: A simple extract data flow<br>' +
        'Loading Data<br>' +
        'The point at which you load data into a database table —<br>' +
        'whether a staging table as in Figure 7-5, or a real data<br>' +
        'warehouse target table — is a potential bottleneck for the<br>' +
        'performance of the ETL application. You want to load data<br>' +
        'as fast as possible, but you also don’t want the load to fail<br>' +
        'because of one bad row in a billion.<br>' +
        'As we discuss in Chapter 5, SQL Server can load data<br>' +
        'quite fast, but certain conditions must be in place for the<br>' +
        'fastest possible loads:<br>' +
        '• Slow: Row-by-row or very small batches<br>' +
        '• Medium: Reasonable sized batches (1000, 10k, 100k),<br>' +
        'depending on your system<br>' +
        '395<br>' +
        '• Fast: Large batches plus the target table either has no data or<br>' +
        'no indexes (or both)<br>' +
        'We can meet the conditions for fast bulk load during the<br>' +
        'initial extract to staging tables, and for the one-time<br>' +
        'historical load into the data warehouse tables. But medium<br>' +
        'bulk load is still pretty good, and a lot faster than row by<br>' +
        'row. The problem with medium bulk load is that if we<br>' +
        'encounter a bad row — violating a constraint in the target<br>' +
        'table — the entire batch is rejected.<br>' +
        'Our recommended design pattern for loading data is<br>' +
        'illustrated in Figure 7-5. First, always build and test your<br>' +
        'packages so that you test and fix data before inserting. In<br>' +
        'other words, a design goal is to create an insert stream into<br>' +
        'the target table that will insert without errors. That said,<br>' +
        'one of our mantras is Bad Things Happen, so design a<br>' +
        'failsafe.<br>' +
        'With SSIS, our favorite feature of error flows provides that<br>' +
        'failsafe. Build in an error flow from all of your destination<br>' +
        'adaptors. The first destination (labeled Stage SpecialOffer<br>' +
        'in Figure 7-5) uses large batches. If there are no bad rows<br>' +
        'in a batch, all the data is inserted with good performance.<br>' +
        'However, any batch that contains an error row has all its<br>' +
        'rows flow into the error flow (labeled Error Batch). We<br>' +
        'immediately attempt to insert those rows again, this time<br>' +
        'one-by-one. The first and second destination transforms<br>' +
        'are identical, except the first one uses a batch (in this case,<br>' +
        '1,000 rows), and the second inserts row by row.<br>' +
        'Finally, we must do something with the true error rows.<br>' +
        'There might be only one error, or perhaps many. In this<br>' +
        'case, we’re writing the error rows into a raw file.<br>' +
        '396<br>' +
        'NOTE<br>' +
        'A raw file is a data file stored in a format<br>' +
        'unique to SSIS. It’s effectively the memory<br>' +
        'image of the data flow written down onto<br>' +
        'disk. There are several advantages and<br>' +
        'disadvantages of raw files:<br>' +
        '• Good: It is a fast write, because there is no<br>' +
        'conversion between the memory data<br>' +
        'stream and the disk file.<br>' +
        '• Good: It won’t throw a data type error,<br>' +
        'because there’s no conversion.<br>' +
        '• Bad: It can only be written onto the server<br>' +
        'where SSIS is running.<br>' +
        '• Bad: It can only be read by an SSIS<br>' +
        'package. However, there are inexpensive<br>' +
        'third-party utilities that will read raw files<br>' +
        'for you.<br>' +
        'Our sample packages make ample use of<br>' +
        'raw files, but that’s merely to make it easy<br>' +
        'to install the sample packages. Most<br>' +
        'organizations write data to structured flat<br>' +
        'files instead of using raw files, but raw files<br>' +
        'are a good choice in the real world if you<br>' +
        'are strongly constrained by performance.<br>' +
        'If you’re working with large data volumes, you may create<br>' +
        'larger batches, say of 100,000 rows. In that case, you can<br>' +
        'design a cascade of successively smaller batches on the<br>' +
        'error flows before you implement the row by row insert as<br>' +
        'the final error step.<br>' +
        '397<br>' +
        'Figure 7-6 illustrates how to configure a data flow<br>' +
        'destination with batches, whose controls are revealed by<br>' +
        'selecting Table or view - fast load in the Data access mode<br>' +
        'box. Figure 7-7 shows a data flow destination that will<br>' +
        'load row by row when the Data access mode is set to Table<br>' +
        'or view.<br>' +
        'Figure 7-6: SSIS Destination Editor, batches<br>' +
        'Figure 7-7: SSIS Destination Editor, row by row<br>' +
        '398<br>' +
        'Cleaning and Conforming Data<br>' +
        'Cleaning and conforming data are critical ETL system<br>' +
        'tasks. These are the steps where the ETL system adds<br>' +
        'value to the data. The other activities, extracting and<br>' +
        'delivering data, are obviously necessary, but they simply<br>' +
        'move and load the data. The cleaning and conforming<br>' +
        'subsystems actually change data and enhance its value to<br>' +
        'the organization. In addition, these subsystems can be<br>' +
        'designed to create metadata used to diagnose problems<br>' +
        'with the source systems. Such diagnoses should eventually<br>' +
        'lead to business process reengineering initiatives to<br>' +
        'address the root causes of dirty data and improve data<br>' +
        'quality over time.<br>' +
        'Subsystem 4: Data Cleaning System<br>' +
        '399<br>' +
        'The ETL data cleaning process is often expected to fix<br>' +
        'dirty data, yet provide an accurate picture of the data as it<br>' +
        'was captured by the organization’s production systems.<br>' +
        'Striking the proper balance between these conflicting goals<br>' +
        'is essential.<br>' +
        'The heart of the ETL architecture is a set of quality screens<br>' +
        'that act as diagnostic filters in the data flow pipelines.<br>' +
        'Each quality screen is a test. If the test against the data is<br>' +
        'successful, nothing happens and the screen has no effect.<br>' +
        'But if the test fails, the package must either attempt to fix<br>' +
        'the data, tag the data, or halt the process. The SSIS error<br>' +
        'flow is well suited for this architecture.<br>' +
        'There are three categories of quality screens:<br>' +
        '• Column screens: Test the data within a single column, for<br>' +
        'example for nulls, data type, or range violations.<br>' +
        '• Structure screens: Test data relationships, for example<br>' +
        'lookup failures.<br>' +
        '• Business rule screens: Test complex business logic, for<br>' +
        'example requiring that a Platinum customer has a high<br>' +
        'lifetime value and has made a purchase in the last two years.<br>' +
        'Business rule screens often require a historical time series or<br>' +
        'comparison to aggregate data, and as such may be run on a<br>' +
        'periodic (perhaps monthly) basis on data already in the data<br>' +
        'warehouse.<br>' +
        'RESOURCES<br>' +
        'You can learn more about data quality and<br>' +
        'data screens by reading:<br>' +
        '400<br>' +
        '• The Data Warehouse ETL Toolkit, pages<br>' +
        '131–147<br>' +
        '• The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, pages 381–383.<br>' +
        '• Data Quality: The Accuracy Dimension,<br>' +
        'Jack Olson (Morgan Kaufmann, 2002).<br>' +
        'In most cases you want to identify data problems and fix<br>' +
        'them as quickly and smoothly as possible. Only in the<br>' +
        'most extreme cases would you want to halt processing.<br>' +
        'Most of the “pull the plug” tests that we build into the ETL<br>' +
        'process are actually looking for incomplete or corrupted<br>' +
        'extracts.<br>' +
        'Cleaning Data in the Data Flow<br>' +
        'The data flow step is the logical place to perform data<br>' +
        'cleaning operations. Recall from earlier in this chapter that<br>' +
        'we advocate a separate package for the extract of each<br>' +
        'table. The data flow for the cleaning step will consist of a<br>' +
        'data source, several transformation steps, and then a load<br>' +
        'into the destination table.<br>' +
        'The vast majority of the data cleaning screens and<br>' +
        'subsequent transformations are column screens. Of these,<br>' +
        'we see many examples of data type conversions and filling<br>' +
        'in null values. Checking for and fixing up range or domain<br>' +
        'violations is less common, as even the simplest transaction<br>' +
        'system data entry interfaces tend to do an adequate job of<br>' +
        'this task.<br>' +
        'Within the SSIS framework, there are two main methods<br>' +
        'for fixing the column screen violations:<br>' +
        '401<br>' +
        '• Make conversions within the source query, using for<br>' +
        'example the ISNULL or CAST function.<br>' +
        '• Implement conversions explicitly using SSIS data flow<br>' +
        'transforms such as the derived column transform.<br>' +
        'We have a slight preference for the second approach, but<br>' +
        'in most cases either approach is just fine. Many ETL<br>' +
        'developers prefer to implement column screen transforms<br>' +
        'in SQL because they are comfortable with SQL. Our<br>' +
        'reasons for preferring the alternative approach include:<br>' +
        '• Readability: Transforms are explicitly called out and<br>' +
        'graphically displayed.<br>' +
        '• Error event logging: As we discuss in the next section, you<br>' +
        'may decide to log data quality screening errors in an error<br>' +
        'event schema. Usually you’d do this only if required for<br>' +
        'compliance purposes. If you’re logging quality screen errors,<br>' +
        'it’s more elegantly done by using SSIS transforms, because<br>' +
        'you can perform the test, log the error, and fix the data in a<br>' +
        'single pass over the row. Not only is the SSIS transform<br>' +
        'solution more elegant, but it also should perform better than<br>' +
        'the SQL approach when you’re logging errors.<br>' +
        'The second most common type of data quality screen and<br>' +
        'data cleaning are the structure screens, for example<br>' +
        'translating a code into a more readable text string. It’s<br>' +
        'surprisingly common for decode tables to be missing<br>' +
        'values. It seems even with the best source systems, there’s<br>' +
        'always some historical data floating around that violates<br>' +
        'the rules for full and complete text descriptions of all<br>' +
        'coded values.<br>' +
        'As with the column screens, there are two methods for<br>' +
        'implementing structure screens and transformations: SQL<br>' +
        'and SSIS transforms. If you implement a structure screen<br>' +
        'and cleanup in SQL, you’d write your source SQL<br>' +
        '402<br>' +
        'statement with an outer join to the decode table, and fill in<br>' +
        'any missing values with a default. Figure 7-8 illustrates a<br>' +
        'data flow task that uses the SQL-based approach. It has a<br>' +
        'relatively complex source query and a simple data flow<br>' +
        'structure. This is the clean and transform package for the<br>' +
        'product dimension, whose logical flow was outlined in<br>' +
        'Figure 7-2.<br>' +
        'Figure 7-8: Using SQL to join and clean data<br>' +
        'All the work in the package illustrated in Figure 7-8 occurs<br>' +
        'in the SQL query in the source adapter:<br>' +
        'SELECT [ProductID] AS ProductSKU<br>' +
        ',p.[Name] AS ProductName<br>' +
        ',p.[Name] AS ProductDescr<br>' +
        '403<br>' +
        ',ISNULL(m.[ProductModelID],0) AS ProductModelID<br>' +
        ',ISNULL(m.[Name], N\'Unknown Model\') AS ProductModel<br>' +
        ',p.[ProductSubcategoryID]<br>' +
        ',s.[Name] AS ProductSubcategory<br>' +
        ',s.[ProductCategoryID]<br>' +
        ',c.[Name] AS ProductCategory<br>' +
        ',CASE p.ProductLine<br>' +
        'WHEN \'T\' THEN N\'Touring\'<br>' +
        'WHEN \'M\' THEN N\'Mountain\'<br>' +
        'WHEN \'R\' THEN N\'Road\'<br>' +
        'WHEN \'S\' THEN N\'Accessory\'<br>' +
        'ELSE N\'Bike part\'<br>' +
        'END AS ProductLine<br>' +
        ',ISNULL(p.Color, N\'None\') AS [Color]<br>' +
        ',CASE p.[Class]<br>' +
        'WHEN \'H\' THEN N\'High\'<br>' +
        'WHEN \'M\' THEN N\'Medium\'<br>' +
        'WHEN \'L\' THEN N\'Low\'<br>' +
        'ELSE N\'No product class\'<br>' +
        'END AS [Class]<br>' +
        ',CASE p.[Style]<br>' +
        'WHEN \'M\' THEN N\'Men\'<br>' +
        'WHEN \'W\' THEN N\'Women\'<br>' +
        '404<br>' +
        'WHEN \'U\' THEN N\'Unisex\'<br>' +
        'ELSE N\'No product style\'<br>' +
        'END AS [Style]<br>' +
        ',CASE p.[FinishedGoodsFlag]<br>' +
        'WHEN 1 THEN N\'Finished good\'<br>' +
        'ELSE N\'Unfinished good\'<br>' +
        'END AS IsFinishedGood<br>' +
        ',CASE p.[Size]<br>' +
        'WHEN \'L\' THEN N\'Large\'<br>' +
        'WHEN \'M\' THEN N\'Medium\'<br>' +
        'WHEN \'S\' THEN N\'Small\'<br>' +
        'ELSE ISNULL(p.[Size], N\'N/A\')<br>' +
        'END AS [Size]<br>' +
        ',cast(ISNULL(p.[SizeUnitMeasureCode], N\'N/A\') AS NCHAR(5))<br>' +
        'AS SizeUnitMeasureCode<br>' +
        ',p.[Weight]<br>' +
        ',CASE p.[WeightUnitMeasureCode]<br>' +
        'WHEN \'LB\' THEN N\'pound\'<br>' +
        'WHEN \'G\' then N\'gram\'<br>' +
        'ELSE ISNULL(p.[WeightUnitMeasureCode], N\'N/A\')<br>' +
        'END AS WeightUnitMeasureCode<br>' +
        ',p.[DaysToManufacture]<br>' +
        ',p.[StandardCost]<br>' +
        '405<br>' +
        ',p.[ListPrice]<br>' +
        ',p.[SafetyStockLevel]<br>' +
        ',p.[ReorderPoint]<br>' +
        ',p.[SellStartDate]<br>' +
        ',p.[SellEndDate]<br>' +
        ',CASE<br>' +
        'WHEN SellEndDate IS NULL THEN N\'Current\'<br>' +
        'ELSE N\'Discontinued\'<br>' +
        'END AS ProductCurrentStatus<br>' +
        'FROM [StageProduct] p<br>' +
        'LEFT OUTER JOIN StageProductSubcategory s ON<br>' +
        '(p.ProductSubcategoryID=s.ProductSubcategoryID)<br>' +
        'LEFT OUTER JOIN StageProductCategory c ON<br>' +
        '(s.ProductCategoryID=c.ProductCategoryID)<br>' +
        'LEFT OUTER JOIN StageProductModel m ON<br>' +
        '(p.ProductModelID=m.ProductModelID)<br>' +
        'The data flow displayed in Figure 7-8 is simple, but the<br>' +
        'mess is swept under the rug and hidden in the SQL query<br>' +
        'embedded in the source adapter.<br>' +
        'The alternative approach is to use a lot of SSIS data flow<br>' +
        'transforms. Begin with a source query that simply pulls the<br>' +
        'StageProduct table, without performing any joins or<br>' +
        'transformations. Set up a Lookup transform for each code<br>' +
        'lookup. Transform the stream of lookup failures, logging<br>' +
        '406<br>' +
        'to the error event schema if required. Then union the error<br>' +
        'flow back into the main stream and implement the next<br>' +
        'lookup. Figure 7-9 illustrates the same ETL process as<br>' +
        'Figure 7-8, but uses the SSIS transform approach. As you<br>' +
        'can see, the data flow design surface is filled with<br>' +
        'transforms as each error screen is handled separately.<br>' +
        'Comparing Figures 7-8 and 7-9, the first method is<br>' +
        'probably more appealing. But recognize that we’ve chosen<br>' +
        'a very simple example for the purposes of exposition. We<br>' +
        'do not like to see 300-line SQL statements joining 30<br>' +
        'tables as the source query of a data flow step. No one<br>' +
        'wants to maintain such a thing. Also, recognize that your<br>' +
        'ability to perform complex actions, such as logging the<br>' +
        'error to an error schema, is very limited within a single<br>' +
        'SQL statement.<br>' +
        'As part of your ETL design process, you should decide on<br>' +
        'a general approach to data quality screening and<br>' +
        'transformation. An example policy would be:<br>' +
        '• Use the source SQL statement to:<br>' +
        '• Replace a null with a single default value.<br>' +
        '• Cast data types.<br>' +
        '• Rename columns.<br>' +
        '• Use simple (non-outer) joins to decode lookups<br>' +
        'where referential integrity is strictly enforced in<br>' +
        'the source systems.<br>' +
        '• Use outer joins to decode lookups where all<br>' +
        'lookup failures are assigned a simple text such as<br>' +
        '“Unknown: <code>”.<br>' +
        '• Join fewer than six tables. Queries that join six or<br>' +
        'more tables must be reviewed and justified in the<br>' +
        'system documentation.<br>' +
        '407<br>' +
        '• Pull data only from the extract tables in the staging<br>' +
        'database.<br>' +
        '• Use SSIS transform logic to:<br>' +
        '• Log data quality violations to an error event<br>' +
        'schema.<br>' +
        '• Perform complex transformations (other than<br>' +
        'isnull or cast).<br>' +
        '• Handle lookup violation errors in a more complex<br>' +
        'way than can be handled in SQL.<br>' +
        'In most cases, performance is not the deciding factor in<br>' +
        'determining whether to use SQL or SSIS transforms to test<br>' +
        'and clean the data. Both approaches should have similar<br>' +
        'performance characteristics.<br>' +
        'Figure 7-9: Using SSIS transforms to join and clean data<br>' +
        'Halting Package Execution<br>' +
        '408<br>' +
        'When you’re finished with the data quality screening and<br>' +
        'subsequent transformation, you’re still not quite ready to<br>' +
        'write the data into the destination data warehouse table.<br>' +
        'After all, data cleaning is only subsystem 4 of the 34<br>' +
        'subsystems. Even the simplest ETL system still needs to<br>' +
        'add audit information, manage changes to dimension<br>' +
        'attributes, and manage fact table surrogate key<br>' +
        'substitution.<br>' +
        'Post-cleaning is a good checkpoint for staging the data a<br>' +
        'second time. Staging data at this point can improve<br>' +
        'restartability. Also, if you’re processing data in small<br>' +
        'batches during the day, you may accumulate the cleaned<br>' +
        'data and perform the final delivery steps at midnight.<br>' +
        'That said, we often don’t bother to stage the data after<br>' +
        'cleaning. Instead the data flow goes on to perform the<br>' +
        'delivery steps and write directly into the target data<br>' +
        'warehouse tables. In many implementations, we’ll stage<br>' +
        'the data once on extract, and write it again only when it<br>' +
        'goes into the data warehouse tables.<br>' +
        'However, at the beginning of this section on data cleaning,<br>' +
        'we described data quality screens that might cause you to<br>' +
        'halt processing or at least to notify an operator. A simple<br>' +
        'example would be a huge drop in data volume in today’s<br>' +
        'load, indicating a potential problem with an extract or<br>' +
        'transfer. If you need to halt processing because of a data<br>' +
        'quality screen violation, you must stage the data to a table<br>' +
        'and exit the data flow task. In SSIS, the only place you can<br>' +
        'gracefully halt execution is in the control flow. The data<br>' +
        'flow task will halt only if it triggers a fatal error, which is<br>' +
        'not the same as a data quality screen violation.<br>' +
        '409<br>' +
        'Figure 7-10 illustrates the control flow for a package that<br>' +
        'conditionally halts after data quality screening. It contains<br>' +
        'two data flow steps; the first for the data quality screening<br>' +
        'and the second for the delivery into the target fact table.<br>' +
        'Between the two data flow tasks is a simple script that<br>' +
        'evaluates whether or not processing should continue. If all<br>' +
        'is good, the second data flow is launched. If not, page the<br>' +
        'operator and return to the master package with an error<br>' +
        'condition.<br>' +
        'Figure 7-10: Halting package execution based on a data<br>' +
        'quality failure<br>' +
        'Fun with Scripts<br>' +
        '410<br>' +
        'You can’t develop a production-ready ETL system<br>' +
        'in SSIS without writing a few scripts. It’s really not<br>' +
        'hard. Let’s take a closer look at the two scripts in<br>' +
        'Figure 7-10.<br>' +
        'The first script, DQ Continue, performs a few<br>' +
        'basic checks on the data after the data flow task.<br>' +
        'Table 7-1 shows the SSIS variables used in the<br>' +
        'script.<br>' +
        'Table 7-1: SSIS variables used in sample script<br>' +
        'Variable Use of the Variable<br>' +
        'ProdLowLimit Default value for variable<br>' +
        'ProdCount<br>' +
        'Count of distinct products loaded into the staging<br>' +
        'table in the first data flow step (populated in an<br>' +
        'Execute SQL task)<br>' +
        'XtrctLowLimit Default value for variable<br>' +
        'RC_Xtrct<br>' +
        'Count of rows loaded into the staging table in the<br>' +
        'first data flow step (populated in an Execute<br>' +
        'SQL task)<br>' +
        'bXtrctOK Set by the script<br>' +
        'All the script does is evaluate whether the number<br>' +
        'of rows extracted today is above some minimum<br>' +
        'threshold, and the same for the number of product<br>' +
        'types in the extract. If so, continue with processing,<br>' +
        'and set the value of bXtrctOK to TRUE; otherwise<br>' +
        'bXtrctOK remains FALSE.<br>' +
        '411<br>' +
        'When you create a script task or transform, SSIS<br>' +
        'creates the outline of the script for you. The<br>' +
        'quantity of code that you need to write is often<br>' +
        'quite small: in this case, the seven lines shown<br>' +
        'here:<br>' +
        'Dim ProdCount As Integer =<br>' +
        'CType(Dts.Variables("ProdCount").Value, Integer)<br>' +
        'Dim ProdLowLimit As Integer =<br>' +
        'CType(Dts.Variables("ProdLowLimit").Value, Integer)<br>' +
        'Dim RC_Xtrct As Integer =<br>' +
        'CType(Dts.Variables("RC_Xtrct").Value, Integer)<br>' +
        'Dim XtrctLowLimit As Integer =<br>' +
        'CType(Dts.Variables("XtrctLowLimit").Value, Integer)<br>' +
        'If ProdCount > ProdLowLimit AndAlso<br>' +
        'RC_Xtrct > XtrctLowLimit Then<br>' +
        'Dts.Variables("bXtrctOK").Value = True<br>' +
        'End If<br>' +
        'Looking back at Figure 7-10 you can see two<br>' +
        'precedence constraints labeled GOOD and BAD.<br>' +
        'The expression for the “good” path is<br>' +
        '@bXtrctOK==True. This example is simple<br>' +
        'enough that we could have eliminated the script<br>' +
        'and conducted our test in the precedence<br>' +
        '412<br>' +
        'expression. But we recommend using a script,<br>' +
        'because it is much easier to read, understand, and<br>' +
        'edit.<br>' +
        'The second script in Figure 7-10 is absolutely<br>' +
        'trivial. On the BAD path, after we notify the<br>' +
        'operator of the error, we want to pass an error<br>' +
        'condition up to the master package. We need to<br>' +
        'translate the business logic that identified our<br>' +
        'extract as a bad one into a message to SSIS so the<br>' +
        'master package can stop processing of other<br>' +
        'packages. This script contains one line of code:<br>' +
        'Dts.TaskResult = ScriptResults.Failure<br>' +
        'The package ends at this point and reports failure<br>' +
        'to the master package.<br>' +
        'Subsystem 5: Error Event Schema<br>' +
        'The error event schema is a centralized dimensional<br>' +
        'schema whose purpose is to record every error event<br>' +
        'thrown by a quality screen. The error event schema<br>' +
        'consists of a fact table at the grain of an error event — at<br>' +
        'the grain of a failure of any quality screen. For each row<br>' +
        'that fails, for example, a referential integrity check would<br>' +
        'add a row to the error event schema. The error event<br>' +
        'schema holds error events from across the ETL pipeline,<br>' +
        'and as such is a great place to evaluate overall data quality.<br>' +
        '413<br>' +
        'RESOURCES<br>' +
        'You can find more information about<br>' +
        'logging error events in The Data<br>' +
        'Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition, pages 383–385.<br>' +
        'In addition to the integrated error event schema, it is often<br>' +
        'very useful to create error tables for each target table. The<br>' +
        'error table is designed to hold rows that have failed a<br>' +
        'critical data screening, and will hold the image of the row<br>' +
        'at the time of the failure. Error tables don’t have to be SQL<br>' +
        'Server tables — a raw file or flat file can be good choices<br>' +
        'for error tables.<br>' +
        'Note that error tables are usually limited to critical data<br>' +
        'screening errors, such as a lookup failure. We seldom log<br>' +
        'an error image row for something as simple as a null<br>' +
        'violation. There are several characteristics of error tables<br>' +
        'that you need to include in your design:<br>' +
        '• Table layout: Columns and data types.<br>' +
        '• Table location: Database, table name, and/or file name.<br>' +
        '• Error type: You should develop a taxonomy for the types of<br>' +
        'errors to track. Possible error types include, but aren’t<br>' +
        'limited to:<br>' +
        '• RI violation (specify which foreign key).<br>' +
        '• Numeric value out of bounds (specify which<br>' +
        'column).<br>' +
        '• Business rule violation (specify which rule).<br>' +
        '• Error level: Usually critical or moderate. Critical error tables<br>' +
        'contain data rows that were not inserted into the target table.<br>' +
        '414<br>' +
        'Moderate error tables contain data rows that were inserted<br>' +
        'into the target table, but we want to hold the image of the<br>' +
        'row at the time of data screening.<br>' +
        '• Resolution information: For critical error tables, was the row<br>' +
        'eventually “fixed” and added to the data warehouse? When<br>' +
        'and how?<br>' +
        'The error tables are part of the back room — we don’t<br>' +
        'expect the business user community to use them. One<br>' +
        'possible exception is an auditing user or a data steward.<br>' +
        'DOWNLOAD<br>' +
        'Several of the sample packages on the<br>' +
        'book’s web site, including ExchangeRates,<br>' +
        'Orders_SQL, and Orders_SSIS, use error<br>' +
        'tables.<br>' +
        'Subsystem 6: Audit Dimension Assembler<br>' +
        'The audit dimension is a special dimension that’s<br>' +
        'assembled in the back room by the ETL system. The audit<br>' +
        'dimension in Figure 7-11 contains the metadata context at<br>' +
        'the moment a specific row is created. Every data<br>' +
        'warehouse table contains two keys to the audit dimension:<br>' +
        'the key for the batch in which the row was originally<br>' +
        'inserted, and the key for the batch where the row was most<br>' +
        'recently updated.<br>' +
        'The audit dimension receives one row each time an SSIS<br>' +
        'package is executed. The audit dimension is a small table.<br>' +
        'You might notice that the sample packages on the web site<br>' +
        '415<br>' +
        'also use the audit dimension for loads into staging and<br>' +
        'error tables.<br>' +
        'The mechanics of implementing the audit dimension in<br>' +
        'SSIS are very straightforward. Each package begins with a<br>' +
        'few steps that insert a new row into the audit dimension<br>' +
        'table. Those steps update an SSIS variable with the key for<br>' +
        'the new audit dimension row. The work of the ETL occurs<br>' +
        'in the data flow task, and any inserts or update are flagged<br>' +
        'with today’s audit key for this table. Finally, after the data<br>' +
        'flow, end the package with a few steps that update the<br>' +
        'audit table with information such as the number of rows<br>' +
        'processed and the time processing finished.<br>' +
        'The audit dimension contains a foreign key to itself: the<br>' +
        'audit key points to the parent audit key. When a master<br>' +
        'package begins, it creates a row in the audit table, and<br>' +
        'generates its own audit key. It passes that audit key down<br>' +
        'to any child package. You can use the parent audit key to<br>' +
        'tie together all the packages that ran in a single batch,<br>' +
        'called from a single parent.<br>' +
        'DOWNLOAD<br>' +
        'Every package on the book’s web site<br>' +
        'contains the same design pattern for using<br>' +
        'the audit dimension.<br>' +
        'Figure 7-11: Sample audit dimension<br>' +
        '416<br>' +
        'Subsystem 7: Deduplication System<br>' +
        'Often dimensions are derived from several sources. This is<br>' +
        'a common situation for organizations that have many<br>' +
        'customer-facing source systems that create and manage<br>' +
        'separate customer master tables. Customer information<br>' +
        'may need to be integrated from several lines of business<br>' +
        'and outside sources. Sometimes the data can be matched<br>' +
        'through identical values in important columns. However,<br>' +
        'even when a definitive match occurs, other columns in the<br>' +
        'data might contradict one another, requiring a decision on<br>' +
        'which data should survive.<br>' +
        'Unfortunately, there is seldom a universal column that<br>' +
        'makes the merge operation easy. Sometimes the only clues<br>' +
        'available are the similarity of several columns. SSIS<br>' +
        'contains data flow transforms that can help develop a<br>' +
        '417<br>' +
        'solution to this problem — the Fuzzy Lookup and Fuzzy<br>' +
        'Grouping transforms.<br>' +
        'As we discussed in Chapter 6, it is extraordinarily difficult<br>' +
        'to build deduplication into the standard automated ETL<br>' +
        'stream. The ETL jobs must run without human<br>' +
        'intervention, but some deduplication problems cannot be<br>' +
        'resolved without human eyes. We strongly advocate that<br>' +
        'the deduplication system be initiated before the nightly<br>' +
        'ETL process runs. Within the Microsoft SQL Server<br>' +
        'product, we suggest that you implement a deduplication<br>' +
        'system in Master Data Services.<br>' +
        'Subsystem 8: Conforming System<br>' +
        'Conforming consists of all the steps required to align the<br>' +
        'content of some or all of the columns in a dimension with<br>' +
        'the columns in similar or identical dimensions in other<br>' +
        'parts of the data warehouse. For instance, in a large<br>' +
        'organization you may have fact tables capturing invoices<br>' +
        'and customer service calls that both use the customer<br>' +
        'dimension. The source systems for invoices and customer<br>' +
        'service often have separate customer databases, with little<br>' +
        'consistency between the two sources of customer<br>' +
        'information. The data from these two customer sources<br>' +
        'needs to be conformed to make some or all of the columns<br>' +
        'describing customers share the same domains.<br>' +
        'The conforming subsystem is responsible for creating and<br>' +
        'maintaining conformed dimensions and conformed facts.<br>' +
        'Incoming data from multiple systems need to be combined<br>' +
        'and integrated so that it is structurally identical,<br>' +
        'deduplicated, filtered, and standardized in terms of content<br>' +
        '418<br>' +
        'rows in a conformed image. A large part of the conforming<br>' +
        'process is the deduplicating, matching, and survivorship<br>' +
        'processes described previously. In the SQL Server data<br>' +
        'warehouse, this work is best implemented in a master data<br>' +
        'management system that serves as a source to the ETL<br>' +
        'process.<br>' +
        'The hardest effort of the conforming system is political —<br>' +
        'wresting agreement across the enterprise about entity and<br>' +
        'attribute names and business rules. These political<br>' +
        'problems fall under the umbrella of data governance, and<br>' +
        'they need to be solved during the design phase of your<br>' +
        'DW/BI project.<br>' +
        'The conformed dimension should be managed in one place<br>' +
        'and distributed to “subscribers” of the dimension. The<br>' +
        'easiest way to distribute copies of conformed dimensions<br>' +
        'is to set up a simple SSIS package to copy the updated<br>' +
        'dimension tables to the other servers in a distributed<br>' +
        'environment. The packages can end with a notification to<br>' +
        'the subscribers that the dimensions have been successfully<br>' +
        'published. Alternatively, you can set up SQL Server<br>' +
        'replication to publish the centrally managed dimension to<br>' +
        'other databases.<br>' +
        'NOTE<br>' +
        'The Kimball Approach supports a<br>' +
        'distributed data warehouse, with separate<br>' +
        'databases for, say, accounting and<br>' +
        '419<br>' +
        'customer care. Where separate business<br>' +
        'process dimensional models have<br>' +
        'dimensions that are completely identical —<br>' +
        'such as customer — those copies of the<br>' +
        'dimension should have the same keys.<br>' +
        'Each can be a subset of the master<br>' +
        'dimension — each subset can filter rows<br>' +
        'and/or columns — but they should use the<br>' +
        'same warehouse surrogate keys.<br>' +
        'The payoff for building conformed<br>' +
        'dimensions is the ability to drill across<br>' +
        'separate business processes, assembling an<br>' +
        'integrated final result. The bare minimum<br>' +
        'requirement for drilling across is that the<br>' +
        'separate conformed dimensions have at<br>' +
        'least one common field with the same<br>' +
        'contents. When such a common field is<br>' +
        'used in the SELECT list of each of the SQL<br>' +
        'or MDX queries in the drill across report,<br>' +
        'the results can then be merged to produce<br>' +
        'the integrated final result. This simple<br>' +
        'recipe has a profound result. It is the core<br>' +
        'of enterprise data warehouse integration.<br>' +
        'Delivering Data for Presentation<br>' +
        'The primary mission of the ETL system is the handoff of<br>' +
        'the dimension and fact tables in the delivery step. For this<br>' +
        'reason, the delivery subsystems are the most pivotal<br>' +
        'subsystems in your ETL architecture. Though there is<br>' +
        '420<br>' +
        'considerable variation in source data structures and<br>' +
        'cleaning and conforming logic, the delivery processing<br>' +
        'techniques for preparing the dimensional table structures<br>' +
        'are more defined and disciplined. Use of these techniques<br>' +
        'is critical to building a successful dimensional data<br>' +
        'warehouse that is reliable, scalable, and maintainable.<br>' +
        'Many of these subsystems focus on dimension table<br>' +
        'processing. Dimension tables are the heart of the DW/BI<br>' +
        'system. They provide context for the fact tables. Although<br>' +
        'dimension tables are usually smaller than the fact tables,<br>' +
        'they are critical to the success of the DW/BI system<br>' +
        'because they provide the entry points into the fact tables,<br>' +
        'through constraints and grouping specifications.<br>' +
        'The delivery process begins with the cleaned and<br>' +
        'conformed data resulting from the subsystems just<br>' +
        'described. For many dimensions, the delivery plan is<br>' +
        'simple. Perform basic transformations to the data to build<br>' +
        'dimension rows for loading into the target presentation<br>' +
        'table. This typically includes surrogate key assignment,<br>' +
        'splitting or combining columns to present the appropriate<br>' +
        'data values, and joining underlying third normal form table<br>' +
        'structures into denormalized flat dimensions. Dimension<br>' +
        'tables are usually small, and subject to many<br>' +
        'transformations.<br>' +
        'Preparing fact tables is important because fact tables hold<br>' +
        'the key measurements of the business. Fact tables can be<br>' +
        'very large and time consuming to load. However,<br>' +
        'preparing fact tables for presentation is typically<br>' +
        'straightforward.<br>' +
        '421<br>' +
        'Subsystem 9: Slowly Changing Dimension Manager<br>' +
        'One of the more important elements of the ETL<br>' +
        'architecture is the capability to implement slowly changing<br>' +
        'dimension (SCD) logic. The ETL system must determine<br>' +
        'how to handle a dimension attribute value that has changed<br>' +
        'from the value already stored in the data warehouse.<br>' +
        'In Chapter 2, we talked about the two main techniques for<br>' +
        'handling changes in dimension attributes:<br>' +
        '• Type 1: Restate history by updating the dimension row when<br>' +
        'attributes change.<br>' +
        '• Type 2: Track history by propagating a new dimension row<br>' +
        'when attributes change.<br>' +
        'Standard Handling for Slowly Changing Dimensions<br>' +
        'Any dimension that contains a type 2 attribute should track<br>' +
        'the date range for which each dimension row is valid. For<br>' +
        'any dimension with a type 2 attribute, add three columns:<br>' +
        'RowStartDate, RowEndDate, and IsRowCurrent. For<br>' +
        'every dimension member like customer, there should be<br>' +
        'one and only one current row at any one time. Older rows<br>' +
        'have their RowStartDate and RowEndDate set<br>' +
        'appropriately. Figure 7-12 illustrates the logic for handling<br>' +
        'updates to a dimension with both type 1 and type 2<br>' +
        'attributes during the daily incremental load.<br>' +
        'We’ve seen companies get so intimidated by this<br>' +
        'complexity that they decide to manage all dimensions as<br>' +
        'type 1, even if that’s not what the users want. The SSIS<br>' +
        'Slowly Changing Dimension transform is a useful feature.<br>' +
        'It does most of this work for you.<br>' +
        '422<br>' +
        'Figure 7-12: Logic flow for handling dimension updates<br>' +
        '423<br>' +
        'Using the Slowly Changing Dimension<br>' +
        'Transform<br>' +
        'The SSIS Slowly Changing Dimension (SCD)<br>' +
        'transform is available in the data flow. Typically,<br>' +
        'your dimension’s data flow begins by sourcing data<br>' +
        'from the extracted data in the staging database, and<br>' +
        'performs any necessary cleaning and<br>' +
        'transformation steps. The final section of the data<br>' +
        'flow is where you insert and update this flow into<br>' +
        'the target dimension table.<br>' +
        'When you drag the SCD transform into the data<br>' +
        'flow design palette, it consists of a single rectangle<br>' +
        'like all the other transforms. When you edit it, it<br>' +
        'launches a wizard with several pages of questions.<br>' +
        'And when you finally click Finish, the wizard<br>' +
        'generates a bunch of transforms and flows for you.<br>' +
        'The generated transforms and flows do the work<br>' +
        'that’s outlined in Figure 7-12.<br>' +
        'The wizard starts by asking you to specify the<br>' +
        'dimension table you’ll be loading. Next, identify<br>' +
        'the business key, used to tie together all the<br>' +
        'instances of a particular entity. In a customer<br>' +
        'dimension the business key is usually the account<br>' +
        'number or customer ID. Map the other columns in<br>' +
        'the input flow to the attributes in the target<br>' +
        'dimension.<br>' +
        '424<br>' +
        'The wizard next has you identify how to manage<br>' +
        'the changes in each attribute. In addition to the<br>' +
        'types 1 and 2 (restate and track history) described<br>' +
        'previously, Integration Services includes a Fixed<br>' +
        'attribute, which should never be updated. Set the<br>' +
        'attribute change type for all the columns in your<br>' +
        'target table.<br>' +
        'On the next screen you’re asked several<br>' +
        'housekeeping questions. Do you want the<br>' +
        'processing to fail when you encounter a change to<br>' +
        'a Fixed attribute? Answer No (you rarely want<br>' +
        'processing to fail outright). Do you want the bad<br>' +
        'row to go into an error flow? Answer Yes. You’re<br>' +
        'also asked if a type 1 change, when encountered,<br>' +
        'should update all the historical rows for the<br>' +
        'dimension entity, or just the current row. The<br>' +
        'textbook definition of a type 1 attribute indicates<br>' +
        'you should update all the rows, and this is the<br>' +
        'recommended setting.<br>' +
        'If you have any type 2 attributes in your<br>' +
        'dimension, you’re next asked how to identify the<br>' +
        'current row. Do you use row start and end dates, or<br>' +
        'an indicator like IsRowCurrent?<br>' +
        'You can’t have the SCD Wizard maintain both the<br>' +
        'row start and end date and the row current<br>' +
        'indicator. It’s one or the other. As we discuss in the<br>' +
        'next section, you can edit the results of the wizard,<br>' +
        '425<br>' +
        'and you can generally make it do what you want.<br>' +
        'We find it easier to generate the SCD Wizard using<br>' +
        'the row current indicator technique, and then edit<br>' +
        'the resulting transforms to add the row date<br>' +
        'handling.<br>' +
        'Perhaps you’re exasperated with the complexity of<br>' +
        'the SCD wizard, although we’ll point out that it’s a<br>' +
        'complex problem. Your reward comes when you<br>' +
        'click Finish and see all the transforms that have<br>' +
        'been created for you. These objects insert new<br>' +
        'rows, update rows that have a type 1 change, and<br>' +
        'perform the update and insert steps for existing<br>' +
        'rows that have a type 2 change.<br>' +
        'The Slowly Changing Dimension transform will meet<br>' +
        'many projects’ requirements without any further changes.<br>' +
        'However, there are circumstances where you need to do<br>' +
        'something tricky, or circumvent the wizard altogether. The<br>' +
        'next few sections discuss some advanced topics around<br>' +
        'handling dimension changes.<br>' +
        'Custom Handling for Slowly Changing Dimensions<br>' +
        'You will probably want to customize the output from the<br>' +
        'SCD Wizard. It may even make sense for you to develop<br>' +
        'custom handling for dimension changes.<br>' +
        'The SCD Wizard will identify the current row for an entity<br>' +
        'like customer in one of two ways: with a True/False (or<br>' +
        'Yes/No) indicator, or with a range of dates for which the<br>' +
        '426<br>' +
        'row is valid. If you choose the date range approach, the<br>' +
        'SCD transform will look for the single row for each<br>' +
        'natural key that has a null end date.<br>' +
        'We recommend that you use both the row current indicator<br>' +
        'technique and the valid date range technique. Also, set the<br>' +
        'end date to a date far in the future, rather than leave it null.<br>' +
        'Can you still use the SCD Wizard? Yes you can, but you<br>' +
        'need to modify the generated objects in order to populate<br>' +
        'your dimension the way you want. This is easy to do, but<br>' +
        'be warned: If you need to go through the wizard again,<br>' +
        'perhaps to switch an attribute from type 1 to type 2<br>' +
        'handling, you’ll lose any customizations you’ve made to<br>' +
        'the generated objects.<br>' +
        'NOTE<br>' +
        'The objects generated by the SCD Wizard<br>' +
        'are standard Integration SSIS transforms.<br>' +
        'You can edit them to do whatever you like.<br>' +
        'The only logic that you can’t modify is the comparison<br>' +
        'logic that’s hidden in the SCD transform itself. Under the<br>' +
        'covers, this transform takes a row in the pipeline and<br>' +
        'compares it to the current row in the dimension table;<br>' +
        'determines if the pipeline row is new or an update; and<br>' +
        'sends the row to the correct downstream path or paths.<br>' +
        'You can’t change any of the logic in this transform without<br>' +
        'going through the wizard again.<br>' +
        'Alternatives to the Slowly Changing Dimension Transform<br>' +
        '427<br>' +
        'We like the SCD transform because it’s well thought out<br>' +
        'and reasonably flexible. However, it’s not perfect. The two<br>' +
        'main issues are:<br>' +
        '• Performance: The SCD transform, particularly the<br>' +
        'comparison logic, does not perform as fast as some<br>' +
        'alternative approaches. The slow performance is a cost you<br>' +
        'pay every time the package executes.<br>' +
        '• Resilience: If you need to change the dimension table, for<br>' +
        'example to add a column or change a column from type 1 to<br>' +
        'type 2, you will have to reapply any edits you’ve made to the<br>' +
        'objects generated by the transform. As we have already<br>' +
        'described, you’re very likely to want to edit the objects<br>' +
        'generated by the transform. This problem doesn’t affect the<br>' +
        'execution of the package in production, but it can be quite<br>' +
        'frustrating for the developers. This is particularly true if you<br>' +
        'rushed into ETL development before your data model design<br>' +
        'was finalized.<br>' +
        'There are many alternatives to using the SSIS SCD<br>' +
        'transform. You can develop the logic yourself, or there are<br>' +
        'a variety of free and moderate cost transforms that you can<br>' +
        'download.<br>' +
        'DOWNLOADS<br>' +
        'The two most popular third-party tools for<br>' +
        'replacing the Microsoft SCD transform are:<br>' +
        '• Kimball Method SCD Transform, available<br>' +
        'for free download from<br>' +
        'www.codeplex.com/kimballscd/ and<br>' +
        'developed by Todd McDermid. This<br>' +
        'transform offers more functionality than<br>' +
        'the Microsoft SCD transform, and<br>' +
        'performs much better. Edits are not<br>' +
        '428<br>' +
        'destructive to downstream elements. This<br>' +
        'transform is not associated with the<br>' +
        'Kimball Group, though the developer<br>' +
        'followed Kimball published best practices<br>' +
        'in his design.<br>' +
        '• TableDifference, available for purchase<br>' +
        'from www.cozyroc.com and designed by<br>' +
        'the folks at SQLBI.eu. This component<br>' +
        'does only the “comparison” step of the<br>' +
        'SCD problem. You need to build out the<br>' +
        'updates, inserts, and so on. The comparison<br>' +
        'step is notably faster than the Microsoft<br>' +
        'SCD transform, and edits are not<br>' +
        'destructive to downstream elements.<br>' +
        'The good news is that the problem of managing SCDs has<br>' +
        'been solved many times by many people. Our advice is to<br>' +
        'use the Microsoft component unless you find its faults<br>' +
        'intolerable. If you choose an alternative approach, you’ll<br>' +
        'assume the risk of incorporating a third-party object into<br>' +
        'your solution. Are you better off with a very functional,<br>' +
        'free solution with limited support? Or a low cost solution<br>' +
        'backed by a company that — we can safely say — does<br>' +
        'not have the financial resources of Microsoft? No solution<br>' +
        'is free of risk.<br>' +
        'Another approach to handling SCDs is to perform the work<br>' +
        'not in the SSIS data flow, but in the database. To<br>' +
        'implement this technique, the final step of the clean and<br>' +
        'transform data flow would write the dimension data to a<br>' +
        'staging table. Out in the control flow, write an Execute<br>' +
        'SQL task that uses the MERGE statement to perform the<br>' +
        'SCD processing.<br>' +
        '429<br>' +
        'RESOURCES<br>' +
        'You can find a discussion of how to use the<br>' +
        'MERGE statement for slowly changing<br>' +
        'dimensions in the Kimball Group design tip<br>' +
        '#107, available at www.kimballgroup.com/html/<br>' +
        'designtips.html.<br>' +
        'Subsystem 10: Surrogate Key Generator<br>' +
        'As you recall from Chapter 2, we strongly recommend the<br>' +
        'use of surrogate keys for all dimension tables. This implies<br>' +
        'that you use a robust mechanism for producing surrogate<br>' +
        'keys in your ETL system. The goal of the surrogate key<br>' +
        'generator is to create a meaningless integer key to serve as<br>' +
        'the primary key of the dimension row.<br>' +
        'In SQL Server, we implement the surrogate key generator<br>' +
        'in the database, by defining the table’s primary key with<br>' +
        'the keyword IDENTITY. Inserts into the table exclude the<br>' +
        'primary key column, and each new row generates a new<br>' +
        'surrogate key. We have implemented this technique with<br>' +
        'very large dimensions, and have not experienced problems<br>' +
        'with performance or logic.<br>' +
        'Subsystem 11: Hierarchy Manager<br>' +
        'It’s normal for a dimension to have multiple, simultaneous,<br>' +
        'embedded hierarchical structures. These multiple<br>' +
        'hierarchies coexist as dimension attributes within the<br>' +
        'denormalized dimension table. The hierarchy manager<br>' +
        '430<br>' +
        'ensures that each attribute be a single value in the presence<br>' +
        'of the dimension’s primary key. For example, a product<br>' +
        'rolls up to one and only one brand; a brand to one and only<br>' +
        'one product subcategory; and a subcategory to one and<br>' +
        'only one product category.<br>' +
        'Hierarchies are either fixed or ragged. A fixed hierarchy<br>' +
        'has a consistent number of levels and is modeled and<br>' +
        'populated as a separate dimension attribute for each of the<br>' +
        'levels. Slightly ragged hierarchies like postal addresses are<br>' +
        'most often modeled as a fixed hierarchy, with placeholders<br>' +
        'for the “missing” levels. Profoundly ragged hierarchies are<br>' +
        'typically used for organization structures that are<br>' +
        'unbalanced and of indeterminate depth. The data model<br>' +
        'and ETL solution required to support ragged hierarchies in<br>' +
        'relational databases require the use of a bridge table<br>' +
        'containing the organization map. Ragged hierarchies in<br>' +
        'Analysis Services are supported more directly, not<br>' +
        'requiring a bridge table. Please see Chapter 8.<br>' +
        'Snowflakes or normalized data structures are not<br>' +
        'recommended for the presentation level. However, the use<br>' +
        'of a normalized design may be appropriate in the ETL<br>' +
        'staging area to assist in the maintenance of the ETL data<br>' +
        'flow for populating and maintaining the hierarchy<br>' +
        'attributes. The ETL system is responsible for enforcing the<br>' +
        'business rules to assure the hierarchy is populated<br>' +
        'appropriately in the dimension table.<br>' +
        'One of the biggest challenges with hierarchies is that<br>' +
        'transaction systems typically contain stripped down<br>' +
        'hierarchies that contain only the groupings needed to enter<br>' +
        'transactions. Business users often have alternative ways of<br>' +
        '431<br>' +
        'viewing and analyzing the data, and there is no official<br>' +
        'source for these alternate hierarchies. They are often<br>' +
        'sourced from Excel.<br>' +
        'The main problem with sourcing from Excel is that it’s an<br>' +
        'unstructured source. Referential integrity violations are<br>' +
        'common. And the solutions usually require a person’s<br>' +
        'intervention — it’s not a problem that is well solved by the<br>' +
        'overnight ETL loading process.<br>' +
        'The new SQL Server 2008 R2 feature Master Data<br>' +
        'Services, discussed in Chapter 6, is the best place to<br>' +
        'manage the integration of this kind of user information.<br>' +
        'Redefine your business processes so that Master Data<br>' +
        'Services maintains good clean hierarchies, and have your<br>' +
        'ETL job stream subscribe to those hierarchies.<br>' +
        'Subsystem 12: Special Dimensions Manager<br>' +
        'The special dimensions manager is a catch-all subsystem:<br>' +
        'a placeholder in the ETL architecture for supporting an<br>' +
        'organization’s specific dimensional design characteristics.<br>' +
        'These design techniques were introduced in Chapter 2.<br>' +
        'Some organizations’ ETL systems will require all of the<br>' +
        'capabilities discussed here, whereas others will be<br>' +
        'concerned with few of these design techniques:<br>' +
        '• Date and time dimensions: Date and time are unique in that<br>' +
        'they are completely specified at the beginning of the data<br>' +
        'warehouse project and they don’t have a conventional<br>' +
        'source. Most often, these dimensions are built in a<br>' +
        'spreadsheet.<br>' +
        '432<br>' +
        'DOWNLOADS<br>' +
        'The book’s web site contains a sample date<br>' +
        'dimension in Excel, and a package for<br>' +
        'loading that dimension into the<br>' +
        'MDWT_2008R2 database.<br>' +
        '• Junk dimensions: Junk dimensions are made up from text and<br>' +
        'miscellaneous flags left over in the fact table after you’ve<br>' +
        'removed all the fields related to the other dimensions. As we<br>' +
        'describe in Chapter 2, you sometimes combine unrelated flags<br>' +
        'and text into a junk dimension to avoid creating dozens of tiny<br>' +
        'dimensions. If the theoretical number of rows in the dimension<br>' +
        'is fixed and a relatively small number, you can populate the<br>' +
        'junk dimension in advance. In other cases, you must load the<br>' +
        'junk dimension as you load the fact table, inserting dimension<br>' +
        'rows as you observe new combinations of flags and text. As<br>' +
        'illustrated in Figure 7-13, this process requires assembling the<br>' +
        'junk dimension attributes and comparing them to the existing<br>' +
        'junk dimension rows to see if the row already exists. If not, a<br>' +
        'new dimension row must be assembled, and the row loaded into<br>' +
        'the junk dimension on the fly during the fact table load process.<br>' +
        'In SSIS, this design pattern is easily implemented as part of the<br>' +
        'fact table surrogate key pipeline. There is a more thorough<br>' +
        'discussion of this design solution later in the chapter, in the<br>' +
        'section on the surrogate key pipeline.<br>' +
        '• Shrunken dimensions: Shrunken dimensions are conformed<br>' +
        'dimensions that contain a subset of rows or columns of one of<br>' +
        'your base dimensions. Shrunken dimensions are used to support<br>' +
        'data at different levels of granularity, for example monthly<br>' +
        'budgets. The ETL data flow should build conformed shrunken<br>' +
        'dimensions from the base dimension to assure consistency.<br>' +
        '• Small static dimensions: A few dimensions are created entirely<br>' +
        'by the ETL system without a real outside source. These are<br>' +
        'usually small lookup dimensions where an operational code is<br>' +
        '433<br>' +
        'translated into words. In these cases, there is no real ETL<br>' +
        'processing. The lookup dimension is simply created by the ETL<br>' +
        'team as a relational table in its final form.<br>' +
        'Figure 7-13: Architecture for building junk dimension<br>' +
        'rows<br>' +
        'Subsystem 13: Fact Table Builders<br>' +
        'The fact table builder subsystem focuses on the ETL<br>' +
        'architectural requirements to effectively build the three<br>' +
        'primary types of fact tables: transaction grain, periodic<br>' +
        'snapshot, and accumulating snapshot fact tables. Your fact<br>' +
        'table load packages must maintain referential integrity with<br>' +
        'the associated dimensions. The surrogate key pipeline<br>' +
        '(subsystem 14) is designed to support this need.<br>' +
        '434<br>' +
        'You will almost certainly need two sets of fact table<br>' +
        'builder packages: one for the one-time historical load and a<br>' +
        'second set for the ongoing incremental loads. First, it’s<br>' +
        'common for transaction systems to evolve over time, so<br>' +
        'the historical load packages need to track the changes to<br>' +
        'the data structures and meanings. In addition, the historical<br>' +
        'fact table surrogate key pipeline must associate each fact<br>' +
        'row with the type 2 dimension member in effect when the<br>' +
        'fact event occurred. The problem is discussed in greater<br>' +
        'detail in the late-arriving fact problem (subsystem 16). As<br>' +
        'far as the surrogate key pipeline is concerned, the<br>' +
        'historical load is one huge set of late arriving fact data!<br>' +
        'NOTE<br>' +
        'As you design your fact table packages,<br>' +
        'you need to remember the demands of SQL<br>' +
        'Server Analysis Services (SSAS)<br>' +
        'processing. As we describe in Chapter 8,<br>' +
        'SSAS can be a very effective database<br>' +
        'engine for the presentation area. But the<br>' +
        'ETL team needs to be aware that during<br>' +
        'processing of the SSAS database, Analysis<br>' +
        'Services cannot easily handle fact table<br>' +
        'updates or deletes. The only way to process<br>' +
        'a fact row update or delete is to fully<br>' +
        'process the SSAS partition that contains<br>' +
        'that fact row.<br>' +
        '435<br>' +
        'The easiest approach is to fully process the<br>' +
        'fact table into SSAS every day. This is a<br>' +
        'fine idea for modest data volumes (in the<br>' +
        'tens of gigabytes), but doesn’t scale well.<br>' +
        'For larger data volumes, you need to:<br>' +
        '• Partition the SSAS fact data by date.<br>' +
        '• In the fact table ETL, keep track of the<br>' +
        'dates that are receiving updates.<br>' +
        '• Fully process the SSAS partitions with<br>' +
        'updates (or deletes, though we usually<br>' +
        'don’t design hard deletes into the data<br>' +
        'warehouse).<br>' +
        'Transaction Grain Fact Table Loader<br>' +
        'The transaction grain represents a measurement event<br>' +
        'defined at a particular instant. A line item on an invoice is<br>' +
        'an example of a transaction event. A scanner event at a<br>' +
        'cash register is another. Transaction grain fact tables are<br>' +
        'the largest and most detailed of the three types of fact<br>' +
        'tables. The transaction grain fact table loader receives data<br>' +
        'from the changed data capture system and loads it with the<br>' +
        'proper dimensional foreign keys. Most rows are inserted.<br>' +
        'In the SSIS data flow, use the bulk load options of the<br>' +
        'destination, as we described in subsystem 3.<br>' +
        'Loading into a Partitioned Table<br>' +
        '436<br>' +
        'Large fact tables are often partitioned, usually by<br>' +
        'date. Monthly partitions are the most common. For<br>' +
        'most fact tables, it is perfectly fine to use the<br>' +
        'partitioned table in the data flow’s destination.<br>' +
        'SSIS performs the load into the destination table,<br>' +
        'and SQL Server determines to which physical<br>' +
        'partition each row belongs. It’s not the fastest<br>' +
        'possible load performance, but in many cases it’s<br>' +
        'just fine.<br>' +
        'However, transaction grain fact tables may be very<br>' +
        'large, and loading the data into the target table may<br>' +
        'become a bottleneck. We have worked with some<br>' +
        'organizations that are loading a billion rows a day<br>' +
        'into a single fact table. Obviously, such<br>' +
        'implementations need to be careful to optimize<br>' +
        'load performance.<br>' +
        'To get the very fastest load performance, you need<br>' +
        'to load into an empty table. For daily loads,<br>' +
        'partition the fact table on a daily basis, load into an<br>' +
        'empty pseudo-partition, and then swap that<br>' +
        'partition into the fact table. Refer back to Chapter 5<br>' +
        'for a discussion of partitioning.<br>' +
        'Implementing this load-and-swap technique in<br>' +
        'SSIS requires some steps in both the control flow<br>' +
        'and the data flow. First, in the control flow, you<br>' +
        'need to perform the following steps:<br>' +
        '437<br>' +
        '• Modify the partitioning scheme, creating an empty<br>' +
        'partition for “yesterday’s” data by splitting the last<br>' +
        'empty partition.<br>' +
        '• Create new standalone tables (and indexes) as<br>' +
        'pseudo-partitions that correspond to the real<br>' +
        'partitions.<br>' +
        '• Define a view on the standalone pseudo-partition<br>' +
        'table with a static name. Each pseudo-partition table<br>' +
        'usually includes the date in the table name; you need<br>' +
        'an unchanging view name to use inside the data flow<br>' +
        'step, as you cannot change the target table name in<br>' +
        'SSIS at runtime.<br>' +
        'The data flow task is completely normal, except<br>' +
        'you set up the destination adapter to load into the<br>' +
        'view rather than either the partitioned fact table or<br>' +
        'one of the pseudo-partitions. The data will flow<br>' +
        'into the view, which points to the newly created<br>' +
        'empty pseudo-partition.<br>' +
        'Once the data flow task is complete and the new<br>' +
        'pseudo-partition is populated, return to the control<br>' +
        'flow. There, you need to complete a few more<br>' +
        'steps:<br>' +
        '• Index the pseudo-partition, if you didn’t load it with<br>' +
        'indexes in place.<br>' +
        '• Switch the new pseudo-partition into the partitioned<br>' +
        'table. The pseudo-partition table now has zero rows<br>' +
        'in it; all those rows are in the partitioned table.<br>' +
        '• Periodically, consolidate the daily partitions into<br>' +
        'weekly or monthly partitions. This is usually a<br>' +
        'separate process.<br>' +
        'All the partition management activities in the<br>' +
        'control flow are straightforward to set up. You can<br>' +
        '438<br>' +
        'write scripts to handle the logic, but a much better<br>' +
        'choice is to use the partition management utility<br>' +
        'designed and built by the SQL Server customer<br>' +
        'advisory team. It’s available for download from<br>' +
        'CodePlex.com (search for Partition Management).<br>' +
        'The Partition Management utility does not have an<br>' +
        'SSIS task wrapper. It’s a command line tool, so to<br>' +
        'call it from SSIS you need to use the Execute<br>' +
        'Process task. Write a simple script to construct the<br>' +
        'list of arguments.<br>' +
        'The addition of late arriving records is more difficult,<br>' +
        'requiring processing capabilities described in subsystem<br>' +
        '16.<br>' +
        'As much as we like to think of a transaction grain fact<br>' +
        'table as only receiving new rows, updates are necessary. In<br>' +
        'some cases, updates are relatively unusual, driven from<br>' +
        'fixing an earlier error. But in other cases there are<br>' +
        'systematic business rules that call for updating a fact. If<br>' +
        'you have a small volume of fact table updates, you may<br>' +
        'implement them in SSIS within the data flow, by using the<br>' +
        'OLE DB Command transform.<br>' +
        'The OLE DB Command transform is the only way to<br>' +
        'perform database updates and deletes within the data flow.<br>' +
        'The advantage of the OLE DB Command transform is the<br>' +
        'availability of the error flow discussed previously. Any<br>' +
        'row for which the attempted update fails will go into the<br>' +
        'error flow and can be handled or at the very least captured.<br>' +
        '439<br>' +
        'There is one huge disadvantage of the OLE DB Command<br>' +
        'transform: it operates row by row and hence is very slow.<br>' +
        'If you need to update many rows, you should instead flow<br>' +
        'the entire image of the row to be updated into yet another<br>' +
        'staging table. Once you exit the data flow, use an Execute<br>' +
        'SQL task to execute a bulk UPDATE or MERGE statement.<br>' +
        'WARNING<br>' +
        'The OLE DB Command transform is one<br>' +
        'of the first things we look for when<br>' +
        'reviewing the design of SSIS packages that<br>' +
        'have performance problems. It’s great for<br>' +
        'small data volumes because of the error<br>' +
        'flow. But it is several orders of magnitude<br>' +
        'slower than the SQL-based bulk update<br>' +
        'technique.<br>' +
        'Periodic Snapshot Fact Table Loader<br>' +
        'The periodic snapshot grain represents a regular repeating<br>' +
        'set of measurements, such as financial account balances.<br>' +
        'Periodic snapshots are a common fact table type and are<br>' +
        'frequently used for monthly account balances, standard<br>' +
        'financial reporting, and inventory balances. The<br>' +
        'periodicity of a periodic snapshot is typically daily,<br>' +
        'weekly, or monthly. Periodic snapshots are often paired<br>' +
        'with a transaction fact table that details the movements<br>' +
        'into and out of the snapshot.<br>' +
        '440<br>' +
        'Periodic snapshots have similar loading characteristics to<br>' +
        'those of the transaction grain fact tables, especially if you<br>' +
        'load the latest snapshot at the end of the period. A<br>' +
        'common design for periodic snapshots is to maintain the<br>' +
        'current period daily, then freeze that snapshot at the end of<br>' +
        'the period. This design creates a twist for the ETL<br>' +
        'processing, because you need to completely replace<br>' +
        '(update!) the current period’s set of data.<br>' +
        'Luckily, this problem is rendered very simple by the use of<br>' +
        'table partitions. If the periodic snapshot is monthly, design<br>' +
        'monthly partitions with the usual pseudo-partition staging<br>' +
        'tables. Load into the pseudo-partition and swap it into the<br>' +
        'partitioned tables. For each month you only need one<br>' +
        'pseudo-partition table, which you truncate, load, and swap<br>' +
        'every day.<br>' +
        'If you’re not using table partitions, you can either bulk<br>' +
        'delete the current month’s rows before entering the data<br>' +
        'flow, or you can use the bulk update technique described<br>' +
        'previously.<br>' +
        'Accumulating Snapshot Fact Table Loader<br>' +
        'The accumulating snapshot grain represents the current<br>' +
        'evolving status of a process that has a finite beginning and<br>' +
        'end. Order processing is the classic example of an<br>' +
        'accumulating snapshot. The order is placed, shipped, and<br>' +
        'paid for at different points in time. The transaction grain<br>' +
        'provides too much detail separated into individual records<br>' +
        'in multiple fact tables. Business users want to see all the<br>' +
        'events related to a given order in a single record so they<br>' +
        'can easily explore bottlenecks in processes.<br>' +
        '441<br>' +
        'The design and administration of the accumulating<br>' +
        'snapshot is different from the first two fact table types. All<br>' +
        'accumulating snapshot fact tables have many roles of the<br>' +
        'date dimension, usually four to ten. These are the dates<br>' +
        'that the snapshot tracks, such as order date, order fulfilled<br>' +
        'date, ship date, payment date, and delivery date. At the<br>' +
        'time the fact row for the new order is created, only the<br>' +
        'order date is known. The other date keys point to a row in<br>' +
        'the date dimension that indicates “Hasn’t happened yet.”<br>' +
        'Each time a significant event occurs, the same row in the<br>' +
        'accumulating snapshot fact table is updated.<br>' +
        'The ETL design must account for the heavy updates. If<br>' +
        'you’re using Enterprise Edition or Data Center Edition,<br>' +
        'you can partition the accumulating snapshot fact, say by<br>' +
        'order month. In many implementations, it is faster to<br>' +
        'completely reload the most recent month or two (into an<br>' +
        'empty pseudo-partition) than to perform even a bulk<br>' +
        'update.<br>' +
        'There’s inevitably a trickle of data from months ago. The<br>' +
        'older updates are best handled through the bulk update<br>' +
        'technique described previously.<br>' +
        'Accumulating snapshot fact tables present a particular<br>' +
        'challenge for Analysis Services cubes. Even if you get just<br>' +
        'one fact row update for an order opened six months ago,<br>' +
        'you need to fully process its entire SSAS partition. You<br>' +
        'can increase the granularity of your SSAS partitions, but<br>' +
        'accumulating snapshot fact tables in SSAS are often faced<br>' +
        'with fully processing a large chunk of the cube.<br>' +
        'Subsystem 14: Surrogate Key Pipeline<br>' +
        '442<br>' +
        'Every ETL system must include a step for replacing the<br>' +
        'operational natural keys in the incoming fact table record<br>' +
        'with the appropriate dimension surrogate keys. Referential<br>' +
        'integrity (RI) means that for each foreign key in the fact<br>' +
        'table, one and only one entry exists in the corresponding<br>' +
        'dimension table.<br>' +
        'WARNING<br>' +
        'Maintain referential integrity! Of all the<br>' +
        'rules and advice included in the Kimball<br>' +
        'Method, this is the one we feel most<br>' +
        'strongly about: Never build a system that<br>' +
        'does not maintain referential integrity<br>' +
        'between facts and dimensions. You should<br>' +
        'never have a null foreign key in the fact<br>' +
        'table. You should never have a fact key<br>' +
        'that does not correspond to a row in the<br>' +
        'dimension table. Never. Not ever.<br>' +
        'A dimensional model without RI is<br>' +
        'unusable for ad hoc business users, and is<br>' +
        'highly dangerous even for expert users like<br>' +
        'IT staff. It is too easy to construct a query<br>' +
        'that unintentionally drops rows.<br>' +
        'As we discuss in Chapter 5, you might<br>' +
        'choose not to have the SQL Server<br>' +
        'database enforce the foreign key<br>' +
        'constraints. But the underlying data<br>' +
        '443<br>' +
        'absolutely must not violate RI, and the<br>' +
        'surrogate key pipeline is the ETL<br>' +
        'subsystem where you do that work.<br>' +
        'After the fact data has been cleaned and transformed, just<br>' +
        'before loading into the presentation layer, a surrogate key<br>' +
        'lookup needs to occur to substitute the source system keys<br>' +
        'in the incoming fact table record with the proper current<br>' +
        'surrogate key. To preserve referential integrity, complete<br>' +
        'dimension processing before starting the surrogate key<br>' +
        'pipeline. The dimension tables are the legitimate source of<br>' +
        'primary keys to be replaced in the fact table.<br>' +
        'There are several methods of handling an RI failure. Your<br>' +
        'choice depends on the business requirements. This is<br>' +
        'seldom a requirement that you collect during the initial<br>' +
        'design, but the ETL team needs to discuss the design<br>' +
        'issues and decisions with a representative of the user<br>' +
        'community. The possible approaches include:<br>' +
        '• Throw away the fact rows. This is rarely a good solution.<br>' +
        '• Write the bad rows to an error table. This is the most<br>' +
        'common solution, but it should not be implemented unless<br>' +
        'you have procedures (both human and technical) to get the<br>' +
        'bad rows out of prison and into the fact table.<br>' +
        '• Insert a placeholder row into the dimension. The<br>' +
        'placeholder dimension row contains the only information<br>' +
        'you have at the moment: the natural key. It receives a<br>' +
        'surrogate key, which is inserted in the fact table. All other<br>' +
        'attributes are set to appropriate defaults agreed to by the<br>' +
        'business users. This is an excellent solution if you can<br>' +
        'identify a legitimate business process or technical reason<br>' +
        'why you can receive a fact row before its corresponding<br>' +
        '444<br>' +
        'dimension member. Keeping such “early arriving facts” in<br>' +
        'the fact table preserves overall totals and measures of<br>' +
        'activity even while you are waiting for the dimension detail<br>' +
        'to arrive. Once you do receive the dimension row, simply<br>' +
        'update the default labels with the correct ones. No changes<br>' +
        'need to be made to the fact table.<br>' +
        '• Fail the package and abort processing. This seems a bit<br>' +
        'draconian.<br>' +
        'WARNING<br>' +
        'In our consulting practice we review<br>' +
        'existing DW/BI implementations. We’ve<br>' +
        'often seen another technique for handling<br>' +
        'RI failures, which we cannot recommend.<br>' +
        'That solution is to map all the bad fact<br>' +
        'rows to a single Unknown member (such as<br>' +
        '-1). The single unknown member solution<br>' +
        'is popular because it gets the bad rows into<br>' +
        'the fact table so amounts add up, and it’s<br>' +
        'incredibly easy to implement. But it makes<br>' +
        'no business sense.<br>' +
        'Most often in normal daily batch processing designs, we<br>' +
        'write the bad rows to an error table. For a minority of the<br>' +
        'dimensions, and in aggressive low latency situations where<br>' +
        'early arriving facts are more common, we use the<br>' +
        'placeholder dimension row technique. We almost never<br>' +
        'throw away the bad rows, and failing the package just<br>' +
        'seems silly.<br>' +
        '445<br>' +
        'The surrogate key pipeline is one of the heaviest<br>' +
        'processing activities of the ETL system. With large data<br>' +
        'volumes, it’s important to implement it as efficiently as<br>' +
        'possible. As with most of the ETL subsystems, there are<br>' +
        'two main approaches to implementing the surrogate key<br>' +
        'pipeline in SSIS: use SSIS lookups, or use database joins.<br>' +
        'Surrogate Key Pipeline Technique #1: Cascading Lookups<br>' +
        'The Cascading Lookups technique can be implemented in<br>' +
        'the same fact table data flow where cleaning and<br>' +
        'transforming occurs. Alternatively, if you chose to stage<br>' +
        'the fact data after cleaning and transforming, the OLE DB<br>' +
        'source query would pull in the entire staging table.<br>' +
        'Figure 7-14 illustrates a sample package for populating the<br>' +
        'FactOrders table. Displayed here is a portion of the data<br>' +
        'flow, including the surrogate key pipeline for six keys. All<br>' +
        'of the lookups illustrated here use the technique of flowing<br>' +
        'error rows to an error table.<br>' +
        'Figure 7-14: Example fact table surrogate key pipeline<br>' +
        'with RI violations flowing to an error table<br>' +
        '446<br>' +
        'The main work of the surrogate key pipeline occurs in the<br>' +
        'Lookup transform, which looks up the source system key<br>' +
        'from the fact table flow into a database table, in this case,<br>' +
        'the dimension table. The lookup adds the surrogate key to<br>' +
        'the flow. The Lookup transform does the same thing as a<br>' +
        'simple SQL join, but performs that operation on the data in<br>' +
        'the flow, rather than having to stage the data to write SQL.<br>' +
        'The lookup can be cached, and in most cases you want to<br>' +
        'fully cache the lookup.<br>' +
        'WARNING<br>' +
        'Always specify a SQL query for the lookup<br>' +
        'data source. In most cases you need just<br>' +
        'two or three columns. For example, the<br>' +
        '447<br>' +
        'source query for a lookup to the date<br>' +
        'dimension for the surrogate key pipeline is<br>' +
        'SELECT DateKey, FullDate FROM DimDate<br>' +
        'The default behavior is to bring in the<br>' +
        'entire dimension table. The lookup table is<br>' +
        'cached in memory, so you want it to be as<br>' +
        'small and efficient as possible.<br>' +
        'There are two misleading characteristics of this cascade of<br>' +
        'lookups:<br>' +
        '• It appears that the lookups will operate in series, one after<br>' +
        'the other. Actually, SSIS is smart enough to perform the<br>' +
        'work in parallel.<br>' +
        '• It appears that the referential integrity failures are not<br>' +
        'handled. Each lookup is set to ignore failures, with missing<br>' +
        'surrogate keys set to null. However, the failures are being<br>' +
        'handled. They are collected near the end of the data flow and<br>' +
        'placed in an error table. The conditional split labeled “Look<br>' +
        'for bad rows” strips out rows with a null value in any<br>' +
        'surrogate key column. This design lets you collect an error<br>' +
        'row only once, even if it has problems with RI to multiple<br>' +
        'dimensions. The target error table looks just like the real fact<br>' +
        'table, but it also includes columns for the source system<br>' +
        'keys.<br>' +
        'The sample package Orders_SSIS also includes a sample<br>' +
        'of the design pattern for creating a placeholder row in the<br>' +
        'dimension table. Figure 7-15 illustrates this design pattern.<br>' +
        'The data flow splits the unmatched data: the rows for<br>' +
        'which there was no corresponding entry in the currency<br>' +
        'dimension. The OLE DB Command transform adds a<br>' +
        '448<br>' +
        'placeholder row to DimCurrency, followed by an<br>' +
        'uncached lookup to pick up the key for that new row. (You<br>' +
        'can do this in the OLE DB Command transform. It’s the<br>' +
        'same work either way.) Finally, the Union transform<br>' +
        'combines the main flow with the error flow.<br>' +
        'WARNING<br>' +
        'The Union transform can be troublesome<br>' +
        'during development. If you change the<br>' +
        'shape of the data flow above the transform,<br>' +
        'for example by adding a column, the Union<br>' +
        'transform often doesn’t adjust properly.<br>' +
        'We’ve found a measure of peace by always<br>' +
        'deleting and replacing the Union transform,<br>' +
        'rather than trying to fix it. Most other<br>' +
        'transforms can be fixed up, often as easily<br>' +
        'as opening the object. But you occasionally<br>' +
        'have to delete and replace more<br>' +
        'complicated objects than the Union<br>' +
        'transform as well.<br>' +
        'Figure 7-15: RI violations create placeholder row in<br>' +
        'dimension<br>' +
        '449<br>' +
        'Surrogate Key Pipeline Technique #2: Database Joins<br>' +
        'The database intensive technique requires that you stage<br>' +
        'the data before the surrogate key pipeline. As we discussed<br>' +
        'in subsystem 4, you may already have chosen to stage the<br>' +
        'data after cleaning and transformation, for auditing or<br>' +
        'restarting the ETL process.<br>' +
        'The design pattern starts with a big query, joining the clean<br>' +
        'staged data to each of the dimensions. In the case of the<br>' +
        'Orders fact table, this query joins the fact table to eight<br>' +
        'dimension tables (date is used twice), using outer joins to<br>' +
        'pick up the RI violations. The output flow from the source<br>' +
        'adapter contains columns for all eight surrogate keys, all<br>' +
        'eight natural keys, and all the other columns in the fact<br>' +
        'table.<br>' +
        '450<br>' +
        'Stage the clean, transformed data to a table, then write a<br>' +
        'SQL query joining the staged data to the dimension tables.<br>' +
        'This approach is illustrated in Figure 7-16. Note that the<br>' +
        'surrogate key lookups that are handled in the standard way<br>' +
        '— by flowing into an error table — do not explicitly show<br>' +
        'up in the data flow. Their design pattern here is the same<br>' +
        'as in Figure 7-15, using the conditional split near the<br>' +
        'bottom of the flow.<br>' +
        'The SQL-based design pattern for adding a placeholder<br>' +
        'row to the dimension, as we saw in the currency lookup, is<br>' +
        'very similar to before. But here we use a conditional split<br>' +
        'to identify unmatched rows (where the CurrencyKey is<br>' +
        'null). The OLE DB Command, Lookup, and Union All<br>' +
        'transforms are the same as previously. You would add a<br>' +
        'conditional split branch for each dimension that may need<br>' +
        'a placeholder entry.<br>' +
        'Figure 7-16: Database joins perform surrogate key<br>' +
        'lookups<br>' +
        '451<br>' +
        'Which Technique Is Best?<br>' +
        'We don’t have a strong opinion about which is the best<br>' +
        'technique for incremental loads. Both techniques use the<br>' +
        'data flow task, and so you still have an opportunity to<br>' +
        'perform complex manipulations on error flows. As a rule<br>' +
        'of thumb, if you’re already staging the cleaned and<br>' +
        'transformed fact data for other reasons, the database join<br>' +
        'technique probably performs better. If not, you need to<br>' +
        'count the cost of staging and indexing the fact data in your<br>' +
        'assessment of relative performance.<br>' +
        'When you’re designing the packages for the one-time<br>' +
        'historic load, the database join technique is likely to be<br>' +
        'your best choice. The historic load requires a complex join<br>' +
        'to each dimension with type 2 attributes, in order to pick<br>' +
        'up the surrogate key associated with the dimension<br>' +
        '452<br>' +
        'member at the time the fact event occurred. As we discuss<br>' +
        'in subsystem 16, the data volumes associated with the<br>' +
        'historic load push you toward using the database join<br>' +
        'technique.<br>' +
        'Subsystem 15: Multi-Valued Dimension Bridge Table<br>' +
        'Builder<br>' +
        'Sometimes a fact table must support a dimension that takes<br>' +
        'on multiple values at the lowest granularity of the fact<br>' +
        'table, as described in Chapter 2. If the grain of the fact<br>' +
        'table cannot be changed to directly support this dimension,<br>' +
        'the multi-valued dimension must be linked to the fact table<br>' +
        'via a bridge table. Bridge tables are common in the<br>' +
        'healthcare industry, financial services, insurance, and for<br>' +
        'supporting variable depth hierarchies (see subsystem 11).<br>' +
        'The challenge for the ETL team is building and<br>' +
        'maintaining the bridge table. As multi-valued relationships<br>' +
        'to the fact row are encountered, the ETL system has the<br>' +
        'choice of either making each set of observations a unique<br>' +
        'group, or reusing groups when an identical set of<br>' +
        'observations occurs. Reusing groups is more work for the<br>' +
        'ETL process, but results in a much faster user query<br>' +
        'experience. In the event the multi-valued dimension has<br>' +
        'type 2 attributes, the bridge table must also be<br>' +
        'time-varying, such as a patient’s time-varying set of<br>' +
        'diagnoses.<br>' +
        'Within SSIS, the bridge table row can be constructed on<br>' +
        'the fly during fact table loading. If a row does not already<br>' +
        'exist in the bridge table for the current set of observations,<br>' +
        'construct it, add it to the bridge table, and return the key to<br>' +
        '453<br>' +
        'the fact table flow. The design is almost identical to the<br>' +
        '“placeholder row” technique detailed in subsystem 14.<br>' +
        'Subsystem 16: Late Arriving Data Handler<br>' +
        'Data warehouses are built around the ideal assumption that<br>' +
        'fact rows arrive around the same time the activity occurs.<br>' +
        'When the fact rows and dimension rows both refer to<br>' +
        '“yesterday,” the ETL design for the incremental load is<br>' +
        'straightforward and performs well. However, there may be<br>' +
        'cases where either fact or dimension rows trickle in long<br>' +
        'after the event occurs.<br>' +
        'Late Arriving Dimension Members<br>' +
        'We’ve already described the design pattern for late<br>' +
        'arriving dimension data. If your business processes or<br>' +
        'technical environment are such that you can reasonably<br>' +
        'expect to see fact rows arrive before their corresponding<br>' +
        'dimension rows, the best approach is to insert the<br>' +
        'placeholder row in the dimension table, as detailed in<br>' +
        'subsystem 14. When the dimension information eventually<br>' +
        'shows up, all you need to do is update the attributes in the<br>' +
        'dimension row. The fact row doesn’t have to change.<br>' +
        'NOTE<br>' +
        'There’s a nuance here around how you<br>' +
        'treat the first update to the dimension row,<br>' +
        'in the case where the dimension has any<br>' +
        'type 2 attributes. You should be able to<br>' +
        '454<br>' +
        'make the initial update to the dimension<br>' +
        'attributes as a type 1 change. The best way<br>' +
        'to handle this is to add a metadata column<br>' +
        'to the dimension table that tracks whether<br>' +
        'the row was added by the fact table<br>' +
        'processing, and flips a bit when you update<br>' +
        'the attributes the first time. The SSIS<br>' +
        'slowly changing dimension transform can<br>' +
        'handle this situation, with the feature called<br>' +
        'Inferred Member Support.<br>' +
        'Late Arriving Dimension Updates<br>' +
        'What if you have a dimension member already, but you<br>' +
        'receive a late notification of a change to a type 2 attribute?<br>' +
        'This happened with a recent client who was building a<br>' +
        'schema around employees’ projects and activities. The<br>' +
        'human resources business process queued up changes until<br>' +
        'the end of the month and then made those changes en<br>' +
        'masse. But the employee might have had new<br>' +
        'responsibilities for a full month by the time the systems<br>' +
        'received the information! The monthly update included a<br>' +
        'retroactive effective date.<br>' +
        'Although it’s easy to say the business process is stupid and<br>' +
        'should change (you can rest assured we did say that, but a<br>' +
        'little more diplomatically), the reality is that a change to<br>' +
        'the underlying business process is unlikely in the<br>' +
        'necessary timeframe. In this scenario, the ETL system<br>' +
        'needs to monitor for retroactive type 2 changes to<br>' +
        'dimension rows. The unfortunate cascading implication is<br>' +
        '455<br>' +
        'that the existing fact rows for events after the late arriving<br>' +
        'change may need to update their surrogate key to point to<br>' +
        'the dimension member in effect at the time the fact event<br>' +
        'occurred.<br>' +
        'The ripples of the late arriving update continues into<br>' +
        'Analysis Services. The fact table updates imply that you’ll<br>' +
        'usually be fully processing the most recent month or two<br>' +
        'of partitions in SSAS (or the entire cube if you’re not using<br>' +
        'cube partitions).<br>' +
        'Note that late arriving dimension members and updates are<br>' +
        'far more likely to occur the closer to real time you’re<br>' +
        'pushing your data warehouse. There can be a significant<br>' +
        'performance issue associated with frequent updates of the<br>' +
        'fact table keys.<br>' +
        'Late Arriving Facts<br>' +
        'At first glance, late arriving facts don’t appear to be a<br>' +
        'problem. It seems the only difference is that the surrogate<br>' +
        'key for the date dimension isn’t yesterday, but some other<br>' +
        'date in the past. The wrinkle comes with a dimension that<br>' +
        'has type 2 attributes. You need to assign the late arriving<br>' +
        'fact row the dimension surrogate key that was in effect at<br>' +
        'the time the fact event occurred, not the surrogate key<br>' +
        'that’s in effect today.<br>' +
        'It turns out this problem is not a difficult one to solve in<br>' +
        'SSIS. The solution depends on which surrogate key<br>' +
        'pipeline technique you’re using (subsystem 14):<br>' +
        '456<br>' +
        '• Database joins: If you’re using a large outer join query to<br>' +
        'perform the work of the surrogate key pipeline, you need to<br>' +
        'make that query a bit more complex. The join to any type 2<br>' +
        'dimensions for fact tables that may experience late arriving<br>' +
        'data needs to use a BETWEEN clause. Join on the source<br>' +
        'system key, where the event date on the fact table is<br>' +
        'BETWEEN RowStartDate and RowEndDate: the dates for<br>' +
        'which this is the active row for this dimension member.<br>' +
        'DOWNLOADS<br>' +
        'You can see the database joins design<br>' +
        'pattern in the Orders_SQL package on the<br>' +
        'book’s web site. The cascading lookups<br>' +
        'design pattern is illustrated in the<br>' +
        'Orders_Lookups package.<br>' +
        '• Cascading lookups: The situation is a little trickier if you’re<br>' +
        'using cascading lookups to implement your surrogate key<br>' +
        'pipeline. The reason is that the Lookup transform doesn’t have<br>' +
        'an easy way to set up a BETWEEN clause on the implicit join. But<br>' +
        'you can trick the Lookup transform into doing what you want.<br>' +
        'An overview of the steps are to:<br>' +
        '• Use a conditional split to divide the data flow into<br>' +
        'current and late arriving fact data streams.<br>' +
        '• Use the standard cached lookup on the current stream,<br>' +
        'as described in subsystem 14.<br>' +
        '• Use a derived column transform to add a copy of the<br>' +
        'event date to the late-arriving stream.<br>' +
        '• Use an uncached lookup on the late arriving stream.<br>' +
        'On the mappings tab, map one event date to<br>' +
        'RowStartDate and the second derived event date to<br>' +
        'RowEndDate. Finally, on the Advanced tab, you can<br>' +
        'modify the query to be parameterized.<br>' +
        '457<br>' +
        '• Use the Union All transform to combine the current<br>' +
        'and late-arriving data streams, and continue<br>' +
        'processing.<br>' +
        'In most cases of late arriving facts, the late arriving stream<br>' +
        'is less than 3% of the incremental load. The cascading<br>' +
        'lookups technique works surprisingly well for incremental<br>' +
        'loads where a small minority of fact data is late arriving.<br>' +
        'Make sure there’s an index on the natural key in the<br>' +
        'dimension table.<br>' +
        'Don’t use the cascading lookups technique to perform the<br>' +
        'surrogate key pipeline work for the one time historic load,<br>' +
        'unless you have no history of type 2 dimension changes.<br>' +
        'For the historic load, the vast majority of data is not<br>' +
        'current, and hence would go through the uncached lookup.<br>' +
        'Use the database join technique to implement the surrogate<br>' +
        'key pipeline for the historic load.<br>' +
        'Subsystem 17: Dimension Manager<br>' +
        'The dimension manager is a centralized authority who<br>' +
        'prepares and publishes conformed dimensions to the data<br>' +
        'warehouse community. A conformed dimension is by<br>' +
        'necessity a centrally managed resource; each conformed<br>' +
        'dimension must have a single, consistent source. The<br>' +
        'dimension manager’s responsibilities include the following<br>' +
        'ETL processing tasks:<br>' +
        '• Implement the common descriptive labels agreed to by the<br>' +
        'data stewards and stakeholders during the dimensional<br>' +
        'design.<br>' +
        '• Add new rows to the conformed dimension for new source<br>' +
        'data, generating new surrogate keys.<br>' +
        '458<br>' +
        '• Manage attribute changes, generating new surrogate keys or<br>' +
        'updating in place, as appropriate.<br>' +
        '• Distribute the revised dimension simultaneously to all fact<br>' +
        'table providers.<br>' +
        'If you have a single centralized data warehouse, perhaps<br>' +
        'feeding downstream Analysis Services cubes, the<br>' +
        'dimension manager’s job is easy: there’s only one copy of<br>' +
        'the dimension table. The job is harder in a distributed<br>' +
        'environment. As described in subsystem 8, many of the<br>' +
        'challenges are political as much as technical.<br>' +
        'RESOURCES<br>' +
        'For a longer discussion of the Dimension<br>' +
        'Manager system, see The Data Warehouse<br>' +
        'Lifecycle Toolkit, Second Edition, page<br>' +
        '402.<br>' +
        'Subsystem 18: Fact Provider System<br>' +
        'The fact provider owns the administration of one or more<br>' +
        'fact tables, and is responsible for their creation,<br>' +
        'maintenance, and use. If fact tables are used in any<br>' +
        'drill-across applications, then by definition the fact<br>' +
        'provider must be using conformed dimensions provided by<br>' +
        'the dimension manager. The fact provider’s<br>' +
        'responsibilities include:<br>' +
        '• Receive duplicated dimensions from the dimension manager.<br>' +
        '• Add new records to fact tables after replacing their source<br>' +
        'system keys with surrogate keys.<br>' +
        '459<br>' +
        '• Modify records in all fact tables for error correction,<br>' +
        'accumulating snapshots, and late arriving dimension<br>' +
        'changes.<br>' +
        '• Remove and recalculate prestored aggregates that have<br>' +
        'become invalidated (see subsystem 19).<br>' +
        '• Assure the quality of all base and aggregate fact tables.<br>' +
        '• Bring updated fact and dimension tables online.<br>' +
        '• Inform users that the database has been updated. Notify them<br>' +
        'of any major changes or issues.<br>' +
        'Subsystem 19: Aggregate Builder<br>' +
        'Aggregates are the single most dramatic way to affect<br>' +
        'performance in a large data warehouse environment.<br>' +
        'Aggregates are like indexes: they are specific data<br>' +
        'structures created to improve performance.<br>' +
        'In SQL Server implementations, most organizations that<br>' +
        'need performance aggregates implement them in Analysis<br>' +
        'Services. As we describe in Chapter 8, SSAS has a wealth<br>' +
        'of features for designing, defining, and updating<br>' +
        'performance aggregates.<br>' +
        'RESOURCES<br>' +
        'For a longer discussion of building<br>' +
        'aggregates when you are deploying to a<br>' +
        'relational database, see The Data<br>' +
        'Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition, page 134–137, and also detailed<br>' +
        'articles in The Kimball Group Reader,<br>' +
        'pages 536–546.<br>' +
        '460<br>' +
        'Subsystem 20: OLAP Cube Builder<br>' +
        'OLAP cubes present dimensional data in an intuitive way,<br>' +
        'enabling analytic users to slice and dice data. SQL Server<br>' +
        'Analysis Services is a sibling of dimensional models in the<br>' +
        'relational database, with intelligence about relationships<br>' +
        'and calculations defined on the server that enable faster<br>' +
        'query performance and more interesting analytics from a<br>' +
        'broad range of query tools. You shouldn’t think of SSAS<br>' +
        'as a competitor to a relational data warehouse, but rather<br>' +
        'an extension. Let the relational database do what it does<br>' +
        'best: provide storage and management.<br>' +
        'The relational dimensional schema is the foundation for<br>' +
        'the Analysis Services cubes. It is easy to design cubes on<br>' +
        'top of clean, conformed, well maintained dimensional<br>' +
        'models. If you try to perform ETL and data cleaning<br>' +
        'during cube design and processing, you’re asking for<br>' +
        'trouble.<br>' +
        'You can launch Analysis Services processing from within<br>' +
        'your SSIS packages. Most often, the master package<br>' +
        'launches dimension table processing, then fact table<br>' +
        'processing, and then ends with a package or packages that<br>' +
        'perform SSAS processing. An alternative approach is to<br>' +
        'integrate SSAS processing with each object: end the<br>' +
        'customer dimension package with a task to process the<br>' +
        'customer dimension in SSAS. We’ll return to this question<br>' +
        'in Chapter 8.<br>' +
        'Subsystem 21: Data Propagation Manager<br>' +
        '461<br>' +
        'The data propagation manager is responsible for the ETL<br>' +
        'processes required to transfer conformed, integrated<br>' +
        'enterprise data from the data warehouse presentation<br>' +
        'server to other environments for special purposes. Many<br>' +
        'organizations need to extract data from the presentation<br>' +
        'layer to share with business partners, customers, vendors,<br>' +
        'data mining applications, or government organizations.<br>' +
        'These situations require extraction from the data<br>' +
        'warehouse, possibly some light transformation, and<br>' +
        'loading into a target format: ETL. The big difference<br>' +
        'between data propagation projects and normal data<br>' +
        'warehouse projects is usually that the target format is<br>' +
        'completely non-negotiable. Consider data propagation as<br>' +
        'part of your ETL system and leverage SSIS to provide this<br>' +
        'capability. The SSIS packages to deliver data warehouse<br>' +
        'data to downstream BI applications are usually much<br>' +
        'simpler than the “real” ETL packages.<br>' +
        'Managing the ETL Environment<br>' +
        'A DW/BI system can have a great dimensional model,<br>' +
        'compelling BI applications, and strong sponsorship. But it<br>' +
        'will not be a success until it can be relied upon as a<br>' +
        'dependable source for business decision making. One of<br>' +
        'the goals for the data warehouse is to build a reputation for<br>' +
        'delivering timely, consistent, and reliable information to<br>' +
        'the user community. The ETL system must strive for<br>' +
        'reliability, availability, and manageability.<br>' +
        'The remaining thirteen ETL subsystems have to do with<br>' +
        'managing your ETL environment. We briefly introduce<br>' +
        '462<br>' +
        'them here, and talk about many of these issues in more<br>' +
        'depth in Chapter 17.<br>' +
        '• Subsystem 22 — Job Scheduler: Every enterprise data<br>' +
        'warehouse needs a robust ETL scheduler. The ETL process<br>' +
        'should be managed through a metadata-driven job control<br>' +
        'environment. In production, many organizations launch the<br>' +
        'SSIS master package from SQL Agent. SQL Agent supports<br>' +
        'simple job scheduling and dependency, and the master<br>' +
        'package itself provides more complex control. There is<br>' +
        'nothing special about using SQL Agent to launch the master<br>' +
        'package. An alternative approach is to use command line<br>' +
        'utility dtexec to start the package; use a script or other<br>' +
        'enterprise scheduling software to call dtexec.<br>' +
        '• Subsystem 23 — Backup System: The data warehouse is<br>' +
        'subject to the same risks as any other computer system. Disk<br>' +
        'drives fail and power supplies go out. Though typically not<br>' +
        'managed by the ETL team, the backup and recovery process<br>' +
        'is often designed as part of the ETL system. We discuss<br>' +
        'backup and recovery at greater length in Chapter 17.<br>' +
        '• Subsystem 24 — Recovery and Restart System: After your<br>' +
        'ETL system is in production, failures can occur for countless<br>' +
        'reasons beyond the control of your ETL process. Common<br>' +
        'causes of ETL process failures include network, database,<br>' +
        'disk, and memory failures. As we have discussed throughout<br>' +
        'this chapter, you need staged data sets as a basis for<br>' +
        'restarting the system. Without staged data, you’ll have to<br>' +
        'redo all the hard ETL work, and may be in a situation where<br>' +
        'you do not have a consistent starting point. SSIS contains<br>' +
        'features for checkpoints and binding multiple tasks into<br>' +
        'transactions. These features are useful, but you need to<br>' +
        'design your packages for restartability; there’s no magic.<br>' +
        'One of the easiest recovery and restart systems to implement<br>' +
        'is simply to snapshot the data warehouse database as the first<br>' +
        'step in the ETL process. If there is a failure in ETL, the<br>' +
        'database can simply roll back to the snapshot. The snapshot<br>' +
        'adds a bit of overhead to the system, but remarkably little for<br>' +
        'such a convenient technique.<br>' +
        '463<br>' +
        '• Subsystem 25 — Version Control System: The version<br>' +
        'control system is a “snapshotting” capability for archiving<br>' +
        'and recovering your SSIS packages. As we discuss at the<br>' +
        'beginning of this chapter, the BI Development Studio<br>' +
        'integrates with a wide variety of source control applications<br>' +
        'including Microsoft’s Team Foundation Server.<br>' +
        '• Subsystem 26 — Version Migration System: After the ETL<br>' +
        'team gets past the difficult process of designing and<br>' +
        'developing the ETL process and manages to complete the<br>' +
        'creation of the jobs required to load the data warehouse, the<br>' +
        'jobs must be bundled and migrated to the next environment<br>' +
        '— from development to test and on to production. Migrating<br>' +
        'SSIS packages between environments is quite<br>' +
        'straightforward. These issues, and the use of package<br>' +
        'configurations to modify variables and connection<br>' +
        'information at runtime, are discussed in Chapter 16.<br>' +
        '• Subsystem 27 — Workflow Monitor: The ETL system must<br>' +
        'be constantly monitored to ensure the ETL processes are<br>' +
        'operating efficiently and the warehouse is being loaded on a<br>' +
        'timely basis. The audit system, ETL logs, and database<br>' +
        'monitoring information are your key tools. In Chapter 17 we<br>' +
        'provide an overview of the kind of monitors to place on your<br>' +
        'system, and some items to keep an eye out for.<br>' +
        '• Subsystem 28 — Sorting System: Certain common ETL<br>' +
        'processes call for data to be sorted in a particular order.<br>' +
        'Because sorting is such a fundamental ETL processing<br>' +
        'capability, it’s called out as a separate subsystem. SSIS<br>' +
        'contains a Sort transform within the data flow. This<br>' +
        'transform works well for small to medium data volumes, but<br>' +
        'should be avoided for very large data sets. The Sort<br>' +
        'transform functions well when the entire data set fits in<br>' +
        'memory; its performance degrades significantly when data<br>' +
        'must be paged to disk. You have two alternatives to the<br>' +
        'built-in Sort transform. If the data is in a staging table, use<br>' +
        'an ORDER BY clause on the source query to pre-sort the data.<br>' +
        'You need to go into the Source Editor and tell SSIS that the<br>' +
        'data is sorted. This is easy to do; you just need to remember<br>' +
        'to do it. Alternatively, there are third-party sort utilities<br>' +
        'available, including SyncSort, CoSort and NSort. These<br>' +
        '464<br>' +
        'utilities constitute the “high end” for extreme high<br>' +
        'performance sorting applications. In some cases, it makes<br>' +
        'sense to drop data out of a relational format just in order to<br>' +
        'use these packages. As ever, you’ll need to evaluate your<br>' +
        'specific problem to reach a conclusion as to the best course<br>' +
        'for you.<br>' +
        '• Subsystem 29 — Lineage and Dependency Analyzer: Two<br>' +
        'increasingly important elements requested of enterprise-class<br>' +
        'ETL systems are lineage and dependency. Lineage is the<br>' +
        'ability to look at a data element — for example a number in<br>' +
        'a report — and see exactly how it was populated.<br>' +
        'Dependency is the other direction: look at a source table or<br>' +
        'column, and identify all the packages, data warehouse tables,<br>' +
        'cubes, and reports that might be affected by a change.<br>' +
        'Although you can lay the groundwork for a lineage and<br>' +
        'dependency analyzer by following the practices outlined in<br>' +
        'this chapter, Microsoft does not provide such an analyzer.<br>' +
        'The good news is that all SQL Server components rely on<br>' +
        'well structured metadata, so the problem is theoretically<br>' +
        'solvable. Someday.<br>' +
        '• Subsystem 30 — Problem Escalation System: The execution<br>' +
        'of the ETL system should be a hands-off operation, running<br>' +
        'like clockwork without human intervention. If a problem<br>' +
        'occurs, the ETL process should handle it gracefully: logging<br>' +
        'minor errors, and notifying the operator of major errors. In<br>' +
        'subsystem 4 we discussed the design pattern for halting<br>' +
        'package execution in the event of a major error. Before you<br>' +
        'go to production, you should have processes in place for<br>' +
        'examining the error logs, and rehabilitating bad data.<br>' +
        '• Subsystem 31 — Parallelizing/Pipelining System: The goal<br>' +
        'of the ETL system, in addition to providing high quality<br>' +
        'data, is to load the data warehouse within the allocated<br>' +
        'processing window. For many organizations this can be a<br>' +
        'challenge, and you should always look for opportunities to<br>' +
        'parallelize your process flows. SSIS is good at doing this on<br>' +
        'your behalf. There are several properties of the Data Flow<br>' +
        'task that you can adjust for each task to improve<br>' +
        'performance. These are DefaultBufferMaxRows,<br>' +
        'DefaultBufferSize, and EngineThreads. In addition,<br>' +
        '465<br>' +
        'since each target table has its own package, you can<br>' +
        'experiment with the layout of dependencies in the master<br>' +
        'package, executing different packages at the same time. Of<br>' +
        'course, dimensions must be processed before facts, but<br>' +
        'different fact tables can start at different times. Also consider<br>' +
        'that if you’re staging the data at multiple points in the<br>' +
        'process, you’ll be creating varying loads on the database<br>' +
        'server and SSIS.<br>' +
        '• Subsystem 32 — Security System: The ETL system and<br>' +
        'staging tables should be off limits to all business users, with<br>' +
        'the possible exception of an auditing organization.<br>' +
        'Administer role-based security on the servers and databases<br>' +
        'and backup media. Security is discussed in greater detail in<br>' +
        'Chapter 14.<br>' +
        '• Subsystem 33 — Compliance Manager: In highly regulated<br>' +
        'environments, supporting compliance requirements is a<br>' +
        'significant new requirement for the ETL team. Compliance<br>' +
        'in the data warehouse boils down to maintaining the chain of<br>' +
        'custody of the data. The data warehouse must carefully<br>' +
        'guard the compliance-sensitive data entrusted to it from the<br>' +
        'moment it arrives. The foundation of your compliance<br>' +
        'system is the interaction of several subsystems already<br>' +
        'described: lineage and dependency analysis, version control,<br>' +
        'backup and restore, security, the audit dimension, and<br>' +
        'logging and monitoring.<br>' +
        '• Subsystem 34 — Metadata Repository Manager: We<br>' +
        'continue to hope that Microsoft will deliver a Metadata<br>' +
        'Repository Manager someday. Metadata is discussed in<br>' +
        'greater detail in Chapter 15.<br>' +
        'Summary<br>' +
        'Developing the ETL system is one of the greatest<br>' +
        'challenges of the DW/BI project. No matter how thorough<br>' +
        'your interviews and analyses, you’ll uncover data quality<br>' +
        'problems when you start building the ETL system. Some<br>' +
        'of those data quality problems may be bad enough to force<br>' +
        'a redesign of the schema.<br>' +
        '466<br>' +
        'One of the key early steps in the ETL system development<br>' +
        'is to write the ETL plan. Ideally, we like to see a<br>' +
        'specification document that describes your standard<br>' +
        'approaches for the subsystems presented in this chapter<br>' +
        'and that documents nonstandard techniques for specific<br>' +
        'challenges. All of these issues need to be addressed before<br>' +
        'you go into production, and it’s best to give some thought<br>' +
        'before you start writing your first SSIS package. At the<br>' +
        'very least, you need to finish the logical and physical data<br>' +
        'models, complete the source to target maps, and write a<br>' +
        'brief overview of your planned approach.<br>' +
        'The extract logic is a challenging step. You need to work<br>' +
        'closely with the source system programmers to develop an<br>' +
        'extract process that generates quality data, does not place<br>' +
        'an unbearable burden on the transaction system, and can<br>' +
        'be incorporated into the flow of your automated ETL<br>' +
        'system.<br>' +
        'The ETL system may seem mysterious, but hopefully at<br>' +
        'this point you realize it’s fairly straightforward. We’ve<br>' +
        'tried to demystify the jargon around implementing<br>' +
        'dimensional designs, emphasizing the importance of using<br>' +
        'surrogate keys and maintaining referential integrity.<br>' +
        'We hope that the many ideas presented in this chapter have<br>' +
        'sparked your creativity for designing your ETL systems.<br>' +
        'More than anything else, we want to leave you with the<br>' +
        'notion that there are many ways to solve any problem. The<br>' +
        'most difficult problems require patience and perseverance.<br>' +
        '467<br>';
    document.getElementById('chapter6').innerHTML = 'Chapter 6<br>' +
        'Master Data Management<br>' +
        '“The truth will set you free, but first it will make you<br>' +
        'miserable.”<br>' +
        '— Attributed to James A. Garfield<br>' +
        'If you are reading this book from start to finish, you should<br>' +
        'now have the target dimensional model built in the<br>' +
        'relational database and be ready to start working on the<br>' +
        'ETL system. One of the big issues you may have struggled<br>' +
        'with in designing the dimensional model was figuring out<br>' +
        'where all the attributes in a given conformed dimension<br>' +
        'were going to come from. In many organizations, there are<br>' +
        'multiple sources for the same attribute, and multiple<br>' +
        'versions of the same entity. In addition, you probably<br>' +
        'identified attributes the business users need in the<br>' +
        'dimension that only exist in an Excel spreadsheet.<br>' +
        'If this describes your source environment, you would do<br>' +
        'well to deal with these data integration issues before you<br>' +
        'pull the dimensions into the warehouse. In the broader IT<br>' +
        'world, this is called master data management. Master data<br>' +
        'is reference data that is managed centrally for an<br>' +
        'organization. Master data describes the business entities<br>' +
        'that participate in the transaction systems. These business<br>' +
        'entities include people such as customers and employees;<br>' +
        'places such as warehouses, sales offices, and<br>' +
        'manufacturing plants; physical entities such as products<br>' +
        '329<br>' +
        'and assets; and logical entities such as organizational<br>' +
        'structures and charts of accounts.<br>' +
        'Master data sounds a lot like dimensions. Dimension data<br>' +
        'and master data are closely related, but they are not exactly<br>' +
        'the same thing. Ideally, master data is much closer to the<br>' +
        'transaction systems. A master data management system<br>' +
        'can be an excellent source for the dimension data in the<br>' +
        'data warehouse. A data warehouse architect would love to<br>' +
        'have a robust master data management system in place in<br>' +
        'the transaction environment, because such a system would<br>' +
        'solve the hardest problems in the dimension table ETL<br>' +
        'process.<br>' +
        'In this chapter, we describe master data and master data<br>' +
        'management. We introduce a wide range of potential<br>' +
        'master data management implementations, and discuss<br>' +
        'how such implementations can benefit the downstream<br>' +
        'DW/BI system. In SQL Server 2008 R2, Microsoft<br>' +
        'introduced a new component of SQL Server, called Master<br>' +
        'Data Services. As you can infer from its name, you can use<br>' +
        'Master Data Services, or MDS, to build a master data<br>' +
        'management solution. We provide an overview of the new<br>' +
        'MDS, and describe how to implement a simple MDS<br>' +
        'deployment.<br>' +
        'In this chapter, you will learn:<br>' +
        '• What master data management is, and how it may fit into<br>' +
        'your enterprise.<br>' +
        '• What are the scenarios in which it makes sense to manage<br>' +
        'master data before the ETL process.<br>' +
        '• What is Microsoft’s Master Data Services product.<br>' +
        '330<br>' +
        '• How to get started with simple applications in Master Data<br>' +
        'Services.<br>' +
        'Managing Master Reference Data<br>' +
        'Master data should more properly be called master<br>' +
        'reference data. Master data is the centrally managed<br>' +
        'reference data for your organization. Few organizations<br>' +
        'have a clear system in place to manage the master<br>' +
        'reference data in the transaction environment. This has led<br>' +
        'to multiple versions of the same data entities being created<br>' +
        'and maintained in different transaction subsystems. Data<br>' +
        'warehouses have long managed the process of<br>' +
        'reintegrating these entities for the purposes of reporting<br>' +
        'and analysis. This is sometimes called downstream master<br>' +
        'data management. Only a few organizations have<br>' +
        'leveraged this downstream master data management to<br>' +
        'improve the transactional data because rarely are there<br>' +
        'effective pipelines for the reverse flow of data from the<br>' +
        'data warehouse back to the transaction systems. Practically<br>' +
        'speaking, the best place for managing your organization’s<br>' +
        'master reference data is before the data warehouse, in a<br>' +
        'separate though related management system.<br>' +
        'RESOURCES<br>' +
        'For more information on master data<br>' +
        'management system architectural<br>' +
        'approaches, see The Kimball Group<br>' +
        'Reader, pp. 515–520, or search<br>' +
        'KimballGroup.com for “MDM.”<br>' +
        '331<br>' +
        'Master data management is also related to data<br>' +
        'governance: the processes and rules for managing data in<br>' +
        'your organization. These rules include required fields,<br>' +
        'standard values, data retention policies, and so on. Some<br>' +
        'organizations have mature data governance policies in<br>' +
        'place; others have scarcely given it a thought. Most<br>' +
        'organizations are a patchwork, with some areas strongly<br>' +
        'managed, such as definition of a chart of accounts. Within<br>' +
        'the same organization, other data is managed in a more ad<br>' +
        'hoc fashion. The more rigorous your organization is about<br>' +
        'data governance, the easier it will be to implement a<br>' +
        'master data management system. Indeed, it’s impossible to<br>' +
        'be truly serious about data governance without some kind<br>' +
        'of master data management system in place. But even<br>' +
        'organizations with little rigorous data governance can<br>' +
        'benefit from a new master data management solution, as<br>' +
        'we’ll describe.<br>' +
        'An effective master data management solution must enable<br>' +
        'effective data governance, while providing the technical<br>' +
        'infrastructure to solve a wide range of data problems. The<br>' +
        'most common master data management problems include<br>' +
        'incomplete attributes, data integration, and systems<br>' +
        'integration.<br>' +
        'Incomplete Attributes<br>' +
        'Few transaction systems include all the descriptive<br>' +
        'attributes needed for rich reporting and analytics. After all,<br>' +
        'transaction systems are built to process transactions, for<br>' +
        'which a rich set of descriptive attributes is seldom<br>' +
        'necessary. Most organizations use purchased ERP systems<br>' +
        '332<br>' +
        'for some or all of their key operations, and these packaged<br>' +
        'systems can be difficult to extend.<br>' +
        'As you gather the business requirements for the data<br>' +
        'warehouse, the business users often voice complaints<br>' +
        'about incomplete attributes. Most often the missing<br>' +
        'attributes are alternate rollups or hierarchies. Business<br>' +
        'users maintain additional product rollups or organizational<br>' +
        'hierarchies in various unmanaged applications, typically<br>' +
        'Excel or Access.<br>' +
        'NOTE<br>' +
        'One recent client, a large multinational<br>' +
        'corporation, was limited by its accounting<br>' +
        'software to a 5-level organizational<br>' +
        'hierarchy. The top three levels were Global<br>' +
        'Area (Americas, EMEA, Asia), Region<br>' +
        '(such as Western Europe and SE Asia), and<br>' +
        'Country. This left just two levels to<br>' +
        'describe the organization inside a country,<br>' +
        'which worked fine for Switzerland or<br>' +
        'Columbia but not so well for the U.S. or<br>' +
        'China where this company did most of its<br>' +
        'business.<br>' +
        'As crazy as this sounds, the organizational<br>' +
        'structure enabled transactions to be<br>' +
        'processed, and senior management to get<br>' +
        'the top line reports they needed. But as<br>' +
        '333<br>' +
        'you’d expect, middle managers throughout<br>' +
        'the world had created all sorts of rollups to<br>' +
        'help them analyze their operations in<br>' +
        'sufficient detail. Consolidating those<br>' +
        'rollups into a single hierarchy — or at<br>' +
        'worst two or three hierarchies — presents<br>' +
        'challenges both technical and managerial.<br>' +
        'The data warehouse team attempts to address the problem<br>' +
        'of incomplete attributes. Often, the team begins by<br>' +
        'importing the missing attributes from an Excel file on a<br>' +
        'business user’s desktop. There are two advantages to the<br>' +
        'Excel solution: it puts the business people in charge of<br>' +
        'defining the analytic attributes, and it’s quick and easy<br>' +
        '(until it breaks). But Excel is a fragile building block for<br>' +
        'systems, and we all know where this will lead. Eventually,<br>' +
        'either the process breaks causing a failed load, or the data<br>' +
        'is incorrectly attributed causing data quality problems and<br>' +
        'retroactive updates.<br>' +
        'A better solution is to build a custom applet for business<br>' +
        'users to maintain the attributes. Many data warehouse<br>' +
        'teams have done so, building a handful of custom<br>' +
        'applications for the most important attributes that need to<br>' +
        'be maintained outside of the transaction system. As we<br>' +
        'describe later in this chapter, you can use SQL Server’s<br>' +
        'Master Data Services as the platform for business users to<br>' +
        'maintain dimensional attributes and hierarchies.<br>' +
        '334<br>' +
        'There are plenty of data governance issues to consider<br>' +
        'even in such a simple master data management project as<br>' +
        'adding attributes to existing members. These include:<br>' +
        '• Which user or users are responsible for maintaining the<br>' +
        'attributes?<br>' +
        '• As new members, or rows, are added to the master data, such<br>' +
        'as new products or new customers, what are the procedures<br>' +
        'for ensuring new attributes are assigned and confirmed?<br>' +
        '• How will the organization manage changes to the attributes?<br>' +
        '• What is the process for managing a structural change to the<br>' +
        'attributes or hierarchies, for example adding a new attribute<br>' +
        'or a new level to a hierarchy?<br>' +
        '• Even if the master data management application exists solely<br>' +
        'to create new attributes for the data warehouse, other<br>' +
        'legitimate uses of the same information may arise. How will<br>' +
        'you manage those external dependencies?<br>' +
        'Data Integration<br>' +
        'One of the goals of a data warehouse is to integrate<br>' +
        'information from multiple source systems. It’s all too<br>' +
        'common for an entity such as customer to be sourced from<br>' +
        'multiple transaction systems. This is certainly true if your<br>' +
        'organization has grown by acquisition. Even within a<br>' +
        'unified company, the sales system and the customer care<br>' +
        'system may both maintain their own copies of customer<br>' +
        'attributes. The business wants a single master source for<br>' +
        'customer data, combining information from all transaction<br>' +
        'systems and subsidiaries.<br>' +
        'Data integration is often attempted as part of a data<br>' +
        'warehouse project. The initial project to map and combine<br>' +
        'data is bad enough, and often takes two to three times as<br>' +
        'long as anticipated. Nonetheless, there are tools to help<br>' +
        '335<br>' +
        'uniquely identify persons and organizations from one or<br>' +
        'more sources. As long as you can tolerate some level of<br>' +
        'unresolved noise, the initial cleanup and mapping is<br>' +
        'generally doable.<br>' +
        'In the dimensional modeling world, we often talk about<br>' +
        'conformed dimensions as the key enablers of enterprise<br>' +
        'integration. A conformed dimension is a dimension, or part<br>' +
        'of a dimension, that’s common to many different business<br>' +
        'processes. The conformed attributes in such a dimension<br>' +
        'have the same names, values, and keys wherever they<br>' +
        'appear. Using these conformed attributes, business users<br>' +
        'can drill across multiple business processes and assemble<br>' +
        'integrated reports. But we must be careful to not make the<br>' +
        'process of deploying conformed dimensions be too<br>' +
        'ambitious. These integration projects can easily take too<br>' +
        'long, and raise political resistance if the content of the<br>' +
        'conformed dimension appears to override the favorite<br>' +
        'dimensional attributes of each separate business process.<br>' +
        'The best approach for achieving data integration through<br>' +
        'conformed dimensions is to start modestly and proceed<br>' +
        'incrementally, ideally with techniques drawn from the<br>' +
        'agile development community. Start by proposing a small<br>' +
        'number of “enterprise attributes” that will be added<br>' +
        'non-destructively to a key dimension, such as customer.<br>' +
        'Introduce these attributes into the local copies of<br>' +
        'dimension tables maintained by separate business<br>' +
        'processes. Expand the coverage of business processes one<br>' +
        'at a time, perhaps as agile “sprints.” In each sprint, show<br>' +
        'the business users how to build BI reports drilling across<br>' +
        'the supported subject areas, constraining and grouping on<br>' +
        'the conformed attributes. Once the business users<br>' +
        '336<br>' +
        'understand the value of analyses that leverage conformed<br>' +
        'dimensions and integrated drill across, they will eagerly<br>' +
        'await each sprint!<br>' +
        'The biggest problem with data integration is the process of<br>' +
        'maintaining it over time. So often we’ve seen massive data<br>' +
        'integration efforts generate a lovely mapping, which<br>' +
        'begins to be polluted the day it goes live.<br>' +
        'It’s extraordinarily difficult to maintain data integration<br>' +
        'over time within a “lights-out” (fully automated) data<br>' +
        'warehouse ETL process. The lights-out ETL process<br>' +
        'requires that data integration be algorithmic and<br>' +
        'deterministic. No matter how good your algorithms,<br>' +
        'there’s an inevitable trickle of transactions that a person<br>' +
        'will need to eyeball. You need these integration decisions<br>' +
        'to be made before the ETL load — before the fact data is<br>' +
        'associated with the wrong dimension key, and before<br>' +
        'performance aggregations are computed or updated. For<br>' +
        'the majority of data warehouses on a nightly ETL process,<br>' +
        'it makes sense to manage mapping changes during the day,<br>' +
        'before the nightly load begins. And the technical solution<br>' +
        'to the mapping maintenance problem must include a loop<br>' +
        'for some or all mapping decisions to be evaluated by a<br>' +
        'person before being finalized and passed downstream.<br>' +
        'Data integration is a second kind of master data<br>' +
        'management system, more complex than merely adding<br>' +
        'missing attributes and hierarchies. You can use SQL<br>' +
        'Server MDS to host the entities, attributes, and mapping<br>' +
        'tables, and use its workflow features to interact with the<br>' +
        'decision maker.<br>' +
        '337<br>' +
        'A data integration master data management system must<br>' +
        'address all the data governance issues we’ve already<br>' +
        'discussed, and many new ones, including:<br>' +
        '• What level of confidence from a matching algorithm is<br>' +
        'accepted without triggering a review by a person? The<br>' +
        'decision depends on the cost of incorrect matches relative to<br>' +
        'the cost of the review.<br>' +
        '• Some attributes will be maintained in multiple systems. If<br>' +
        'these attribute values are inconsistent, what are the rules for<br>' +
        'determining a winner?<br>' +
        '• What is the process for undoing an incorrect match?<br>' +
        'Systems Integration<br>' +
        'A master data management system that performs data<br>' +
        'integration is always going to be a struggle to maintain.<br>' +
        'The system and business users must continually react to<br>' +
        'data entered in a variety of source systems, with a broad<br>' +
        'range of data quality. Of course, as you build a data<br>' +
        'integration master data management system you will<br>' +
        '(hopefully) improve your data governance. Nonetheless,<br>' +
        'data integration is a stopgap measure — though in most<br>' +
        'cases it’s a necessary step along the road to a more<br>' +
        'complete solution.<br>' +
        'The real solution should be obvious: in a perfect world the<br>' +
        'various source transaction systems are integrated with each<br>' +
        'other. Entities are managed centrally, and various systems<br>' +
        'subscribe to the master data. How can you reach that<br>' +
        'utopia?<br>' +
        'Most organizations have a mixture of transaction systems<br>' +
        'from various vendors, including some systems developed<br>' +
        '338<br>' +
        'internally. In such a heterogeneous environment, it makes<br>' +
        'sense to have a separate master data management system<br>' +
        'as the communications hub — the system of record<br>' +
        'connecting the various transaction systems. You may resist<br>' +
        'the notion of yet another major system in your IT<br>' +
        'environment. But the more heterogeneous your<br>' +
        'environment, the more attractive a separate master data<br>' +
        'management solution becomes.<br>' +
        'Implementing a systems integration master data<br>' +
        'management solution is not for the faint of heart. Most<br>' +
        'existing transaction systems are designed as if they’re the<br>' +
        'center of the universe. You need to coerce these systems<br>' +
        'into accepting new entities such as products or customers<br>' +
        'from an external source — the master data management<br>' +
        'system — rather than always creating new entities<br>' +
        'themselves.<br>' +
        'Often, one transaction system is identified as the source for<br>' +
        'new entities; other systems cannot create a new entity but<br>' +
        'can modify the attributes of an existing entity. Your<br>' +
        'organization may decide that only the ERP system can<br>' +
        'create a new customer; the CRM system — even if it has<br>' +
        'functionality to create an account — is configured only to<br>' +
        'update customer attributes. The master data management<br>' +
        'system must communicate in both directions. As with the<br>' +
        'data integration scenario, it receives feeds of new entities<br>' +
        'and updated attributes. It also distributes the updated<br>' +
        'attributes to the various systems, though not all attributes<br>' +
        'must be consumed by all systems.<br>' +
        'An even stronger form of systems integration master data<br>' +
        'management is where all of the transaction systems turn<br>' +
        '339<br>' +
        'over control of entity creation and updates to the master<br>' +
        'data management system. Any request to create a new<br>' +
        'customer account is passed to the master data management<br>' +
        'system, which creates the basic structure and hands back<br>' +
        'the new customer account key.<br>' +
        'Master Data Management Systems and the Data<br>' +
        'Warehouse<br>' +
        'The master data management system straddles the line<br>' +
        'between the data warehouse and the operational systems.<br>' +
        'The data warehouse team can certainly develop an entry<br>' +
        'level master data management system to manage attributes<br>' +
        'and hierarchies that are not collected in the operational<br>' +
        'systems.<br>' +
        'More complete systems integration implementations feel<br>' +
        'much closer to the operational systems than to the data<br>' +
        'warehouse. The data warehouse will subscribe to the<br>' +
        'integrated information, but the architecture of the solution<br>' +
        'belongs on the transactional side of the IT organization. In<br>' +
        'our experience, the role of the data warehouse team is<br>' +
        'usually to set the stage for robust master data management<br>' +
        'and data governance. The early efforts of the data<br>' +
        'warehouse team will prove the value of integrated<br>' +
        'information to the organization.<br>' +
        'The remainder of this chapter focuses on the simplest<br>' +
        'master data management applications, and does not<br>' +
        'address the issues of bi-directional communication,<br>' +
        'latency, and systems modification that are characteristic of<br>' +
        'the systems integration master data management<br>' +
        'implementations.<br>' +
        '340<br>' +
        'Introducing SQL Server Master Data Services<br>' +
        'People have been managing master data for decades<br>' +
        'without the benefit of a class of software called master<br>' +
        'data management. Yet this class of software has been<br>' +
        'growing, and now Microsoft has entered the game with its<br>' +
        'Master Data Services (MDS). What features does MDS<br>' +
        'offer?<br>' +
        '• User interface to define the master data structures, called<br>' +
        'models<br>' +
        '• Database structure to hold the master data<br>' +
        '• Security for both model definition and data management<br>' +
        '• Hierarchy management, including security that can limit a<br>' +
        'user to a portion of the tree<br>' +
        '• Programmability<br>' +
        '• Full versioning of models<br>' +
        '• User interface to manage the master data, including<br>' +
        'workflow<br>' +
        '• Mechanisms to import and update elements<br>' +
        '• Mechanisms to export data<br>' +
        '• Full versioning of all attributes<br>' +
        'SQL Server Master Data Services is an Enterprise Edition<br>' +
        'feature, and can be installed only on 64-bit hardware.<br>' +
        'Model Definition Features<br>' +
        'The first set of MDS features support the creation of new<br>' +
        'models, and the improvement of existing models. These<br>' +
        'features work at the metadata level, manipulating the<br>' +
        'structure of the data rather than its contents.<br>' +
        'User Interface: Defining the Master Data Model<br>' +
        '341<br>' +
        'Somewhat surprisingly, the user interface to define and<br>' +
        'manipulate the MDS data models is not part of SQL Server<br>' +
        'BI Development Studio. Instead, it’s a web application.<br>' +
        'The same web application can be accessed by business<br>' +
        'users to maintain data in an existing MDS model, though<br>' +
        'of course business users will have fewer permissions than<br>' +
        'the developer of a model.<br>' +
        'Figure 6-1 illustrates the home screen of the Master Data<br>' +
        'Manager web application, which is included in the MDS<br>' +
        'installation and configuration. As an administrator, a user<br>' +
        'can define models, manage security, import and export<br>' +
        'data, enter or update data through the Master Data<br>' +
        'Manager user interface, or browse the structure of the<br>' +
        'models.<br>' +
        'A Master Data Services model corresponds roughly to a<br>' +
        'dimension in the data warehouse. It’s the base level entity<br>' +
        'that you’ll be managing in MDS. A complete deployment<br>' +
        'will have many models, one for each conformed dimension<br>' +
        'you’re managing through the system.<br>' +
        'The initial tasks involved in defining the model are:<br>' +
        '• Create the model; for example, the Product model. This step<br>' +
        'creates the basic structure of the master data management<br>' +
        'system.<br>' +
        '• Create attributes whose values are limited to a specific set,<br>' +
        'called a domain, such as product type. MDS refers to these<br>' +
        'as entities.<br>' +
        '• Create attributes that do not have a domain, such as product<br>' +
        'description. MDS refers to these as attributes.<br>' +
        '• Create hierarchical relationships.<br>' +
        '342<br>' +
        '• Define business rules for data validation. A simple rule<br>' +
        'would check that a numeric field called percent ownership is<br>' +
        'less than or equal to 100. Rules can also send email to notify<br>' +
        'a user of violations.<br>' +
        'Later in this chapter we provide additional detail on how to<br>' +
        'define models.<br>' +
        'Figure 6-1: Master Data Manager home screen<br>' +
        'Creating Database Structures<br>' +
        '343<br>' +
        'There is no explicit data modeling task to create database<br>' +
        'objects that will hold the master data. All of the database<br>' +
        'work is handled by the Master Data Services application.<br>' +
        'Security<br>' +
        'As we’ve already discussed, one of the key advantages of a<br>' +
        'master data management solution is that it provides an<br>' +
        'opportunity for users, even business users, to participate in<br>' +
        'the management of the data. This is particularly important<br>' +
        'for adding attributes and managing hierarchies that are not<br>' +
        'sourced from existing transaction systems. Clearly, this<br>' +
        'access must be secured.<br>' +
        'You can assign permissions to modify the structure of a<br>' +
        'model, or to add or modify the data within a model. You<br>' +
        'can even specify which users are permitted to modify a<br>' +
        'branch of a hierarchy definition.<br>' +
        'The security definition tasks can be accomplished from<br>' +
        'within the Master Data Manager web application.<br>' +
        'Programmability of Model Definition<br>' +
        'Every task that you undertake in the Master Data Manager<br>' +
        'web application can be accomplished instead by<br>' +
        'programming to the MDS API. You can completely<br>' +
        'replace the Master Data Manager web application with<br>' +
        'custom code, if you wish. Most organizations that build<br>' +
        'master data management applications will use the Master<br>' +
        'Data Manager to define the model structure.<br>' +
        'Programmability will be very important for the ongoing<br>' +
        '344<br>' +
        'data governance, especially for systems integration<br>' +
        'applications.<br>' +
        'Full Versioning of Models<br>' +
        'Master Data Services contains features for you to easily<br>' +
        'archive the structure of a model, explore the ancestry of a<br>' +
        'model, and validate a model.<br>' +
        'Data Management Features<br>' +
        'Once an MDS model has been defined, efforts turn to<br>' +
        'populating it with data. Most of the Master Data<br>' +
        'Manager’s features are focused on managing data contents.<br>' +
        'User Interface: Exploring and Managing the Master Data<br>' +
        'The administrator of an MDS model will grant browsing<br>' +
        'and editing privileges to a handful of users. Those users<br>' +
        'can use the Master Data Manager web application to<br>' +
        'maintain the data. Although users can create new<br>' +
        'members, such as a new product, their main tasks will be<br>' +
        'to resolve data conflicts and add or change attributes that<br>' +
        'are not sourced from the transaction systems.<br>' +
        'It’s easy for users to find data that fails business rules, in<br>' +
        'order to resolve those issues. In addition, the system can be<br>' +
        'set up to email notifications to users.<br>' +
        'NOTE<br>' +
        '345<br>' +
        '“Easy” is probably not the right word. We<br>' +
        'find the Master Data Manager user<br>' +
        'interface a bit confusing. As a web<br>' +
        'application, it doesn’t have all the<br>' +
        'navigational richness that we’d like.<br>' +
        'However, when you use Master Data<br>' +
        'Manager with user privileges (rather than<br>' +
        'full administrative or developer privileges),<br>' +
        'it’s a lot less confusing.<br>' +
        'As we’ve already described, reporting and analysis<br>' +
        'hierarchies are a common initial master data management<br>' +
        'application. The Master Data Manager web application<br>' +
        'contains hierarchy maintenance functionality, including<br>' +
        'the ability to move a branch of a hierarchy from one node<br>' +
        'to another — and helps prevent the inevitable mistakes that<br>' +
        'arise when this activity is managed in Excel.<br>' +
        'You can see the structure of a sample customer model and<br>' +
        'hierarchy in Figure 6-2. If the user browsing the hierarchy<br>' +
        'has editing privileges, she can browse the data elements<br>' +
        'and restructure the hierarchy by dragging and dropping a<br>' +
        'hierarchy branch from one parent to another.<br>' +
        'Figure 6-2: Master Data Manager Model Explorer<br>' +
        '346<br>' +
        'As we will describe subsequently, all MDS transactions,<br>' +
        'including those entered directly into the UI, can and should<br>' +
        'be logged.<br>' +
        'Importing and Updating Data<br>' +
        '347<br>' +
        'Although you can use the Master Data Manager web<br>' +
        'application to type in the contents of a model, that’s not a<br>' +
        'scalable method to add or update members. MDS relies on<br>' +
        'several staging tables to populate and maintain a model.<br>' +
        'Staging tables are a comfortable interface for the DW<br>' +
        'team, which certainly has the skills to load data into a<br>' +
        'table.<br>' +
        'There are three staging tables. The tables are in the<br>' +
        'canonical form of attribute/value pairs. In other words, the<br>' +
        'same three tables work for all master data models, no<br>' +
        'matter their structure.<br>' +
        '• tblStgMember is used to stage new members, such as a<br>' +
        'new product or customer. You provide only the member<br>' +
        'name and identifying code or source system key in this table,<br>' +
        'plus a little metadata to describe the target model, entity,<br>' +
        'data owner, and so on.<br>' +
        '• tblStgMemberAttribute is used to stage attribute values<br>' +
        'for each member. Stage the data with one row per attribute: a<br>' +
        'product model with 100 attributes will use 100 rows in this<br>' +
        'staging table for each product. The column in the staging<br>' +
        'table that contains the attribute value is nvarchar(2000),<br>' +
        'so you may need to explicitly cast numeric or date attributes.<br>' +
        'You must also supply identifying metadata, including the<br>' +
        'member code to map the attribute to the applicable member.<br>' +
        'If the attribute exists already, it will be updated and the prior<br>' +
        'value logged. Use a system attribute called<br>' +
        'MDMMemberStatus to “delete” a member by turning its<br>' +
        'status to inactive.<br>' +
        '• tblSTGRelationship is used to add or maintain<br>' +
        'parent-child or explicit hierarchies.<br>' +
        'You can use any method you wish to populate these<br>' +
        'staging tables. The obvious candidates include:<br>' +
        '348<br>' +
        '• Write a SQL INSERT … SELECT statement. This is a good<br>' +
        'choice for demo and experimentation purposes, where the<br>' +
        'source database is on the same server as Master Data<br>' +
        'Services.<br>' +
        '• Use SQL Server Integration Services.<br>' +
        '• Export to a text file and bulk load.<br>' +
        'Once you have candidate rows in the staging tables, you<br>' +
        'can use the Master Data Manager to launch the process to<br>' +
        'bring them into MDS. Navigate to Data Integration and<br>' +
        'choose Import, as illustrated in Figure 6-3. Batches are<br>' +
        'logged, and each staged row is updated with the batch that<br>' +
        'loaded it, the current status, and any error code. You can<br>' +
        'keep appending data into the staging tables; MDS is smart<br>' +
        'enough to grab only unprocessed rows. However, you<br>' +
        'should develop a process for periodically pruning the fully<br>' +
        'staged rows.<br>' +
        'In most production scenarios, you should schedule an<br>' +
        'automated process to import the staged data, rather than<br>' +
        'relying on a person to click the button in the Master Data<br>' +
        'Manager. The easiest way to do this is to use the<br>' +
        'processing stored procedures in the MDS database, notably<br>' +
        'updStagingSweep.<br>' +
        'Figure 6-3: Master Data Manager Import Data screen<br>' +
        '349<br>' +
        'Once the data has been imported, validate it by processing<br>' +
        'it against the business rules embedded in the model.<br>' +
        'Within Master Data Manager, you can navigate to Version<br>' +
        'Management⇒ Validate Version to manually validate the<br>' +
        'data in the model. Once you’ve automated the import, you<br>' +
        'should automatically launch validation once the import<br>' +
        'batches have completed by executing the<br>' +
        'updValidateModel stored procedure.<br>' +
        'RESOURCES<br>' +
        'None of these stored procedures nor the<br>' +
        'steps to automate and schedule processing<br>' +
        'is documented in the early releases of<br>' +
        'Books Online. Luckily the Microsoft MDS<br>' +
        'team is good about monitoring and<br>' +
        'responding to the official MDS forum,<br>' +
        'which you can find at<br>' +
        '350<br>' +
        'http://social.msdn.microsoft.com/Forums/en-US/<br>' +
        'sqlmds/threads.<br>' +
        'Exporting Data<br>' +
        'Master Data Services is a back room, helper application.<br>' +
        'It’s of no use to your organization unless you can easily<br>' +
        'extract the master data in a form that other applications can<br>' +
        'consume. Our discussion will of course focus on extracting<br>' +
        'the data for loading into the data warehouse. But other<br>' +
        'systems — even operational transaction systems — can<br>' +
        'extract information from MDS as well.<br>' +
        'Master Data Services makes exporting data very easy, by<br>' +
        'providing a tool to create data export views. These views<br>' +
        'are simply relational database views that the UI creates for<br>' +
        'you in the database that’s installed with MDS. Unlike the<br>' +
        'strange attribute/value pair format for the import tables, the<br>' +
        'export views look perfectly normal: one row for each<br>' +
        'member, with its attributes displayed as columns.<br>' +
        'There is one slight oddity, which is that you need to create<br>' +
        'additional views for each hierarchy that’s derived from<br>' +
        'attribute relationships. The main attribute view contains<br>' +
        'only the first level of the hierarchy; the second view<br>' +
        'contains the first level and continues up the tree. To get all<br>' +
        'the columns in your flattened star dimension table, you<br>' +
        'need to join the main attribute view with the hierarchy<br>' +
        'view. No doubt there’s a good reason they didn’t create<br>' +
        'one view that spans all the attributes (hierarchical or not),<br>' +
        '351<br>' +
        'but it’s so easy to combine them that it doesn’t really<br>' +
        'matter.<br>' +
        'The nicest feature of these export views is that they’re<br>' +
        'created with interesting metadata like:<br>' +
        '• Version: Filter on this to extract only the current model’s<br>' +
        'data.<br>' +
        '• Member entered and last updated date and time, user,<br>' +
        'and model version: Filter on these to extract only the rows<br>' +
        'that have been added or changed since your last data<br>' +
        'warehouse update.<br>' +
        '• Validation status: Filter on this to extract only rows that<br>' +
        'meet all the model’s business rules.<br>' +
        'Figure 6-4 illustrates the output from an attribute view on<br>' +
        'the Product model.<br>' +
        'Figure 6-4: Sample rows from an export view for product<br>' +
        'attributes<br>' +
        '352<br>' +
        'Full Versioning of All Attributes<br>' +
        'Master Data Services keeps a complete audit trail of each<br>' +
        'attribute: every value it’s ever been assigned, where it<br>' +
        'came from, who changed the attribute’s value, and when.<br>' +
        'This is a useful feature that can simplify the ETL<br>' +
        'application and associated staging area.<br>' +
        'Part of the dimensional design process for the data<br>' +
        'warehouse is to determine, based on business user<br>' +
        'requirements, which dimension attributes are to be<br>' +
        'managed as type 1 (restate history) and which as type 2<br>' +
        '(track history). The decisions should be made once and<br>' +
        'apply to all subject areas of the eventual complete<br>' +
        'conformed data warehouse, even those for business<br>' +
        'processes you’ve not yet analyzed during the iterative<br>' +
        'design process. Inevitably, some of the attributes you<br>' +
        'initially identify as type 1 are later determined to require<br>' +
        '353<br>' +
        'type 2 tracking. Often we keep the history of all attributes,<br>' +
        'even type 1 attributes, in the data warehouse staging area.<br>' +
        'Being able to rely on Master Data Services for that history<br>' +
        'makes the ETL system that much simpler.<br>' +
        'Creating a Simple Application<br>' +
        'Now that we have described master data, master data<br>' +
        'management, and Master Data Services, we will walk<br>' +
        'through the development process for an extremely simple<br>' +
        'MDS application. Our goal is not to present a tutorial, but<br>' +
        'rather to provide a picture of what it’s like to create the<br>' +
        'simplest possible application.<br>' +
        'The Business Scenario<br>' +
        'The business problem that we’re illustrating in this<br>' +
        'scenario is the common problem of business users wanting<br>' +
        'to see attributes that don’t have a systematic source. The<br>' +
        'data warehouse currently has a product dimension, and a<br>' +
        'development project is under way to improve that<br>' +
        'dimension by adding a few attributes, as illustrated in<br>' +
        'Figure 6-5.<br>' +
        'Figure 6-5: Product dimension, with new attributes<br>' +
        'highlighted<br>' +
        '354<br>' +
        'The data warehouse team is going to become familiar with<br>' +
        'Master Data Services by maintaining these new attributes<br>' +
        'in MDS. There will be several new attributes: product<br>' +
        'category manager name and email, product category<br>' +
        'group, and alternate category.<br>' +
        'NOTE<br>' +
        'You should always create a logical model<br>' +
        'of your dimension before you get started<br>' +
        'building the MDS model, as illustrated in<br>' +
        'Figure 6-5. The model highlights the<br>' +
        'attribute relationships that imply<br>' +
        'hierarchies (product, brand, subcategory,<br>' +
        'and so on). You must clarify your thinking<br>' +
        'about which entities an attribute is<br>' +
        'associated with. You should develop a<br>' +
        'taxonomy for which attributes are freeform,<br>' +
        'and which belong to a domain.<br>' +
        'If you don’t have access to modeling<br>' +
        'software, simply draw a picture as<br>' +
        'illustrated in Figure 6-5.<br>' +
        '355<br>' +
        'Keep It Simple<br>' +
        'The data warehouse team doesn’t want to have to sell<br>' +
        'MDS and data governance throughout their organization.<br>' +
        'That’s an eventual goal, but for this little project they just<br>' +
        'want to kick the tires of MDS while solving a real<br>' +
        'problem. The team recognizes it will take a long time and<br>' +
        'many small steps to get from where they are — near-zero<br>' +
        'data governance — to their desired goal of integrated<br>' +
        'master data.<br>' +
        'With that goal in mind, the team decides to keep the MDS<br>' +
        'model as small as possible. They initially assumed they’d<br>' +
        'model the entire product dimension, and add these new<br>' +
        'attributes to that model. But management decided that was<br>' +
        'too risky, and asked the team to modify the current<br>' +
        'dimension ETL process rather than replace it entirely. If<br>' +
        'this little test is successful, that will be the next step.<br>' +
        'The MDM model will encompass only a piece of the<br>' +
        'product dimension. If you look back at Figure 6-5 you can<br>' +
        'see that all the new attributes are based on hierarchy levels<br>' +
        'above subcategory. So the MDM model will start at<br>' +
        'subcategory and include category, category group, and alt<br>' +
        'category (and all their attributes).<br>' +
        'Create the MDS Model<br>' +
        'Installing Master Data Services from the SQL Server<br>' +
        'Developer Edition media took a little time because it<br>' +
        'wasn’t immediately obvious that they needed to run a<br>' +
        'separate install process. This is such a small project that<br>' +
        'the team initially decided to use an old 32-bit server. Once<br>' +
        '356<br>' +
        'the team read the documentation and switched to a 64-bit<br>' +
        'machine, things went more smoothly.<br>' +
        'NOTE<br>' +
        'MDS has its own installation process that’s<br>' +
        'not integrated with the main SQL Server<br>' +
        'installation. You need to browse the install<br>' +
        'DVD and navigate to the<br>' +
        'MasterDataServices directory. MDS only<br>' +
        'runs on 64-bit hardware. Search MSDN for<br>' +
        '“Install Master Data Services.” You can<br>' +
        'download sample MDS models from<br>' +
        'MSDN that are useful for exploring the<br>' +
        'product.<br>' +
        'The team downloaded the sample master data services<br>' +
        'models and explored their contents. After a few hours of<br>' +
        'poking around, they felt ready to move ahead to create<br>' +
        'their own application.<br>' +
        'The first piece of work was to create a new model: Product<br>' +
        'Subcategory. That work took two seconds, and is<br>' +
        'illustrated in Figure 6-6.<br>' +
        'Figure 6-6: Creating the Product Subcategory model<br>' +
        '357<br>' +
        'REFERENCE<br>' +
        'The simple model described here is<br>' +
        'available for download from the book’s<br>' +
        'website.<br>' +
        'Looking at Figure 6-5, you can see that the model needed<br>' +
        'to have additional entities for category, category group,<br>' +
        'and alt category. The basic structure for the subcategory<br>' +
        'entity was automatically created with the model itself. If<br>' +
        'you want to declare referential integrity between the levels<br>' +
        'of these hierarchies, the level attributes must be created as<br>' +
        '358<br>' +
        'separate entities. Creating those entities was equally easy,<br>' +
        'as illustrated in Figure 6-7.<br>' +
        'Figure 6-7: Creating an entity for a hierarchical level<br>' +
        'The next step was to hook these entities together, declaring<br>' +
        'referential integrity between the levels of the hierarchy. To<br>' +
        'do so, the team edited the Product Subcategory entity to<br>' +
        'add a leaf attribute, told MDS that it’s constrained to a<br>' +
        'domain, and pointed it to the category entity created<br>' +
        'earlier. Figure 6-8 illustrates the screen for creating a<br>' +
        'domain attribute.<br>' +
        '359<br>' +
        'Figure 6-8: Creating an attribute that’s associated with a<br>' +
        'domain<br>' +
        'The team finished up the subcategory entity by creating a<br>' +
        'freeform attribute for short name, as illustrated in Figure<br>' +
        '6-9.<br>' +
        'Figure 6-9: Creating a freeform attribute<br>' +
        '360<br>' +
        'With similar steps they created links between category and<br>' +
        'category group, between subcategory and alt category, and<br>' +
        'between alt category and category group. Finally, they<br>' +
        'defined freeform attributes for category manager, category<br>' +
        'manager email, and the short name for the alt category.<br>' +
        'Load the Subcategory Members<br>' +
        'There are 37 subcategories in the transaction database,<br>' +
        'which the team deems far too many to type by hand. They<br>' +
        '361<br>' +
        'choose to populate the staging tables with a SQL script<br>' +
        '(assuming the source database is on the same server as<br>' +
        'MDS):<br>' +
        '-- insert 37 Subcategory members into the staging table<br>' +
        'INSERT INTO mdm.tblStgMember<br>' +
        '(<br>' +
        'ModelName<br>' +
        ', EntityName<br>' +
        ', MemberType_ID<br>' +
        ', MemberName<br>' +
        ', MemberCode<br>' +
        ')<br>' +
        'SELECT<br>' +
        '\'Product Subcategory\'<br>' +
        ', \'Product Subcategory\'<br>' +
        ', 1<br>' +
        ', Name<br>' +
        ', ProductSubcategoryID<br>' +
        'FROM AdventureWorks2008R2.Production.ProductSubcategory<br>' +
        'By navigating to the Integration Management⇒ Import<br>' +
        'page from the Master Data Manager home page, the team<br>' +
        'quickly figured out how to load the subcategories into the<br>' +
        'model. Loading only took a few seconds, and the newly<br>' +
        'imported members can be viewed by navigating to<br>' +
        '362<br>' +
        'Explorer⇒ Entities⇒ Product Subcategory, as illustrated<br>' +
        'in Figure 6-10.<br>' +
        'Figure 6-10: Exploring the newly added members<br>' +
        'The initial data import process added only the new<br>' +
        'members; none of the attributes is populated yet. There are<br>' +
        'categories in the source database, but the other attributes<br>' +
        'don’t exist anywhere else and must be added by hand.<br>' +
        'Remember that loading the categories uses the<br>' +
        'mdm.tblStgMemberAttributes staging table.<br>' +
        'Polish the Model<br>' +
        'The basic structure of this model was very simple to<br>' +
        'create, but there are some additional steps to make the<br>' +
        '363<br>' +
        'model production-ready. Most important, the team needs<br>' +
        'to add business rules to ensure the data is correct. This is<br>' +
        'particularly important in the current business scenario,<br>' +
        'where some of the attributes must be added by hand. The<br>' +
        'team has the following wish list for business rules:<br>' +
        '• Null or empty values are not allowed.<br>' +
        '• Referential integrity between levels is maintained at all<br>' +
        'times. The team lead notes that linking together the<br>' +
        'hierarchy levels as domain-driven entities ensures this will<br>' +
        'always be the case.<br>' +
        '• The category manager name should look like a person’s<br>' +
        'name, containing 2–4 capitalized words. Similarly, the<br>' +
        'category manager email should look like an email.<br>' +
        '• The names and emails should be validated in Active<br>' +
        'Directory.<br>' +
        '• If the short name attributes aren’t supplied in an import, the<br>' +
        'business rule should copy the standard name.<br>' +
        '• An email should be sent to team members when any data<br>' +
        'element violates these business rules.<br>' +
        'The team should define named hierarchies for subcategory,<br>' +
        'category, category group and subcategory, alt category,<br>' +
        'category group. Even though the attribute relationships<br>' +
        'exist, the hierarchies must be defined in order for Master<br>' +
        'Data Manager to create the export view for the hierarchical<br>' +
        'levels.<br>' +
        'Export to the Data Warehouse<br>' +
        'The team created three export views: one for the base level<br>' +
        'attributes for subcategory, one for the main hierarchy, and<br>' +
        'one for the alternative hierarchy.<br>' +
        '364<br>' +
        'The data warehouse load process for the product<br>' +
        'dimension is a straightforward SSIS package. That<br>' +
        'package must be modified to pick up the category,<br>' +
        'category group, and alt category information from the<br>' +
        'MDS export views. This is a minor change, and is<br>' +
        'expected to take only a few days to code and thoroughly<br>' +
        'test.<br>' +
        'Even though the data in this model is fairly static, it’s<br>' +
        'possible that new subcategories will be added in the future.<br>' +
        'The team hasn’t done a good job of thinking through how<br>' +
        'they will learn about new subcategories, other than a<br>' +
        'failure in the ETL processing. They will probably begin by<br>' +
        'querying the subcategory table in the transaction database<br>' +
        'on a daily basis. Longer term, they need to better<br>' +
        'understand the business process that leads to the creation<br>' +
        'of new products, subcategories, and other product-related<br>' +
        'entities. They need a clear notification that something has<br>' +
        'been added or changed, and default strategies for dealing<br>' +
        'with those changes.<br>' +
        'Summary<br>' +
        'The information stored in Master Data Services can be the<br>' +
        'perfect input for the periodic — usually nightly — data<br>' +
        'warehouse ETL process. Many of the cleaning steps of the<br>' +
        'ETL process that we perform on dimension data can be<br>' +
        'accomplished by Master Data Services.<br>' +
        'What MDS does particularly well is provide a mechanism<br>' +
        'for people to augment data and fix data problems. ETL, by<br>' +
        'contrast, is typically a batch process. There’s certainly no<br>' +
        'built-in mechanism in SSIS for someone to review,<br>' +
        '365<br>' +
        'correct, and validate data. Some of the problems that are<br>' +
        'most intractable for the data warehouse ETL system,<br>' +
        'especially data integration problems, are well suited for a<br>' +
        'master data management system.<br>' +
        'For MDS to work well in a data warehouse environment,<br>' +
        'the MDS model for a dimension must be updated in<br>' +
        'advance of the data warehouse ETL cycle. There must be<br>' +
        'time for users to react to data validity issues before the<br>' +
        'new or updated information is used in the data warehouse.<br>' +
        'For this to really work, management and business users<br>' +
        'must commit to data governance. They may have to<br>' +
        'change their workflows in order to ensure there’s enough<br>' +
        'lead time for the data to be well managed.<br>' +
        'This is a hard sell for the data warehouse team to make.<br>' +
        'The carrot is improved data quality, and a mechanism by<br>' +
        'which user-driven attributes can be maintained and<br>' +
        'included in the data warehouse. As ever, the impetus must<br>' +
        'come from the user community, because in the long run<br>' +
        'they are the ones who must maintain the data.<br>' +
        'As we described at the beginning of this chapter, a full<br>' +
        'master data management system falls outside the purview<br>' +
        'of the data warehouse team. For most organizations, such a<br>' +
        'system is simply infeasible at the moment. We need to<br>' +
        'solve simpler problems, and nurture the organization’s<br>' +
        'commitment to data governance, a little bit at a time.<br>' +
        'Successful MDS implementations, however small, will<br>' +
        'move us toward that goal.<br>' +
        '366<br>';
    document.getElementById('chapter5').innerHTML = 'Part 2: Building and Populating the Databases<br>' +
        'Chapter 5: Creating the Relational Data Warehouse<br>' +
        'Chapter 6: Master Data Management<br>' +
        'Chapter 7: Designing and Developing the ETL System<br>' +
        'Chapter 8: The Core Analysis Services OLAP Database<br>' +
        'Chapter 9: Design Requirements for Real-Time BI<br>' +
        'The second part of the Lifecycle is where you create the<br>' +
        'back room infrastructure for the data warehouse, where<br>' +
        'you realize the designs described in Part 1. The first big<br>' +
        'step is creating the relational data warehouse database. The<br>' +
        'logical model you’ve already developed translates directly<br>' +
        'to the physical database, though there are still plenty of<br>' +
        'physical design decisions for the database administrator to<br>' +
        'make.<br>' +
        'Once you’ve built the relational data warehouse, it’s time<br>' +
        'to start populating it. The most successful data warehouse<br>' +
        'projects take place in organizations with strong data<br>' +
        'governance practices. You can use SQL Server Master<br>' +
        'Data Services to implement a wide range of master data<br>' +
        'management systems, including simple applications for<br>' +
        'those organizations new to data governance. Most of your<br>' +
        'DW/BI team’s time and effort is spent using Integration<br>' +
        'Services to build the ETL system. The cost and quality of<br>' +
        'that system are greatly improved by thinking through the<br>' +
        '272<br>' +
        'architectural issues before you start writing packages to<br>' +
        'move data.<br>' +
        'The last chapter in Part 2 describes how to build and<br>' +
        'populate the core Analysis Services OLAP database,<br>' +
        'which is a recommended component of any Microsoft<br>' +
        'DW/BI system. The Analysis Services database includes<br>' +
        'rich metadata that helps business users navigate the data,<br>' +
        'which is so vital for successful ad-hoc use.<br>' +
        'The Kimball Lifeycle steps covered in Part 2<br>' +
        '273<br>' +
        'Chapter 5<br>' +
        'Creating the Relational Data Warehouse<br>' +
        'Where the rubber starts to meet the road.<br>' +
        'This chapter is about instantiating the target relational<br>' +
        'dimensional model in a SQL Server database. Now that<br>' +
        'you have your servers set up, software installed,<br>' +
        'development environment in place, dimensional model<br>' +
        'designed and well documented, and, of course, your<br>' +
        'business requirements and priorities clearly defined, you<br>' +
        'can actually start writing some code in this chapter, if<br>' +
        'T-SQL counts as code in your book. Figure 5-1 provides a<br>' +
        'graphical example of the lifecycle context just described<br>' +
        'for this chapter.<br>' +
        'Our goal in this lifecycle step is to get the physical<br>' +
        'structures in place so they can be populated by the ETL<br>' +
        'process in the next step. Creating the database and building<br>' +
        'the tables is a task for the DBA role. Folks who have prior<br>' +
        'SQL Server DBA experience should have no problem with<br>' +
        'this chapter. Those of you who are not really DBAs, but<br>' +
        'ended up with this role because no one else volunteered<br>' +
        'shouldn’t panic. Basic DBA work in SQL Server is<br>' +
        'generally straightforward. Although this chapter is not a<br>' +
        'tutorial, we will tell you the key tasks and provide some<br>' +
        'examples to point you in the right direction.<br>' +
        'In this chapter, you will learn:<br>' +
        '274<br>' +
        '• How to deal with specific table and column implementation<br>' +
        'decisions such as surrogate keys, string data types, Unicode,<br>' +
        'NULLs, and default values<br>' +
        '• How to use table and column extended properties to capture<br>' +
        'descriptive metadata<br>' +
        '• About adding housekeeping columns that may not be in the<br>' +
        'logical dimensional model to support change tracking,<br>' +
        'process auditing, and sort orders<br>' +
        '• What to do about declaring entity and integrity constraints<br>' +
        '• A good starting point for indexes and an approach to<br>' +
        'creating relational aggregate tables<br>' +
        '• The value and cost of data compression<br>' +
        '• What table partitioning is and how to implement it in SQL<br>' +
        'Server<br>' +
        'Figure 5-1: The Business Dimensional Lifecycle<br>' +
        'Getting Started<br>' +
        'Before you start the relational database physical design<br>' +
        'process, you should have completed the logical model. In<br>' +
        'fact, you should have very little design work to do in<br>' +
        'translating from the logical model to the physical model.<br>' +
        'Here are some of the key elements that should already be<br>' +
        '275<br>' +
        'in place as deliverables from your dimensional modeling<br>' +
        'process:<br>' +
        '• Object names should be clearly defined and agreed to by<br>' +
        'your data governance team. Your logical model should<br>' +
        'already be using good, clear, sensible names that conform to<br>' +
        'your naming conventions. You should already have defined<br>' +
        'naming conventions for database objects such as tables and<br>' +
        'columns.<br>' +
        '• All columns should have the correct data types. Start off<br>' +
        'with the column definitions from the modeling process, but<br>' +
        'you may need to modify data types later, after you’ve<br>' +
        'completed the data profiling discussed in Chapter 2. For<br>' +
        'example, you may learn that some customer surnames take<br>' +
        'more than the 35 characters called for in the logical model.<br>' +
        '• Primary keys and foreign key relationships should be<br>' +
        'identified.<br>' +
        'The best tool for developing the physical model is a data<br>' +
        'modeling tool such as ERwin, PowerDesigner, or ER/<br>' +
        'Studio. These tools can generate the data definition<br>' +
        'language (DDL) needed to create your database, a feature<br>' +
        'known as forward engineering. Using a modeling tool<br>' +
        'makes it very easy to modify the database during the early,<br>' +
        'iterative design phases.<br>' +
        'NOTE<br>' +
        'At this time, Microsoft is not selling an<br>' +
        'effective data modeling tool. Old versions<br>' +
        'of Visio were usable (though hardly best of<br>' +
        'class), but that functionality has been<br>' +
        'removed. There are various tools in the<br>' +
        '276<br>' +
        'Microsoft world that offer modeling<br>' +
        'capabilities, but they are not as effective as<br>' +
        'the standalone database modeling tools.<br>' +
        'Visio 2010 can document models, but has<br>' +
        'no forward engineering capabilities. Visual<br>' +
        'Studio 2010 has a database project type<br>' +
        'that is meant more to support DBAs with<br>' +
        'ongoing database management and<br>' +
        'maintenance. You can add an ADO .NET<br>' +
        'Entity Data Model template to a Visual<br>' +
        'Studio project to get a graphical design<br>' +
        'environment, but you need the full Visual<br>' +
        'Studio product. Or, you can use the<br>' +
        'Database Diagram feature in SQL<br>' +
        'Management Studio, but it is tightly linked<br>' +
        'to the database objects.<br>' +
        'The high-end data modeling tools can be expensive, and<br>' +
        'some small companies will balk at purchasing them. In<br>' +
        'Chapter 2, we introduced the Excel workbook and macro<br>' +
        'to support the logical modeling process. You can use the<br>' +
        'workbook and macro to generate the DDL to create the<br>' +
        'physical database if you don’t have a data modeling tool.<br>' +
        'We provide this spreadsheet because not all readers will<br>' +
        'have a real data modeling tool.<br>' +
        'Complete the Physical Design<br>' +
        'There are always some decisions the DBA must make with<br>' +
        'regard to the makeup of the tables and columns. These<br>' +
        'include managing surrogate keys, working with string data<br>' +
        '277<br>' +
        'types, handling NULLs, adding housekeeping columns,<br>' +
        'and creating extended property entries to capture basic<br>' +
        'metadata. We’ll walk through each of these topics in this<br>' +
        'section.<br>' +
        'Surrogate Keys<br>' +
        'The primary key for dimension tables should be a<br>' +
        'surrogate key assigned and managed by the DW/BI<br>' +
        'system. The most common method for creating surrogate<br>' +
        'keys in SQL Server is to enable the IDENTITY property on<br>' +
        'the surrogate key column. Every time a row is inserted, the<br>' +
        'identity column populates itself by incrementing.<br>' +
        'Check to ensure the surrogate key column is an integer<br>' +
        'data type. If you have Enterprise Edition and will use page<br>' +
        'compression, just use regular integers, unless you need the<br>' +
        'larger size. Be a little more thoughtful with Standard<br>' +
        'Edition. Choose the appropriate integer type given the<br>' +
        'anticipated size of the dimension:<br>' +
        '• Tinyint takes values in the range 0 to 255 and requires 1 byte<br>' +
        'of storage. Note that tinyint does not support negative<br>' +
        'numbers.<br>' +
        '• Smallint ranges from –215 (–32,768) to 2<br>' +
        '15<br>' +
        '– 1 (32,767) and<br>' +
        'takes 2 bytes.<br>' +
        '• Int ranges from –231 to 231 – 1 and takes 4 bytes.<br>' +
        '• Bigint ranges from –263 to 263 – 1 and takes 8 bytes.<br>' +
        'Choose the smallest integer type that will work for your<br>' +
        'dimension, keeping in mind how much the dimension will<br>' +
        'grow over time. This isn’t important for the dimension<br>' +
        'table itself, but it makes a difference for the fact table’s<br>' +
        'size and performance. These same surrogate keys show up<br>' +
        '278<br>' +
        'as foreign keys in the fact table. Using the small data types<br>' +
        'is also important for minimizing memory use during data<br>' +
        'processing. Make sure you use the same integer types for<br>' +
        'the foreign key columns in the fact table as for the<br>' +
        'corresponding dimension tables.<br>' +
        'We usually frown on using meaningful surrogate keys —<br>' +
        'which is something of an oxymoron — but we make an<br>' +
        'exception in every DW/BI system we build. The date<br>' +
        'dimension should use a surrogate key. That surrogate key<br>' +
        'should be a 32-bit integer. But it’s awfully convenient for<br>' +
        'it to be a meaningful integer of the form year-month-day,<br>' +
        'such as 20110723. Developers are people, too.<br>' +
        'String Columns<br>' +
        'If you didn’t pay close attention to string column lengths<br>' +
        'during the modeling process because you were focused on<br>' +
        'business meaning, you need to clean this up in the physical<br>' +
        'model. This is particularly true for columns in very large<br>' +
        'dimensions, and the occasional string column in fact<br>' +
        'tables. The relational database stores variable length string<br>' +
        'columns, type varchar and nvarchar, efficiently. It doesn’t<br>' +
        'pad these columns with spaces. However, other parts of the<br>' +
        'SQL Server toolset will fill these columns out to their full<br>' +
        'width. Notably, Integration Services and Analysis Services<br>' +
        'pad string columns with spaces as they are loaded into<br>' +
        'memory. Both Integration Services and Analysis Services<br>' +
        'love physical memory, so there’s a cost to declaring string<br>' +
        'columns that are far wider than they need to be.<br>' +
        '279<br>' +
        'TIP<br>' +
        'Start by making the string columns in the<br>' +
        'data warehouse database the same length as<br>' +
        'in the source database. We’ve seen source<br>' +
        'systems that routinely use varchar(100) for<br>' +
        'all string columns. In this case, investigate<br>' +
        'the actual lengths of string data. Make the<br>' +
        'data warehouse columns wider than any<br>' +
        'historical width, just for insurance. Add a<br>' +
        'data quality screen to your ETL application<br>' +
        'to catch especially long strings in the<br>' +
        'future.<br>' +
        'Don’t get carried away by the varchar data type. Any<br>' +
        'column smaller than 5 (some people say 10) characters<br>' +
        'should just be a char data type, even if the data length<br>' +
        'varies somewhat. If the string length varies little, say from<br>' +
        '10 to 13 characters, simply use the char type.<br>' +
        'Char and varchar can be used to support text in many<br>' +
        'languages, including most European languages. The<br>' +
        'corresponding Unicode data types, nchar and nvarchar, can<br>' +
        'store any character including cursive and glyph languages<br>' +
        'such as Arabic or Korean. Unicode accomplishes this feat<br>' +
        'by assigning two bytes to store each character, instead of<br>' +
        'one byte used by char and varchar. Microsoft uses Unicode<br>' +
        'data types in AdventureWorks because the same sample<br>' +
        'database structure needs to work worldwide.<br>' +
        '280<br>' +
        'There are several good reasons to use the Unicode data<br>' +
        'types for strings, and one reason not to. On the plus side,<br>' +
        'using Unicode will ensure you can load any strings that get<br>' +
        'thrown at you in the future, even if you are storing only<br>' +
        'ASCII characters today. Unicode doesn’t require any<br>' +
        'conversion or code page when you move data from one<br>' +
        'environment to another, and it is the standard encoding for<br>' +
        'web-based systems. Finally, SQL Server Integration<br>' +
        'Services is heavily biased toward working with Unicode<br>' +
        'data. On the minus side, all your strings will require twice<br>' +
        'as much space.<br>' +
        'Our recommendation is to use Unicode data types<br>' +
        'wherever possible. Most dimensions are so small that the<br>' +
        'extra space hit won’t matter. If you have large dimensions<br>' +
        'with lots of strings, you should still consider Unicode if<br>' +
        'you are using SQL Server 2008 R2 Enterprise Edition or<br>' +
        'higher because you can apply row or page compression,<br>' +
        'which will recover most of the space. In the worst case, go<br>' +
        'ahead and use char and varchar if you will never need to<br>' +
        'support non-Latin character sets, and you don’t have<br>' +
        'Enterprise Edition.<br>' +
        'If the character column is in a fact table, for example to<br>' +
        'hold a degenerate dimension such as an invoice number,<br>' +
        'you should use a char data type. Unless, of course, your<br>' +
        'invoice numbers require Unicode.<br>' +
        'To Null, or Not to Null?<br>' +
        'At this point in creating the tables, you need to decide<br>' +
        'whether you want the database to allow null values or not.<br>' +
        'When you define columns, you can tell the database<br>' +
        '281<br>' +
        'engine to not allow NULLs. In general, you should avoid<br>' +
        'null values in the data warehouse dimension tables. They<br>' +
        'are confusing to business users, especially when<br>' +
        'constructing queries that filter or report on a dimension<br>' +
        'attribute that’s sometimes null. If you decide to let a<br>' +
        'column be null, document your rationale in your system<br>' +
        'documentation.<br>' +
        'Numeric fields in fact tables, on the other hand, generally<br>' +
        'act more gracefully with null values. All of the aggregate<br>' +
        'operators (SUM, MIN, MAX, COUNT, and AVG) do the<br>' +
        '“right thing” when a null is encountered. Generally the<br>' +
        'null value is ignored by all these operators as if the record<br>' +
        'didn’t exist. In almost all situations, a zero value (instead<br>' +
        'of a null value) would produce undesirable and misleading<br>' +
        'results.<br>' +
        'It’s not strictly necessary to enforce nullability in the<br>' +
        'database. The data warehouse tables are loaded through an<br>' +
        'ETL process and only through the ETL process. Your ETL<br>' +
        'process should look for nulls and substitute an appropriate<br>' +
        'value, so no nulls should sneak through. One easy way to<br>' +
        'do this is to define a default value constraint on any<br>' +
        'column that might have a null. Something simple, such as<br>' +
        '“Missing Region” will usually do the trick. As long as you<br>' +
        'don’t actually load any null values, you should be okay.<br>' +
        'But let’s be professional here. If a column isn’t supposed<br>' +
        'to be null, it should be declared NOT NULL in the database.<br>' +
        'Housekeeping Columns<br>' +
        '282<br>' +
        'There are several columns that should be added to the<br>' +
        'physical model, if they’re not already included in the<br>' +
        'logical model. Any dimension that includes an attribute<br>' +
        'that’s tracked as type 2 needs columns to track the date<br>' +
        'range for which each dimension row is valid.<br>' +
        'NOTE<br>' +
        'You don’t have to add these columns to a<br>' +
        'dimension table that has no type 2 (track<br>' +
        'history) slowly changing dimension<br>' +
        'attributes in it. Although, you should have<br>' +
        'a only few dimensions that don’t have type<br>' +
        '2 attributes.<br>' +
        'The RowStartDate and RowEndDate columns indicate<br>' +
        'the date range for which the dimension row is valid. Make<br>' +
        'these dates be inclusive, so that a SQL statement that<br>' +
        'includes a BETWEEN clause works as expected. If these<br>' +
        'columns are at the grain of a calendar day, make sure that<br>' +
        'the RowEndDate is one day less than the RowStartDate<br>' +
        'of the succeeding record for that dimension member so<br>' +
        'that the BETWEEN syntax fetches back only a single<br>' +
        'dimension record. If these columns must be at the grain of<br>' +
        'minute or second, then the RowStartDateTime and<br>' +
        'RowEndDateTime fields have to be administered more<br>' +
        'carefully. In this case the RowEndDateTime of a record<br>' +
        'must be set exactly equal to the RowStartDateTime of<br>' +
        'the succeeding record, and you must replace the BETWEEN<br>' +
        '283<br>' +
        'constraint with<br>' +
        'RowStartDateTime <= YOURDATETIME < RowEndDateTime.<br>' +
        'For the current row, you can leave RowEndDate as a NULL,<br>' +
        'but it works better if you make it the maximum date for<br>' +
        'your data type. These maximum dates are 12/31/2079 for<br>' +
        'smalldatetime, and 12/31/9999 for datetime, datetime2, or<br>' +
        'date data types. Populate a third column, RowIsCurrent,<br>' +
        'with the value yes or no (Y/N). Although this column can<br>' +
        'be inferred from the row’s end date, sometimes it’s easier<br>' +
        'to use the current indicator. In the MDWT_2008R2 case<br>' +
        'study data model, the customer and employee dimensions<br>' +
        'include type 2 attributes. Therefore, their table definitions<br>' +
        'include these tracking columns.<br>' +
        'Occasionally it’s interesting to the business users to be<br>' +
        'able to easily tell which of several columns propagated a<br>' +
        'type 2 change in the dimension. A simple way to do this is<br>' +
        'to add a column to track the RowChangeReason: the<br>' +
        'columns that changed on the date this row was added. The<br>' +
        'customer and employee dimensions in our case study<br>' +
        'database include this column.<br>' +
        'RESOURCES<br>' +
        'For more information on how to populate<br>' +
        'the RowChangeReason column, search for<br>' +
        'Design Tip #80 Adding a Row Change<br>' +
        'Reason Attribute at www.kimballgroup.com.<br>' +
        '284<br>' +
        'The audit dimension keeps track of when and how a row<br>' +
        'was added to the DW/BI system. It is closely tied to the<br>' +
        'ETL system, and keeps track of the package and step that<br>' +
        'loaded the data. Using an audit dimension for both fact and<br>' +
        'dimension rows is becoming more important, as<br>' +
        'increasingly strict compliance regulations mean we must<br>' +
        'track the lineage of the data in our warehouse. In Chapter<br>' +
        '7, we provide examples of how to populate a simple audit<br>' +
        'dimension. All dimension and fact tables in our case study<br>' +
        'database include a key to the audit dimension. In fact, we<br>' +
        'add two audit dimension key columns to each table: one<br>' +
        'for the process that initially loaded a row, and one for the<br>' +
        'latest update to that row.<br>' +
        'REFERENCE<br>' +
        'The Data Warehouse ETL Toolkit (Wiley,<br>' +
        '2004) outlines more complex auditing<br>' +
        'procedures. You need to evaluate your<br>' +
        'business and auditing requirements against<br>' +
        'the cost and complexity of maintaining a<br>' +
        'richer data auditing system.<br>' +
        'Consider adding one or more sorting columns to your<br>' +
        'dimension tables. There may be some sort order other than<br>' +
        'alphabetical that makes sense to your business users. This<br>' +
        'is especially true of chart of accounts dimensions.<br>' +
        'Table and Column Extended Properties<br>' +
        '285<br>' +
        'When your team developed the DW/BI system’s logical<br>' +
        'model, you specified several metadata elements including<br>' +
        'sources, descriptions, and how each dimension tracks<br>' +
        'history. We like to capture this information in the physical<br>' +
        'database using column extended properties. For tables and<br>' +
        'views, we recommend storing a business description and<br>' +
        'possibly a second technical description as table extended<br>' +
        'properties at a minimum.<br>' +
        '• For each table, create a table extended property called Table<br>' +
        'Description to hold the business description of the table.<br>' +
        '• For each column in each dimension table, create a column<br>' +
        'extended property called Description to hold the business<br>' +
        'description of the column. Create a column extended<br>' +
        'property called Source System to hold a business-oriented<br>' +
        'summary of the source system.<br>' +
        '• For each non-key column in each dimension table, create a<br>' +
        'column extended property called SCD Type. The value of the<br>' +
        'SCD Type extended property should be 1-Overwrite History<br>' +
        'or 2-Track History.<br>' +
        'REFERENCE<br>' +
        'The Excel spreadsheet that we’ve already<br>' +
        'talked about, and which is posted on the<br>' +
        'book’s website at<br>' +
        'shttp://www.kimballgroup.com/html/<br>' +
        'booksMDWTtools.html automatically creates<br>' +
        'these and other extended properties for<br>' +
        'you.<br>' +
        'Define Storage and Create Constraints and Supporting<br>' +
        'Objects<br>' +
        '286<br>' +
        'The core definition of the tables and columns are enough<br>' +
        'to get started, and for a small data warehouse, they are<br>' +
        'probably all you need. DBAs building larger data<br>' +
        'warehouses will need to spend some time working on<br>' +
        'creating files and filegroups for storage, determining a data<br>' +
        'compression strategy, declaring primary and foreign keys,<br>' +
        'building indexes and aggregates for performance, creating<br>' +
        'views to support user access, and adding data rows to<br>' +
        'support missing values.<br>' +
        'Create Files and Filegroups<br>' +
        'Before you can create your tables and indexes, you have to<br>' +
        'know where you are going to put them. Where data gets<br>' +
        'stored in the file system can dramatically impact<br>' +
        'performance. Files and filegroups are the mechanisms for<br>' +
        'determining where SQL Server stores the data. Files are<br>' +
        'the physical operating system files where data and<br>' +
        'transaction logs are stored. Filegroups are sets of files that<br>' +
        'SQL Server manages together. There are more layers in<br>' +
        'the file subsystem that determine where the data actually<br>' +
        'gets written, but they are determined outside of SQL<br>' +
        'Server by the file system and the disk subsystem. You<br>' +
        'need to deal with files and filegroups when you create your<br>' +
        'tables because the CREATE TABLE statement includes a<br>' +
        'filegroup assignment.<br>' +
        'SQL Server creates two default files when you create a<br>' +
        'database: the main datafile with a suffix of .mdf and the<br>' +
        'transaction log file with a suffix of .ldf. It puts these files<br>' +
        'in the default location, which is in the /MSSQL/DATA/<br>' +
        'directory in SQL Server directory in Program Files. SQL<br>' +
        'Server also creates the default filegroup called PRIMARY,<br>' +
        '287<br>' +
        'which contains the main datafile. Log files can be located<br>' +
        'wherever you like, but they cannot be assigned to<br>' +
        'filegroups. By default, all data will be written to the<br>' +
        'PRIMARY filegroup and into the default datafile it<br>' +
        'contains.<br>' +
        'You will need to determine where your files should live<br>' +
        'and how many you should have per filegroup. This is<br>' +
        'based on how many disk drives you have and how they are<br>' +
        'configured in the disk subsystem, along with the nature<br>' +
        'and usage of your data and indexes. Here are a few<br>' +
        'guidelines:<br>' +
        '• Create at least one additional filegroup and make it the<br>' +
        'default.<br>' +
        '• Create files on all available local disks and add them to the<br>' +
        'filegroup.<br>' +
        '• Pre-allocate a generous amount of space evenly across all<br>' +
        'files; enough for all the history you are loading, plus at least<br>' +
        'an additional year. Three years of additional space would be<br>' +
        'even better if you can see that far into the future.<br>' +
        '• Put the transaction log file on a separate disk from your data<br>' +
        'files.<br>' +
        'If a filegroup has multiple files, SQL Server writes data to<br>' +
        'them in a round robin fashion, but prorated based on the<br>' +
        'space available in each file. Keep the file sizes the same so<br>' +
        'the data will be distributed evenly across the disks.<br>' +
        'Much of what you do with files and filegroups will depend<br>' +
        'on the nature of your disk subsystem. You can achieve the<br>' +
        'goal of distributing data across multiple disks using files<br>' +
        'and filegroups, or by how you define your disk subsystem<br>' +
        'or SAN. If you create one file on a logical disk on the<br>' +
        'SAN, that logical disk can be designed to stripe the data<br>' +
        '288<br>' +
        'across an array of physical disks. In other words, the SAN<br>' +
        'can accomplish the same data distribution you get with<br>' +
        'multiple files and filegroups within SQL Server.<br>' +
        'Our recommendation is to get help from someone who is<br>' +
        'an expert at configuring disk subsystems for SQL Server<br>' +
        'data warehousing on your hardware.<br>' +
        'REFERENCE<br>' +
        'Search msdn.microsoft.com for the MSDN<br>' +
        'page titled “Using Files and Filegroups”<br>' +
        'and search TechNet.Microsoft.com for the<br>' +
        'white paper titled “Fast Track Data<br>' +
        'Warehouse 2.0 Architecture” for two good<br>' +
        'starting points on files and filegroups.<br>' +
        'Data Compression<br>' +
        'Data compression came to SQL Server with the 2008<br>' +
        'release. Data compression offers several benefits and a few<br>' +
        'costs. The benefits are space related: Compressed data<br>' +
        'takes up less disk space. Often the compressed table is less<br>' +
        'than half the original table size and it can be as small as 10<br>' +
        'percent of the original size depending on the table<br>' +
        'definition and data distribution. Compression uses less<br>' +
        'memory because table rows are kept in a compressed form<br>' +
        'in the buffer cache until the contents of a row are needed<br>' +
        'by the relational engine. More rows on each page and more<br>' +
        'rows in memory also mean less I/O needed to resolve any<br>' +
        'given query. As a result, you will typically see both a<br>' +
        '289<br>' +
        'space savings and query speed improvement with data<br>' +
        'compression.<br>' +
        'There are two major costs to data compression. The first is<br>' +
        'a true dollar cost: Data compression is an Enterprise<br>' +
        'Edition feature. The second is a CPU cost. All this<br>' +
        'compression and uncompression requires additional CPU<br>' +
        'cycles, so if your system is already CPU bound, data<br>' +
        'compression will actually reduce performance.<br>' +
        'SQL Server offers two major types of compression: row<br>' +
        'and page. Row compression essentially applies the concept<br>' +
        'of variable-width encoding to all columns in the table.<br>' +
        'Row compression may not make a huge difference because<br>' +
        'most data warehouse dimensions are relatively small, and<br>' +
        'they already use varchar for the character attributes. Large<br>' +
        'dimensions with many Unicode fields will benefit from a<br>' +
        'special Unicode compression added to row and page<br>' +
        'compression in SQL 2008 R2. Row compression is<br>' +
        'generally not recommended for the data warehouse.<br>' +
        'Page compression applies three different algorithms to<br>' +
        'compress all the data on a given page. First it applies row<br>' +
        'level compression. Next it applies what is known as prefix<br>' +
        'compression, where a value is stored in the compression<br>' +
        'information section after the page header for each column<br>' +
        'with a commonly used prefix. Some columns can take<br>' +
        'better advantage of this than others. Donald Farmer uses<br>' +
        'the example of the page in the Scottish phone book with all<br>' +
        'the last names that start with ‘Mac’. By storing ‘Mac’ in<br>' +
        'the page header, you can save significant space.<br>' +
        '290<br>' +
        'The third step in page compression, called dictionary<br>' +
        'compression, looks for repeating values within a page and<br>' +
        'stores a single copy of frequent values in a dictionary<br>' +
        'section of the page. In individual rows that contain these<br>' +
        'values, an index is stored that points back to the value in<br>' +
        'the page dictionary. In the phone book example, dictionary<br>' +
        'compression might store “MacDonald” in the dictionary<br>' +
        'with pointers to that entry from all fields in the page that<br>' +
        'contain “MacDonald.”<br>' +
        'Page compression can make a big difference on fact tables<br>' +
        'and large dimensions. You can explore the impact of<br>' +
        'compression using a stored procedure called<br>' +
        'sp_estimate_data_compression_savings that<br>' +
        'estimates compression results by sampling rows from the<br>' +
        'target table. For example, the stored procedure predicted a<br>' +
        'data size reduction of the Adventure Works<br>' +
        'FactInternetSales table and its associated indexes as<br>' +
        'follows:<br>' +
        'Compression Type Size (KB) % Reduction in Size<br>' +
        'None 19,272 0%<br>' +
        'Row 11,000 43%<br>' +
        'Page 7,104 63%<br>' +
        'You’ll want to experiment with data compression in terms<br>' +
        'of table size and query performance, but in general, you<br>' +
        'should expect a size reduction from page compression of<br>' +
        '50% or more.<br>' +
        '291<br>' +
        'REFERENCE<br>' +
        'For more information on data compression,<br>' +
        'search Technet (www.technet.com) for the<br>' +
        'topic titled Creating Compressed Tables<br>' +
        'and Indexes.<br>' +
        'Backup compression is separate from table compression.<br>' +
        'Most DBAs who deal with larger datasets have used third<br>' +
        'party backup utilities, which have included compression<br>' +
        'for years. Backup compression was introduced in the core<br>' +
        'SQL Server product with the SQL Server 2008 release, as<br>' +
        'an Enterprise Edition feature.<br>' +
        'Entity and Referential Integrity Constraints<br>' +
        'In the data warehouse, most of the tables have certain rules<br>' +
        'and relationships they must follow. All tables have a<br>' +
        'primary key, which is that column or set of columns that<br>' +
        'will identify a single row when constrained to a single<br>' +
        'value. This is known as entity integrity. For the dimension<br>' +
        'tables, the primary key is obviously the surrogate key. For<br>' +
        'the fact tables, the primary key is usually a combination of<br>' +
        'all of the foreign keys from each dimension.<br>' +
        'Foreign keys define the relationships between the fact and<br>' +
        'dimension tables. It says every value for a given foreign<br>' +
        'key found in the fact table is guaranteed to have an entry in<br>' +
        'the associated dimension table. This is known as<br>' +
        'referential integrity.<br>' +
        '292<br>' +
        'Declaring the primary key on a table means the database<br>' +
        'will not allow the insertion of duplicate rows. The database<br>' +
        'will look up each new row to make sure it doesn’t already<br>' +
        'exist before it is inserted. With foreign key constraints in<br>' +
        'place, every time a row is added to the fact table the SQL<br>' +
        'Server engine will check that each dimension key exists in<br>' +
        'its corresponding dimension table.<br>' +
        'The issue before you at this point is deciding whether you<br>' +
        'want the database to enforce these constraints for you.<br>' +
        'Declaring the primary key on the dimensions makes sense,<br>' +
        'but what about the primary key on the fact table? And<br>' +
        'what about the foreign key constraints between the fact<br>' +
        'table and the dimension tables? The textbook DBA answer<br>' +
        'is that of course you should declare the fact table keys,<br>' +
        'both primary and foreign. Even though you should do so, a<br>' +
        'few paragraphs from now we’ll talk about why you often<br>' +
        'don’t.<br>' +
        'Let’s start with the technically right, but practically wrong<br>' +
        'approach. Declaring primary and foreign keys in the<br>' +
        'database is technically the right thing to do. Database<br>' +
        'administrators without data warehouse experience will<br>' +
        'look at you funny if you suggest anything different. In the<br>' +
        'case of a fact table, there is usually little value in defining<br>' +
        'a surrogate (integer) primary key on the fact table. Instead,<br>' +
        'you can enforce uniqueness by creating a unique index<br>' +
        'over the set of columns, usually dimension keys, that<br>' +
        'makes a fact row unique. In Figure 5-2 it’s all three<br>' +
        'dimensions, but that’s not always the case.<br>' +
        '293<br>' +
        'NOTE<br>' +
        'The unique, primary key index on a fact<br>' +
        'table should never be a clustered index.<br>' +
        'The primary key index is a big,<br>' +
        'multicolumn index. If it’s a clustered index,<br>' +
        'all other indexes on the fact table will be<br>' +
        'huge and inefficient because they will use<br>' +
        'the clustered index as their row identifier.<br>' +
        'In the indexing section coming up, you will create<br>' +
        'single-column indexes on some of the individual foreign<br>' +
        'key columns in the fact table, and their primary key<br>' +
        'reference columns in the corresponding dimension tables.<br>' +
        'If you are doing things by the book you should add a<br>' +
        'foreign key constraint between the fact table and its<br>' +
        'dimensions. You need to let SQL Server check referential<br>' +
        'integrity, usually when you add the constraints, but you<br>' +
        'can schedule this task for later. If SQL Server doesn’t<br>' +
        'check referential integrity, the constraints are just window<br>' +
        'dressing and don’t do anything.<br>' +
        'In practice, data warehouse DBAs often do not create the<br>' +
        'primary key and foreign key constraints on the fact table.<br>' +
        'Maintaining these structures is extremely expensive and<br>' +
        'slows down the data loads. As we describe in Chapter 7,<br>' +
        'one of the most important jobs of your ETL system is to<br>' +
        'prevent referential integrity violations — and substituting<br>' +
        'surrogate keys in the fact table is a nearly foolproof way to<br>' +
        'do that. Having the database look up foreign keys in every<br>' +
        'dimension for every incoming fact row is a very expensive<br>' +
        '294<br>' +
        'check for something that you did just moments before<br>' +
        'when you looked up the fact table’s surrogate keys. Along<br>' +
        'the same lines, for SQL Server to maintain a<br>' +
        'multiple-column unique index is obviously expensive.<br>' +
        'Since SQL Server resolves most queries by using the<br>' +
        'single-column indexes that you’ve created on the more<br>' +
        'selective dimension keys in the fact table, this unique<br>' +
        'index provides very little value for its cost of maintenance.<br>' +
        'NOTE<br>' +
        'Generally, you won’t declare referential<br>' +
        'integrity constraints. If you feel it’s<br>' +
        'important, test the options in your<br>' +
        'environment to understand the cost. Build<br>' +
        'your fact tables the right way, with key<br>' +
        'constraints defined and enforced in the<br>' +
        'database. For the initial historical load,<br>' +
        'disable the constraints, load all the data,<br>' +
        'and then re-enable and check the<br>' +
        'constraints. Test your incremental load<br>' +
        'process. If it’s too slow, and you’re<br>' +
        'positive the slowness is occurring during<br>' +
        'the insert step, test the performance gains<br>' +
        'that result from removing the primary key<br>' +
        'index and foreign key constraints. If you<br>' +
        'decide to run without these constraints,<br>' +
        'check periodically (weekly or daily) for<br>' +
        'referential integrity violations, which can<br>' +
        '295<br>' +
        'creep in no matter how beautifully you’ve<br>' +
        'designed your ETL system.<br>' +
        'Initial Indexing and Database Statistics<br>' +
        'Indexes are one of two major performance tools available<br>' +
        'in the data warehouse database platforms, the other being<br>' +
        'aggregates. In this section we provide a simple starting<br>' +
        'point indexing plan for your relational data warehouse<br>' +
        'database. You will need to evaluate your query workload<br>' +
        'against your data on your test system, in order to optimize<br>' +
        'your indexing plan. We’ll talk more about performance<br>' +
        'tuning in Chapter 17 on maintenance.<br>' +
        'Dimension Indexes<br>' +
        'Dimension tables with a single column integer surrogate<br>' +
        'primary key should have a clustered primary key index.<br>' +
        'The clustered part defines the physical order of the rows.<br>' +
        'In this case, the clustered index actually becomes the<br>' +
        'physical table itself. The primary key part is that column<br>' +
        '(or columns) that uniquely identify a single row in the<br>' +
        'table: the surrogate key. When you define a primary key<br>' +
        'on a table in SQL Server it also creates a clustered index<br>' +
        'by default. Unless the dimension is small, you should also<br>' +
        'create an index on the business key to support the ETL fact<br>' +
        'table key substitution process.<br>' +
        'For small dimensions, the only other index you might want<br>' +
        'to define at the outset is a single column index on any<br>' +
        'foreign keys. In the case study database, all dimensions<br>' +
        '296<br>' +
        'have two foreign keys to DimAudit, so we created indexes<br>' +
        'on those columns. All of the dimensions in our sample<br>' +
        'database are small enough that it’s unlikely to be<br>' +
        'worthwhile creating any additional indexes.<br>' +
        'For larger dimensions, your indexing plan depends on how<br>' +
        'the relational data warehouse database will be used. The<br>' +
        'hardest case to tune for is if the relational data warehouse<br>' +
        'will support significant reporting and ad hoc queries.<br>' +
        'Insofar as you know what that query and reporting load<br>' +
        'will be, you can tune the index plan for the expected use.<br>' +
        'Any attribute commonly used in query constraints or select<br>' +
        'lists is a candidate for a single-column index. Often,<br>' +
        'columns in a hierarchy are such candidates. You should<br>' +
        'already have identified the hierarchical relationships in<br>' +
        'your dimension. For a large dimension supporting direct<br>' +
        'queries, these hierarchical attributes are probably the first<br>' +
        'non-key attributes that you’ll want to index. A very simple<br>' +
        'dimensional model is illustrated in Figure 5-2. If<br>' +
        'DimProduct is large and heavily used, consider<br>' +
        'single-column indexes on ProductCategory,<br>' +
        'ProductSubcategory, and ProductName.<br>' +
        'Figure 5-2: Simple dimensional diagram illustrating key<br>' +
        'constraints<br>' +
        '297<br>' +
        'Indexing Very Large Dimensions<br>' +
        'A very large dimension that contains some type 2<br>' +
        'slowly changing dimension attributes should have<br>' +
        'a four-column index on the business key, row<br>' +
        'begin date, row end date, and the surrogate key.<br>' +
        'The row end date and surrogate key can be created<br>' +
        'as INCLUDE columns in the index. This index will<br>' +
        'speed the performance of surrogate key<br>' +
        'management during the ETL process, especially for<br>' +
        'loading historical fact rows.<br>' +
        'See the Books Online topic “Indexes with Included<br>' +
        'Columns.”<br>' +
        'Microsoft SQL Server’s query engine will use multiple<br>' +
        'indexes on a single table when resolving a query. With a<br>' +
        'specific workload of predefined queries, you can probably<br>' +
        'define multicolumn indexes that are very useful. However,<br>' +
        '298<br>' +
        'if you know very little about how users will access the<br>' +
        'data, the single-column index approach is the most<br>' +
        'sensible starting point.<br>' +
        'Fact Table Indexes<br>' +
        'For fact tables, the standard starting point is to create a<br>' +
        'single-column clustered index on the date key. If your fact<br>' +
        'table has multiple date foreign keys, start with the<br>' +
        'transaction date or, if your fact table is partitioned, start<br>' +
        'with the field that’s used in the partitioning strategy. This<br>' +
        'will keep your fact table from getting too fragmented<br>' +
        'because new transactions will be added to the end of the<br>' +
        'table. However, business queries may not perform well if<br>' +
        'they refer to some other date, such as effective date. In this<br>' +
        'case, you may need to index the queried date and rebuild<br>' +
        'your index every so often.<br>' +
        'Next, create a single-column index on most of the other<br>' +
        'foreign keys to the dimension tables. If you have a low<br>' +
        'cardinality dimension that won’t limit the selected rowset,<br>' +
        'such as a transaction type with only five values, it’s<br>' +
        'probably not worth creating an index.<br>' +
        'In the simple database illustrated in Figure 5-2, you would<br>' +
        'create a clustered index on OrderDateKey and<br>' +
        'single-column indexes on CustomerKey and ProductKey<br>' +
        'in the FactOrders table. Most DW/BI systems do use the<br>' +
        'relational database for some queries and reports, so the<br>' +
        'simple fact table indexing described here is a good starting<br>' +
        'point. As with the dimensions, you should tune the fact<br>' +
        'table indexing plan with your data and query mix.<br>' +
        '299<br>' +
        'Statistics<br>' +
        'The optimizer looks at indexes in conjunction with a set of<br>' +
        'statistics it keeps about the cardinality and distribution of<br>' +
        'column values across the tables in a query. In most cases,<br>' +
        'you can rely on the default settings for statistics which will<br>' +
        'autocreate and autoupdate statistics as the system sees fit.<br>' +
        'In larger data warehouse databases, especially when you<br>' +
        'are loading data into a date partition or your fact table is<br>' +
        'clustered by date, you should update statistics for that date<br>' +
        'column after every load. Otherwise, the optimizer won’t<br>' +
        'realize the data is available. You may also want to consider<br>' +
        'creating multicolumn statistics on combinations of fact<br>' +
        'table foreign keys that are often used together. You will<br>' +
        'need to script these and update them on a regular basis<br>' +
        'because they will not be included in the autoupdate.<br>' +
        'NOTE<br>' +
        'If your relational data warehouse is being<br>' +
        'used only to stage the Analysis Services<br>' +
        'database, you can get away with building<br>' +
        'fewer indexes. You’ll certainly want to<br>' +
        'keep the primary key index on dimensions.<br>' +
        'Fact tables can be left largely unindexed.<br>' +
        'Because the queries Analysis Services uses<br>' +
        'to find data for processing are always the<br>' +
        'same, you can run the Database Tuning<br>' +
        'Advisor and tune the relational database<br>' +
        'exactly for that set of queries.<br>' +
        '300<br>' +
        'Aggregate Tables<br>' +
        'Most BI reports and analyses start at a summary level, and<br>' +
        'then allow the users to drill down into greater detail. A<br>' +
        'time series report showing sales by month for the last two<br>' +
        'years returns only 24 rows, but if it is built on the atomic<br>' +
        'level detail, it may have to sum up hundreds of millions, or<br>' +
        'even billions of rows each time it is run. If you can<br>' +
        'preaggregate this data during the ETL process, any report<br>' +
        'that uses the aggregates will run dramatically faster. In<br>' +
        'fact, aggregate tables are the single most useful way to<br>' +
        'improve query performance on a dimensional DW/BI<br>' +
        'system. An aggregate table summarizes data at a higher<br>' +
        'level than the atomic data maintained in the detailed fact<br>' +
        'table. You create aggregate tables by either omitting one or<br>' +
        'more of the dimension foreign keys entirely, or by<br>' +
        'summing to a parent level in a dimension, say at category<br>' +
        'of product, country of geography, or month of date. Some<br>' +
        'or all of the other dimensions would remain at their leaf<br>' +
        'levels.<br>' +
        'RESOURCES<br>' +
        'The process of designing and maintaining<br>' +
        'aggregate tables in the relational data<br>' +
        'warehouse database is discussed in the<br>' +
        'books The Data Warehouse Toolkit<br>' +
        '(Chapter 14), The Data Warehouse ETL<br>' +
        'Toolkit (Chapter 6), and Mastering Data<br>' +
        'Warehouse Aggregates by Chris Adamson<br>' +
        '301<br>' +
        '(Wiley, 2006), and in several articles in<br>' +
        'The Kimball Group Reader.<br>' +
        'Most Microsoft DW/BI systems that would benefit from<br>' +
        'aggregates use Analysis Services to manage those<br>' +
        'aggregates. As we describe in Chapter 8, the Analysis<br>' +
        'Services OLAP functionality has a host of features for<br>' +
        'designing and maintaining aggregates. This is one of the<br>' +
        'core features of Analysis Services. Even if Analysis<br>' +
        'Services provided no other benefits, its usefulness as an<br>' +
        'aggregate manager and navigator makes implementing it<br>' +
        'worthwhile.<br>' +
        'Microsoft has a few tools to help you maintain aggregate<br>' +
        'tables in the relational database. We’ve seen people use<br>' +
        'indexed views as a substitute for aggregate tables. You can<br>' +
        'define an indexed view on a fact table which will<br>' +
        'summarize the detailed data and physically store the<br>' +
        'summary in an indexed view. This approach is most<br>' +
        'appealing to people who have done a lot of data<br>' +
        'warehousing in Oracle, which has a similar feature. One<br>' +
        'big advantage of an indexed view over simply creating a<br>' +
        'summary table is the optimizer understands that the view is<br>' +
        'based on the atomic fact table. When a user submits a<br>' +
        'query against the atomic fact table, the optimizer will<br>' +
        'automatically try to use the summary level indexed view if<br>' +
        'it can resolve the query. The incoming query does not have<br>' +
        'to be aware of the existence of a separate indexed view.<br>' +
        'This query redirection is known as aggregate navigation<br>' +
        'and is a key component to making aggregates broadly<br>' +
        'usable.<br>' +
        '302<br>' +
        'There are a lot of little rules about creating indexed views.<br>' +
        'Most importantly, they work automatically only in<br>' +
        'Enterprise Edition. In Standard Edition, you need to refer<br>' +
        'to the indexed view by name and use the NOEXPAND hint to<br>' +
        'make the optimizer use it as if it was a stored table. At that<br>' +
        'point, you might as well simply create a stored table. Even<br>' +
        'in Enterprise Edition, you may need to use the NOEXPAND<br>' +
        'hint to get the optimizer to choose the indexed view.<br>' +
        'NOTE<br>' +
        'Indexed views are not perfect in SQL<br>' +
        'Server. Most importantly, they work<br>' +
        'automatically only in Data Center,<br>' +
        'Enterprise, and Developer editions. Future<br>' +
        'releases beyond SQL Server 2008 R2 will<br>' +
        'likely turn to other technologies, such as<br>' +
        'column store indexes, to accelerate analytic<br>' +
        'queries against the relational database.<br>' +
        'If you are running Standard Edition and just can’t do<br>' +
        'OLAP, and really need to maintain relational aggregate<br>' +
        'tables for performance, we recommend that you don’t use<br>' +
        'indexed views, since they don’t provide automatic<br>' +
        'aggregate navigation. Instead, maintain separate aggregate<br>' +
        'tables the old-fashioned way: in your ETL process. You<br>' +
        'will have to update the aggregates by hand. If you only<br>' +
        'ever add new, incremental rows to your data warehouse<br>' +
        'database, and you only build aggregates on type 2 slowly<br>' +
        'changing dimensions, then maintaining aggregate tables<br>' +
        'isn’t that difficult. In most cases, it’s a bit more effort.<br>' +
        '303<br>' +
        'Again, see the books referenced above for a more<br>' +
        'complete description of the issues and approaches.<br>' +
        'REFERENCE<br>' +
        'For more details, search msdn.microsoft.com<br>' +
        'for the article titled “Improving<br>' +
        'Performance with SQL Server 2008<br>' +
        'Indexed Views.”<br>' +
        'Create Table Views<br>' +
        'All business user access to the relational data warehouse<br>' +
        'database should come through views. Similarly, Analysis<br>' +
        'Services databases should be defined on views, rather than<br>' +
        'the underlying tables. In both cases, the rationale is to<br>' +
        'provide a protective layer between the users and the<br>' +
        'underlying database. This layer can be very helpful when<br>' +
        'you need to modify the DW/BI system after it’s in<br>' +
        'production. It is also a bit of a pain for the developers<br>' +
        'because some of the design tools in the BI studio rely on<br>' +
        'foreign key constraints to identify join paths between<br>' +
        'tables. These design tools are unable to pull these<br>' +
        'relationships up through the views, so you will need to<br>' +
        'draw them in yourself.<br>' +
        'All user access should be through views rather than to the<br>' +
        'underlying tables. The table names shouldn’t even show up<br>' +
        'in a user’s list of database objects. In the simplest case, a<br>' +
        'table’s view would select all the columns from the<br>' +
        'underlying table. You may want to omit some columns<br>' +
        '304<br>' +
        'from the view, especially some of the housekeeping<br>' +
        'columns described previously.<br>' +
        'If you have snowflaked a dimension, create a single view<br>' +
        'for the dimension that collapses the multiple tables into a<br>' +
        'single logical table. To improve performance, you may<br>' +
        'choose to make this an indexed view.<br>' +
        'All of your database object names should be business user<br>' +
        'friendly. But the view definition is an opportunity to<br>' +
        'rename columns, especially when you use a dimension in<br>' +
        'multiple roles, such as Order_Date and Ship_Date.<br>' +
        'Insert an Unknown Member Row<br>' +
        'A corollary to forbidding nulls, especially for the fact table<br>' +
        'foreign keys, is how to handle a fact row that does have a<br>' +
        'null as one of its incoming business keys. For example,<br>' +
        'how would we load a fact row that has a missing customer<br>' +
        'ID? We talk about handling other referential integrity<br>' +
        'failures in Chapter 7, but a good solution to the missing<br>' +
        'source key problem is to add an unknown member row to<br>' +
        'each dimension table. We habitually do this as soon as we<br>' +
        'create the table, and we use –1 as the unknown member’s<br>' +
        'surrogate key (unless the key is a tinyint).<br>' +
        'If you enable the identity column property to generate<br>' +
        'dimension surrogate keys, you can add the unknown<br>' +
        'member row by using the following logic:<br>' +
        'SET IDENTITY_INSERT Dim_MyDim ON<br>' +
        'INSERT<br>' +
        'Dim_MyDim (MyDim_Surrogate_Key, MyDim_Business_Key, Attribute1, Attribute2)<br>' +
        '305<br>' +
        'VALUES (-1, NULL, \'Unknown\', \'Unknown\')<br>' +
        'SET IDENTITY_INSERT Dim_MyDim OFF<br>' +
        'Example CREATE TABLE Statement<br>' +
        'The following T-SQL creates the DimProduct table from<br>' +
        'Figure 5-2 with most of the constraints and associated<br>' +
        'items we’ve discussed thus far. You might not use all of<br>' +
        'these settings for every table, but we wanted to include the<br>' +
        'syntax.<br>' +
        '-- Create the table with an IDENTITY generated surrogate key, default<br>' +
        '-- values, a primary key and clustered index on the surrogate key, on a<br>' +
        '-- specific filegroup with page level data compression<br>' +
        'CREATE TABLE [dbo].[DimProduct](<br>' +
        '[ProductKey] [int] IDENTITY(1,1) NOT NULL,<br>' +
        '[BKProductSKU] [nvarchar] (25) NOT NULL DEFAULT N\'ZZ-000-ZZ\',<br>' +
        '[ProductName] [nvarchar](50) NOT NULL<br>' +
        'DEFAULT N\'Product unknown or not provided\',<br>' +
        '[ProductSubCategory] [nvarchar](50) NOT NULL<br>' +
        'DEFAULT N\'Product Subcategory unknown or not provided\',<br>' +
        '[ProductCategory] [nvarchar](50) NOT NULL<br>' +
        'DEFAULT N\'Product Category unknown or not provided\',<br>' +
        'CONSTRAINT [PK_dbo.DimProduct] PRIMARY KEY<br>' +
        'CLUSTERED ([ProductKey]<br>' +
        'ASC)<br>' +
        '306<br>' +
        ') ON [DimFileGroup]<br>' +
        'WITH ( DATA_COMPRESSION = PAGE ); -- only if this is a very big<br>' +
        'dimension<br>' +
        '-- Create the extended property entry for the table description<br>' +
        'exec sys.sp_addextendedproperty @name=N\'Table Description\',<br>' +
        '@value=N\'Information about products\', @level0type=N\'SCHEMA\',<br>' +
        '@level0name=\'dbo\', level1type=N\'TABLE\', @level1name=DimProduct;<br>' +
        'GO;<br>' +
        '-- Create the user access view of the table<br>' +
        'CREATE VIEW [Product] AS SELECT [ProductKey], [BKProductSKU],<br>' +
        '[ProductName], [ProductSubCategory], [ProductCategory]<br>' +
        'FROM [DimProduct];<br>' +
        'GO;<br>' +
        'You may want to separate the table creation DDL from the<br>' +
        'index DDL for tables with multiple indexes, and especially<br>' +
        'for fact tables. This will allow you to drop and recreate the<br>' +
        'indexes separate from the table itself. Also, note that if you<br>' +
        'create a default value such as \'ZZ-000-ZZ\' for<br>' +
        'BKProductSKU, you must make sure that value will never<br>' +
        'be used to describe a different entry in the table.<br>' +
        'Partitioned Tables<br>' +
        '307<br>' +
        'A partitioned table is essentially a large table split out into<br>' +
        'smaller tables under the covers. Each of these smaller<br>' +
        'tables, called partitions, can be accessed, indexed, and<br>' +
        'managed independently. Meanwhile, the set of partitions<br>' +
        'still looks and behaves like a single table to any incoming<br>' +
        'query. Partitioned tables are important for the scalability of<br>' +
        'the relational data warehouse database. The big win comes<br>' +
        'from greatly increased manageability of very large tables.<br>' +
        'With a very large partitioned table, everything from<br>' +
        'loading data, to indexing, and especially to backing up the<br>' +
        'data, can be much easier and faster than with a single<br>' +
        'monolithic table.<br>' +
        'Because large is the operative word, you would typically<br>' +
        'partition only the fact tables in a relational data warehouse<br>' +
        'database. The definition of large depends on the size and<br>' +
        'strength of your server and disk system, but as a rule of<br>' +
        'thumb, tables with around 100 million rows, and/or ten<br>' +
        'gigabytes of data are low end partition candidates.<br>' +
        'Dimension tables, even very large dimension tables,<br>' +
        'seldom benefit from partitioning.<br>' +
        'Analysis Services databases can also be partitioned. Most<br>' +
        'people use the same partitioning scheme for Analysis<br>' +
        'Services as for the relational database, and create, merge,<br>' +
        'and delete partitions from the two data stores on the same<br>' +
        'schedule. This is just a convenience; there is no<br>' +
        'requirement that Analysis Services partitions be<br>' +
        'synchronized with relational partitions.<br>' +
        'Relational and Analysis Services partitioning are both<br>' +
        'features of SQL Server Enterprise Edition only. Like many<br>' +
        'SQL Server features, there are wizards to help you create<br>' +
        '308<br>' +
        'and maintain partitioned tables. These can be helpful<br>' +
        'learning tools, but in most cases you will want to create<br>' +
        'scripts and use tools you can programmatically invoke<br>' +
        'from your ETL system.<br>' +
        'How Does Table Partitioning Work?<br>' +
        'The classic partitioning scheme is to partition by month.<br>' +
        'Each month of fact data goes into a separate physical table,<br>' +
        'which is tied together with the other months’ identically<br>' +
        'structured tables. There are some specific requirements,<br>' +
        'discussed shortly, for how the partitions are physically<br>' +
        'structured.<br>' +
        'DOWNLOADS<br>' +
        'In this section, we illustrate an extremely<br>' +
        'simple partitioned table that is loaded<br>' +
        'monthly. These scripts, along with example<br>' +
        'data, are available under the Tools and<br>' +
        'Utilities tab on the book’s website:<br>' +
        'http://www.kimballgroup.com/html/<br>' +
        'booksMDWT.html<br>' +
        'There are four basic steps to creating a partitioned table:<br>' +
        '1. Create files and filegroups if needed<br>' +
        '2. Define the partition function<br>' +
        '3. Define the partition scheme<br>' +
        '309<br>' +
        '4. Create the partitioned table on the partition scheme<br>' +
        'We’ll use a simple example of creating a partitioned table<br>' +
        'to hold the first three months of data for 2012 to<br>' +
        'demonstrate each of these steps.<br>' +
        'Create Files and Filegroups<br>' +
        'You can map the partitions in your partitioned table to one<br>' +
        'or more filegroups. The single filegroup approach allows<br>' +
        'for simpler management, and can make it easier to set up<br>' +
        'the underlying storage for faster sequential access.<br>' +
        'However, because backup and restore occur at the<br>' +
        'filegroup level, you must backup and restore the entire<br>' +
        'table. By creating partitions on separate filegroups, you<br>' +
        'can set older filegroups that no longer receive data or<br>' +
        'updates to Read Only. You can create a backup process<br>' +
        'that will recognize these filegroups as unchanged and back<br>' +
        'them up only once, thus greatly speeding the differential<br>' +
        'backup and reducing its size. You can also do a partial<br>' +
        'restore of a single partition if need be.<br>' +
        'In our example, we will create a filegroup for each<br>' +
        'partition. Create files and filegroups with simple ALTER<br>' +
        'DATABASE commands. This code creates a new filegroup,<br>' +
        'MDWT_FG1, and a new file, MDWT_Data1.ndf on the G:<br>' +
        'drive, adding it to the filegroup:<br>' +
        'ALTER DATABASE MDWT_2008R2 ADD FILEGROUP MDWT_FG1;<br>' +
        'ALTER DATABASE MDWT_2008R2<br>' +
        '310<br>' +
        'ADD<br>' +
        'FILE ( NAME = MDWT_2008R2_Data1, FILENAME = \'G:\\MDWT_Data1.ndf\',<br>' +
        'SIZE = 1GB, MAXSIZE = 1200MB, FILEGROWTH = 100MB )<br>' +
        'TO FILEGROUP MDWT_FG1;<br>' +
        'It’s a good idea to define the initial file sizes large enough<br>' +
        'to accommodate your initial historical load plus data for<br>' +
        'some time out into the future. Having files autogrow can<br>' +
        'cause unexpected slowdowns and file fragmentation.<br>' +
        'Since our example involves three months of data, we<br>' +
        'would copy the filegroup and file creation code to create a<br>' +
        'total of five files and filegroups. This will make more<br>' +
        'sense in the next step.<br>' +
        'Create the Partition Function<br>' +
        'Once the filegroups are in place, the first step in defining a<br>' +
        'partitioned table is to define a partition function, using the<br>' +
        'CREATE PARTITION FUNCTION syntax that’s well defined<br>' +
        'in Books Online. If you’re creating monthly partitions for<br>' +
        'a DateKey surrogate key, you would use syntax like:<br>' +
        'CREATE PARTITION FUNCTION PFMonthly (int)<br>' +
        'AS RANGE RIGHT<br>' +
        'FOR VALUES (20120101, 20120201, 20120301, 20120401);<br>' +
        'In this very simple example, we have four breakpoints,<br>' +
        'which results in five partitions. The first partition holds all<br>' +
        'data before January 2012, the second partition holds<br>' +
        'January data, the third holds February data, the fourth<br>' +
        'holds March data, and the fifth holds all data for April<br>' +
        '311<br>' +
        'onward. Note that a partition function automatically<br>' +
        'creates partitions to hold all possible values. Four<br>' +
        'boundary points, as in the preceding example, create five<br>' +
        'partitions, hence the five filegroups.<br>' +
        'At this point, the alert reader is wondering why we created<br>' +
        'five partitions since our goal is to create a partitioned table<br>' +
        'to hold three months of data. Look carefully at the<br>' +
        'preceding paragraph and you will see we have created an<br>' +
        'empty partition on either side of the partitions that will<br>' +
        'have data in them. This allows us to add new data at either<br>' +
        'end of the table without having to split a populated<br>' +
        'partition. Splitting a populated partition is a slow process<br>' +
        'because SQL Server has to examine each row to see which<br>' +
        'of the new partitions it belongs to.<br>' +
        'If you’re using meaningless surrogate date keys, you need<br>' +
        'to create an integer function that uses the appropriate key<br>' +
        'ranges. You can create complex partition functions, but<br>' +
        'most people will just create simple functions like the one<br>' +
        'illustrated here. (This is the main reason to use a<br>' +
        'meaningful surrogate key for Date.)<br>' +
        'TIP<br>' +
        'Always leave yourself at least one empty<br>' +
        'partition that covers the date range that you<br>' +
        'plan to split later. Create new partitions and<br>' +
        'filegroups well before you need them, so<br>' +
        'that you are always splitting an empty<br>' +
        '312<br>' +
        'partition. In most cases, you want the first<br>' +
        'and last partitions always to be empty.<br>' +
        'Create the Partition Scheme<br>' +
        'The next step is to define a partition scheme, which maps<br>' +
        'each partition in a partition function to a specific physical<br>' +
        'location. A simple example is illustrated here:<br>' +
        'CREATE PARTITION SCHEME PSMonthly<br>' +
        'AS PARTITION PFMonthly<br>' +
        'TO (MDWT_FG1, MDWT_FG2, MDWT_FG3, MDWT_FG4, MDWT_FG5);<br>' +
        'This is where the filegroups we created in the first step<br>' +
        'come into play. It is possible to map more than one<br>' +
        'partition to a filegroup. This makes managing partitions<br>' +
        'easier, but reduces flexibility.<br>' +
        'Create the Partitioned Table<br>' +
        'Finally, create your partitioned table on the partition<br>' +
        'scheme. The syntax is really simple: Basically you’re<br>' +
        'replacing the standard ON <filegroups> syntax with an<br>' +
        'ON <PartitionScheme> clause. Here is example DDL for<br>' +
        'the FactOrders table from Figure 5-2:<br>' +
        'CREATE TABLE FactOrders<br>' +
        '( OrderDateKey int NOT NULL,<br>' +
        'CustomerKey int NOT NULL,<br>' +
        '313<br>' +
        'ProductKey int NOT NULL,<br>' +
        'OrderQty int,<br>' +
        'SalesAmt money<br>' +
        ')<br>' +
        '-- The ON clause refers to the partition scheme<br>' +
        'ON PSMonthly(OrderDateKey);<br>' +
        'NOTE<br>' +
        'The partition key (OrderDateKey in our<br>' +
        'example) must be NOT NULL.<br>' +
        'The end result of all this is a single table called<br>' +
        'FactOrders with the characteristics shown in Figure 5-3.<br>' +
        'Figure 5-3: Partitions, files, and filegroups for the<br>' +
        'FactOrders example<br>' +
        '314<br>' +
        'Inserting a row into FactOrders invokes the partition<br>' +
        'scheme, PSMonthly with OrderDateKey as its argument.<br>' +
        'This passes through the partition function to determine the<br>' +
        'partition to which the row belongs. The row is then<br>' +
        'inserted into the proper partition, which lives in a filegroup<br>' +
        'on a particular disk (or set of disks if you are designing for<br>' +
        'high performance).<br>' +
        'RESOURCES<br>' +
        'To examine data and partition information<br>' +
        'within the specific partitions, see the Books<br>' +
        'Online topic “Querying Data and Metadata<br>' +
        'from Partitioned Tables and Indexes.”<br>' +
        '315<br>' +
        'At this point, you can insert the three months of data<br>' +
        'directly into your partitioned table just like any other table.<br>' +
        'However, it shouldn’t surprise you to learn that this is not<br>' +
        'a fast loading process: At best it’s as fast as an insert into a<br>' +
        'normal table because every row has to go through the<br>' +
        'partition function to determine the partition to which it<br>' +
        'belongs. Especially for your initial load, you’ll want to<br>' +
        'know how to load the partitioned fact table as fast as<br>' +
        'possible. There’s an elegant trick that works great, which<br>' +
        'we’ll describe in the next section.<br>' +
        'Managing Partitioned Tables<br>' +
        'Once your partitioned table is defined, you will need to<br>' +
        'manage it on an ongoing basis. This section covers the<br>' +
        'major repeating tasks including adding new partitions,<br>' +
        'loading data quickly, using mixed grain partitions, and<br>' +
        'dropping old data.<br>' +
        'Adding a New Partition<br>' +
        'This very simple example can keep going until the end of<br>' +
        'March, but then what happens? How do you add a new<br>' +
        'partition for April? Remember, you want to keep an empty<br>' +
        'partition out in front of the data, so split the existing empty<br>' +
        'partition at the May 1 breakpoint to create an empty April<br>' +
        'partition and an empty partition starting at May 1.<br>' +
        'Assuming you have already created files and a filegroup<br>' +
        'for the new, split partition, you can use the following<br>' +
        'ALTER PARTITION commands:<br>' +
        '-- Alter the Partition Scheme to use a new filegroup for the next<br>' +
        'partition<br>' +
        '316<br>' +
        'ALTER PARTITION SCHEME PSMonthly<br>' +
        'NEXT USED MDWT_FG6; -- this filegroup must already exist<br>' +
        '-- Split the partition function range at May 1<br>' +
        'ALTER PARTITION FUNCTION PFMonthly ()<br>' +
        'SPLIT RANGE (20120501);<br>' +
        'Just like INSERTING, splitting a partition would be very<br>' +
        'slow if it contained data because SQL Server runs each<br>' +
        'row through the partition function to determine the<br>' +
        'partition to which it belonged. Splitting an empty partition<br>' +
        'is clearly the way to go.<br>' +
        'A second syntax for the ALTER PARTITION FUNCTION<br>' +
        'command will merge two partitions. As with splitting<br>' +
        'partitions, you’d prefer to merge empty partitions. We’ll<br>' +
        'discuss how to do so most effectively.<br>' +
        'Use Table Partitions for Fast Data Loads and Minimal<br>' +
        'Downtime<br>' +
        'Now that there’s an empty partition in place for April data,<br>' +
        'we will unveil our fast data load trick. The trick is to create<br>' +
        'a separate table structured exactly like the target partition,<br>' +
        'load data into it, add indexes and constraints, and then<br>' +
        'swap it with the actual partition. We’ll call this separate<br>' +
        'table a pseudo-partition. It has to structurally match the<br>' +
        'target partition before we can swap it — same columns,<br>' +
        'data types, indexes, filegroups, and so on. Note the<br>' +
        'CREATE TABLE script that follows is exactly the same as<br>' +
        '317<br>' +
        'we used for the partitioned table, excluding the ON<br>' +
        '<PartitionScheme> clause:<br>' +
        '-- Create an empty table nearly identical to the partitioned table<br>' +
        'CREATE TABLE PseudoPartition_201204<br>' +
        '(OrderDateKey int NOT NULL,<br>' +
        'CustomerKey int NOT NULL,<br>' +
        'ProductKey int NOT NULL,<br>' +
        'OrderQty int,<br>' +
        'SalesAmt money,<br>' +
        'CONSTRAINT CKPseudoPartition_201204 CHECK<br>' +
        '(OrderDateKey >= 20120401 and OrderDateKey <= 20120431)<br>' +
        ')<br>' +
        'ON MDWT_FG5; -- The same filegroup as the April partition<br>' +
        'Pseudo-Partition Characteristics<br>' +
        'The pseudo-partition table must be defined on the<br>' +
        'same filegroup as the partition it’s destined to<br>' +
        'replace. At the point where you switch the<br>' +
        'partitions, everything about the pseudo-partition<br>' +
        'table must be exactly the same as the target<br>' +
        'partitioned table, with one exception. You must<br>' +
        'define a check constraint for the partition key<br>' +
        '(OrderDateKey in our example), to ensure that the<br>' +
        '318<br>' +
        'pseudo-partition table contains only data<br>' +
        'appropriate for the partition.<br>' +
        'If you have indexes on keys other than the<br>' +
        'partitioning key, you must INCLUDE the<br>' +
        'partitioning key in the indexes of the<br>' +
        'pseudo-partition. SQL Server automatically adds<br>' +
        'the partitioning key as an INCLUDE column to any<br>' +
        'partitioned index that doesn’t already have it<br>' +
        'included, but it’s better to do it in advance so the<br>' +
        'partition switch can be very fast.<br>' +
        'See the Books Online topic “Index with Included<br>' +
        'Columns.”<br>' +
        'Use standard fast load techniques to load the data into that<br>' +
        'pseudo-partition:<br>' +
        '• Set the database to Bulk-Logged or Simple recovery mode if<br>' +
        'it’s not already.<br>' +
        '• Confirm the pseudo-partition is empty and/or disable all<br>' +
        'indexes and constraints.<br>' +
        '• Bulk Insert or BCP from a file, or develop an Integration<br>' +
        'Services package using a SQL Server Destination task in the<br>' +
        'data flow.<br>' +
        '• Enable (or create) indexes and constraints.<br>' +
        '• Return the database to the desired recovery mode.<br>' +
        '• Perform appropriate backups.<br>' +
        'Once the data is loaded into the pseudo-partition, the<br>' +
        'indexes rebuilt, and the constraints re-enabled, switch the<br>' +
        'pseudo-partition into the partitioned fact table. This switch<br>' +
        '319<br>' +
        'is a metadata operation and executes very quickly,<br>' +
        'although it requires a schema-modification lock and can be<br>' +
        'blocked while other DML locks complete.<br>' +
        '-- The magic switch – very fast even with large data volumes<br>' +
        'ALTER TABLE PseudoPartition_201204 SWITCH TO FactOrders<br>' +
        'PARTITION 5;<br>' +
        'FactOrders now contains data through April, 2012, and<br>' +
        'the table PseudoPartition_201204 is empty. When we<br>' +
        'executed the ALTER TABLE … SWITCH TO command, no<br>' +
        'data actually moved. Instead, the system’s metadata<br>' +
        'logically swapped the places of the empty partition and the<br>' +
        'populated pseudo-partition. This is why they have to be<br>' +
        'structured identically and located on the same filegroup.<br>' +
        'NOTE<br>' +
        'For populating the initial historical data,<br>' +
        'it’s usually fastest to create all indexes after<br>' +
        'the entire partitioned table is populated and<br>' +
        'stitched together. SQL Server will build the<br>' +
        'indexes in parallel.<br>' +
        'This partition switching technique minimizes overall load<br>' +
        'time and system resources because you can perform a fast,<br>' +
        'non-logged load of large volumes of data. At the same<br>' +
        'time, you’re minimizing the impact on the database’s<br>' +
        'users. The data is loaded into a table that’s invisible to<br>' +
        'users; during that load the partitioned table remains<br>' +
        'available for query, and the switch step executes extremely<br>' +
        '320<br>' +
        'fast. In addition, the pseudo partition can be backed up as a<br>' +
        'separate table, improving system manageability.<br>' +
        'The Broader Parallel Structure Design Pattern<br>' +
        'Partition switching is an example of using parallel<br>' +
        'structures to enable high availability. You can use<br>' +
        'the same basic technique at the table, database,<br>' +
        'disk subsystem, or virtual machine level. At the<br>' +
        'table level, for example, you can load incremental<br>' +
        'data into Fact_Sales_Load, manage its indexes,<br>' +
        'indexed views, and any other dependencies, and<br>' +
        'then, when all is ready, change the table names like<br>' +
        'so:<br>' +
        'EXEC sp_rename ‘Fact_Sales’,<br>' +
        '‘Fact_Sales_Temp’;<br>' +
        'EXEC sp_rename ‘Fact_Sale_Load’,<br>' +
        '‘Fact_Sales’;<br>' +
        'EXEC sp_rename ‘Fact_Sales_Temp’,<br>' +
        '‘Fact_Sale_Load’;<br>' +
        'Users would be able to query Fact_Sales while<br>' +
        'Fact_Sales_Load is being loaded without<br>' +
        'blocking the load process.<br>' +
        'The first step of the load process would be to bring<br>' +
        'Fact_Sales_Load up to date. The easiest way to<br>' +
        '321<br>' +
        'do this might be to insert the last two incremental<br>' +
        'loads because Fact_Sales_Load will start out<br>' +
        'two days behind.<br>' +
        'All you need to provide this high availability is<br>' +
        'enough disk space to copy your tables, and enough<br>' +
        'CPU and memory to process user queries and data<br>' +
        'loads at the same time.<br>' +
        'Using Mixed Grain Partitions<br>' +
        'Most systems with partitioned fact tables will partition<br>' +
        'monthly by date. Most will implement the fast load and<br>' +
        'partition switching technique for the historical load, but<br>' +
        'not bother to do so for the daily incremental loads. If your<br>' +
        'DW/BI system has extreme data volumes, on the order of<br>' +
        '10 million new fact rows a day or more, you may need to<br>' +
        'play this game on your daily loads.<br>' +
        'If you’re in this situation, the easiest thing to do is to<br>' +
        'partition by day instead of by month. However, you want<br>' +
        'to keep the total number of partitions for any one table to<br>' +
        'several hundred. There’s a default limit of 1,000 partitions<br>' +
        'per table.<br>' +
        'Clearly daily partitioning is in conflict with the 1,000<br>' +
        'partition limit; keeping even one year of data, or 365 daily<br>' +
        'partitions, is a bit worrisome. If you need daily partitions<br>' +
        'to support the load process, you should consolidate the<br>' +
        'daily partitions into weekly or monthly partitions as they<br>' +
        'age. Obviously you want to do this the fast way, using<br>' +
        '322<br>' +
        'partition switching, rather than by merging populated<br>' +
        'partitions. The recommended approach is to build a<br>' +
        'monthly merge Integration Services package that would<br>' +
        'automate the following steps:<br>' +
        '1. SELECT INTO the new monthly pseudo-partition<br>' +
        'from all the daily partitions (you can do this right from<br>' +
        'the partitioned table).<br>' +
        '2. Switch the empty daily partitions into the partitioned<br>' +
        'table.<br>' +
        '3. Change the partition function and partition scheme so<br>' +
        'the breakpoints fall on the new boundaries — a whole<br>' +
        'month instead of days for last month, and individual<br>' +
        'days for the next month.<br>' +
        '4. Switch the merged month pseudo-partition into the<br>' +
        'partitioned table.<br>' +
        '5. Clean up the leftover daily pseudo-partitions that are<br>' +
        'still lying around.<br>' +
        'Have fun.<br>' +
        'Dropping Old Data from a Partitioned Table<br>' +
        'One of the great advantages of a partitioned table is that it<br>' +
        'makes it so easy — and fast — to drop aged data. It’s a<br>' +
        'common practice to keep a rolling window of data in the<br>' +
        'fact table, usually in multiples of a year plus one month<br>' +
        '(for instance, 61 months). Without table partitioning,<br>' +
        '323<br>' +
        'dropping the oldest month of data requires a resource<br>' +
        'intensive DELETE FROM statement.<br>' +
        'The best way to drop an old partition is to create an empty<br>' +
        'pseudo-partition, and swap it into the partitioned table for<br>' +
        'the old partition. As before, the pseudo-partition table must<br>' +
        'be structured identically to the partition it replaces,<br>' +
        'although in this case you don’t need to add a check<br>' +
        'constraint on the partition key.<br>' +
        'After you execute the ALTER TABLE … SWITCH<br>' +
        'PARTITION command, the pseudo-partition will contain<br>' +
        'the aged data and can be backed up, truncated, and<br>' +
        'dropped from the database if you wish.<br>' +
        'Using Partitioned Tables in the Data Warehouse Database<br>' +
        'If you’ve read this whole section on partitioned tables,<br>' +
        'you’ve already figured out that partitioned tables are too<br>' +
        'complicated to use unless you really need them. So when<br>' +
        'do you need them? As we said earlier, 100 million rows is<br>' +
        'a reasonable point. If you have a fact table that contains a<br>' +
        'billion rows, you certainly want it to be partitioned. You<br>' +
        'should seriously consider partitioning for smaller fact<br>' +
        'tables if you can’t find a better way to reduce your backup<br>' +
        'window or load window to an acceptable target. At the low<br>' +
        'end, it’s hard to imagine that a 10 million row fact table<br>' +
        'would truly need to be partitioned.<br>' +
        'If you decide to partition your fact table, you must<br>' +
        'automate the process of loading and managing the table<br>' +
        'and partitions. Your ETL system should be<br>' +
        '324<br>' +
        'partition-aware, and automatically add new partitions as<br>' +
        'needed to accommodate new data.<br>' +
        'DOWNLOADS<br>' +
        'Books Online provides examples of best<br>' +
        'practices for how to manage partitions.<br>' +
        'You can also download a utility from<br>' +
        'Microsoft’s Codeplex site that automates<br>' +
        'the creation of new partitions and the roll<br>' +
        'out of aged partitions at<br>' +
        'http://sqlpartitionmgmt.codeplex.com/.<br>' +
        'Partitioned Table Limitations<br>' +
        'We really like the partitioned table feature. But<br>' +
        'there are some limitations that it’s important to<br>' +
        'know about.<br>' +
        '• There’s a limit of 1,000 partitions per table in most<br>' +
        'SQL Server editions. (SQL Server 2008 SP2 can<br>' +
        'support 15,000 partitions, but this is not part of SQL<br>' +
        'Server 2008 R2, and is expected to be a Data Center<br>' +
        'Edition feature in the future.)<br>' +
        '• Partitioned tables can have indexed views defined on<br>' +
        'them, but they add a layer of management overhead<br>' +
        'when you use partition switching to load data.<br>' +
        '• Either the target or source partition/table needs to be<br>' +
        'empty to use the partition switch technique.<br>' +
        '• Issue the ALTER PARTITION FUNCTION …<br>' +
        'MERGE RANGE and SPLIT RANGE commands<br>' +
        'only against empty partitions. SQL Server will let<br>' +
        '325<br>' +
        'you issue the command against a populated partition,<br>' +
        'and it will move the data around, but it will do it very<br>' +
        'slowly. If you have enough data that you’re using<br>' +
        'partitions, you need to move the data yourself, most<br>' +
        'likely with an Integration Services package.<br>' +
        '• For simplicity, the examples in this section did not<br>' +
        'include indexes on the partitioned table. Indexes can<br>' +
        '— and should — be partitioned!<br>' +
        'RESOURCES<br>' +
        'There are two useful white papers on<br>' +
        'TechNet.Microsoft.com with additional<br>' +
        'information on partitioned tables: The<br>' +
        '“Fast Track Data Warehouse 2.0<br>' +
        'Architecture” paper previously mentioned,<br>' +
        'and “Partitioned Table and Index Strategies<br>' +
        'Using SQL Server 2008”.<br>' +
        'Finishing Up<br>' +
        'With all the tables, indexes, aggregates, storage, partitions,<br>' +
        'and related items created, there are still a few more items<br>' +
        'to get in place before the initial physical setup is complete.<br>' +
        'You need to set up the staging tables that will be used in<br>' +
        'the ETL system, and you need to set up your business and<br>' +
        'process metadata structures.<br>' +
        'Staging Tables<br>' +
        'One of the last steps in your physical design process is to<br>' +
        'develop staging tables. Staging tables are relational tables<br>' +
        '326<br>' +
        'that are used to hold data during the ETL process. Use<br>' +
        'staging tables for data that you want to use as a lookup for<br>' +
        'other processing. You may create a staging table based on<br>' +
        'each dimension that contains today’s version of the key<br>' +
        'lookups, for use during the fact table surrogate key<br>' +
        'assignment process. You may create a staging table to tie<br>' +
        'together similar members such as people from multiple<br>' +
        'source systems.<br>' +
        'Because staging tables are intimately tied to the ETL<br>' +
        'process, the ETL developer is usually the person who<br>' +
        'specifies the requisite structure. A defining characteristic<br>' +
        'of a staging table is that it is not available to business users<br>' +
        'for querying. Best practice puts staging tables in a separate<br>' +
        'database from the relational data warehouse, though<br>' +
        'usually on the same server.<br>' +
        'During your staging area design process, it’s particularly<br>' +
        'important to stage in relational tables any data that is used<br>' +
        'for lookups, and to index it appropriately to speed those<br>' +
        'lookups. You can use the file system, and especially the<br>' +
        'Integration Services raw file format, to stage data that’s<br>' +
        'not used for lookups.<br>' +
        'Metadata Setup<br>' +
        'At this point in the process, when you are setting up your<br>' +
        'other databases and file system, you should also define<br>' +
        'your metadata database. We have an entire chapter devoted<br>' +
        'to metadata, in which we suggest some structures that<br>' +
        'integrate with the way we like to build Microsoft DW/BI<br>' +
        'systems. See Chapter 15 for details.<br>' +
        '327<br>' +
        'Aside from the data model for metadata that you define,<br>' +
        'the other issue with metadata is where it is stored. We like<br>' +
        'to store user-defined metadata in its own relational<br>' +
        'database. Usually this database is on the same server as the<br>' +
        'data warehouse database. The reason you want it in a<br>' +
        'separate database is that the metadata database is more<br>' +
        'transactional than the data warehouse or staging databases.<br>' +
        'You should back it up frequently — which is easy to do<br>' +
        'because it’s small.<br>' +
        'Summary<br>' +
        'This chapter discussed setup and design issues for<br>' +
        'physically instantiating the target dimensional model in the<br>' +
        'relational data warehouse database. We described how to<br>' +
        'convert the logical data model into a physical data model,<br>' +
        'including surrogate keys, string data types, and NULLs.<br>' +
        'We also discussed the initial index recommendations,<br>' +
        'statistics, creating aggregates with indexed views or<br>' +
        'separate summary tables, data compression, and primary<br>' +
        'key/foreign key enforcement. We described relational table<br>' +
        'partitioning, which can greatly improve the manageability<br>' +
        'of large fact tables. We finished up with a brief mention of<br>' +
        'staging tables and metadata structures.<br>' +
        'Once you have completed the initial database setup and<br>' +
        'table creation, you should be ready to start the ETL<br>' +
        'process.<br>' +
        '328<br>';
    document.getElementById('chapter4').innerHTML = 'Chapter 4<br>' +
        'System Setup<br>' +
        'Let’s get physical.<br>' +
        'Up to this point, we’ve been talking about project<br>' +
        'management, business requirements, logical data models,<br>' +
        'and system architectures. Until now, you haven’t needed<br>' +
        'any technology more sophisticated than a laptop with<br>' +
        'Office. That changes in this chapter, as we discuss issues<br>' +
        'surrounding the setup of your development, test, and<br>' +
        'production systems, and get you ready to start the<br>' +
        'development process.<br>' +
        'As you can see in Figure 4-1, the product installation and<br>' +
        'setup issues addressed in this chapter come toward the end<br>' +
        'of the technology track of the Kimball Lifecycle, once<br>' +
        'you’ve created your system architecture, and selected any<br>' +
        'additional products you might need, and worked through<br>' +
        'the dimensional model design.<br>' +
        'We begin by helping you get a handle on the size of your<br>' +
        'business intelligence system, so you can make decisions<br>' +
        'about its basic physical configuration. Will you install all<br>' +
        'the server software components for your DW/BI system on<br>' +
        'a single machine or several? Will you use clustering or<br>' +
        'web farms? Do you need to budget for server hardware or<br>' +
        'expensive storage networks? We can’t answer these<br>' +
        'questions for you, but we’ve provided some guidance that<br>' +
        'should help you answer them for yourself.<br>' +
        '215<br>' +
        'The decisions you make about your production hardware<br>' +
        'and software configuration should be reflected, as much as<br>' +
        'economically feasible, in your test or quality assurance<br>' +
        'system. It may seem wasteful to spend money on test<br>' +
        'systems, but if you’re serious about delivering<br>' +
        'good-quality service to your existing business users, you<br>' +
        'need to be serious about testing before you roll system<br>' +
        'changes into production.<br>' +
        'Figure 4-1: System setup in the Kimball Lifecycle<br>' +
        'We suspect many readers will be tempted to skip this<br>' +
        'chapter. At least one person on your project team should<br>' +
        'read it, even if you have a separate server management<br>' +
        'group who will do all the work for you.<br>' +
        'In this chapter, we address the following specific<br>' +
        'questions:<br>' +
        '• Early in the design process, how can you determine how<br>' +
        'large your DW/BI system will be? What are the usage<br>' +
        '216<br>' +
        'factors that will push you to a larger and more complex<br>' +
        'configuration?<br>' +
        '• How should you configure your system? How much memory<br>' +
        'do you need, how many servers, what kind of storage and<br>' +
        'processors?<br>' +
        '• How do you install the SQL Server software on the<br>' +
        'development, test, and production servers? What do different<br>' +
        'members of the DW/BI team need to install on their<br>' +
        'workstations?<br>' +
        'System Sizing Considerations<br>' +
        'We’re sure that all readers of this chapter are hoping for a<br>' +
        'simple chart that will specify what kind of server machine<br>' +
        'they should buy. Sorry, it’s not going to happen: The<br>' +
        'problem is too difficult to reduce to a simple matrix or<br>' +
        'tool. The best we can do is describe the different options<br>' +
        'and parameters, and the kinds of operations that require<br>' +
        'bigger and more expensive hardware.<br>' +
        'There are four main factors that will push your project to<br>' +
        'more expensive hardware: data volumes, usage<br>' +
        'complexity, number of simultaneous users, and system<br>' +
        'availability requirements. These factors are illustrated in<br>' +
        'Figure 4-2 and discussed in the following sections.<br>' +
        'Calculating Data Volumes<br>' +
        'The first and most obvious characteristic of your DW/BI<br>' +
        'system that will affect your hardware purchases is data<br>' +
        'volumes. By the time you’ve finished your logical model<br>' +
        'and initial data profiling, you should have enough<br>' +
        'information to estimate how big your DW/BI system is<br>' +
        'going to be. Later, during the database setup, your DBAs<br>' +
        'will calculate database sizes in great detail, but for now<br>' +
        '217<br>' +
        'you can just think about fact table row counts. Unless you<br>' +
        'have a monster dimension of 50–100 million rows,<br>' +
        'dimension sizes are insignificant.<br>' +
        'For starters, just figure out what order of magnitude<br>' +
        'number of rows you’ll have for your initial historical load<br>' +
        'of fact data. Multiply that number by 100 bytes, which is<br>' +
        'our generous rule-of-thumb for fact row sizes. You can get<br>' +
        'more precise if you like, but we like arithmetic that we can<br>' +
        'do on our fingers. One hundred million fact rows require<br>' +
        'about 10GB as stored in the relational database, measured<br>' +
        'as uncompressed atomic data only, no indexes. It might be<br>' +
        '7GB; it might be 12GB; it won’t be 100MB or 100GB.<br>' +
        'When you add relational indices and Analysis Services<br>' +
        'indices and MOLAP storage, multiply that base number by<br>' +
        '2 to 4. In the early stages, before we have a specific<br>' +
        'design, we use a factor of 3. The requirements for the ETL<br>' +
        'staging area could conceivably add another factor,<br>' +
        'although the staging area may be relatively small.<br>' +
        'Figure 4-2: DW/BI system sizing considerations and<br>' +
        'configuration options<br>' +
        '218<br>' +
        'The incremental daily or monthly load volume is<br>' +
        'important, too. From the incremental volumes, you can<br>' +
        'compute expected data volumes for each fact table one,<br>' +
        'three, and five years out, which will help you decide what<br>' +
        'class of storage system to buy today.<br>' +
        'Although we’ve talked in this section about counting all<br>' +
        'the fact rows in your system, not all fact tables are created<br>' +
        'equal. A single billion-row fact table is going to be more<br>' +
        'demanding of system resources than ten 100 million–row<br>' +
        'fact tables would be. We don’t have a scientific way to<br>' +
        'quantify this difference, but you should list the fact table<br>' +
        'sizes by fact table, in addition to an overall count.<br>' +
        'In Figure 4-2, a small system is characterized by less than<br>' +
        '500 million rows of fact data, or 50GB using our simple<br>' +
        'multiplier. A large system is more than 5 billion fact rows,<br>' +
        'into the terabyte range and above.<br>' +
        'Determining Usage Complexity<br>' +
        '219<br>' +
        'The next key factor in deciding how big your hardware<br>' +
        'needs to be is to consider how your business users are<br>' +
        'going to use the data. There are two main questions: how<br>' +
        'many users will be working simultaneously, and what will<br>' +
        'they be doing? The usage patterns of a business<br>' +
        'intelligence system are quite different from the familiar<br>' +
        'workload of a transaction system. Even the simplest DW/<br>' +
        'BI query — for example, a query browsing a dimension —<br>' +
        'is more complex than most transactional queries. And the<br>' +
        'most complex DW/BI query is several orders of magnitude<br>' +
        'more complex, and touches more data, than any<br>' +
        'operational transaction.<br>' +
        'You can’t expect to already have a great understanding of<br>' +
        'system usage during the design phase of the DW/BI<br>' +
        'system; however, you do need to think about different<br>' +
        'kinds of usage and the approximate volume of use in each<br>' +
        'category. The data model tests we described in Chapter 2<br>' +
        'are a good place to start. Your business requirements<br>' +
        'document should also contain information about the kinds<br>' +
        'of usage you will need to support.<br>' +
        'Simple or Controlled Access<br>' +
        'The more simple or predictable the users’ queries, the<br>' +
        'more simultaneous users can be supported on the same size<br>' +
        'system. Examples of simple use include:<br>' +
        '• Predefined queries and reports based on highly selective sets<br>' +
        'of relational or Analysis Services data: Because these<br>' +
        'queries are relatively simple and predefined, a relational<br>' +
        'system can easily be tuned to support them. On the other<br>' +
        'hand, it’s particularly hard to understand what’s simple and<br>' +
        'what’s challenging to Analysis Services OLAP databases.<br>' +
        '220<br>' +
        'Please refer to the related discussion in the following two<br>' +
        'subsections.<br>' +
        '• Reporting Services scheduled and cached reports: As<br>' +
        'described in Chapter 10, scheduled reports may be extremely<br>' +
        'complex. But because they are run and cached at night, the<br>' +
        'load placed by users of these reports during business hours is<br>' +
        'relatively light.<br>' +
        '• Data mining forecasting queries: Data mining represents<br>' +
        'another kind of predefined query. As we discuss later,<br>' +
        'training a data mining model is most definitely not simple<br>' +
        'access. But performing a forecasting query on new<br>' +
        'information about a customer is highly selective.<br>' +
        'Moderate Complexity<br>' +
        'Examples of moderately complex use include:<br>' +
        '• Predefined reports based on a broad set of relational data<br>' +
        'not supported by aggregates, such as specific constraints on<br>' +
        'all sales for the past year. On the positive side, the report is<br>' +
        'predefined and so it can be tuned. On the negative side, the<br>' +
        'underlying query touches a lot of data and so is expensive.<br>' +
        'Consider using Analysis Services as the report’s data source,<br>' +
        'scheduling and caching these reports in Reporting Services,<br>' +
        'or experimenting with aggregated indexed views.<br>' +
        '• Ad hoc query and analysis using Analysis Services, where<br>' +
        'the analysis does not need to look at a large portion of the<br>' +
        'atomic data. If a lot of business users are performing ad hoc<br>' +
        'queries, the odds are good that they’re hitting different parts<br>' +
        'of the OLAP database. In this case, the server’s data cache<br>' +
        'will be of limited use (unless you have a lot of memory).<br>' +
        'Contrast this moderately complex ad hoc use of Analysis<br>' +
        'Services with the highly complex situation described next.<br>' +
        'Highly Demanding Use<br>' +
        'Examples of highly complex use include:<br>' +
        '221<br>' +
        '• Ad hoc query and analysis using the relational data<br>' +
        'warehouse database: These queries typically join many<br>' +
        'tables and often access large volumes of data. The business<br>' +
        'users aren’t experts, so they may make mistakes. It’s not<br>' +
        'feasible for them to use query hints.<br>' +
        '• Advanced ad hoc query and analysis on Analysis Services:<br>' +
        'The analysis requires wide queries that access a large portion<br>' +
        'of the atomic data. There is a class of analytic problems that<br>' +
        'by definition must touch very detailed data. For example, a<br>' +
        'query that counts the unique values in a large set is<br>' +
        'unavoidably expensive because it must touch detailed rows.<br>' +
        'Similarly, a query that returns a median or top N percent<br>' +
        'must also touch many more rows than are handed back in the<br>' +
        'result set.<br>' +
        '• Training of a data mining model: As we discuss in Chapter<br>' +
        '13, creating the case sets for a data mining model often<br>' +
        'involves several full table scans of the fact tables.<br>' +
        'Most DW/BI systems will be used in all these ways. The<br>' +
        'mix on a typical DW/BI system will lean toward the<br>' +
        'simple end, with about 60 percent simple, 30 percent<br>' +
        'moderately complex, and 10 percent demanding usage, as<br>' +
        'illustrated in Figure 4-2. At the other end, a challenging<br>' +
        'usage profile has 35 percent simple, 40 percent moderate,<br>' +
        'and 25 percent demanding usage.<br>' +
        'Estimating Simultaneous Users<br>' +
        'The number of potential users of the DW/BI system<br>' +
        'provides only the roughest possible estimate of how many<br>' +
        'people are using the system at the same time. One analyst<br>' +
        'doing very complex work, or a manager opening a<br>' +
        'multi-report dashboard, can use as many resources as<br>' +
        'dozens of users who are accessing simple reports. It’s as<br>' +
        'important to know the system usage characteristics as it is<br>' +
        'to know how many people access the system.<br>' +
        '222<br>' +
        'If you currently have no DW/BI system in place, it will be<br>' +
        'difficult for you to forecast usage frequency and timing.<br>' +
        'Even our old standby recommendation, that you interview<br>' +
        'the business users, will be of little value. During the design<br>' +
        'and development phase, business users are not able to<br>' +
        'guess how much they’ll use the system, and during what<br>' +
        'times of day.<br>' +
        'A few broad patterns are easy to predict. There is usually a<br>' +
        'surge of demand in the morning, when people arrive at<br>' +
        'work and check the reports run on yesterday’s data. Other<br>' +
        'cycles may be based on obvious work patterns, like<br>' +
        'month-end, quarterly, or year-end financial reporting<br>' +
        'calendars. It’s a good idea to plan your system capacity to<br>' +
        'meet these peak times, even if your system is underutilized<br>' +
        'the rest of the year. These peaks are when people need the<br>' +
        'data most urgently and will be most frustrated with delays.<br>' +
        'Even with these patterns, however, remember that a DW/<br>' +
        'BI workload is quite different from the fairly constant<br>' +
        'stream associated with a transaction system. In most cases,<br>' +
        'a business user executes a query or report, and then<br>' +
        'examines and thinks about the information before<br>' +
        'executing a follow up query. You should be careful to<br>' +
        'incorporate this think time into your understanding of<br>' +
        'simultaneous usage. If you buy or develop a performance<br>' +
        'testing suite, make sure it uses randomly generated think<br>' +
        'times of between 30 seconds and several minutes.<br>' +
        'If you have a DW/BI system already in production, the<br>' +
        'current usage statistics will provide a more certain estimate<br>' +
        'of future use. But if the current DW/BI system performs<br>' +
        'poorly, expect increased use with a new higher<br>' +
        '223<br>' +
        'performance system than with the old. A lot of DW/BI<br>' +
        'queries and reports are somewhat optional for business<br>' +
        'users. If the current system is painful to use, they won’t<br>' +
        'use it; they’ll get the data some other way, or do without.<br>' +
        'A small system may have only a dozen simultaneous users<br>' +
        '— people who are issuing queries and reports at more or<br>' +
        'less the same time. A large system, by contrast, will have<br>' +
        'hundreds or even thousands.<br>' +
        'Assessing System Availability Requirements<br>' +
        'The final factor affecting system size and configuration is<br>' +
        'the business requirements for system availability. These<br>' +
        'requirements can range from an 8-hour load window<br>' +
        '(midnight to 8 a.m.) during which the DW/BI system can<br>' +
        'be offline, to the opposite extreme of providing system<br>' +
        'availability 24 hours a day, 7 days a week. If your business<br>' +
        'users require high availability, you may need to purchase a<br>' +
        'substantially larger and more complex system than you’d<br>' +
        'otherwise need.<br>' +
        'The stronger the business need for a high availability<br>' +
        'system, the more likely you will be to cluster some of the<br>' +
        'components, notably the relational database and Analysis<br>' +
        'Services database, and to set up a web farm for Reporting<br>' +
        'Services. You may also need parallel server resources<br>' +
        'allowing you to run ETL jobs against one version of the<br>' +
        'data while users query yesterday’s version at the same<br>' +
        'time. We’ll see one version of using parallel structures<br>' +
        'when we explore partition switching in Chapter 5.<br>' +
        'How Big Will It Be?<br>' +
        '224<br>' +
        'With a little thought and research, you should be able to<br>' +
        'determine where you fall on the spectrum in each of the<br>' +
        'four sizing factors in Figure 4-2. This leads to a general<br>' +
        'sense for the size of the system and your high level scaling<br>' +
        'options. A small system can work with a single server; in<br>' +
        'the mid-range, consider a single large server, or multiple<br>' +
        'good sized commodity servers; and at the high end, you<br>' +
        'may need to look at multiple large servers. We’ll look at<br>' +
        'how all these servers might be configured in the next<br>' +
        'section.<br>' +
        'System Configuration Considerations<br>' +
        'At this point, you might be wondering how you can<br>' +
        'possibly determine what configuration is the most<br>' +
        'appropriate for your workload. It helps to break the<br>' +
        'question down into the major system components and<br>' +
        'configuration options. These include memory, monolithic<br>' +
        'or distributed systems, storage, and high availability<br>' +
        'systems.<br>' +
        'Memory<br>' +
        'All of the SQL Server DW/BI components love physical<br>' +
        'memory. The relational database uses memory at query<br>' +
        'time to resolve the DW/BI style of query, and during ETL<br>' +
        'processing for index restructuring. Analysis Services uses<br>' +
        'memory for resolving queries and performing calculations,<br>' +
        'for caching result sets, and for managing user session<br>' +
        'information. During processing, Analysis Services uses<br>' +
        'memory to compute aggregations, data mining models, and<br>' +
        'any stored calculations. The whole point of Integration<br>' +
        'Services’ data flow pipeline is to avoid temporarily writing<br>' +
        '225<br>' +
        'data to disk during the ETL process. Depending on your<br>' +
        'package design, you may need several times as much<br>' +
        'memory as your largest incremental processing set.<br>' +
        'Reporting Services is probably the least memory-intensive<br>' +
        'of the four major components, but rendering large or<br>' +
        'complex reports will also place a strain on memory<br>' +
        'resources.<br>' +
        'Because all the DW/BI system components are memory<br>' +
        'intensive, the obvious solution is to buy hardware that<br>' +
        'supports a lot of memory. When we wrote this, you could<br>' +
        'purchase a two processor, four or six-core 64-bit server<br>' +
        'with 32GB of memory for around $5,000. A commodity<br>' +
        '64-bit eight or twelve-way machine like this can be an<br>' +
        'all-in-one server for smaller systems, and the basic<br>' +
        'workhorse system for more complex configurations. The<br>' +
        'largest and most complex DW/BI systems will need one or<br>' +
        'more high-performance systems sold directly by the major<br>' +
        'hardware vendors. As a general guide, we try to outfit our<br>' +
        'servers with at least four GB of memory per CPU core.<br>' +
        'More is better. Remember, SQL Server 2008 R2 Standard<br>' +
        'Edition is limited to four CPU sockets and 64 GB total, but<br>' +
        'Windows Server 2008 R2 Standard Edition is limited to 32<br>' +
        'GB.<br>' +
        'Monolithic or Distributed?<br>' +
        'There are two primary strategies for applying more<br>' +
        'horsepower to your DW/BI system: using fewer, larger<br>' +
        'machines or using more, smaller machines. The monolithic<br>' +
        'approach is also known as scaling up; the distributed<br>' +
        'approach is also known as scaling out.<br>' +
        '226<br>' +
        'For smaller systems, an all-in-one configuration is<br>' +
        'appealing: You’ll minimize operating system and SQL<br>' +
        'Server licensing costs, and one server is easiest to manage.<br>' +
        'If you need a larger system and decide to use the scale out<br>' +
        'approach with several distributed servers, the first way to<br>' +
        'do so is by putting one or more SQL Server components<br>' +
        'onto separate machines. This architecture is easier to<br>' +
        'manage than distributing the workload by splitting up the<br>' +
        'data sets and hosting them on separate servers, each with<br>' +
        'its own set of SQL Server components. You need a<br>' +
        'high-bandwidth network between the DW/BI system<br>' +
        'servers, and between the ETL server and the source<br>' +
        'systems, as significant volumes of data are shipped back<br>' +
        'and forth between the components.<br>' +
        'The all-in-one configuration illustrated in Figure 4-3 has<br>' +
        'all server components, including possibly SharePoint<br>' +
        'Server as a reporting portal, running on a single machine.<br>' +
        'Note that SharePoint is resource intensive, and will do<br>' +
        'better on its own server. Most users (clients) will access<br>' +
        'the DW/BI system by connecting to Reporting Services,<br>' +
        'either directly or through SharePoint. Some analytic<br>' +
        'business users will connect directly to Analysis Services or<br>' +
        'the relational database for ad hoc and complex analyses.<br>' +
        'Figure 4-4 illustrates a common step up from the all-in-one<br>' +
        'configuration by creating a reporting server. Consider this<br>' +
        'configuration if your business users make heavy use of<br>' +
        'standardized reports built primarily from Analysis<br>' +
        'Services, and your DW/BI system processing occurs at<br>' +
        'night when users are not on the system. In this<br>' +
        'configuration, the Reporting Services catalog database will<br>' +
        'probably work best on the SQL Server Data Store server,<br>' +
        '227<br>' +
        'although it could be placed on the reporting server. In the<br>' +
        'reporting server configuration, some business users access<br>' +
        'the SQL Server data store directly.<br>' +
        'REFERENCE<br>' +
        'The Microsoft SQL-CAT team has a white<br>' +
        'paper at SQLCAT.com called “Report<br>' +
        'Server Catalog Best Practices” that<br>' +
        'describes how to manage and tune the<br>' +
        'report server catalog in detail.<br>' +
        'Figure 4-3: All-in-one business intelligence system<br>' +
        'Figure 4-4: SQL Server data store and separate reporting<br>' +
        'server<br>' +
        '228<br>' +
        'If your system is allowed only a relatively short downtime,<br>' +
        'you should separate Analysis Services from the relational<br>' +
        'database, as pictured in Figure 4-5. In this configuration,<br>' +
        'the ETL process and relational database load will not<br>' +
        'compete with Analysis Services queries, most of which<br>' +
        'will use the data cache on the reporting and analysis<br>' +
        'server. The Reporting Services catalog, located on the data<br>' +
        'store server, will compete with the ETL process for<br>' +
        'resources, but this is almost certainly better than having it<br>' +
        'compete with the reporting and analysis services. This<br>' +
        'configuration is not appropriate for a very high availability<br>' +
        'operation, which requires the use of clusters as discussed<br>' +
        'later in this chapter.<br>' +
        'Figure 4-5: SQL Server data store and reporting and<br>' +
        'analysis server<br>' +
        '229<br>' +
        'As your system grows larger and places more demands on<br>' +
        'the hardware, you may end up at the logical end point of<br>' +
        'the scale out approach: with each SQL Server component<br>' +
        'on its own server. Each of the chapters about the different<br>' +
        'components discusses some of the issues around<br>' +
        'distributing the system. But in general there is no built-in<br>' +
        'assumption that any two of the components are co-located<br>' +
        'on a single physical server.<br>' +
        'You can even push the SQL Server architecture past the<br>' +
        'point of one server per component. Analysis Services can<br>' +
        'distribute the partitions of an OLAP database across<br>' +
        'multiple servers. Reporting Services can run on a web<br>' +
        'farm, which can greatly enhance scalability. Your network<br>' +
        'of Integration Services packages can also be distributed<br>' +
        'across multiple servers, although we expect only the most<br>' +
        'extreme ETL problems would need to use this architecture.<br>' +
        'After you’ve reached the limit of distributing the SQL<br>' +
        'Server components to multiple machines, you can think<br>' +
        'about partitioning along the lines of business process<br>' +
        'dimensional models. Usually, when a DW/BI system is<br>' +
        '230<br>' +
        'distributed throughout the organization, it’s for political<br>' +
        'rather than performance reasons. Only for really large<br>' +
        'systems is it technically necessary to partition the DW/BI<br>' +
        'system along dimensional model boundaries. Also, various<br>' +
        'components may employ varying approaches to achieve<br>' +
        'uptime guarantees. We discuss how to make a horizontally<br>' +
        'distributed system work, but we prefer and recommend a<br>' +
        'more centralized architecture.<br>' +
        'In the development environment, a common configuration<br>' +
        'is for developers to use a shared instance of the relational<br>' +
        'database and Analysis Services database on a two- or<br>' +
        'four-socket server. Some developers may also choose to<br>' +
        'run local instances of the database servers, in order to<br>' +
        'experiment in isolation from their colleagues.<br>' +
        'Server Architecture Options<br>' +
        'All of the server machines discussed thus far are<br>' +
        'known as symmetric multi-processing (SMP)<br>' +
        'machines. The CPUs on SMP machines generally<br>' +
        'have shared access to all the available system<br>' +
        'memory and disk. As the machine gets bigger, the<br>' +
        'system bus becomes a bottleneck as all these CPUs<br>' +
        'try to process different threads in parallel,<br>' +
        'accessing memory and disk through the same<br>' +
        'system bus. Larger machines use a non-uniform<br>' +
        'memory architecture (NUMA) to mitigate this by<br>' +
        'grouping subsets of memory and CPUs together on<br>' +
        'local busses, called nodes, and tying the nodes<br>' +
        '231<br>' +
        'together with a cross-node bus. If you are facing<br>' +
        'the prospect of a multi-terabyte data warehouse,<br>' +
        'you have another system architecture option called<br>' +
        'massively parallel processing (MPP). MPP systems<br>' +
        'use a network of smaller SMP nodes, each with its<br>' +
        'own memory and disk. Microsoft’s Parallel Data<br>' +
        'Warehouse product allows you to implement an<br>' +
        'MPP SQL Server data warehouse on commodity<br>' +
        'hardware from most of the major vendors.<br>' +
        'Storage System Considerations<br>' +
        'Reading and writing data to and from disk is by far the<br>' +
        'slowest point in the system. There are several issues to<br>' +
        'consider when it comes to data storage. Regardless of what<br>' +
        'kind of storage system you use, you must make sure the<br>' +
        'data pipeline from the CPUs down to the disk drives is<br>' +
        'properly balanced. You also need to make sure your<br>' +
        'storage system provides some level of fault tolerance to<br>' +
        'protect against data loss, and you need to determine<br>' +
        'whether you will use a storage area network, or attach<br>' +
        'storage directly to your server.<br>' +
        'Balancing the Data Pipeline<br>' +
        'Every data bit processed in the data warehouse has to<br>' +
        'travel from the source system through a CPU and out to a<br>' +
        'disk for long-term safekeeping. The data is then retrieved<br>' +
        'from disk, often many times, to answer user queries. There<br>' +
        'are several components along the way that may act as<br>' +
        'bottlenecks if they do not have sufficient capacity. Figure<br>' +
        '232<br>' +
        '4-6 shows the major components in this pipeline, any one<br>' +
        'of which could choke off your throughput if it does not<br>' +
        'have sufficient capacity.<br>' +
        'Figure 4-6: The data pipeline<br>' +
        'There are six major potential bottlenecks in Figure 4-6:<br>' +
        'CPUs, host bus adapters, the fibre channel switch ports,<br>' +
        'the storage subsystem ports, the array controllers, and the<br>' +
        'disk drives. This system is reasonably balanced, with the<br>' +
        '1.6 gigabytes per second maximum consumption rate of<br>' +
        'the CPUs being the overall limiting factor. A system like<br>' +
        'this should be able to scan a 1 billion row fact table in<br>' +
        'about 60 seconds.<br>' +
        'Microsoft has created reference hardware architectures<br>' +
        'with each of the major hardware vendors called SQL<br>' +
        '233<br>' +
        'Server Fast Track Data Warehouse architecture. The<br>' +
        'system architecture concentrates on balancing the data<br>' +
        'pipeline and storing data for sequential access, where disk<br>' +
        'drives perform best.<br>' +
        'REFERENCE<br>' +
        'You can find a detailed white paper<br>' +
        'describing the fast track architecture,<br>' +
        'including throughput calculations, in a<br>' +
        'document titled “Fast Track Data<br>' +
        'Warehouse 2.0 Architecture” on MSDN.<br>' +
        'This is a very helpful document if you are<br>' +
        'responsible for architecting your server<br>' +
        'environment.<br>' +
        'Disk Performance<br>' +
        'The disk drives are the slowest component in the DW/BI<br>' +
        'system; as much as 100 times slower than memory. Disk<br>' +
        'drive system designers have worked to overcome this<br>' +
        'problem for decades. One major technique has been to put<br>' +
        'memory in the disk drive and at the disk controller level.<br>' +
        'Recently requested data is cached in this memory in the<br>' +
        'hopes that it may be needed again soon. SQL Server uses<br>' +
        'system memory to do the same thing on a much larger<br>' +
        'scale, caching whole tables and results sets for future use.<br>' +
        'The basic disk strategy in the data warehouse is to use<br>' +
        'more, smaller disks rather than fewer large disks. When<br>' +
        'configured in an array with data spread across multiple<br>' +
        '234<br>' +
        'disks, several disks can be used to read or write a data set<br>' +
        'in parallel. This is the reason for the 12 disks in Figure 4-6.<br>' +
        'Silicon Storage Devices<br>' +
        'Silicon Storage Devices (SSDs) are basically<br>' +
        'non-volatile memory packaged to look like a disk<br>' +
        'drive. They perform much faster than standard hard<br>' +
        'disk drives for certain operations. In particular,<br>' +
        'random access reads can be an order of magnitude<br>' +
        'faster or more. However, they have some<br>' +
        'limitations in other areas, such as sequential writes,<br>' +
        'which might be a major part of the ETL process.<br>' +
        'They also have some technical limitations in terms<br>' +
        'of the number of times a particular cell can be<br>' +
        'written to (called the program-erase cycle). As<br>' +
        'these technologies improve, they will<br>' +
        'fundamentally change our disk subsystem<br>' +
        'strategies. Meanwhile, SSDs may be a relatively<br>' +
        'cheap and easy way to boost performance for<br>' +
        'certain parts of the DW/BI system. The Analysis<br>' +
        'Services database is an ideal candidate because of<br>' +
        'its heavy random access read patterns.<br>' +
        'Fault Tolerance and RAID<br>' +
        'Almost all DW/BI system servers use a Redundant Array<br>' +
        'of Independent Disks (RAID) storage infrastructure.<br>' +
        'RAID-1, also known as mirroring, makes a complete copy<br>' +
        'of the disk. RAID 1+0 (or RAID 10) is an array of at least<br>' +
        'two mirrored disk sets with the data striped across the<br>' +
        '235<br>' +
        'mirrors. RAID-1 and RAID-1+0 are used for replicating<br>' +
        'and sharing data among disks. They are the configurations<br>' +
        'of choice for performance-critical, fault-tolerant<br>' +
        'environments, but require 100 percent duplication of disks.<br>' +
        'RAID-1 has the same write performance as single disks,<br>' +
        'and twice the read performance. RAID-5’s read<br>' +
        'performance is good, but its write performance suffers in<br>' +
        'comparison to RAID-1. All of these RAID configurations,<br>' +
        'including RAID-5, are vulnerable to a kind of<br>' +
        'second-order disaster that occurs when a failed drive is<br>' +
        'being restored and simultaneously a disk read error occurs.<br>' +
        'In this case the entire data set is lost. An enhanced version<br>' +
        'of RAID-5, known as “RAID-6 with hot spare” avoids this<br>' +
        'situation.<br>' +
        'The RAID array needs to be managed, either by hardware<br>' +
        'or by software. A wide variety of hardware<br>' +
        'RAID-controlling technologies are available, including the<br>' +
        'Fibre Channel used by many SAN solutions. Use hardware<br>' +
        'to control the RAID. If you let the operating system<br>' +
        'control the RAID, those activities will compete with the<br>' +
        'operation and performance of the DW/BI system.<br>' +
        'Don’t skimp on the quality and quantity of the hardware<br>' +
        'controllers. To maximize performance you may need<br>' +
        'multiple controllers; work with your storage vendor to<br>' +
        'develop your requirements and specifications.<br>' +
        'Storage Area Networks<br>' +
        'The most flexible, although most expensive, approach for<br>' +
        'disk storage is to use a storage area network (SAN). A<br>' +
        'storage area network is defined as a set of interconnected<br>' +
        '236<br>' +
        'servers and devices such as disks and tapes, which are<br>' +
        'connected to a common high speed communication and<br>' +
        'data transfer infrastructure such as Fibre Channel. SANs<br>' +
        'allow multiple servers access to a shared pool of storage.<br>' +
        'The SAN management software coordinates security and<br>' +
        'access. Storage area networks are designed to be<br>' +
        'low-latency and high-bandwidth storage that is easier to<br>' +
        'manage than non-shared storage.<br>' +
        'A SAN environment provides the following benefits:<br>' +
        '• Centralization of storage into a single pool: Storage is<br>' +
        'dynamically assigned from the pool as and when it is<br>' +
        'required, without complex reconfiguring.<br>' +
        '• Simplified management infrastructure: Adding capacity,<br>' +
        'setting up RAID configurations, allocating data across<br>' +
        'multiple disks, and providing access to the shared storage to<br>' +
        'multiple machines can all be easily accomplished with the<br>' +
        'SAN’s management tools.<br>' +
        '• Data can be transferred at fibre channel speeds directly<br>' +
        'from device to device without server intervention: For<br>' +
        'example, data can be moved from a disk to a tape without<br>' +
        'first being read into the memory of a backup server.<br>' +
        '• The SAN can be physically implemented on a campus<br>' +
        'several kilometers in size: For example, backup staging<br>' +
        'copies of all data sets can be physically located in a separate<br>' +
        'building from the primary DW/BI servers, thereby<br>' +
        'supporting disaster recovery scenarios. The remote staging<br>' +
        'copies can be updated and refreshed at disk channel speeds<br>' +
        'over the SAN.<br>' +
        'These benefits are valuable to a DW/BI system<br>' +
        'environment. As the DW/BI system grows, it’s much<br>' +
        'easier to allocate storage as required. For very large<br>' +
        'systems, a direct copy of the database files at the SAN<br>' +
        'level is the most effective backup technique. Finally,<br>' +
        '237<br>' +
        'SANs play an important role in a DW/BI system with high<br>' +
        'availability requirements.<br>' +
        'NOTE<br>' +
        'Directly attached RAID disks can offer<br>' +
        'better performance at a much lower price<br>' +
        'than SANs, particularly for sequential I/O.<br>' +
        'However, the advantages of the SAN<br>' +
        'technology usually, although not always,<br>' +
        'outweigh the difference in performance for<br>' +
        'DW/BI applications.<br>' +
        'One common problem with SANs is they are often<br>' +
        'considered a shared resource. This means they are<br>' +
        'managed by a separate group and a single SAN often hosts<br>' +
        'multiple applications. If you use a SAN, insist on a<br>' +
        'dedicated SAN and make sure it is configured properly for<br>' +
        'data warehousing.<br>' +
        'Processors<br>' +
        'To make a gross generalization, DW/BI systems are more<br>' +
        'likely to be limited by memory or I/O than processing<br>' +
        'power. However, all of the SQL Server components<br>' +
        'individually are designed for parallelism, so systems will<br>' +
        'benefit — often significantly — from additional<br>' +
        'processors. It’s hard to imagine anyone taking the time to<br>' +
        'build a DW/BI system that wouldn’t at least benefit from a<br>' +
        'dual processor box, and most systems use four or more<br>' +
        'processors. Multi-core CPUs are an easy way to add<br>' +
        '238<br>' +
        'processing capacity without increasing your SQL Server<br>' +
        'license costs.<br>' +
        'The additional memory address space and processing<br>' +
        'capacity that comes from the 64-bit platform is mandatory<br>' +
        'for the DW/BI system. Given that Windows Server<br>' +
        'stopped offering 32-bit versions with Windows Server<br>' +
        '2008, the option for 32-bit servers is almost a thing of the<br>' +
        'past.<br>' +
        'Setting Up for High Availability<br>' +
        'What does high availability mean for a DW/BI system?<br>' +
        'The answer, as you might expect, lies with the business<br>' +
        'users’ requirements. The job of the DW/BI team is to<br>' +
        'gather requirements, evaluate and price technical options,<br>' +
        'and present the business sponsor with a recommendation<br>' +
        'and cost justification.<br>' +
        'Because the DW/BI system is most often focused on<br>' +
        'longer term decisions and inquiries, the business users may<br>' +
        'be satisfied with access to the system from approximately<br>' +
        '7 or 8 a.m. to 10 p.m. local time. A DW/BI system that’s<br>' +
        'used primarily for strategic and non-operational decision<br>' +
        'making can tolerate an occasional downtime during the<br>' +
        'day. As a result, relatively few DW/BI teams deliver 24x7<br>' +
        'availability for the entire DW/BI system.<br>' +
        'Very high availability for the entire system is most<br>' +
        'common for multinational companies with business users<br>' +
        'spread throughout the globe. Even so, extremely high<br>' +
        'availability is seldom a mandate. If you add real-time data<br>' +
        'to your DW/BI system, you’ll need to design more<br>' +
        '239<br>' +
        'rigorously for high availability. Typically, although not<br>' +
        'always, real time affects a relatively small subset of the<br>' +
        'data, and the high availability requirements might not<br>' +
        'affect the bulk of data and operations.<br>' +
        'It may be necessary to cluster your Analysis Services and<br>' +
        'relational database servers in order to deliver the highest<br>' +
        'availability. A properly configured cluster can provide<br>' +
        'failover in the event of an emergency. A cluster can also<br>' +
        'provide scalability under normal circumstances. You can<br>' +
        'run Reporting Services in a web farm configuration, which<br>' +
        'provides similar advantages for both scalability and<br>' +
        'availability for that portion of your DW/BI system. SQL<br>' +
        'Server Books Online provides clear instructions for<br>' +
        'clustering databases and installing Reporting Services on a<br>' +
        'web farm. But if high availability is mission critical, you<br>' +
        'should contact your Microsoft sales team for references to<br>' +
        'highly qualified consultants who can help with the system<br>' +
        'design and configuration.<br>' +
        'RESOURCES<br>' +
        'The following references help when scaling<br>' +
        'out the individual components across<br>' +
        'multiple servers:<br>' +
        '• SQL Server Books Online topic “How to: Configure a Report<br>' +
        'Server Scale-Out Deployment (Reporting Services<br>' +
        'Configuration).”<br>' +
        '• White paper titled “Scale-Out Querying for Analysis Services<br>' +
        'with Read-Only Databases” can be found at SQLCAT.com.<br>' +
        '240<br>' +
        'For the vast gray area between needing 24x7 availability<br>' +
        'and having an 8-hour load window, there are a lot of things<br>' +
        'that you can do to minimize the system’s downtime. The<br>' +
        'easiest thing to do is to use Analysis Services as the<br>' +
        'primary or only presentation server. The heavy lifting of<br>' +
        'the ETL processing occurs in Integration Services and in<br>' +
        'the relational database. Once the data is cleaned and<br>' +
        'conformed, the process of performing an incremental<br>' +
        'OLAP database update is generally quite fast. And even<br>' +
        'better, Analysis Services performs updates into a shadow<br>' +
        'partition so the database remains open for querying during<br>' +
        'processing. You can’t expect query performance to remain<br>' +
        'at the same level while the database is being processed, but<br>' +
        'in most cases you can schedule the update at a time when<br>' +
        'the system is lightly used.<br>' +
        'Even if you use the relational data warehouse database to<br>' +
        'support queries and reports, you can minimize the amount<br>' +
        'of downtime. Dimension updates are seldom the problem;<br>' +
        'it’s the fact table inserts and especially updates, for<br>' +
        'example for a rolling snapshot fact table, which are most<br>' +
        'problematic. As we discuss in Chapter 5, you can use<br>' +
        'partitioned fact tables to load current data without<br>' +
        'affecting the availability of yesterday’s fact table. If your<br>' +
        'fact table is small and you’re not using partitioning, you<br>' +
        'can use a similar technique to perform inserts and updates<br>' +
        'on a copy of yesterday’s fact table, and then quickly switch<br>' +
        'it into production with an extremely short downtime. See<br>' +
        'the “Partitioned Tables” section in Chapter 5 for a more<br>' +
        'detailed description of this technique.<br>' +
        '241<br>' +
        'NOTE<br>' +
        'Partitioning for Analysis Services and<br>' +
        'relational databases are features of SQL<br>' +
        'Server Enterprise Edition. These features<br>' +
        'are not available in the Standard Edition.<br>' +
        'If you need very high availability for the relational data<br>' +
        'warehouse database, you may need to use the database<br>' +
        'snapshot feature. Users would query the snapshot while the<br>' +
        'underlying database is being loaded. This approach is most<br>' +
        'useful for those delivering 24-hour access to a global<br>' +
        'enterprise.<br>' +
        'Software Installation and Configuration<br>' +
        'The basic installation of the SQL Server and other<br>' +
        'Microsoft components is straightforward and well<br>' +
        'documented in Books Online. We won’t discuss the<br>' +
        'installation experience here. Instead, we’ll describe which<br>' +
        'pieces of software need to be installed on different<br>' +
        'developers’ workstations, depending on what part of the<br>' +
        'DW/BI project they’re working on. Next, we’ll describe<br>' +
        'some best practices for the initial configuration of the<br>' +
        'various SQL Server components, including the relational<br>' +
        'database, Analysis Services, Integration Services, and<br>' +
        'Reporting Services.<br>' +
        'Most multi-person development teams share one or two<br>' +
        'database servers and install only the development tools on<br>' +
        '242<br>' +
        'their personal machines. A common development team<br>' +
        'configuration is illustrated in Figure 4-7.<br>' +
        'Figure 4-7: A common development team configuration<br>' +
        'Depending on their roles, different members of the<br>' +
        'development team will need to install different<br>' +
        'components of SQL Server, and some will need to install<br>' +
        'other Microsoft and third-party software. These<br>' +
        'requirements are outlined in the next section.<br>' +
        'Development Environment Software Requirements<br>' +
        'The following section outlines the software to be installed<br>' +
        'on the development database server, and the workstations<br>' +
        'for different common development roles.<br>' +
        '243<br>' +
        'How Powerful Should Development and Test<br>' +
        'Systems Be?<br>' +
        'In a perfect world, the test system will be<br>' +
        'physically identical to the production system. The<br>' +
        'test system plays two key roles. First, it’s the<br>' +
        'system on which modifications are tested. In this<br>' +
        'first role, it’s as important to test the scripts that<br>' +
        'deploy the system changes as it is to test the<br>' +
        'changes themselves. For testing the deployment<br>' +
        'process, the test system doesn’t need to be identical<br>' +
        'to the production system. It’s not uncommon to use<br>' +
        'a virtual machine in this functional test role.<br>' +
        'The second major role of the test system is to serve<br>' +
        'as a place to experiment with performance<br>' +
        'optimizations, such as indexes and aggregates. For<br>' +
        'performance tests to be valid, the test system<br>' +
        'should have similar physical characteristics as<br>' +
        'production. Virtual machines are not yet as<br>' +
        'effective as performance test environments,<br>' +
        'although they are getting better. Many hardware<br>' +
        'vendors have Technology Centers, and may make<br>' +
        'those resources available to help validate system<br>' +
        'sizing prior to purchasing your production servers.<br>' +
        'Development often takes place on a subset of data.<br>' +
        'If so, the developers’ systems, including shared<br>' +
        'development database servers, can be much less<br>' +
        'powerful than the test and production servers.<br>' +
        '244<br>' +
        'Memory is important. Install at least 8GB of RAM<br>' +
        'on any computer that’s running one of the database<br>' +
        'services.<br>' +
        'The ETL system developer may need to work on<br>' +
        'the development server by way of a Remote<br>' +
        'Desktop session because Integration Services<br>' +
        'packages run in debugging mode on the machine<br>' +
        'where BI Studio is running. With significant data<br>' +
        'volumes, package execution can overwhelm a<br>' +
        'typical developer’s desktop.<br>' +
        'Finally, the screenshots in this book should<br>' +
        'convince you that team members who use BI<br>' +
        'Studio will need big monitors. The screenshots in<br>' +
        'this book were taken at 1024 × 768, and that’s<br>' +
        'really not big enough.<br>' +
        'Development Database Server<br>' +
        'As we described in the previous sidebar, most DW/BI<br>' +
        'teams share a database server for development purposes.<br>' +
        'The SQL Server components to install on the development<br>' +
        'database server are:<br>' +
        '• Relational engine<br>' +
        '• Integration Services<br>' +
        '• Analysis Services<br>' +
        '• Reporting Services<br>' +
        '• BI Studio for remote use by ETL developers<br>' +
        '245<br>' +
        'Most development occurs with small data volumes, so<br>' +
        'co-hosting all the server components on a single machine<br>' +
        'is usually fine from a technical point of view. If you have<br>' +
        'access to plenty of servers, distribute the components in<br>' +
        'the same way for development as is planned for<br>' +
        'production.<br>' +
        'Database Designer<br>' +
        'BI Development Studio (BIDS) is the main design tool for<br>' +
        'Analysis Services databases. The relational data warehouse<br>' +
        'database is primarily developed in Management Studio.<br>' +
        'Relational database designers may choose to install the<br>' +
        'relational database server on their local workstation. The<br>' +
        'SQL Server components to install on the database<br>' +
        'designer’s workstation are:<br>' +
        '• SQL Server Management Studio<br>' +
        '• BI Development Studio<br>' +
        '• Analysis Services<br>' +
        '• Relational database engine (optional)<br>' +
        'In addition, the database designer should install Visual<br>' +
        'Studio Team System, or any other source control product<br>' +
        'that can be integrated with Visual Studio.<br>' +
        'The relational database designer may want a<br>' +
        'data-modeling tool such as ERwin, ER/Studio, or<br>' +
        'PowerDesigner. These tools support visual modeling of the<br>' +
        'database, with excellent forward engineering capabilities.<br>' +
        'Development Database Administrator<br>' +
        '246<br>' +
        'Development databases are usually not managed very well<br>' +
        '— they are for development, after all. However, someone<br>' +
        'needs to perform some basic DBA tasks like ensuring the<br>' +
        'database has enough space. Backups are a really good idea,<br>' +
        'too. Management Studio is the tool for operating and<br>' +
        'maintaining databases (relational and OLAP), and<br>' +
        'managing the operation of Integration Services packages.<br>' +
        'The only SQL Server component required on the<br>' +
        'development DBA’s workstation is Management Studio.<br>' +
        'The development DBA should install and use the team’s<br>' +
        'source control system for the management of any database<br>' +
        'maintenance scripts.<br>' +
        'ETL System Developer<br>' +
        'The ETL system developer will create Integration Services<br>' +
        'packages. BIDS is the tool for developing and debugging<br>' +
        'Integration Services, and the Integration Services<br>' +
        'components also need to be installed on the developer’s<br>' +
        'workstation. The SQL Server components to install on the<br>' +
        'ETL developer’s workstation are:<br>' +
        '• SQL Server Management Studio<br>' +
        '• BI Development Studio<br>' +
        '• Integration Services<br>' +
        'In addition, the ETL system developer should install the<br>' +
        'source control client. Depending on how complex and<br>' +
        'unusual your ETL problems are, the ETL system developer<br>' +
        'may need to install the full Visual Studio product in order<br>' +
        'to develop custom objects in C# or VB. This is relatively<br>' +
        'unusual; the vast majority of ETL systems can be<br>' +
        '247<br>' +
        'developed without any need for custom coding in a Visual<br>' +
        'Studio .NET language.<br>' +
        'The ETL system developer often uses the development<br>' +
        'server to work on packages because packages run in debug<br>' +
        'mode on the same machine where BI Studio is running.<br>' +
        'Some teams start developing packages on the development<br>' +
        'server from the outset. Others may start on the developer’s<br>' +
        'workstation but move packages to the development server<br>' +
        'when it’s time to test with a reasonable volume of data.<br>' +
        'Still other teams find that it’s easiest to simply buy bigger<br>' +
        'workstations for the ETL developers.<br>' +
        'Report Designer<br>' +
        'The DW/BI team members who develop reports need the<br>' +
        'following software on their workstations:<br>' +
        '• BI Development Studio.<br>' +
        '• Source control software.<br>' +
        '• Microsoft Office, especially Excel with the PowerPivot for<br>' +
        'Excel add-in. Excel is often the final delivery platform for<br>' +
        'more complex reports and dashboards.<br>' +
        '• Optional: A non-Microsoft relational ad hoc query tool. It’s<br>' +
        'a matter of taste, but some folks prefer to use a third-party<br>' +
        'tool to formulate the query and then paste the SQL into the<br>' +
        'report designer.<br>' +
        '• Optional: A non-Microsoft Analysis Services query tool. As<br>' +
        'we have discussed elsewhere in this book, many<br>' +
        'Microsoft-based DW/BI systems use a third-party Analysis<br>' +
        'Services query tool to circumvent the limitations of the<br>' +
        'Office suite. The PowerPivot pivot table controls help<br>' +
        'address some of the limitations, but they can’t be used<br>' +
        'directly against standard relational or Analysis Services<br>' +
        'sources.<br>' +
        '248<br>' +
        'Reporting Portal Developer<br>' +
        'As we discuss in Chapter 12, many DW/BI systems embed<br>' +
        'Reporting Services into a reporting portal built using<br>' +
        'SharePoint Server or some other portal software. The<br>' +
        'reporting portal developer needs the following software:<br>' +
        '• BI Development Studio<br>' +
        '• Source control software<br>' +
        '• Microsoft Office Excel and the PowerPivot for Excel add-in<br>' +
        '• Microsoft SharePoint Designer<br>' +
        '• Access to a server running Microsoft SharePoint Server with<br>' +
        'PowerPivot for SharePoint<br>' +
        'It would be unusual for the reporting portal developer to<br>' +
        'need Visual Studio .NET to implement functionality not<br>' +
        'included with SharePoint.<br>' +
        'Data Mining Model Developer<br>' +
        'As we discuss in Chapter 13, the first major step in<br>' +
        'building a data mining application is to develop and train<br>' +
        'data mining models. This is an activity that requires<br>' +
        'knowledge of statistics, of the business problems, and of<br>' +
        'the data, but it does not require actual coding skills. The<br>' +
        'software required includes:<br>' +
        '• BI Development Studio for developing the data mining<br>' +
        'models<br>' +
        '• Write access to Analysis Services<br>' +
        '• Relational and Analysis Services query tools for building the<br>' +
        'input data sets and investigating the data<br>' +
        '• Source control software<br>' +
        'Analytic Application Developer<br>' +
        '249<br>' +
        'Analytic applications embed domain expertise and best<br>' +
        'practices into a guided analytic activity. Although some<br>' +
        'analytic applications are just a collection of predefined<br>' +
        'reports, sometimes they’re more structured than that. The<br>' +
        'more structured the application, the more likely the<br>' +
        'developer is to write code, and perhaps to integrate the<br>' +
        'data mining model described previously with the<br>' +
        'operational systems. The analytic application developer<br>' +
        'needs the following software:<br>' +
        '• SQL Server client and developer tools. The application<br>' +
        'developers typically work with databases and data mining<br>' +
        'models that others have developed. You will need the SQL<br>' +
        'Server object models, which are installed by default with the<br>' +
        'client components.<br>' +
        '• Source control software.<br>' +
        '• Visual Studio, with one or more .NET languages such as C#<br>' +
        'or VB.<br>' +
        'Test and Production Software Requirements<br>' +
        'In a perfect world, your test and production machines will<br>' +
        'have the same physical configuration. We recognize that’s<br>' +
        'not always realistic, but it is realistic and mandatory that<br>' +
        'they have the same software configuration. They must<br>' +
        'have the same operating system and the same SQL Server<br>' +
        'components, with the same configurations and versions,<br>' +
        'including service packs.<br>' +
        'If you’re using your test system only for testing, and not<br>' +
        'for any production use, you can use Developer Edition on<br>' +
        'the test systems. This is extremely appealing for<br>' +
        'components that use Enterprise Edition in production, not<br>' +
        'only because Enterprise Edition is expensive but also<br>' +
        '250<br>' +
        'because the feature set of Developer Edition is the same as<br>' +
        'that of Enterprise Edition. But it’s problematic to use<br>' +
        'Developer Edition on test machines where the production<br>' +
        'machines are running Standard Edition. That’s because the<br>' +
        'Developer Edition feature set is richer, and you might not<br>' +
        'discover a dependency on an Enterprise Edition feature<br>' +
        'until too late. This issue is discussed at greater length in<br>' +
        'Chapter 16.<br>' +
        'If the test system is for functional testing only, it may be a<br>' +
        'virtual machine candidate. Make sure you have someone<br>' +
        'with extensive VM experience to set this up.<br>' +
        'Earlier in this chapter we discussed how a small system<br>' +
        'might build an all-in-one server, with all SQL Server DW/<br>' +
        'BI components on a single machine. A large system will<br>' +
        'require a larger server and/or distribute the components<br>' +
        'across multiple machines.<br>' +
        'Database Server<br>' +
        'The database servers, unsurprisingly, require the SQL<br>' +
        'Server components:<br>' +
        '• Relational database and/or<br>' +
        '• Integration Services and/or<br>' +
        '• Analysis Services and/or<br>' +
        '• Reporting Services<br>' +
        'To manage the system effectively, you should install the<br>' +
        'following Windows Server components:<br>' +
        '• Performance Monitor (also known as System Monitor) is<br>' +
        'installed automatically with Windows. It is the tool that<br>' +
        '251<br>' +
        'exposes the performance counters that are published by the<br>' +
        'database servers, and is an invaluable tool for monitoring<br>' +
        'system performance.<br>' +
        '• Microsoft Systems Center Operations Manager is a Systems<br>' +
        'Center product that provides system monitoring and<br>' +
        'management services. Large or complex systems should<br>' +
        'evaluate whether they should purchase Operations Manager<br>' +
        'or use alternative systems operations software.<br>' +
        'Database Administrator<br>' +
        'The DBAs for the test and production machines will use<br>' +
        'SQL Server Management Studio (Management Studio) to<br>' +
        'operate and maintain the DW/BI system, and manage the<br>' +
        'operation of Integration Services packages.<br>' +
        'Operating Systems<br>' +
        'SQL Server 2008 R2 requires a Windows Server operating<br>' +
        'system in production: Windows Server 2003 SP2, or later.<br>' +
        'In general, you will get more functionality from more<br>' +
        'current versions of Windows Server and from Enterprise<br>' +
        'versus Standard Edition. For example, the Hyper-V virtual<br>' +
        'machine role can dynamically allocate memory across<br>' +
        'virtual machines in Windows 2008 R2 SP1, and Windows<br>' +
        'Server 2008 R2 Standard Edition is limited to 32GB of<br>' +
        'memory, while Enterprise Edition can support up to 2TB.<br>' +
        'The SQL Server setup program will not let you install on a<br>' +
        'machine that is not configured appropriately.<br>' +
        'Server operating systems and software are usually installed<br>' +
        'on a RAID-1 array, for data redundancy and failover.<br>' +
        'SQL Server Relational Database Setup<br>' +
        '252<br>' +
        'Use the following guidelines when installing, configuring,<br>' +
        'and securing SQL Server.<br>' +
        'Install the relational database component of SQL Server<br>' +
        'Enterprise Edition or Standard Edition. You may choose to<br>' +
        'install the Management Studio tools on a server; often the<br>' +
        'server is managed remotely from a DBA’s workstation.<br>' +
        'Upgrade the instance to the latest service pack. You can<br>' +
        'install the relational database as either a default instance or<br>' +
        'a named instance. The only compelling reason we’ve come<br>' +
        'up with for installing multiple instances on a single server<br>' +
        'is to test multiple scenarios on a single machine.<br>' +
        'NOTE<br>' +
        'Multiple SQL Server components can be<br>' +
        'installed on the same machine. Most<br>' +
        'organizations use per-processor licensing<br>' +
        'for DW/BI systems rather than licensing<br>' +
        'based on the number of users. Your<br>' +
        'licensing cost is the same if you install and<br>' +
        'use all SQL Server components on a<br>' +
        'machine, or only one component such as<br>' +
        'the relational engine.<br>' +
        'The SQL Server relational database is resource intensive<br>' +
        'and you’d seldom choose to share the physical server that<br>' +
        'holds the data warehouse database with another data<br>' +
        'application. In production, you usually create the data<br>' +
        'warehouse databases in the default instance. In the<br>' +
        '253<br>' +
        'development and test environments, multiple named<br>' +
        'instances may be very useful.<br>' +
        'The SQL Server relational database is a Windows service<br>' +
        'just like any other Windows service. Use the Management<br>' +
        'Studio tool to start, stop, administer, and manage a SQL<br>' +
        'Server relational database instance. The SQL Server<br>' +
        'database engine service has no dependencies on any other<br>' +
        'component of SQL Server. If you already have other DW/<br>' +
        'BI products in place, you can choose to install and use the<br>' +
        'relational database as the only component from the SQL<br>' +
        'Server product suite.<br>' +
        'Security Options During Installation<br>' +
        'Install and run the SQL Server relational database using<br>' +
        'Windows Authentication mode only. The mixed<br>' +
        'authentication mode is inherently less secure and should be<br>' +
        'avoided if possible. During installation, you are asked to<br>' +
        'provide a Service Account to run each service. This gives<br>' +
        'you greater security control by granting each service<br>' +
        'account only the permissions required to do its job. On the<br>' +
        'other hand, it’s easier to manage if you use the same<br>' +
        'account for all the services. You can modify this choice<br>' +
        'later by using the SQL Server Configuration Manager<br>' +
        'utility.<br>' +
        'warning Use the SQL Server Configuration Manager<br>' +
        'utility to change the characteristics of the service account,<br>' +
        'rather than the Administrative Tools ⇒ Services tool from<br>' +
        'the Control Panel.<br>' +
        '254<br>' +
        'Restrict access to SQL Server data and log files to system<br>' +
        'administrators and the SQL Server and SQL Server Agent<br>' +
        'service accounts. The system databases are secured by<br>' +
        'default by the SQL Server setup program, but we still like<br>' +
        'to check.<br>' +
        'After you’ve installed the SQL Server database engine,<br>' +
        'you may need to run the Configuration Manager to turn on<br>' +
        'or enable some features and services. If, for example,<br>' +
        'you’d like to access the database from another computer,<br>' +
        'you need to go to the Network Configuration directory in<br>' +
        'the Configuration Manager and enable a protocol, such as<br>' +
        'TCP/IP.<br>' +
        'Security issues are discussed in detail in Chapter 14.<br>' +
        'Files, Filegroups, and RAID<br>' +
        'SQL Server data is stored in files, which can be grouped<br>' +
        'into filegroups at the database level. Each file in a<br>' +
        'filegroup is assigned a location on disk. Database objects<br>' +
        'are assigned to filegroups, so when data is written out to a<br>' +
        'table, for example, it will actually write to each file in the<br>' +
        'filegroup in proportion to the available space in the file.<br>' +
        'Filegroups make it easier to balance I/O across multiple<br>' +
        'disks from within SQL Server. If you place database files<br>' +
        'on RAID drives with striping, this I/O balancing is handled<br>' +
        'by the RAID controller.<br>' +
        'Smaller systems can keep their filegroups simple. Start by<br>' +
        'creating a single filegroup for data with a single file on the<br>' +
        'RAID array, and set it to be the default filegroup. If you<br>' +
        'have larger data volumes, you may consider creating a<br>' +
        '255<br>' +
        'filegroup and file for all dimensions and a filegroup and<br>' +
        'file for each fact table. A single file should be adequate for<br>' +
        'the staging database and the metadata database, unless<br>' +
        'your DBAs have strong feelings about an alternative<br>' +
        'configuration. Partitioned tables often have their own<br>' +
        'filegroups. You can mark the older filegroups as read-only<br>' +
        'to reduce your backup workload.<br>' +
        'We strongly recommend that all relational database files<br>' +
        'be placed on RAID arrays, either RAID-1+0 or RAID-5,<br>' +
        'preferably with hardware controllers rather than managed<br>' +
        'by the operating system. RAID-1+0 is significantly better<br>' +
        'than RAID-5, although predictably more expensive,<br>' +
        'because it’s faster both for writes and for recovery after a<br>' +
        'disk failure.<br>' +
        'The SQL Server system master, model, and msdb<br>' +
        'databases can be placed on the RAID-1+0 array that holds<br>' +
        'the operating system. These databases are typically very<br>' +
        'small, and there’s usually plenty of room for them on that<br>' +
        'array. Alternatively, place them on their own small<br>' +
        'RAID-1 array. A third alternative is to place them on the<br>' +
        'same RAID array as user databases. It is vital that these<br>' +
        'databases, especially master, be placed on a fault-tolerant<br>' +
        'array.<br>' +
        'The fourth system database, tempdb, could grow<br>' +
        'significantly as the data warehouse database is being used.<br>' +
        'You should pre-allocate tempdb to a large size to avoid<br>' +
        'auto-growth during query operations. Don’t place tempdb<br>' +
        'on the system RAID-1+0 array, which is usually small.<br>' +
        'Use at least one file per CPU for high-performance tempdb<br>' +
        'operations. Be sure to spread tempdb out over many drives<br>' +
        '256<br>' +
        'to maximize I/O performance. If the total number of drives<br>' +
        'on your system is limited, you can place tempdb on the<br>' +
        'same RAID array as the user databases, especially if you<br>' +
        'use RAID-1+0 for the user databases. However, if you set<br>' +
        'up the user databases on a RAID-5 array, you should<br>' +
        'consider separating out tempdb onto its own RAID-1+0<br>' +
        'array to minimize potential bottlenecks.<br>' +
        'You’ll likely have at least three user databases: the data<br>' +
        'warehouse database, a staging database, and a metadata<br>' +
        'database. Reporting Services has two separate databases,<br>' +
        'which in some circumstances can grow to be quite large.<br>' +
        'You can put all these databases together on a single<br>' +
        'RAID-1+0 or RAID-5 array. Because of the size of the<br>' +
        'data warehouse database and possibly the Reporting<br>' +
        'Services catalog, the size of this array will dwarf the other<br>' +
        'databases’ arrays.<br>' +
        'Assuming you’re using RAID, you could put all of the data<br>' +
        'warehouse’s data into a single file and let the RAID array<br>' +
        'distribute the file across multiple drives. But long before<br>' +
        'you reach 2TB, the maximum drive size that SQL Server<br>' +
        'can address, you’ll want to break the database into<br>' +
        'multiple files and filegroups in order to simplify<br>' +
        'management and backup.<br>' +
        'REFERENCE<br>' +
        'The file layout discussion in this section<br>' +
        'was directed primarily at small- to<br>' +
        '257<br>' +
        'medium-sized systems. If you have larger<br>' +
        'data volumes, in the terabyte range and<br>' +
        'above, you should work with your storage<br>' +
        'vendor to lay out your files and disks very<br>' +
        'carefully. Search for the “Fast Track Data<br>' +
        'Warehouse 2.0 Architecture” white paper<br>' +
        'on MSDN (http://msdn.microsoft.com/) for<br>' +
        'more information.<br>' +
        'Database Recovery Model<br>' +
        'SQL Server logs every change to the structures and<br>' +
        'contents of a database. These log files are used to return<br>' +
        'SQL Server to a consistent state in case of a system crash,<br>' +
        'and in conjunction with backups, to restore a database in<br>' +
        'case of a data loss event, such as the failure of a disk that<br>' +
        'was not part of a RAID array.<br>' +
        'SQL Server provides three recovery modes: Full, Bulk<br>' +
        'Logged, and Simple. All changes are still logged in all<br>' +
        'three modes; the differences lie in the level of detail<br>' +
        'captured, and how the log file is managed. In Simple<br>' +
        'recovery mode, transactions are cleared from the log file<br>' +
        'once they complete and have been committed to the<br>' +
        'database. This keeps the log file relatively small, and<br>' +
        'reduces the maintenance effort. However, a database that<br>' +
        'is operated with Simple recovery mode can be recovered<br>' +
        'only to the point of the last backup.<br>' +
        '258<br>' +
        'NOTE<br>' +
        'Even if you empty out the staging database<br>' +
        'at the beginning of each load, keep a copy<br>' +
        'of the extracted data somewhere. We often<br>' +
        'keep a copy of the extracts in the file<br>' +
        'system, in a file whose name includes the<br>' +
        'date and time of the extract. These extracts<br>' +
        'should be backed up, as should any<br>' +
        'permanent data in the staging database.<br>' +
        'More often than not, the kinds of backups<br>' +
        'you can do with the Simple recovery model<br>' +
        'meet the needs of backing up the staging<br>' +
        'database.<br>' +
        'The Full recovery model keeps every change in the log file<br>' +
        'since the last backup, which enables you to do a full<br>' +
        'recovery after a crash, hence the name. The problem with<br>' +
        'the Full recovery model is the log files can grow to large<br>' +
        'sizes, and need to be actively managed.<br>' +
        'The Bulk Logged recovery model logs individual<br>' +
        'transactions like the Full recovery model, but it treats bulk<br>' +
        'inserts like the Simple recovery model. This is useful for a<br>' +
        'database that supports both transactions and bulk loads.<br>' +
        'The log file is smaller because certain bulk loads are<br>' +
        'minimally logged, but non-bulk transactions are kept. Bulk<br>' +
        'Logged is meant as an adjunct to Full recovery mode and<br>' +
        'has the same log file management requirements. You<br>' +
        'would generally operate in Full recovery mode, switch to<br>' +
        '259<br>' +
        'Bulk Logged when you load a large dataset, and then<br>' +
        'switch back once the load is completed.<br>' +
        'In general, we don’t do individual transactions in the data<br>' +
        'warehouse. If you back up your database immediately after<br>' +
        'the nightly ETL process is finished, there should be few if<br>' +
        'any changes until the next ETL run.<br>' +
        'Given this, the Simple recovery model makes sense for<br>' +
        'most data warehouses. It requires less maintenance and<br>' +
        'supports bulk inserts when the conditions are met. You<br>' +
        'will probably set your staging and development databases<br>' +
        'to Simple recovery mode as well. Just make sure your<br>' +
        'DBAs are backing everything up on a regular basis, and<br>' +
        'that they test those backups and restores.<br>' +
        'There are other implications of the recovery model choice.<br>' +
        'One of the ways to speed up data loading is to invoke<br>' +
        'minimal logging in a bulk load. Minimal logging requires<br>' +
        'either the Simple or Bulk Logged recovery model. On an<br>' +
        'ongoing basis, many of our incremental loads don’t meet<br>' +
        'other conditions for minimal logging: we are not inserting<br>' +
        'into an empty table, or we have not dropped all the<br>' +
        'indexes. However, you may want to create the necessary<br>' +
        'conditions for a minimally logged load because it is so<br>' +
        'much faster. This is particularly true when you’re using<br>' +
        'partitioned tables or other parallel structures, as we discuss<br>' +
        'in Chapter 5.<br>' +
        'Initial Database Size<br>' +
        'Set up your SQL Server database with an initial size<br>' +
        'adequate to hold the initial historical load plus anticipated<br>' +
        '260<br>' +
        'growth for the next year or so. You might permit automatic<br>' +
        'growth up to a certain maximum to avoid a crisis, but you<br>' +
        'should monitor the database size carefully and manually<br>' +
        'increase the database’s file size during a period of slow<br>' +
        'usage. That’s because the initial allocation and subsequent<br>' +
        'increases are resource-intensive. It’s better to pay this file<br>' +
        'initialization price at a time managed by the DBAs than<br>' +
        'during the DW/BI system’s load processing window.<br>' +
        'Windows Server 2003 and higher versions include a<br>' +
        'feature called Instant File Initialization that improves the<br>' +
        'performance of database allocation. However, “instant” is<br>' +
        'a bit of a misnomer; it’s still a resource-intensive process.<br>' +
        'For best performance, especially for very large databases,<br>' +
        'don’t rely on auto-grow. Instead, set up an automated<br>' +
        'process to check for needed file space, programmatically<br>' +
        'increase file sizes if necessary, and set an alert well before<br>' +
        'you run out of space.<br>' +
        'As we discuss in Chapter 17, you should set up an<br>' +
        'automated process to check for disk space on a weekly,<br>' +
        'daily, or load-by-load basis.<br>' +
        'RESOURCES<br>' +
        'See the Books Online topic “Database File<br>' +
        'Initialization” for more information.<br>' +
        'After the database tables’ physical design has been<br>' +
        'finalized, as we discuss in Chapter 5, you can accurately<br>' +
        '261<br>' +
        'assess the storage requirements for the initial database<br>' +
        'setup and storage layout.<br>' +
        'Analysis Services Setup<br>' +
        'The Analysis Services server machine must have at least<br>' +
        'the Analysis Services component of SQL Server installed<br>' +
        'on it. Other components, including the relational database<br>' +
        'engine and the Studio tools, are not required on that server.<br>' +
        'Often, especially in locked down production environments,<br>' +
        'the server is always managed remotely from the DBA’s<br>' +
        'workstation. Make sure you keep up to date with service<br>' +
        'packs.<br>' +
        'You can run Analysis Services alone, if you wish, without<br>' +
        'using any other SQL Server technology because it keeps<br>' +
        'its metadata in XML files. It’s not unusual to run Analysis<br>' +
        'Services as an intermediate data platform between the<br>' +
        'users and a large non-Microsoft relational data warehouse<br>' +
        'that has not been optimized to support ad hoc queries.<br>' +
        'NOTE<br>' +
        'Remember that if you install the relational<br>' +
        'database on one machine and Analysis<br>' +
        'Services on a second machine, you have to<br>' +
        'pay for two licenses.<br>' +
        'Like the relational database, Analysis Services supports<br>' +
        'multiple instances on a single server machine. In a<br>' +
        'standard production environment, we don’t see a<br>' +
        '262<br>' +
        'compelling argument for using multiple instances rather<br>' +
        'than multiple databases within the same instance. Multiple<br>' +
        'instances may be useful during development and testing. If<br>' +
        'you’re building a solution for external parties such as<br>' +
        'vendors, you may find that multiple instances provide an<br>' +
        'extra level of security or comfort to your customers.<br>' +
        'Analysis Services File Locations and Storage<br>' +
        'Requirements<br>' +
        'The main configuration choice to make at or soon after<br>' +
        'installation time is where the program, data, and log files<br>' +
        'are located. These choices are made for an instance; all<br>' +
        'cubes and databases within that instance use the same<br>' +
        'default location. A RAID array, either RAID-1,<br>' +
        'RAID-1+0, or RAID-5, is the best choice for all file<br>' +
        'locations. The program files for a production system<br>' +
        'should be installed on a RAID-1 array, often the operating<br>' +
        'system array.<br>' +
        'The best place for the log files is their default location,<br>' +
        'near the SQL Server program files. We recommend using<br>' +
        'RAID-1 or RAID-1+0 for these files, as you would<br>' +
        'probably use for the program files.<br>' +
        'Analysis Services data files are by far the largest kind of<br>' +
        'files. It is nearly impossible at design time to estimate with<br>' +
        'any degree of accuracy how big the Analysis Services data<br>' +
        'files will be. Let’s start with a simple rule of thumb: an<br>' +
        'Analysis Services database that’s built at the same grain as<br>' +
        'a relational fact table will take approximately 25 percent of<br>' +
        'the space of that fact table (non-compressed atomic data<br>' +
        'only, no indexes). This 25 percent rule includes Analysis<br>' +
        '263<br>' +
        'Services data and indexes, before aggregations are added,<br>' +
        'and again it’s worth emphasizing that this is at the same<br>' +
        'grain as the fact table. We have seen Analysis Services<br>' +
        'atomic data at 15–40 percent of its corresponding<br>' +
        'relational data, but 25 percent is a reasonable midpoint.<br>' +
        'When you add well-designed aggregations, the total data<br>' +
        'size is usually 35–100 percent of the non-compressed<br>' +
        'relational data. You will be at the high end of that range if<br>' +
        'you use distinct count measures. In our experience,<br>' +
        'Analysis Services databases including indexes and<br>' +
        'aggregations typically take 35–50 percent of the data of the<br>' +
        'corresponding non-compressed relational table at the same<br>' +
        'grain, data only, no indexes. This factor was included in<br>' +
        'the very high-level storage space guesstimate that we<br>' +
        'discussed at the beginning of this chapter.<br>' +
        'For small- and medium-sized installations, a 50 percent<br>' +
        'factor should suffice for disk planning. For larger<br>' +
        'installations, you should partition your Analysis Services<br>' +
        'database. Build a test database with several partitions, and<br>' +
        'then scale that storage requirement by the number of<br>' +
        'partitions.<br>' +
        'We recommend that you use RAID-1+0 to store the data<br>' +
        'files. Use a SAN for large installations. To maximize<br>' +
        'processing speed, use a different RAID array, with a<br>' +
        'different physical controller, than the location of the<br>' +
        'relational data warehouse database that feeds the Analysis<br>' +
        'Services database.<br>' +
        'If you’re too cost conscious to use RAID-1+0 for the data<br>' +
        'files, but don’t want to take the write performance hit of<br>' +
        '264<br>' +
        'RAID-5, it’s not as important to use redundant storage for<br>' +
        'the Analysis Services database as for the relational<br>' +
        'database. After all, you can always reprocess the Analysis<br>' +
        'Services database from the relational data warehouse<br>' +
        'database, or restore it from backup. Be warned, however,<br>' +
        'that it could take many hours to process an Analysis<br>' +
        'Services database that covers multiple terabytes of<br>' +
        'relational data. We strongly recommend using some level<br>' +
        'of fault tolerant storage.<br>' +
        'When you create a partition for an Analysis Services cube,<br>' +
        'you can place that partition anywhere in your storage<br>' +
        'system. Assuming you’re using RAID, we see no<br>' +
        'compelling reason for placing data files anywhere but in<br>' +
        'the default location.<br>' +
        'Finally, you may want to avoid all this disk design work<br>' +
        'and just put the Analysis Services data on a silicon storage<br>' +
        'device (SSD). Ad hoc queries against an Analysis Services<br>' +
        'database are primarily random access reads; the sweet spot<br>' +
        'for SSDs. It may actually cost you less because you won’t<br>' +
        'need as many high speed hard disk drives in a large array,<br>' +
        'and your users will be amazed at how well the system<br>' +
        'performs.<br>' +
        'Analysis Services Metadata<br>' +
        'There is no formal repository for Analysis Services<br>' +
        'beginning with SQL Server 2005. Instead, the<br>' +
        'metadata consists of the XML files throughout the<br>' +
        '265<br>' +
        'OLAP Data directory, such as<br>' +
        'DatabaseName.db.xml, CubeName.cub.xml,<br>' +
        'DimensionName.dim.xml, and so on.<br>' +
        'The main implication for Analysis Services<br>' +
        'administrators is improved manageability,<br>' +
        'particularly for backups and restores, as described<br>' +
        'in Chapter 17.<br>' +
        'Analysis Services and Memory<br>' +
        'Analysis Services loves memory. Analysis Services was<br>' +
        'redesigned with SQL Server 2005 to solve the most<br>' +
        'intractable memory problems associated with prior<br>' +
        'versions. Nonetheless, the more data you can cache in<br>' +
        'physical memory, the happier you and Analysis Services<br>' +
        'will be.<br>' +
        'Analysis Services dimensions do not need to be memory<br>' +
        'resident. Information about dimension members will move<br>' +
        'in and out of memory cache as needed. This is great, for<br>' +
        'certainly a server should handle memory contention<br>' +
        'gracefully. Nonetheless, for excellent query performance<br>' +
        'you want plenty of memory for dimension members, a<br>' +
        'result set cache, the computation engine’s cache, and other<br>' +
        'uses. Don’t skimp on memory.<br>' +
        'Integration Services Setup<br>' +
        'Integration Services has two major components: a design<br>' +
        'environment, which is part of the BI Studio; and a runtime<br>' +
        '266<br>' +
        'environment, which is what you install on your production<br>' +
        'servers. The design environment is where you create and<br>' +
        'edit packages. You can see a visual representation of the<br>' +
        'package’s tasks, and run the package in debug mode on the<br>' +
        'development machine, and only on the development<br>' +
        'machine. The only way to remotely execute an Integration<br>' +
        'Services package in development/debugging mode is to<br>' +
        'use a remote desktop connection to the remote machine.<br>' +
        'On the production server, install the Integration Services<br>' +
        'component of SQL Server Enterprise Edition or Standard<br>' +
        'Edition. You may install the Management Studio tools on<br>' +
        'your production server, although often production<br>' +
        'instances of SQL Server are managed remotely from an<br>' +
        'administrator’s workstation.<br>' +
        'You can use Management Studio to interactively execute a<br>' +
        'package that’s been deployed to test or production. But for<br>' +
        'the ETL system, you will use SQL Agent to schedule the<br>' +
        'execution of the DTExecUI or DTExec utility. Using these<br>' +
        'utilities, you can run on one server a package that is stored<br>' +
        'on a second server. In production, Integration Services<br>' +
        'packages can be located anywhere, and can be run on any<br>' +
        'server that has the Integration Services runtime<br>' +
        'executables.<br>' +
        'You can design your ETL system to run multiple packages<br>' +
        'on multiple servers. If you choose this architecture for<br>' +
        'your large scale ETL problem, install Integration Services<br>' +
        'executables (and pay SQL Server licenses) on all of the<br>' +
        'servers on which the packages are running.<br>' +
        '267<br>' +
        'Integration Services has no dependency on any other<br>' +
        'component of the SQL Server product suite. It could even<br>' +
        'be used as the ETL tool for an otherwise non-SQL Server<br>' +
        'DW/BI system.<br>' +
        'Integration Services presents the option of storing package<br>' +
        'definitions in the SQL Server. This is not something you<br>' +
        'need to decide at installation time.<br>' +
        'As we described earlier in this chapter, most DW/BI<br>' +
        'systems will run Integration Services on the same server as<br>' +
        'the relational data warehouse database. It is easy to change<br>' +
        'package locations as your warehouse matures and your<br>' +
        'requirements change.<br>' +
        'Integration Services File Locations and Storage<br>' +
        'Requirements<br>' +
        'You may use a relational database to stage data during<br>' +
        'ETL processing. As we describe in Chapter 7, you will<br>' +
        'probably use both a relational staging area and a file<br>' +
        'system staging area. You may use the file system staging<br>' +
        'area to rest data after it has been extracted from the source<br>' +
        'systems but before the heavy duty ETL processing begins;<br>' +
        'you may also use this staging area for intermediate storage<br>' +
        'and for a kind of backup of changed data before launching<br>' +
        'an update. Many people hold on to the source system<br>' +
        'extracts for days, weeks, or months before deleting them or<br>' +
        'moving them to offline storage. The volume of disk space<br>' +
        'you’ll need for the file-based and relational staging areas<br>' +
        'depends completely on the design of your ETL system.<br>' +
        '268<br>' +
        'If you can extract the exact same data sets from the source<br>' +
        'system as needed, you may not need fault tolerant storage<br>' +
        'for the staging area. However, many source systems<br>' +
        'overwrite data and do not keep history. In these cases, the<br>' +
        'extract is the only record of the source system at that point<br>' +
        'in time. It’s best not to lose it. RAID-1 or RAID-1+0, as<br>' +
        'always, is recommended.<br>' +
        'Reporting Services Setup<br>' +
        'Like all the other components of SQL Server, Reporting<br>' +
        'Services can be installed on a standalone reporting server,<br>' +
        'or it can share a server with one or more other components<br>' +
        'of SQL Server. However, Reporting Services does require<br>' +
        'access to a SQL Server relational database to store its<br>' +
        'metadata, known as the report server catalog.<br>' +
        'When you install Reporting Services, you must supply<br>' +
        'several pieces of configuration information:<br>' +
        '• The location of the report server catalog database: The<br>' +
        'report server catalog is where report definitions, metadata,<br>' +
        'histories, and snapshots are stored. The installation program<br>' +
        'will create the report server database for you. The report<br>' +
        'server database can be located on a different machine, but<br>' +
        'your service account must have appropriate database<br>' +
        'creation privileges on that machine. This catalog database<br>' +
        'must be a SQL Server relational database.<br>' +
        '• Configuration options for email delivery of reports: You will<br>' +
        'probably want to run a subset of standard reports, and use<br>' +
        'email to deliver either the report or a link to the report. The<br>' +
        'email service account information and other configuration<br>' +
        'options are well documented in Books Online.<br>' +
        '269<br>' +
        'You may choose to install the client-side report authoring<br>' +
        'tool, Report Designer, on the server. Report Designer is<br>' +
        'integrated into BI Development Studio, and most<br>' +
        'developers use it on their workstations rather than on the<br>' +
        'server.<br>' +
        'Reporting Services is a Windows service just like the SQL<br>' +
        'Server relational database or any other Windows service. It<br>' +
        'is also implemented as an ASP.NET Web service based on<br>' +
        'http.sys. Both the Windows service and the Web service<br>' +
        'are implemented on the report server. Use the Reporting<br>' +
        'Services Configuration Manager tool to configure, start,<br>' +
        'stop, administer, and manage a Reporting Services<br>' +
        'instance.<br>' +
        'Reporting Services doesn’t require any significant file<br>' +
        'storage other than the report catalog database. Issues<br>' +
        'around the potential size and placement of the report server<br>' +
        'catalog were discussed earlier in this chapter, in the section<br>' +
        'on the SQL Server relational database setup.<br>' +
        'NOTE<br>' +
        'Like the other components of SQL Server,<br>' +
        'Reporting Services can be installed in<br>' +
        'isolation. However, it does need access to a<br>' +
        'SQL Server relational database server for<br>' +
        'the report catalog.<br>' +
        'Summary<br>' +
        '270<br>' +
        'We began this chapter by discussing various options for<br>' +
        'configuring your DW/BI system. System sizing is<br>' +
        'challenging because it depends on so many factors —<br>' +
        'some of which you won’t have much information on until<br>' +
        'your system is in production. The easy factors to predict<br>' +
        'are data volumes and system availability requirements. It’s<br>' +
        'harder to guess how many simultaneous users you’ll have,<br>' +
        'and how many of them will be performing challenging ad<br>' +
        'hoc queries. Nonetheless, we conclude that the vast<br>' +
        'majority of systems will use one or several commodity 2 to<br>' +
        '4 socket, multi-core 64-bit servers. High-end systems will<br>' +
        'use one or more 8 or 16 socket, or even larger 64-bit<br>' +
        'servers. The 64-bit architecture is necessary because it<br>' +
        'allows so much more addressable memory. All the BI<br>' +
        'software components love memory.<br>' +
        'We discussed storage architecture and the importance of<br>' +
        'creating a balanced data pipeline between the disks and<br>' +
        'CPUs. We also stressed the importance of using a fault<br>' +
        'tolerant disk subsystem to reduce the impact of the<br>' +
        'inevitable disk drive failures. These needs emphasize the<br>' +
        'value of a storage area network with some level of<br>' +
        'hardware-controlled redundant RAID.<br>' +
        'We briefly described the SQL Server software installation<br>' +
        'issues for the development, test, and production servers.<br>' +
        'We also touched on standard workstation configurations<br>' +
        'for key roles on the DW/BI team. General installation and<br>' +
        'setup issues are well documented in SQL Server Books<br>' +
        'Online. This chapter is long enough that it makes no sense<br>' +
        'to repeat that information here. Instead, we focused on<br>' +
        'issues that are specific to data warehousing.<br>' +
        '271<br>';
    document.getElementById('chapter3').innerHTML = 'Chapter 3<br>' +
        'The Toolset<br>' +
        '“But lo! Men have become the tools of their tools.”<br>' +
        '— Henry David Thoreau, Walden<br>' +
        'In this chapter, we describe the architecture and product<br>' +
        'selection for the Microsoft data warehouse/business<br>' +
        'intelligence (DW/BI) system. It may seem pointless to talk<br>' +
        'about architecture alternatives and product selection for a<br>' +
        'Microsoft system, but Microsoft offers enough software<br>' +
        'components that there’s a significant element of product<br>' +
        'selection.<br>' +
        'Figure 3-1 repeats the familiar Kimball Lifecycle diagram,<br>' +
        'highlighting the Architecture and Product Selection boxes<br>' +
        'that are the focus of this chapter. In this version of the<br>' +
        'diagram, we’ve included a mapping between the Lifecycle<br>' +
        'boxes and the Microsoft products and components you<br>' +
        'may use during your development and management<br>' +
        'processes.<br>' +
        'The first part of this chapter walks through the overall<br>' +
        'architecture of a Microsoft-based DW/BI system and the<br>' +
        'rationale behind the major components. We next discuss<br>' +
        'the specific product editions and components that make up<br>' +
        'Microsoft’s DW/BI related offerings.<br>' +
        'This chapter continues with a description of the two main<br>' +
        'tools that you’ll use to develop and operate your DW/BI<br>' +
        '186<br>' +
        'system. You’ll use a single integrated environment called<br>' +
        'the Business Intelligence Development Studio (BIDS) to<br>' +
        'develop most of your DW/BI system, and a second<br>' +
        'environment called the SQL Server Management Studio to<br>' +
        'manage it. We introduce the two tools here and provide an<br>' +
        'overview of the elements that are the same no matter what<br>' +
        'part of the DW/BI project you’re working on.<br>' +
        'Figure 3-1: Kimball Lifecycle and Microsoft technologies<br>' +
        'The Microsoft DW/BI Toolset<br>' +
        'The core set of DW/BI tools that Microsoft Corporation<br>' +
        'sells is Microsoft SQL Server. SQL Server includes<br>' +
        'several major components of primary interest for DW/BI<br>' +
        'projects:<br>' +
        '• The relational engine (RDBMS) to manage and store the<br>' +
        'dimensional data warehouse database.<br>' +
        '187<br>' +
        '• SQL Server Integration Services (SSIS) to build the extract,<br>' +
        'transformation, and load (ETL) system.<br>' +
        '• SQL Server Analysis Services (SSAS) analytic database to<br>' +
        'support users’ queries, particularly ad hoc use.<br>' +
        '• SQL Server Analysis Services data mining to develop<br>' +
        'statistical data mining models, and also to include those<br>' +
        'models in advanced analytic applications.<br>' +
        '• SQL Server Reporting Services (SSRS) to build predefined<br>' +
        'reports. The majority of the Reporting Services features are<br>' +
        'most appropriate for the DW/BI team, but you may provide<br>' +
        'some ad hoc query and report building functionality with<br>' +
        'Report Builder.<br>' +
        '• Master Data Services (MDS) to create a range of master data<br>' +
        'management applications to feed the data warehouse, and<br>' +
        'possibly integrate that data management with the source<br>' +
        'transaction systems.<br>' +
        '• Development and management tools, especially SQL Server<br>' +
        'BI Development Studio (BIDS) and SQL Server Management<br>' +
        'Studio to build and manage your DW/BI system.<br>' +
        'The SQL Server product contains the software necessary<br>' +
        'to build, deploy, populate, and manage your DW/BI<br>' +
        'system. Microsoft also offers a second significant set of<br>' +
        'tools beyond the SQL Server product designed for the<br>' +
        'business user, including:<br>' +
        '• Excel is the most common tool for ad hoc users to access<br>' +
        'Analysis Services databases. The Excel pivot table controls<br>' +
        'connect directly into SSAS cubes, in an environment that<br>' +
        'most users are already familiar with. Pivot tables can also<br>' +
        'connect directly to a relational data warehouse.<br>' +
        '• PowerPivot functionality is new in SQL Server 2008 R2. It<br>' +
        'combines the power of Analysis Services and Excel in an<br>' +
        'in-memory desktop experience. It’s very popular with power<br>' +
        'analytic users and provides a strong BI application platform<br>' +
        'in conjunction with SharePoint.<br>' +
        '188<br>' +
        '• SharePoint has several roles within the DW/BI system.<br>' +
        'Many organizations develop their BI portals in SharePoint,<br>' +
        'providing an integrated place to host reports, ad hoc query<br>' +
        'tools, online training, user support, and documentation.<br>' +
        '• PowerPivot for SharePoint expands the usefulness and<br>' +
        'manageability of PowerPivot, by enabling power users to<br>' +
        'share their PowerPivot workbooks via SharePoint.<br>' +
        '• Master Data Services can also be configured to integrate<br>' +
        'with SharePoint, in order to provide workflow functionality<br>' +
        'in the master data management system.<br>' +
        'Some organizations supplement their Microsoft end-user<br>' +
        'tools with third-party query, reporting, and analytic<br>' +
        'software.<br>' +
        'Microsoft Visual Studio is a fundamental tool for the DW/<br>' +
        'BI development team. The SQL Server DW/BI<br>' +
        'development tools are hosted in Visual Studio. The<br>' +
        'necessary Visual Studio components are installed for you,<br>' +
        'and you may not even realize that you are using the<br>' +
        'standard Microsoft development environment.<br>' +
        'You can use Visual Studio to build a custom application,<br>' +
        'such as an analytic application that connects your DW/BI<br>' +
        'system back to transaction systems. The heavy lifting of<br>' +
        'such an application may occur within your Analysis<br>' +
        'Services data mining model, but you’d still need to<br>' +
        'develop a bit of plumbing to connect the two systems.<br>' +
        'Why Use the Microsoft Toolset?<br>' +
        'Before we go on to describe how to build a DW/BI system<br>' +
        'using Microsoft technologies, it’s worth asking the<br>' +
        'question: What is interesting about the Microsoft toolset?<br>' +
        '189<br>' +
        'As it turns out, there are several compelling answers to this<br>' +
        'question:<br>' +
        '• Completeness: From the operating system, database engines,<br>' +
        'and development environment, to a SharePoint portal and<br>' +
        'the Office and Excel desktop, you can build a complete DW/<br>' +
        'BI system using only Microsoft software. You have an extra<br>' +
        'margin of confidence that all the components work together<br>' +
        'effectively.<br>' +
        '• Lower cost of ownership: The licensing cost of SQL Server<br>' +
        'has been less than comparable product suites from other<br>' +
        'vendors, but total cost of ownership depends as much on<br>' +
        'ongoing support, training, and operations costs as on<br>' +
        'licensing costs. Microsoft asserts that SQL Server systems<br>' +
        'need fewer administrative resources than competitive<br>' +
        'products. Your organization may already have .NET<br>' +
        'programming skills. If so, it may be possible for you to<br>' +
        'customize and extend your DW/BI system.<br>' +
        '• Openness: Although you can build a complete DW/BI<br>' +
        'system with Microsoft software — and this book describes<br>' +
        'how to do it — you don’t have to. Any component of the<br>' +
        'Microsoft DW/BI framework can be swapped out for a<br>' +
        'third-party product, and many customers build DW/BI<br>' +
        'systems in heterogeneous environments.<br>' +
        '• High performance and scale: At the time of this writing,<br>' +
        'Microsoft-based DW/BI systems with data volumes of 10<br>' +
        'terabytes are fairly common and 50 TB is not rare. As DW/<br>' +
        'BI systems are built on sub-transactional data like<br>' +
        'clickstreams and RFID data streams, even moderate-sized<br>' +
        'organizations may find themselves in the “terabyte club.”<br>' +
        'Microsoft recognizes this trend, and has engineered and<br>' +
        'tested its products, especially the SQL Server components,<br>' +
        'to perform well at high data volumes. Microsoft has also<br>' +
        'extended its SQL Server product line with the Parallel Data<br>' +
        'Warehouse system that uses a massively parallel processing<br>' +
        'architecture to scale up to hundreds of terabytes.<br>' +
        '• Microsoft investment in business intelligence: The SQL<br>' +
        'Server business intelligence suite consists of real tools that<br>' +
        '190<br>' +
        'work together, if not seamlessly, then at least with seams<br>' +
        'that have been professionally sewn. Some of the tools —<br>' +
        'notably Analysis Services — are best of breed. All of the<br>' +
        'tools are competitive on their own merits with standalone<br>' +
        'products. Microsoft is clearly committed to building tools to<br>' +
        'enable you to build great business intelligence applications.<br>' +
        'And you can be reasonably confident that Microsoft will<br>' +
        'remain in business for a long time.<br>' +
        'Architecture of a Microsoft DW/BI System<br>' +
        'All DW/BI systems consist of several major components,<br>' +
        'as pictured in Figure 3-2: sources of data, an ETL system,<br>' +
        'data warehouse databases, and a wide variety of uses.<br>' +
        'Metadata is the glue that binds together the complete DW/<br>' +
        'BI system.<br>' +
        'As we explained in Chapter 2, the data warehouse<br>' +
        'databases should be in a dimensional form, consisting of<br>' +
        'fact tables and their associated dimension tables.<br>' +
        'Dimensions should be conformed across the enterprise.<br>' +
        'For example, all business processes that are described by<br>' +
        'the customer dimension should use the same customer<br>' +
        'dimension with the same keys.<br>' +
        'Some enterprises take advantage of SQL Server Master<br>' +
        'Data Services (MDS) to build a master data management<br>' +
        'system. As we explain in Chapter 6, simple MDS<br>' +
        'deployments fall squarely within the purview of DW/BI,<br>' +
        'supplementing the ETL system. But complex, enterprise<br>' +
        'master data management systems are more closely aligned<br>' +
        'with the source systems. That’s why the Master Data<br>' +
        'Services box straddles the source systems and ETL in<br>' +
        'Figure 3-2.<br>' +
        '191<br>' +
        'Figure 3-2: Microsoft DW/BI system architecture<br>' +
        'The primary place to store and manage the dimensional<br>' +
        'model is in the relational data warehouse database. In<br>' +
        'Microsoft terms, this is the SQL Server database engine.<br>' +
        'You’ll use Integration Services to develop an ETL system<br>' +
        'that populates that database, performs inserts and updates,<br>' +
        'and also manages system resources such as disk space,<br>' +
        'partitions, and indexes.<br>' +
        'The second place to store and manage the dimensional<br>' +
        'model is in the core online analytic processing (OLAP)<br>' +
        'data warehouse database. In Microsoft terms, this is the<br>' +
        'Analysis Services OLAP engine. You’ll write a small ETL<br>' +
        'module to incrementally populate the core SSAS database<br>' +
        'from the clean, conformed relational data warehouse<br>' +
        'database.<br>' +
        'There are two major categories of DW/BI system usage:<br>' +
        'BI applications and exploratory use. There are many kinds<br>' +
        'of BI applications, ranging from standard predefined<br>' +
        'reports to complex analytic applications that use data<br>' +
        'mining technology to affect business operations. Microsoft<br>' +
        '192<br>' +
        'offers many technologies here, from Reporting Services<br>' +
        'for predefined reports, to Analysis Services data mining<br>' +
        'and the Visual Studio development environment to build<br>' +
        'custom applications.<br>' +
        'The other kind of usage is exploratory or ad hoc. Here,<br>' +
        'Office Excel continues to be popular, although many<br>' +
        'organizations struggle with the data anarchy that comes<br>' +
        'with extensive use of Excel in the enterprise. New to SQL<br>' +
        'Server 2008 R2, the PowerPivot analytic tool that<br>' +
        'combines Excel and Analysis Services may help with the<br>' +
        'data anarchy problem. Or it may make it worse! Many<br>' +
        'organizations use non-Microsoft tools to deliver more<br>' +
        'structured, yet still highly flexible, ad hoc query<br>' +
        'functionality. As we discuss in Chapter 10, the Report<br>' +
        'Builder component of Reporting Services is designed to<br>' +
        'provide some ad hoc functionality. Data mining is another<br>' +
        'kind of exploratory use, delivered by the Analysis Services<br>' +
        'data mining features. Most business users will access<br>' +
        'SSAS data mining via the Excel add-in.<br>' +
        'All these tools use metadata for development and<br>' +
        'operations, but there’s no specific metadata feature or<br>' +
        'central metadata management capability that we can point<br>' +
        'to. That doesn’t mean the metadata is missing or even that<br>' +
        'it’s unavailable; it’s just not as easy to get to, nor as<br>' +
        'integrated as we’d like.<br>' +
        'Most readers understand why the relational data warehouse<br>' +
        'database is important. Let’s talk first about why your<br>' +
        'architecture should include Analysis Services.<br>' +
        'Why Analysis Services?<br>' +
        '193<br>' +
        'What functionality is addressed by the OLAP database<br>' +
        'engine? Why would you want an OLAP engine — an<br>' +
        'Analysis Services implementation — in addition to the<br>' +
        'dimensional model stored in the relational database?<br>' +
        'All DW/BI systems need a user-oriented layer on top of<br>' +
        'the dimensional data stored in the relational database. This<br>' +
        'layer can simply be a set of predefined reports. But for<br>' +
        'successful ad hoc access by business users, you need a<br>' +
        'layer that performs the following basic functions:<br>' +
        '• Easy user navigation: User-oriented names for database<br>' +
        'objects, and transparent join paths between dimensions and<br>' +
        'facts and between multiple fact tables<br>' +
        '• Complex calculations: Centralized storage of calculation<br>' +
        'logic and execution of calculations<br>' +
        '• Fast user query performance: Usually accomplished through<br>' +
        'aggregate navigation and aggregate management<br>' +
        '• Data security definition and enforcement: Preferably<br>' +
        'managed on a server rather than on users’ desktops<br>' +
        'For many years, people have used relational techniques<br>' +
        'such as views and client-side query tools to deliver this<br>' +
        'functionality. An OLAP engine such as Analysis Services<br>' +
        'provides a better way. Analysis Services supports the basic<br>' +
        'user-oriented functions already described, with two key<br>' +
        'additional features:<br>' +
        '• Query language: OLAP engines use a different — and better<br>' +
        '— query language than SQL to express complex<br>' +
        'calculations.<br>' +
        '• Computational performance: The OLAP engine has been<br>' +
        'designed as a high-performance server — its enterprise<br>' +
        'functionality, data capacity, and capability to resolve the<br>' +
        'most complex calculations far outstrips any client-based<br>' +
        'tool.<br>' +
        '194<br>' +
        'The concepts introduced here are discussed more<br>' +
        'thoroughly in Chapter 8, which describes how to design<br>' +
        'the core Analysis Services database.<br>' +
        'Although Analysis Services is an extremely popular<br>' +
        'component of SQL Server, there are still several common<br>' +
        'objections to using it:<br>' +
        '• Scalability: Relational data warehouses can scale higher than<br>' +
        'Analysis Services. We wouldn’t hesitate to implement<br>' +
        'systems with several terabytes of data in Analysis Services,<br>' +
        'but we’d be cautious at scales approaching 10 TB. We’ve<br>' +
        'seen implementations larger than 5 TB, but they are rare.<br>' +
        '• Duplication of data: Many users dislike the notion of<br>' +
        'duplicating all the relational data warehouse data into a<br>' +
        'second database management system.<br>' +
        '• Changes to the user applications: Your business users are<br>' +
        'accustomed to using a SQL-based query and reporting tool,<br>' +
        'which might not work the same way (or at all) against an<br>' +
        'Analysis Services database. There is significant cost in<br>' +
        'purchasing new tools and retraining your users.<br>' +
        'Of the three common arguments against using Analysis<br>' +
        'Services, we find only the third to be broadly compelling.<br>' +
        'Worries about scalability and data duplication shouldn’t<br>' +
        'prevent the vast majority of SQL Server implementations<br>' +
        'from reaping the very real benefits of a DW/BI system<br>' +
        'that’s built on Analysis Services.<br>' +
        'Why a Relational Store?<br>' +
        'Perhaps you’re convinced that Analysis Services is a vital<br>' +
        'part of your DW/BI system architecture. Your next<br>' +
        'question may be: why do you need to store the<br>' +
        'dimensional data in the relational database? You aren’t<br>' +
        '195<br>' +
        'required to do so: Microsoft provides several mechanisms<br>' +
        'for populating cubes directly from non-dimensional source<br>' +
        'systems. Why go to the trouble and expense of<br>' +
        'implementing a relational data warehouse database?<br>' +
        'Here’s why:<br>' +
        '• Manageability: As discussed in Chapter 17, it’s much easier<br>' +
        'to handle changes in structure and content in the relational<br>' +
        'database than in Analysis Services.<br>' +
        '• Conforming dimensions and facts: In a hypothetical, simple<br>' +
        'example, you can conform data on the way into the Analysis<br>' +
        'Services database. In the real world, you’ll have to update<br>' +
        'and delete some data in the ETL pipeline, and you really<br>' +
        'want to do this in a relational database.<br>' +
        '• Comfort: DBAs and power users are familiar with SQL and<br>' +
        'relational databases and will violently resist the elimination<br>' +
        'of the relational layer.<br>' +
        '• Future flexibility: The notion of eliminating the relational<br>' +
        'data warehouse database and populating the Analysis<br>' +
        'Services database directly from transaction systems may<br>' +
        'sound appealing. But if you choose this approach, you’re<br>' +
        'committing to an architecture that’s difficult to transfer to<br>' +
        'another database platform, not that you would ever want to<br>' +
        'do that.<br>' +
        'There are scenarios, particularly around the real-time<br>' +
        'delivery of analytic data, where it may make sense to skip<br>' +
        'the relational storage of the dimensional data and populate<br>' +
        'the Analysis Services database directly from transaction<br>' +
        'systems. But these are edge cases. Most of us, most of the<br>' +
        'time, should plan to store and manage the dimensional data<br>' +
        'in the relational database, and use that store to feed<br>' +
        'Analysis Services. Think of the Analysis Services layer as<br>' +
        'metadata for the dimensional database, which possibly<br>' +
        'includes a data cache for enhanced performance.<br>' +
        '196<br>' +
        'ETL Is Not Optional<br>' +
        'We expect that most of the readers of this book are<br>' +
        'interested in building a data warehouse system. We will<br>' +
        'only briefly review the reasons that most in the<br>' +
        'information management industry agree that a data<br>' +
        'warehouse is a good idea. In short, the data warehouse:<br>' +
        '• Separates analytic workloads from transaction processing<br>' +
        '• Integrates multiple transaction data sources<br>' +
        '• Reduces the complexity of data models for easier reporting<br>' +
        'and ad hoc analysis<br>' +
        '• Improves query performance for reporting and ad hoc<br>' +
        'analysis by redesigning the data models<br>' +
        '• Enhances analytics by supporting the addition of information<br>' +
        'that’s not managed in the transaction systems<br>' +
        '• Maintains a longer time series of data for facts and selective<br>' +
        'dimension attributes than is typically found in the transaction<br>' +
        'systems<br>' +
        '• Presents a single version of the truth to the business user<br>' +
        'community<br>' +
        'More to the point, the data warehouse adds significant<br>' +
        'value to the data. A well designed and built data<br>' +
        'warehouse provides data capture, searching, extracting,<br>' +
        'staging, archiving, cleaning, conforming, duplicating,<br>' +
        'allocating, transforming, computing, arranging, packaging,<br>' +
        'aggregating, presenting, analyzing, modeling, estimating,<br>' +
        'projecting, recommending, and connecting the decision to<br>' +
        'operations!<br>' +
        'From time to time, observers argue that a data warehouse<br>' +
        'is too expensive. Instead, they propose a virtual data<br>' +
        'warehouse to provide many of the advantages of a real<br>' +
        'data warehouse, with presumably less cost. The ETL<br>' +
        '197<br>' +
        'system is particularly expensive; isn’t it possible to<br>' +
        'provide a user navigation layer on top of the transaction<br>' +
        'systems, and skip ETL altogether? In fact, Microsoft<br>' +
        'provides tools that could do that. It’s theoretically possible<br>' +
        'to build an Analysis Services database, a PowerPivot<br>' +
        'workbook, or a Report Builder model directly on top of a<br>' +
        'transaction database or even multiple databases.<br>' +
        'In all but the most trivial cases, this approach is so deeply<br>' +
        'flawed as to be unworkable. You’ll be shifting the burden<br>' +
        'of ETL onto the business users, who will do the same thing<br>' +
        'multiple times, with inefficient tools (usually Excel), and<br>' +
        'in inconsistent ways. Attempting to implement a virtual<br>' +
        'warehouse technique on even a simple and clean sample<br>' +
        'database such as AdventureWorks is an exercise in<br>' +
        'frustration.<br>' +
        'ETL is a job for IT professionals, not the user community.<br>' +
        'Build it once, build it right, and the downstream uses such<br>' +
        'as cubes, reports, and PowerPivot snap into place much<br>' +
        'more easily. It’s easy to create a cube or PowerPivot<br>' +
        'workbook from very clean, well structured, and integrated<br>' +
        'data, such as the data in a dimensional data warehouse.<br>' +
        'With PowerPivot, it’s not terribly difficult to integrate one<br>' +
        'or two additional data sources, such as demographic<br>' +
        'information or custom groupings.<br>' +
        'Even if your user community is saying they just want tools<br>' +
        'such as PowerPivot without an underlying infrastructure,<br>' +
        'they most assuredly will not enjoy doing IT’s job in the<br>' +
        'long term.<br>' +
        'The Role of Master Data Services<br>' +
        '198<br>' +
        'SQL Server Master Data Services has an unusual position<br>' +
        'in the data warehouse architecture, as pictured in Figure<br>' +
        '3-2. It straddles the source transaction systems and the data<br>' +
        'warehouse ETL system.<br>' +
        'Although most information architectures do not yet include<br>' +
        'a formal master data management system, almost every<br>' +
        'existing data warehouse does perform some master data<br>' +
        'management. Functionality to integrate customer names or<br>' +
        'product lists from multiple source systems and applets to<br>' +
        'manage reporting hierarchies or custom attributes are<br>' +
        'required by most data warehouse environments.<br>' +
        'We’re advocating several changes from the status quo:<br>' +
        '• Manage this master data externally and explicitly, rather<br>' +
        'than folding it into the batch ETL operations. One of the key<br>' +
        'advantages of explicit management is the opportunity to<br>' +
        'leverage experts’ judgment as entities are created, rather<br>' +
        'than during the nightly batch ETL process.<br>' +
        '• Consider using Master Data Services to replace most of the<br>' +
        'complex logic for dimension table ETL. Master Data<br>' +
        'Services can process changes during the day, and then<br>' +
        'submit the new and changed rows to the batch ETL system<br>' +
        'for processing into the dimension tables.<br>' +
        '• Set the stage for your organization to implement “real”<br>' +
        'master data management, which integrates tightly with the<br>' +
        'source transaction systems and eliminates the need for<br>' +
        'downstream data integration. Data is integrated at the source.<br>' +
        'As we discuss in Chapter 6, this admirable goal is out of<br>' +
        'immediate reach for many organizations, which haven’t even<br>' +
        'begun to take a serious view of data stewardship.<br>' +
        'Delivering BI Applications<br>' +
        '199<br>' +
        'Reporting Services is the primary platform in the SQL<br>' +
        'Server toolset for delivering BI applications. It is<br>' +
        'essentially a standalone enterprise report definition,<br>' +
        'storage, execution, and delivery service. It offers a<br>' +
        'reasonable set of report creation and distribution tools,<br>' +
        'primarily targeted at the developer community.<br>' +
        'Reporting Services includes Report Builder, a desktop tool<br>' +
        'that offers almost the same report design experience as the<br>' +
        'BIDS-based report designer without the need for BIDS or<br>' +
        'Visual Studio. This makes it more accessible to power<br>' +
        'users, and it has an optional metadata model to help<br>' +
        'simplify the data set definitions even further. However, it<br>' +
        'is still a fairly complex tool and does not find broad use in<br>' +
        'most organizations. We explore Reporting Services and<br>' +
        'Report Builder in greater detail in Chapter 10.<br>' +
        'If you want to deliver more flexible data interaction to<br>' +
        'your users, you need to include tools outside the SQL<br>' +
        'Server toolset. The obvious candidate under the Microsoft<br>' +
        'umbrella is Excel. Its PivotTable control connects directly<br>' +
        'to Analysis Services cubes and shows the available facts<br>' +
        'and dimensions based on the cube’s metadata. This makes<br>' +
        'it easy for business users to create their own simple queries<br>' +
        'and reports against Analysis Services cubes. PivotTables<br>' +
        'can also access relational data sources, but it is difficult to<br>' +
        'present the dimensional model because there is no<br>' +
        'associated metadata in the relational environment to define<br>' +
        'it.<br>' +
        'The PowerPivot add-in for Excel 2010 takes PivotTables<br>' +
        'to a new level. It is a desktop-based Analysis Services<br>' +
        'cube that uses an in-memory column store technology to<br>' +
        '200<br>' +
        'support fast access to millions of rows of data in<br>' +
        'combination with an enhanced PivotTable control called<br>' +
        'the PowerPivot Field List. The big leap forward of the<br>' +
        'PowerPivot Field List is its capability to allow the user to<br>' +
        'define new columns that are computed in the PowerPivot<br>' +
        'database. This enables users to embed computed columns<br>' +
        'in their reports that are completely context-aware, a critical<br>' +
        'function for creating flexible, modifiable reports.<br>' +
        'PowerPivot has a SharePoint component that allows you to<br>' +
        'share access to PowerPivot reports across your<br>' +
        'organization. In order to reach this level of reporting, you<br>' +
        'need Microsoft Office 2010 or higher on every user<br>' +
        'desktop and a SharePoint server. We will explore the<br>' +
        'implications of these additional tools in Chapter 11 on<br>' +
        'PowerPivot and Chapter 12 on the BI Portal and<br>' +
        'SharePoint.<br>' +
        'Overview of the Microsoft Tools<br>' +
        'Many readers will start to work with SQL Server by<br>' +
        'experimenting with its functionality in a single machine<br>' +
        'sandbox. If you have the time and bandwidth to do so, this<br>' +
        'is a great way to determine which product features are<br>' +
        'important to your business and users. In this environment,<br>' +
        'we recommend that you acquire a true sandbox machine: a<br>' +
        'new or rebuilt machine with a clean operating system and<br>' +
        'no other applications. You can use virtual machine<br>' +
        'technology to simulate a clean machine and run SQL<br>' +
        'Server well enough to evaluate functionality.<br>' +
        'Other readers are launching real projects and need to be<br>' +
        'more rigorous and thoughtful in setting up their<br>' +
        '201<br>' +
        'environments. As we discuss in Chapter 16, plan from the<br>' +
        'outset for the standard three-tier system, with separate<br>' +
        'servers for development, test, and production. Your test<br>' +
        'system should be as similar to the production system as<br>' +
        'you can possibly make it. The more different your test and<br>' +
        'production systems are, the more difficult it is for your<br>' +
        'database administrators to evaluate alternative approaches<br>' +
        'to tuning and configuration before rolling those changes<br>' +
        'into production.<br>' +
        'Most teams set up their development environment with a<br>' +
        'central server or two to hold relational and Analysis<br>' +
        'Services databases. Developers install the development<br>' +
        'tools — the studio workbenches described in the next<br>' +
        'section — on their own machines, and point those tools to<br>' +
        'the development database server. Early in the development<br>' +
        'cycle, developers may have personal databases, either on<br>' +
        'the shared server or on their own machines.<br>' +
        'Which Products Do You Need?<br>' +
        'There are four editions available that are interesting for<br>' +
        'DW/BI projects, starting at the high end:<br>' +
        '• Data Center Edition<br>' +
        '• Enterprise Edition<br>' +
        '• Standard Edition<br>' +
        '• Developer Edition<br>' +
        'You will need to purchase and run either Data Center,<br>' +
        'Enterprise, or Standard Edition on your production servers.<br>' +
        'Enterprise Edition contains almost the entire product<br>' +
        'feature set, and — unsurprisingly — costs several times as<br>' +
        'much as Standard Edition. Enterprise Edition lacks a few<br>' +
        '202<br>' +
        'features that are available in Data Center Edition, having<br>' +
        'to do with managing multiple instances. But the main<br>' +
        'difference between Enterprise and Data Center is the<br>' +
        'hardware: Enterprise Edition is limited to 8 CPU sockets<br>' +
        'and 2 TB of memory.<br>' +
        'RESOURCES<br>' +
        'You can find detailed information on each<br>' +
        'edition at www.microsoft.com/sql, and in<br>' +
        'Books Online. See the Books Online topic<br>' +
        '“Features Supported by the Editions of<br>' +
        'SQL Server.”<br>' +
        'There is no hard and fast rule for which edition you should<br>' +
        'purchase. A simple rule of thumb suggests that Standard<br>' +
        'Edition is probably sufficient for most small and some<br>' +
        'medium implementations. If your data volume, measured<br>' +
        'as data only without indexes, is 50 gigabytes (GB) or less,<br>' +
        'then you can do without the scalability features in<br>' +
        'Enterprise Edition. Depending on incremental load<br>' +
        'volumes, frequency, and uptime requirements, a<br>' +
        'medium-sized implementation of up to 250 GB can also<br>' +
        'work on Standard Edition. Any large, real-time, or<br>' +
        'otherwise challenging implementation should plan to use<br>' +
        'Enterprise Edition. Extremely large deployments, which<br>' +
        'plan to use more than 8 CPU sockets for a component of<br>' +
        'the DW/BI system, should use Data Center Edition.<br>' +
        'Whichever edition you use in production, your developers<br>' +
        'should use Developer Edition. Developer Edition is<br>' +
        '203<br>' +
        'extremely inexpensive, it will run on desktop operating<br>' +
        'systems such as Windows 7, and it contains all the<br>' +
        'functionalities of the Enterprise and Data Center Editions.<br>' +
        'Editions and Features of SQL Server 2008 R2<br>' +
        'The features that are excluded from Standard<br>' +
        'Edition support scalability in the enterprise.<br>' +
        'Mostly, scalability refers to data volumes, and<br>' +
        'often has more to do with maintaining and<br>' +
        'operating very large systems than actually storing<br>' +
        'and querying them. Another aspect of scalability is<br>' +
        'complexity; some of the excluded features would<br>' +
        'help your business users navigate a complex<br>' +
        'enterprise-level system more easily.<br>' +
        'Here, we list and comment on our favorite<br>' +
        'Enterprise Edition features:<br>' +
        '• Relational database engine<br>' +
        'Relational database partitioned tables are a key<br>' +
        'feature for fast loading and improved<br>' +
        'maintainability of large tables. We talk about<br>' +
        'partitioning in Chapter 4.<br>' +
        'Relational database maintenance functionality,<br>' +
        'including online index operations and parallel<br>' +
        'index operations, is particularly important for<br>' +
        'loading new data into large tables in a short time<br>' +
        'frame and performing periodic maintenance.<br>' +
        '204<br>' +
        'Resource Governor lets you define groups of users<br>' +
        'and types of usage, and define ceilings for their use<br>' +
        'of the relational database. The Resource Governor<br>' +
        'can help prevent one or two killer queries from<br>' +
        'taking over the server. Resource Governor is<br>' +
        'described in Chapter 17.<br>' +
        'Change Data Capture is defined on the source<br>' +
        'transaction database. It will generate tables with<br>' +
        'images for rows that have been added, updated, or<br>' +
        'deleted.<br>' +
        'Star join query optimization can dramatically<br>' +
        'improve the performance of queries on the<br>' +
        'dimensional or star schema structures in the<br>' +
        'relational data warehouse.<br>' +
        'Data compression can reduce storage and have a<br>' +
        'significant improvement on query performance<br>' +
        'from the relational data warehouse.<br>' +
        '• Integration Services<br>' +
        'Integration with Analysis Services dimensions,<br>' +
        'cubes, and data mining models is a really nice<br>' +
        'feature, but not absolutely necessary. You could<br>' +
        'have SSIS launch a script that updates the cube or<br>' +
        'data mining model.<br>' +
        'Advanced transforms such as fuzzy lookup and<br>' +
        'text mining are very cool, but they may be too<br>' +
        '205<br>' +
        'complicated for most small projects to deal with<br>' +
        'anyway.<br>' +
        '• Analysis Services OLAP engine<br>' +
        'Scalability and performance features, such as<br>' +
        'automatic parallel processing and partitioned<br>' +
        'cubes, are very important for delivering great<br>' +
        'performance with medium and large data volumes.<br>' +
        'Features such as Account Intelligence, Writeback<br>' +
        'Dimensions, and particularly Perspectives,<br>' +
        'Semi-additive Measures, and Translations let you<br>' +
        'build more usable and complex OLAP databases.<br>' +
        '• Analysis Services data mining<br>' +
        'Parallelism for processing and prediction is<br>' +
        'important for large data volumes and heavy usage<br>' +
        'scenarios.<br>' +
        'The statisticians in your organization will<br>' +
        'appreciate the advanced tuning and configuration<br>' +
        'options for the algorithms.<br>' +
        'The integration with Integration Services, as we’ve<br>' +
        'already described, is useful but not absolutely<br>' +
        'necessary.<br>' +
        '• Reporting Services<br>' +
        '206<br>' +
        'Scale-out report servers are an important feature<br>' +
        'for large-scale deployments. This is basically<br>' +
        'creating a web farm for the report servers.<br>' +
        'Data-driven subscriptions may be useful for a large<br>' +
        'enterprise. With this feature, you can email a<br>' +
        'personalized budget variance report to a dynamic<br>' +
        'list of managers, run from a single report<br>' +
        'definition.<br>' +
        '• PowerPivot for SharePoint<br>' +
        'PowerPivot for SharePoint is not available in<br>' +
        'Standard Edition.<br>' +
        '• Master Data Services<br>' +
        'Master Data Services is not available in Standard<br>' +
        'Edition.<br>' +
        'SQL Server Development and Management Tools<br>' +
        'Two toolsets are installed as part of the client tools<br>' +
        'installation. The SQL Server Management Studio<br>' +
        '(Management Studio) is used to operate and manage your<br>' +
        'DW/BI system. The Business Intelligence Development<br>' +
        'Studio (BIDS) is used to design and develop your business<br>' +
        'intelligence system.<br>' +
        'SQL Server Management Studio<br>' +
        '207<br>' +
        'Management Studio is the primary tool for database<br>' +
        'administrators. In most cases, the Management Studio<br>' +
        'client tools are the only component of SQL Server that is<br>' +
        'installed on database administrators’ workstations. The<br>' +
        'Management Studio screen is pictured in Figure 3-3.<br>' +
        'Figure 3-3: SQL Server Management Studio<br>' +
        'Relational Database Management Operations<br>' +
        'Within Management Studio, you perform most<br>' +
        'development and management activities for the relational<br>' +
        'database. There are point-and-click user interfaces to:<br>' +
        '• Create, delete, back up, and restore databases.<br>' +
        '• Create tables.<br>' +
        '• View the data in tables.<br>' +
        '• Create database diagrams.<br>' +
        '• Create and manage stored procedures, views, security, and<br>' +
        'other database objects.<br>' +
        '• Generate a T-SQL script to create any database object.<br>' +
        '208<br>' +
        'Alternatively, from within Management Studio, you can<br>' +
        'open a SQL window to perform all of these functions<br>' +
        'through T-SQL.<br>' +
        'Management Studio includes many predefined server<br>' +
        'management reports. The server dashboard report is<br>' +
        'illustrated in Figure 3-3. There are additional reports<br>' +
        'available for the server, each database, and other objects<br>' +
        'such as security.<br>' +
        'Analysis Services Management Operations<br>' +
        'When you connect to an existing Analysis Services<br>' +
        'database in Management Studio, you can perform<br>' +
        'management operations including:<br>' +
        '• Delete the SSAS database.<br>' +
        '• Back up and restore SSAS databases.<br>' +
        '• Define storage characteristics for cube partitions.<br>' +
        '• Create and process partitions.<br>' +
        '• Manage security.<br>' +
        '• Run aggregation design wizards, and create and process<br>' +
        'performance aggregations.<br>' +
        'You can also open three types of query windows for an<br>' +
        'Analysis Services database:<br>' +
        '• Multidimensional Expressions (MDX): MDX is the query<br>' +
        'language for Analysis Services. Use a simple control to<br>' +
        'browse the cube, or type your own MDX.<br>' +
        '• Data Mining Extensions (DMX): Using a syntax similar to<br>' +
        'SQL, issue queries on the data management views for<br>' +
        'Analysis Services. Alternatively, from within this same<br>' +
        'query window, use SQL syntax or a simple UI to issue<br>' +
        'queries into an existing data mining model.<br>' +
        '209<br>' +
        '• XML for Analysis (XMLA): Issue a query to browse the<br>' +
        'structure of the SSAS structure, modify that structure, or<br>' +
        'process the cube. As you might guess from the name,<br>' +
        'XMLA uses XML syntax.<br>' +
        'It is technically possible to edit and re-issue the Analysis<br>' +
        'Services object creation XMLA. In fact, it is technically<br>' +
        'possible to write a complete cube definition or Integration<br>' +
        'Services package by typing an XML file. Just because<br>' +
        'something is possible doesn’t mean it’s a good idea. Use<br>' +
        'BIDS to design and debug BI objects (except for the<br>' +
        'relational database). In the rare cases when you need to<br>' +
        'automatically generate an object, you should use the<br>' +
        'appropriate programming object model rather than attempt<br>' +
        'to manipulate the XML directly.<br>' +
        'Reporting Services Management Operations<br>' +
        'When you connect Management Studio to a Reporting<br>' +
        'Services instance, you can manage security and shared<br>' +
        'schedules. Many organizations use the Reporting Services<br>' +
        'default web application to perform these activities, rather<br>' +
        'than Management Studio.<br>' +
        'Integration Services Management Operations<br>' +
        'When you connect Management Studio to an Integration<br>' +
        'Services instance, you see the packages that are currently<br>' +
        'executing. You can also see all packages that have been<br>' +
        'installed in the SQL Server database or package store. As<br>' +
        'we discuss in Chapter 7, many organizations simply leave<br>' +
        'their packages in the file system, in which case they do not<br>' +
        'show up here in Management Studio.<br>' +
        '210<br>' +
        'Business Intelligence Development Studio (BIDS)<br>' +
        'BIDS is designed for all BI system designers and<br>' +
        'developers, with user interfaces for designing and<br>' +
        'debugging Analysis Services databases, data mining<br>' +
        'models, Integration Services packages, and Reporting<br>' +
        'Services reports. The obvious omission from this list is the<br>' +
        'design and development of the relational data warehouse<br>' +
        'database. Use Management Studio for the relational part of<br>' +
        'the project and BIDS for the rest.<br>' +
        'Like Management Studio, BIDS is integrated with Visual<br>' +
        'Studio. This is great news for developers who already use<br>' +
        'Visual Studio because the interface will be somewhat<br>' +
        'familiar. Even though the Visual Studio environment<br>' +
        'appears complex at first, everyone benefits from this<br>' +
        'integration. Your team can use integrated source control to<br>' +
        'manage project files; you can set breakpoints and debug<br>' +
        'Integration Services packages and MDX scripts; any code<br>' +
        'you may need to develop is integrated in the same<br>' +
        'environment; and all projects benefit from a unified<br>' +
        'approach to separating development from deployment.<br>' +
        'BIDS uses the same kind of hierarchical grouping as<br>' +
        'Visual Studio. You will create one or more solutions; each<br>' +
        'solution has one or more projects; each project has one or<br>' +
        'more associated files. Each project is associated with a<br>' +
        'technology such as Integration Services or Reporting<br>' +
        'Services. In theory, you can have one BI solution that<br>' +
        'spans your DW/BI system, with multiple projects inside it<br>' +
        'for each component of that system (SSIS, SSAS, and<br>' +
        'SSRS). Practically speaking, most teams keep a one-to-one<br>' +
        'correspondence between projects and solutions, or at most<br>' +
        '211<br>' +
        'a handful of projects for each solution. That’s because the<br>' +
        'solution takes a long time to open up if there are hundreds<br>' +
        'of files in it.<br>' +
        'DW/BI development teams should manage their files<br>' +
        'under source control, and BIDS makes that very easy to<br>' +
        'do. You won’t see this functionality unless you’ve<br>' +
        'integrated your source control product into Visual Studio.<br>' +
        'The files in the project folder completely define the<br>' +
        'project: They are the source code for the project. During<br>' +
        'the development process (especially if you’re not using<br>' +
        'source control!), you may want to share your project<br>' +
        'definition with a colleague so she can view your work.<br>' +
        'You can simply send her a copy of the project folder,<br>' +
        'ensure she has appropriate database permissions, and she is<br>' +
        'set.<br>' +
        'BIDS shows different windows depending on whether<br>' +
        'you’re working on an Analysis Services, Integration<br>' +
        'Services, or Reporting Services project. Figure 3-4<br>' +
        'illustrates the BIDS window for an Integration Services<br>' +
        'project. This is a new project that doesn’t have much<br>' +
        'content yet.<br>' +
        'All the different types of projects use a similar layout,<br>' +
        'imposed by Visual Studio. The Solution Explorer window,<br>' +
        'located by default in the upper right, lists the files in the<br>' +
        'project and lets you navigate between them. The Properties<br>' +
        'pane, located by default in the lower right, shows all the<br>' +
        'properties associated with an object; read-only properties<br>' +
        'are gray. As you’ll see in subsequent chapters, BIDS<br>' +
        '212<br>' +
        'contains extensive wizards whose job, in effect, is to help<br>' +
        'you set these properties.<br>' +
        'Figure 3-4: Basic layout of the BIDS windows and panes<br>' +
        'BIDS gobbles up screen real estate, and is a good excuse<br>' +
        'to get an upgraded monitor and video card. You can<br>' +
        'maximize screen real estate by setting windows such as the<br>' +
        'Solution Explorer and Properties pane to “auto-hide.”<br>' +
        'Many of the BIDS components use the Toolbox pane on<br>' +
        'the left-hand side. The big section in the middle, which<br>' +
        'currently displays the control flow for a new SSIS<br>' +
        'package, is used as a design surface. In here you’ll design<br>' +
        'Integration Services packages, Analysis Services<br>' +
        'dimensions, cubes, and data mining models, Reporting<br>' +
        '213<br>' +
        'Services reports, and so on. You generally want this<br>' +
        'central area to be as big as possible.<br>' +
        'Finally, the BIDS tools use Visual Studio’s build, deploy,<br>' +
        'and debugging features such as watch windows.<br>' +
        'Sometimes it feels like there are dozens of extra little<br>' +
        'windows, located by default at the bottom of the screen.<br>' +
        'The other chapters in this book focus on the specific BI<br>' +
        'features such as Analysis Services and Integration<br>' +
        'Services. In those chapters, we spend more time describing<br>' +
        'how to use BIDS, although this book is not intended to be<br>' +
        'a product tutorial. The tutorials that ship with SQL Server<br>' +
        'do a very good job of explaining how to use the product.<br>' +
        'Summary<br>' +
        'In this chapter, we described how the Microsoft BI toolset<br>' +
        'maps to industry standard terminology. We made a<br>' +
        'compelling argument that you can build your entire<br>' +
        'business intelligence system using software only from<br>' +
        'Microsoft. You’re not tied to Microsoft for your entire<br>' +
        'project, however: the components are “open,” in the sense<br>' +
        'that they are linked together by published interfaces. If you<br>' +
        'wish, you can use a non-Microsoft product for any<br>' +
        'component of your DW/BI system.<br>' +
        'We described the basic recommended architecture of a<br>' +
        'Microsoft DW/BI system and introduced the studio tools<br>' +
        '— SQL Server Management Studio and BI Development<br>' +
        'Studio. We will provide much more information about<br>' +
        'these tools in upcoming chapters.<br>' +
        '214<br>';
    document.getElementById('chapter2').innerHTML = 'Chapter 2<br>' +
        'Designing the Business Process Dimensional Model<br>' +
        '“To arrive at the simple is difficult.”<br>' +
        '— Rashid Elisha<br>' +
        'This chapter is about the basic concepts of dimensional<br>' +
        'modeling and the process of designing a business process<br>' +
        'dimensional model. Designing the dimensional model falls<br>' +
        'within the central section of the Kimball Lifecycle, as<br>' +
        'shown in Figure 2-1. This middle row of the Lifecycle’s<br>' +
        'central section focuses on data, hence the clever name: the<br>' +
        'data track. The main objective of the data track is to make<br>' +
        'sure users get the data they need to meet ongoing business<br>' +
        'requirements. The key word in this objective is ongoing:<br>' +
        'Your goal in this step is to create a usable, flexible,<br>' +
        'extensible data model. This model needs to support the full<br>' +
        'range of analyses, both now and for the foreseeable future.<br>' +
        'The dimensional model is the true heart of the DW/BI<br>' +
        'system. It is the target for the ETL system, the structure of<br>' +
        'the database, and the model behind the user query and<br>' +
        'reporting experience. Clearly, the model must be well<br>' +
        'designed. The first part of this chapter is a brief primer on<br>' +
        'dimensional modeling, including an overview of facts,<br>' +
        'dimensions, the data warehouse bus matrix, and other core<br>' +
        'concepts. The second major section of the chapter delves<br>' +
        'into more detail on several important design techniques,<br>' +
        'such as slowly changing dimensions, hierarchies, and<br>' +
        'bridge tables. Once the basic concepts are in place, the<br>' +
        '97<br>' +
        'third section presents a process for building dimensional<br>' +
        'models.<br>' +
        'The fourth part of the chapter describes a dimensional<br>' +
        'model for the Adventure Works Cycles orders business<br>' +
        'process, providing an opportunity to explore several<br>' +
        'common dimensional modeling issues and their solutions.<br>' +
        'Figure 2-1: The dimensional modeling step in the<br>' +
        'Lifecycle context<br>' +
        'RESOURCES<br>' +
        'This chapter describes what a dimensional<br>' +
        'model is, why it’s a useful design technique<br>' +
        'for a DW/BI system, and how to go about<br>' +
        'designing a strong data foundation. You<br>' +
        'cannot possibly learn everything you need<br>' +
        'to know about dimensional modeling in a<br>' +
        '98<br>' +
        'single chapter — even a long one like this.<br>' +
        'For additional detailed guidance on the<br>' +
        'techniques, including industry case studies,<br>' +
        'we refer you to The Data Warehouse<br>' +
        'Toolkit, Second Edition, Ralph Kimball and<br>' +
        'Margy Ross (Wiley, 2002). We provide<br>' +
        'page references for more information on<br>' +
        'specific concepts and techniques<br>' +
        'throughout this chapter.<br>' +
        'Dimensional Modeling Concepts and Terminology<br>' +
        'We approach the modeling process with three primary<br>' +
        'design goals in mind. We want our models to accomplish<br>' +
        'the following:<br>' +
        '• Present the needed information to users as simply as possible<br>' +
        '• Return query results to the users as quickly as possible<br>' +
        '• Provide relevant information that accurately tracks the<br>' +
        'underlying business processes<br>' +
        'Albert Einstein captured the main reason we use the<br>' +
        'dimensional model when he said, “Make everything as<br>' +
        'simple as possible, but not simpler.” As it turns out,<br>' +
        'simplicity is relative. There is broad agreement in data<br>' +
        'warehousing and business intelligence that the dimensional<br>' +
        'model is the preferred structure for presenting information<br>' +
        'to users. The dimensional model is much easier for users to<br>' +
        'understand than the typical source system normalized<br>' +
        'model even though a dimensional model typically contains<br>' +
        'exactly the same content as a normalized model. It has far<br>' +
        'fewer tables, and information is grouped into coherent<br>' +
        '99<br>' +
        'business categories that make sense to users. These<br>' +
        'categories, which we call dimensions, help users navigate<br>' +
        'the model because entire categories can be disregarded if<br>' +
        'they aren’t relevant to a particular analysis.<br>' +
        'Unfortunately, as simple as possible doesn’t mean the<br>' +
        'model is necessarily simple. The model must reflect the<br>' +
        'business, and businesses are complex. If you simplify too<br>' +
        'much, typically by presenting only aggregated data, the<br>' +
        'model loses information that’s critical to understanding the<br>' +
        'business. No matter how you model data, the intrinsic<br>' +
        'complexity of the data content is ultimately why most<br>' +
        'people will use structured reports and analytic applications<br>' +
        'to access the DW/BI system.<br>' +
        'Achieving our second goal of good performance is a bit<br>' +
        'more platform-specific. In the relational environment, the<br>' +
        'dimensional model helps query performance because of<br>' +
        'the denormalization involved in creating the dimensions.<br>' +
        'By pre-joining the various hierarchies and lookup tables,<br>' +
        'the optimizer considers fewer join paths and creates fewer<br>' +
        'intermediate temporary tables. Analytic queries against the<br>' +
        'SQL Server relational database generally perform better —<br>' +
        'often far better — against a dimensional structure than<br>' +
        'against a fully normalized structure. At a more<br>' +
        'fundamental level, the optimizer can recognize the<br>' +
        'dimensional model and leverage its structure to<br>' +
        'dramatically reduce the number of rows it returns. This is<br>' +
        'known as star join optimization, and is, of course, an<br>' +
        'Enterprise Edition feature.<br>' +
        'In the Analysis Services OLAP environment, the engine is<br>' +
        'specifically designed to support dimensional models.<br>' +
        '100<br>' +
        'Performance is achieved in large part by pre-aggregating<br>' +
        'within and across dimensions.<br>' +
        'Achieving the third goal requires a full range of design<br>' +
        'patterns that allow us to create models that accurately<br>' +
        'capture and track the business. Let’s start with the basic<br>' +
        'patterns first. A dimensional model is made up of a central<br>' +
        'fact table (or tables) and its associated dimensions. The<br>' +
        'dimensional model is also called a star schema because it<br>' +
        'looks like a star with the fact table in the middle and the<br>' +
        'dimensions serving as the points on the star. We stick to<br>' +
        'the term dimensional model in this book to avoid<br>' +
        'confusion.<br>' +
        'From a relational data modeling perspective, the<br>' +
        'dimensional model consists of a normalized fact table with<br>' +
        'denormalized dimension tables. This section defines the<br>' +
        'basic components of the dimensional model, facts and<br>' +
        'dimensions, along with some of the key concepts involved<br>' +
        'in handling changes over time.<br>' +
        'Facts<br>' +
        'Each fact table contains the measurements associated with<br>' +
        'a specific business process, like taking an order, displaying<br>' +
        'a web page, admitting a patient, or handling a customer<br>' +
        'support request. A record in a fact table is a measurement<br>' +
        'event. These events usually have numeric values that<br>' +
        'quantify the magnitude of the event, such as quantity<br>' +
        'ordered, sale amount, or call duration. These numbers are<br>' +
        'called facts (or measures in Analysis Services).<br>' +
        '101<br>' +
        'The primary key to the fact table is usually a multi-part<br>' +
        'key made up of a subset of the foreign keys from each<br>' +
        'dimension table involved in the business event.<br>' +
        'Just the Facts<br>' +
        'Most facts are numeric and additive (such as sales amount<br>' +
        'or unit sales), meaning they can be summed up across all<br>' +
        'dimensions. Additivity is important because DW/BI<br>' +
        'applications seldom retrieve a single fact table record. User<br>' +
        'queries generally select hundreds or thousands of records<br>' +
        'at a time and add them up. A simple query for sales by<br>' +
        'month for the last year returns only 12 rows in the answer<br>' +
        'set, but it may sum up across hundreds of thousands of<br>' +
        'rows (or more!). Other facts are semi-additive (such as<br>' +
        'market share or account balance), and still others are<br>' +
        'non-additive (such as unit price).<br>' +
        'Not all numeric data are facts. Exceptions include discrete<br>' +
        'descriptive information like package size or weight<br>' +
        '(describes a product) or store square footage (describes a<br>' +
        'store). Generally, these less volatile numeric values end up<br>' +
        'as descriptive attributes in dimension tables. Such<br>' +
        'descriptive information is more naturally used for<br>' +
        'constraining a query, rather than being summed in a<br>' +
        'computation. This distinction is helpful when deciding<br>' +
        'whether a data element is part of a dimension or fact.<br>' +
        'Some business processes track events without any real<br>' +
        'measures. If the event happens, we get an entry in the<br>' +
        'source system; if not, there is no row. Common examples<br>' +
        'of this kind of event include employment activities, such<br>' +
        'as hiring and firing, and event attendance, such as when a<br>' +
        '102<br>' +
        'student attends a class. The fact tables that track these<br>' +
        'events typically do not have any actual fact measurements,<br>' +
        'so they’re called factless fact tables. We usually add a<br>' +
        'column called something like event count that contains the<br>' +
        'number 1. This provides users with an easy way to count<br>' +
        'the number of events by summing the event count fact.<br>' +
        'Some facts are derived or computed from other facts, just<br>' +
        'as a net sale number might be calculated from gross sales<br>' +
        'minus sales tax. Some semi-additive facts can be handled<br>' +
        'using a derived column that is based on the context of the<br>' +
        'query. Month end balance would add up across accounts,<br>' +
        'but not across date, for example. The non-additive unit<br>' +
        'price example could be addressed by defining it as an<br>' +
        'average unit price, which is total amount divided by total<br>' +
        'quantity. There are several options for dealing with these<br>' +
        'derived or computed facts. You can calculate them as part<br>' +
        'of the ETL process and store them in the fact table, you<br>' +
        'can put them in the fact table view definition, or you can<br>' +
        'include them in the definition of the Analysis Services<br>' +
        'database. The only way we find unacceptable is to leave<br>' +
        'the calculation to the user.<br>' +
        'NOTE<br>' +
        'Using Analysis Services to calculate<br>' +
        'computed measures has a significant<br>' +
        'benefit in that you can define complex<br>' +
        'MDX calculations for semi-additive facts<br>' +
        '103<br>' +
        'that will automatically calculate correctly<br>' +
        'based on the context of each query request.<br>' +
        'The Grain<br>' +
        'The level of detail contained in the fact table is called the<br>' +
        'grain. We strongly urge you to build your fact tables with<br>' +
        'the lowest level of detail that is possible from the original<br>' +
        'source — generally this is known as the atomic level.<br>' +
        'Atomic fact tables provide complete flexibility to roll up<br>' +
        'the data to any level of summary needed across any<br>' +
        'dimension, now or in the future. You must keep each fact<br>' +
        'table at a single grain. For example, it would be confusing<br>' +
        'and dangerous to have individual sales order line items in<br>' +
        'the same fact table as the monthly forecast.<br>' +
        'NOTE<br>' +
        'Designing your fact tables at the lowest<br>' +
        'practical level of detail, the atomic level, is<br>' +
        'a major contributor to the flexibility of the<br>' +
        'design.<br>' +
        'Fact tables are very efficient. They are highly normalized,<br>' +
        'storing little redundant data. For most transaction-driven<br>' +
        'organizations, fact tables are also the largest tables in the<br>' +
        'data warehouse database, often making up 95 percent or<br>' +
        'more of the total relational database size. The relational<br>' +
        '104<br>' +
        'fact table corresponds to a measure group in Analysis<br>' +
        'Services.<br>' +
        'Dimensions<br>' +
        'Dimensions are the nouns of the dimensional model,<br>' +
        'describing the objects that participate in the business, such<br>' +
        'as employee, subscriber, publication, customer, physician,<br>' +
        'vehicle, product, service, author, and store. Each<br>' +
        'dimension table joins to all the business processes in which<br>' +
        'it participates. For example, the product dimension<br>' +
        'participates in supplier orders, inventory, shipments, and<br>' +
        'returns business processes. A single dimension that is<br>' +
        'shared across all these processes is called a conformed<br>' +
        'dimension. We talk more about conformed dimensions in a<br>' +
        'bit.<br>' +
        'Think about dimensions as tables in a database because<br>' +
        'that’s how you’ll implement them. Each table contains a<br>' +
        'list of homogeneous entities — products in a<br>' +
        'manufacturing company, patients in a hospital, vehicles on<br>' +
        'auto insurance policies, or customers in just about every<br>' +
        'organization. Usually, a dimension includes all<br>' +
        'occurrences of its entity — all the products the company<br>' +
        'sells, for example. There is only one active row for each<br>' +
        'particular occurrence in the table at any time, and each row<br>' +
        'has a set of attributes that identify, describe, define, and<br>' +
        'classify the occurrence. A product will have a certain size<br>' +
        'and a standard weight, and belong to a product group.<br>' +
        'These sizes and groups have descriptions, like a food<br>' +
        'product might come in “Mini-Pak” or “Jumbo size.” A<br>' +
        'vehicle is painted a certain color, like “White,” and has a<br>' +
        'certain option package, such as the “Jungle Jim sports<br>' +
        '105<br>' +
        'utility” package (which includes side impact air bags,<br>' +
        'six-disc CD player, DVD system, and simulated leopard<br>' +
        'skin seats).<br>' +
        'Some descriptive attributes in a dimension relate to each<br>' +
        'other in a hierarchical or one-to-many fashion. A vehicle<br>' +
        'has a manufacturer, brand, and model (such as GM<br>' +
        'Chevrolet Silverado, or Toyota Lexus RX Hybrid).<br>' +
        'Dimensions often have more than one such embedded<br>' +
        'hierarchy.<br>' +
        'The underlying data structures for most relational<br>' +
        'transaction systems are designed using a technique known<br>' +
        'as normalization. This approach removes redundancies in<br>' +
        'the data by moving repeating attributes into their own<br>' +
        'tables. The physical process of recombining all the<br>' +
        'attributes of a business object, including its hierarchies,<br>' +
        'into a single dimension table is known as denormalization.<br>' +
        'As we described earlier, this simplifies the model from a<br>' +
        'user perspective. It also makes the join paths much simpler<br>' +
        'for the database query optimizer than a fully normalized<br>' +
        'model. The denormalized dimension still presents exactly<br>' +
        'the same information and relationships found in the<br>' +
        'normalized model — nothing is lost from an analytic<br>' +
        'perspective except complexity.<br>' +
        'You can spot dimensions or their attributes in conversation<br>' +
        'with the business folks because they are often the “by”<br>' +
        'words in a query or report request. For example, a user<br>' +
        'wants to see sales by month by product. The natural ways<br>' +
        'users describe their business should be included in the<br>' +
        'dimensional model as dimensions or dimension attributes.<br>' +
        'This is important because many of the ways users analyze<br>' +
        '106<br>' +
        'the business are often not captured in the transaction<br>' +
        'system. Including these attributes in the warehouse is part<br>' +
        'of the added value you can provide.<br>' +
        'The Power of Dimensions<br>' +
        'Dimensions provide the entry points into the data.<br>' +
        'Dimensional attributes are used in two primary<br>' +
        'ways: as the target for constraints and as the labels<br>' +
        'on the rows and columns of a report. If the<br>' +
        'dimensional attribute exists, you can constrain and<br>' +
        'label. If it doesn’t exist, you simply can’t.<br>' +
        'Bringing Facts and Dimensions Together<br>' +
        'The completed dimensional model has a characteristic<br>' +
        'appearance, with the fact table in the middle surrounded by<br>' +
        'the dimensions. Figure 2-2 shows a simple dimensional<br>' +
        'model for the classic example: the retail grocery sales<br>' +
        'business process. Note that this is one of the business<br>' +
        'process rows from the retail bus matrix shown in Figure<br>' +
        '1-6 back in Chapter 1. It all ties together if you do it right.<br>' +
        'This model allows users across the business to analyze<br>' +
        'retail sales activity from various perspectives. Category<br>' +
        'managers can look at sales by product for different stores<br>' +
        'and different dates. Store planners can look at sales by<br>' +
        'store format or location. Store managers can look at sales<br>' +
        'by date or cashier. There is something for everyone in the<br>' +
        'organization in this dimensional model. While this model<br>' +
        'is reasonably robust, a large retail grocer would have a few<br>' +
        '107<br>' +
        'more dimensions, notably customer, and many more<br>' +
        'attributes.<br>' +
        'Figure 2-2: A basic dimensional model for Retail Grocery<br>' +
        'Sales<br>' +
        'In Figure 2-2, fields labeled PK are primary keys. In other<br>' +
        'words, these fields are the basis of uniqueness for their<br>' +
        'tables. In a dimensional model, the primary keys of<br>' +
        'dimensions are always implemented physically as single<br>' +
        '108<br>' +
        'columns. The fields labeled FK are foreign keys, and must<br>' +
        'always match the corresponding PKs in the dimensions in<br>' +
        'order to ensure referential integrity. The field labeled DD<br>' +
        'is a special degenerate dimension, which is described later.<br>' +
        'RESOURCES<br>' +
        'To grasp the concept of dimensions and<br>' +
        'facts, it’s helpful to see examples of<br>' +
        'dimensional models from a variety of<br>' +
        'industries and business processes. The Data<br>' +
        'Warehouse Toolkit, Second Edition has<br>' +
        'example dimensional models from many<br>' +
        'different industries and business processes,<br>' +
        'including retail sales, inventory,<br>' +
        'procurement, order management, CRM,<br>' +
        'accounting, HR, financial services,<br>' +
        'telecommunications and utilities,<br>' +
        'transportation, education, health care,<br>' +
        'e-commerce, and insurance.<br>' +
        'The Bus Matrix, Conformed Dimensions, and Drill Across<br>' +
        'The idea of re-using dimensions across multiple business<br>' +
        'processes is the foundation of the enterprise DW/BI<br>' +
        'system and the heart of the Kimball Bus Matrix. In the<br>' +
        'retail grocery example, a dimension such as product will<br>' +
        'be used in both the retail sales and the store inventory<br>' +
        'dimensional models. Because they are exactly the same<br>' +
        'products, both models must use the same dimension with<br>' +
        'the same keys to reliably support true, cross-business<br>' +
        '109<br>' +
        'process analysis. If the logistics folks at the grocer’s<br>' +
        'headquarters want to calculate inventory turns, they need<br>' +
        'to combine data from retail sales and inventory at the<br>' +
        'product level. This works only if the two business<br>' +
        'processes use the exact same product dimension with the<br>' +
        'same keys; that is, they use a conformed dimension.<br>' +
        'Conformed dimensions are the cornerstone of the<br>' +
        'enterprise-enabled DW/BI system. This kind of analysis<br>' +
        'involving data from more than one business process is<br>' +
        'called drill across.<br>' +
        'NOTE<br>' +
        'The precise technical definition of<br>' +
        'conformed dimensions is that two<br>' +
        'dimensions are conformed if they contain<br>' +
        'one or more fields with the same names<br>' +
        'and contents. These “conformed fields”<br>' +
        'must then be used as the basis for the<br>' +
        'drill-across operation.<br>' +
        'Note that this idea of drilling across<br>' +
        'multiple fact tables and combining the<br>' +
        'answer sets requires a front-end tool<br>' +
        'capable of supporting this function. A<br>' +
        'powerful reason to use Analysis Services is<br>' +
        'that conformed dimensions are part of the<br>' +
        'basic architecture of the cube, so its<br>' +
        'calculation engine smoothly supports drill<br>' +
        'across.<br>' +
        '110<br>' +
        'Examine the Adventure Works Cycles high-level bus<br>' +
        'matrix shown in Figure 2-3. Each row of the bus matrix<br>' +
        'represents a business process and defines at least one fact<br>' +
        'table and its associated dimensions. Often, a row in the<br>' +
        'matrix will result in several related fact tables that help<br>' +
        'track the business process from different perspectives. The<br>' +
        'orders business process might have an orders transaction<br>' +
        'fact table at the line-item level and an orders snapshot fact<br>' +
        'table at the order level. Both of these orders-based<br>' +
        'dimensional models belong to the orders business process.<br>' +
        'We call this grouping a business process dimensional<br>' +
        'model. The fully populated enterprise DW/BI system<br>' +
        'contains sets of dimensional models that describe all the<br>' +
        'business processes in an organization’s value chain. As<br>' +
        'you create the business process dimensional models for<br>' +
        'each row in the bus matrix, you end up with a much more<br>' +
        'detailed version of the matrix. Each dimensional model has<br>' +
        'its own row grouped by business process. Order<br>' +
        'transactions and order snapshot would be separate rows<br>' +
        'under the orders business process.<br>' +
        'Figure 2-3: Adventure Works Cycles high-level enterprise<br>' +
        'bus matrix<br>' +
        '111<br>' +
        'The bus matrix is the enterprise business intelligence data<br>' +
        'roadmap. Creating the bus matrix is mandatory for any<br>' +
        'enterprise-wide DW/BI effort. Getting enterprise<br>' +
        'agreement on conformed dimensions is an organizational<br>' +
        'challenge for the data modeler and data steward. Having a<br>' +
        'single dimension table to describe the company’s products,<br>' +
        'customers, or facilities means the organization has to agree<br>' +
        '112<br>' +
        'on how each dimension table is defined. This includes the<br>' +
        'list of attributes, attribute names, hierarchies, and the<br>' +
        'business rules needed to define and derive each attribute in<br>' +
        'the table. This is politically hard work, and the effort<br>' +
        'grows as a function of the number of employees and<br>' +
        'divisions. But it is not optional. Conformed dimensions<br>' +
        'ensure that you are comparing apples to apples (assuming<br>' +
        'you are selling apples).<br>' +
        'Additional Design Concepts and Techniques<br>' +
        'Even though the dimensional modeling concepts we’ve<br>' +
        'described are fairly simple, they are applicable to a wide<br>' +
        'range of business scenarios. However, there are a few<br>' +
        'additional dimensional modeling concepts and techniques<br>' +
        'that are critical to implementing viable dimensional<br>' +
        'models. We start this section with a couple of key<br>' +
        'concepts: surrogate keys and slowly changing dimensions.<br>' +
        'Then we look at several techniques for modeling more<br>' +
        'complex business situations. Finally, we review the<br>' +
        'different types of fact tables. We briefly describe each<br>' +
        'concept or technique and provide references so you can<br>' +
        'find more detailed information if you need it.<br>' +
        'Surrogate Keys<br>' +
        'You will need to create a whole new set of keys in the data<br>' +
        'warehouse database, separate from the keys in the<br>' +
        'transaction source systems. We call these keys surrogate<br>' +
        'keys, although they are also known as meaningless keys,<br>' +
        'substitute keys, non-natural keys, or artificial keys. A<br>' +
        'surrogate key is a unique value, usually an integer,<br>' +
        'assigned to each row in the dimension. This surrogate key<br>' +
        '113<br>' +
        'becomes the primary key of the dimension table and is<br>' +
        'used to join the dimension to the associated foreign key<br>' +
        'field in the fact table. Using surrogate keys in all<br>' +
        'dimension tables reaps the following benefits (and more):<br>' +
        '• Surrogate keys help protect the DW/BI system from<br>' +
        'unexpected administrative changes in the keys coming from<br>' +
        'the source system.<br>' +
        '• Surrogate keys allow the DW/BI system to integrate the<br>' +
        'same data, such as customer, from multiple source systems<br>' +
        'where they have different keys.<br>' +
        '• Surrogate keys enable you to add rows to dimensions that do<br>' +
        'not exist in the source system. For example, the date table<br>' +
        'might have a “Date not known” row.<br>' +
        '• Surrogate keys provide the means for tracking changes in<br>' +
        'dimension attributes over time.<br>' +
        '• Integer surrogate keys can improve query and processing<br>' +
        'performance compared to larger character or GUID keys.<br>' +
        'The ability to track changes in dimension attributes over<br>' +
        'time is reason enough to implement surrogate keys that are<br>' +
        'managed by the data warehouse. We’ve regretted it more<br>' +
        'than once when we decided not to track changes in<br>' +
        'attribute values over time, and later found out the historical<br>' +
        'values were important to support certain business analyses.<br>' +
        'We had to go back and add surrogate keys and re-create<br>' +
        'the dimension’s change history. This is not a fun project;<br>' +
        'we encourage you to do it right the first time. If you use<br>' +
        'surrogate keys for all dimensions at the outset, it’s easier to<br>' +
        'change a dimension later so that it tracks history.<br>' +
        'The biggest cost of using surrogate keys is the burden it<br>' +
        'places on the ETL system. Assigning the surrogate keys to<br>' +
        'the dimension rows is easy. The real effort lies in mapping<br>' +
        'those keys into the fact table rows. A fact row comes to the<br>' +
        '114<br>' +
        'DW/BI system with its source transaction keys in place.<br>' +
        'We call the transaction system key the business key (or<br>' +
        'natural key), although it is usually not business-like. In<br>' +
        'order to join these fact rows to the dimension tables, the<br>' +
        'ETL system must take each business key in each fact row<br>' +
        'and look up its corresponding surrogate key in the<br>' +
        'appropriate dimension. We call this lookup process the<br>' +
        'surrogate key pipeline. Integration Services, used to build<br>' +
        'the ETL system, provides functionality to support this<br>' +
        'lookup, as we describe in Chapter 7.<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about surrogate keys:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 58–62.<br>' +
        '• Search http://msdn.microsoft.com using<br>' +
        'the search string “surrogate key.”<br>' +
        'Slowly Changing Dimensions<br>' +
        'Although we like to think of the attribute values in a<br>' +
        'dimension as fixed, any attribute can change over time. In<br>' +
        'an employee dimension, the date of birth should not<br>' +
        'change over time (other than error corrections, which your<br>' +
        'ETL system should expect). However, other fields, such as<br>' +
        'the employee’s department, might change several times<br>' +
        'over the length of a person’s employment. Many of these<br>' +
        'changes are critical to understanding the dynamics of the<br>' +
        'business. The ability to track these changes over time is<br>' +
        '115<br>' +
        'one of the fundamental reasons for the existence of the<br>' +
        'DW/BI system.<br>' +
        'Almost all dimensions have attributes whose values will<br>' +
        'change over time. Therefore, you need to be prepared to<br>' +
        'deal with a change to the value of any attribute. The<br>' +
        'techniques we use to manage attribute change in a<br>' +
        'dimension are part of what we call slowly changing<br>' +
        'dimensions (SCDs). However, just because something<br>' +
        'changes, it doesn’t mean that change has significance to<br>' +
        'the business. The choice of which dimension attributes you<br>' +
        'need to track and how you track them is a business<br>' +
        'decision. The main technique used to deal with changes<br>' +
        'the business doesn’t care about is called type 1, and the<br>' +
        'main technique used to handle changes the business wants<br>' +
        'to track is called type 2.<br>' +
        'How Slow Is Slow?<br>' +
        'We call this concept a slowly changing dimension<br>' +
        'attribute because the attributes that describe a<br>' +
        'business entity generally don’t change very often.<br>' +
        'Take customer address, for example. The US<br>' +
        'Census Bureau did an in-depth study of migration<br>' +
        'patterns (www.census.gov/population/www/pop-profile/<br>' +
        'geomob.html), and found that 16.8 percent of<br>' +
        'Americans move in a given year, and 62 percent of<br>' +
        'these moves are within the same county. If a<br>' +
        'change in zip code is considered to be a significant<br>' +
        'business event, a simple customer dimension with<br>' +
        '116<br>' +
        'name and address information should generate less<br>' +
        'than 16.8 percent change rows per year. If an<br>' +
        'attribute changes rapidly, causing the dimension to<br>' +
        'grow at a dramatic rate, this usually indicates the<br>' +
        'presence of a business process that should be<br>' +
        'tracked separately, either as a separate dimension,<br>' +
        'called a mini-dimension, or as a fact table rather<br>' +
        'than as a dimension attribute.<br>' +
        'Handling changes using the type 1 technique overwrites<br>' +
        'the existing attribute value with the new value. Use this<br>' +
        'method if the business users don’t care about keeping track<br>' +
        'of historical values when the value of an attribute changes.<br>' +
        'The type 1 change does not preserve the attribute value<br>' +
        'that was in place at the time a historical transaction<br>' +
        'occurred. For example, the Adventure Works customer<br>' +
        'dimension has an attribute for commute distance. Using<br>' +
        'type 1, when a customer moves, their old commute<br>' +
        'distance is overwritten with the new value. The old value<br>' +
        'is gone. All purchases, including purchases made prior to<br>' +
        'the change, will be associated with the new value for<br>' +
        'commute distance.<br>' +
        'If you need to track the history of attribute changes, use<br>' +
        'the type 2 technique. Type 2 change tracking is a powerful<br>' +
        'method for capturing the attribute values that were in<br>' +
        'effect at a point in time and relating them to the business<br>' +
        'events in which they participated. When a change to a type<br>' +
        '2 attribute occurs, the ETL process creates a new row in<br>' +
        'the dimension table to capture the new values of the<br>' +
        '117<br>' +
        'changed dimension attribute. The attributes in the new row<br>' +
        'are in effect as of the time of the change moving forward.<br>' +
        'The previously existing row is marked to show that its<br>' +
        'attributes were in effect right up until the appearance of the<br>' +
        'new row.<br>' +
        'Using the type 2 technique to track changes to the example<br>' +
        'commute distance attribute will preserve history. The ETL<br>' +
        'system writes a new row to the customer dimension, with a<br>' +
        'new surrogate key and date stamps to show when the row<br>' +
        'came into effect and when it expires. (The system also<br>' +
        'updates the expiration date stamp for the old row.) All fact<br>' +
        'rows moving forward will be assigned the surrogate key<br>' +
        'for this new row. All the existing fact rows keep their old<br>' +
        'customer surrogate key, which joins to the old row.<br>' +
        'Purchases made prior to the change will be associated with<br>' +
        'the commute distance that was in effect when the purchase<br>' +
        'was made.<br>' +
        'Type 2 change tracking is more work to manage in the<br>' +
        'ETL system, although it’s transparent to the user queries.<br>' +
        'Most of the popular ETL tools, including Integration<br>' +
        'Services, have these techniques built in. The commute<br>' +
        'distance example is a good one in terms of its business<br>' +
        'impact. If a marketing analyst does a study to understand<br>' +
        'the relationship between commute distance and bicycle<br>' +
        'purchasing, type 1 tracking will yield very different results<br>' +
        'than type 2. In fact, type 1 will yield incorrect results, but<br>' +
        'that may not be apparent to the analyst.<br>' +
        'Here’s a good guide for deciding if it’s worth the effort to<br>' +
        'use type 2 change tracking for an attribute: Ask yourself if<br>' +
        'the data has to be right.<br>' +
        '118<br>' +
        'NOTE<br>' +
        'A third change tracking technique, called<br>' +
        'type 3, keeps separate columns for both the<br>' +
        'old and new attribute values — sometimes<br>' +
        'called “alternate realities.” In our<br>' +
        'experience, type 3 is less common because<br>' +
        'it involves changing the physical tables and<br>' +
        'is not very extensible. If you choose to use<br>' +
        'type 3 tracking, you will need to add a new<br>' +
        'type 3 column for every major change,<br>' +
        'which can lead to a wide table. The<br>' +
        'technique is most often used for an<br>' +
        'organization hierarchy that changes<br>' +
        'seldom, perhaps annually. Often, only two<br>' +
        'versions are kept (current and prior).<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about slowly changing<br>' +
        'dimensions:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 95–105.<br>' +
        '• Books Online: A search for “slowly<br>' +
        'changing dimension support” will return<br>' +
        'several related topics.<br>' +
        '119<br>' +
        'DOWNLOADS<br>' +
        'You can find a more detailed version of the<br>' +
        'commute distance example with example<br>' +
        'data on the book’s web site<br>' +
        '(kimballgroup.com/html/booksMDWTtools.html).<br>' +
        'Dates<br>' +
        'Date is the fundamental business dimension across all<br>' +
        'organizations and industries, although many times a date<br>' +
        'table doesn’t exist in the operational environment.<br>' +
        'Analyses that trend across dates or make comparisons<br>' +
        'between periods (that is, nearly all business analyses) are<br>' +
        'best supported by creating and maintaining a robust date<br>' +
        'dimension.<br>' +
        'Every dimensional DW/BI system has a date (or calendar)<br>' +
        'dimension, typically with one row for every day for which<br>' +
        'you expect to have data in a fact table. Calling it the date<br>' +
        'dimension emphasizes that its grain is at the day level<br>' +
        'rather than time of day. In other words, the Date table will<br>' +
        'have 365 or 366 rows in it per year.<br>' +
        'NOTE<br>' +
        'The date dimension is a good example of a<br>' +
        'role-playing dimension. It is common for a<br>' +
        '120<br>' +
        'date dimension to be used to represent<br>' +
        'different dates, such as order date, due date,<br>' +
        'and ship date. To support users who will be<br>' +
        'directly accessing the relational database,<br>' +
        'you can either define a view for each role<br>' +
        'named appropriately (e.g., Order_Date),<br>' +
        'or define synonyms on the base view of the<br>' +
        'date dimension. The synonym approach is<br>' +
        'simple, but it only allows you to rename<br>' +
        'the entire table, not the columns within the<br>' +
        'table. Reports that can’t distinguish<br>' +
        'between order date and due date can be<br>' +
        'confusing.<br>' +
        'If your users will access the data through Analysis<br>' +
        'Services only, you don’t need to bother with views or<br>' +
        'synonyms to handle multiple roles. Analysis Services has<br>' +
        'built-in support for the concept of role-playing dimensions.<br>' +
        'As we discuss in Chapter 8, however, the Analysis<br>' +
        'Services role-playing dimensions are akin to creating<br>' +
        'relational synonyms: You can name the overall dimension<br>' +
        'role, but not each column within the dimension role.<br>' +
        'We strongly recommend using a surrogate key for date<br>' +
        'because of the classic problem often faced by technical<br>' +
        'people: Sometimes you don’t have a date. While we can’t<br>' +
        'help the technical people here, we can say that transactions<br>' +
        'often arrive at the data warehouse database without a date<br>' +
        'because the value in the source system is missing,<br>' +
        'unknowable, or the event hasn’t happened yet. Surrogate<br>' +
        'keys on the date dimension help manage the problem of<br>' +
        '121<br>' +
        'missing dates. Create a few rows in the date dimension that<br>' +
        'describe such events, and assign the appropriate surrogate<br>' +
        'date key to the fact rows with the missing dates.<br>' +
        'In the absence of a surrogate date key, you’ll create date<br>' +
        'dimension members with strange dates such as 1-Jan-1900<br>' +
        'to mean Unknown and 31-Dec-9999 to mean Hasn’t<br>' +
        'happened yet. Overloading the fact dates in this way isn’t<br>' +
        'the end of the world, but it can confuse users and cause<br>' +
        'erroneous results.<br>' +
        'The date dimension surrogate key has one slight deviation<br>' +
        'from the rule. Where other surrogate keys are usually a<br>' +
        'meaningless sequence of integers, it’s a good idea to use a<br>' +
        'meaningful value for the date surrogate key. Specifically,<br>' +
        'use an integer that corresponds to the date in<br>' +
        'year-month-day order, so September 22, 2010 would be<br>' +
        '20100922. This can lead to more efficient queries against<br>' +
        'the relational database. It also makes implementing<br>' +
        'date-based partitioning much easier, and the partition<br>' +
        'management function will be more understandable.<br>' +
        'Just to prove that we’re not entirely dogmatic and<br>' +
        'inflexible, we don’t always use surrogate keys for dates<br>' +
        'that appear as dimension attributes. Generally, if a<br>' +
        'particular date attribute has business meaning, we use the<br>' +
        'date surrogate key. Otherwise we use a date or<br>' +
        'smalldatetime data type. One way to spot a good<br>' +
        'candidate for a date surrogate key is if the attribute’s date<br>' +
        'values fall within the date range of the organizational<br>' +
        'calendar, and therefore are all in the Date table.<br>' +
        '122<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about the date dimension:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 38–41.<br>' +
        '• Books Online: Search for the topic “Time<br>' +
        'Dimensions (SSAS)” and related topics.<br>' +
        'Degenerate Dimensions<br>' +
        'Transaction identifiers often end up as degenerate<br>' +
        'dimensions without joining to an actual dimension table. In<br>' +
        'the retail grocery example, all the individual items you<br>' +
        'purchase in a trip through the checkout line are assigned a<br>' +
        'transaction ID. The transaction ID is not a dimension — it<br>' +
        'doesn’t exist outside the transaction and it has no<br>' +
        'descriptive attributes of its own because they’ve already<br>' +
        'been handled in separate dimensions. It’s not a fact — it<br>' +
        'doesn’t measure the event in any way, and is not additive.<br>' +
        'We call attributes such as transaction ID a degenerate<br>' +
        'dimension because it’s like a dimension without attributes.<br>' +
        'And because there are no associated attributes, there is no<br>' +
        'dimension table. We include it in the fact table because it<br>' +
        'serves a purpose from an analytic perspective. You can use<br>' +
        'it to tie together all the line items in a market basket to do<br>' +
        'some interesting data mining, as we discuss in Chapter 13.<br>' +
        'You can also use it to tie back to the transaction system if<br>' +
        'additional orders-related data is needed. The degenerate<br>' +
        'dimension is known as a fact dimension in Analysis<br>' +
        'Services.<br>' +
        '123<br>' +
        'Snowflaking<br>' +
        'In simple terms, snowflaking is the practice of connecting<br>' +
        'lookup tables to fields in the dimension tables. At the<br>' +
        'extreme, snowflaking involves re-normalizing the<br>' +
        'dimensions to the third normal form level, usually under<br>' +
        'the misguided belief that this will improve maintainability,<br>' +
        'increase flexibility, or save space. We discourage<br>' +
        'snowflaking. It makes the model more complex and<br>' +
        'therefore less usable, and it actually makes it more difficult<br>' +
        'to maintain, especially for type 2 slowly changing<br>' +
        'dimensions.<br>' +
        'In a few cases we support the idea of connecting lookup or<br>' +
        'grouping tables to the dimensions. One of these cases<br>' +
        'involves rarely used lookups, as in the example of joining<br>' +
        'the Date table to the date of birth field in the customer<br>' +
        'dimension so we can count customers grouped by their<br>' +
        'month of birth. We call this purpose-specific snowflake<br>' +
        'table an outrigger table. When you’re building your<br>' +
        'Analysis Services database, you’ll see this same concept<br>' +
        'referred to as a reference dimension.<br>' +
        'Sometimes it’s easier to maintain a dimension in the ETL<br>' +
        'process when it’s been partially normalized or snowflaked.<br>' +
        'This is especially true if the source data is a mess and<br>' +
        'you’re trying to ensure the dimension hierarchy is<br>' +
        'correctly structured. In this case, there’s nothing wrong<br>' +
        'with using the normalized structure in the ETL application.<br>' +
        'Just make sure the business users never have to deal with<br>' +
        'it.<br>' +
        '124<br>' +
        'NOTE<br>' +
        'Analysis Services can handle snowflaked<br>' +
        'dimensions and hides the added complexity<br>' +
        'from the business users. In the interest of<br>' +
        'simplicity, we encourage you to fully<br>' +
        'populate the base dimensions rather than<br>' +
        'snowflaking. The one exception is that the<br>' +
        'Analysis Services build process can go<br>' +
        'faster for large dimensions, say more than 1<br>' +
        'million rows, when the source is<br>' +
        'snowflaked or fully denormalized in the<br>' +
        'ETL staging database. Test it first before<br>' +
        'you do all the extra work.<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about snowflaking:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 55–57.<br>' +
        '• Books Online: Search for the topic<br>' +
        '“Dimension Structure” and other Books<br>' +
        'Online topics on dimensions.<br>' +
        'Many-to-Many or Multivalued Dimensions<br>' +
        'The standard relationship between a dimension table and<br>' +
        'fact table is called one-to-many. This means one row in the<br>' +
        'dimension table will join to many rows in the fact table,<br>' +
        '125<br>' +
        'but one row on the fact table will join to only one row in<br>' +
        'the dimension table. This relationship is important because<br>' +
        'it keeps us from double counting. Fortunately, in most<br>' +
        'cases this relationship holds true.<br>' +
        'There are two common instances where the real world is<br>' +
        'more complex than one-to-many:<br>' +
        '• Many-to-many between the fact table and a dimension<br>' +
        '• Many-to-many between dimensions<br>' +
        'These two instances are essentially the same, except the<br>' +
        'fact-to-dimension version is missing an intermediate<br>' +
        'dimension that uniquely describes the group. In both cases,<br>' +
        'we introduce an intermediate table called a bridge table<br>' +
        'that supports the more complex many-to-many<br>' +
        'relationship.<br>' +
        'The Many-to-Many Relationship Between a Fact and<br>' +
        'Dimension<br>' +
        'A many-to-many relationship between a fact table and a<br>' +
        'dimension occurs when multiple dimension values can be<br>' +
        'assigned to a single fact transaction. A common example is<br>' +
        'when multiple sales people can be assigned to a given sale.<br>' +
        'This often happens in complex, big-ticket sales such as<br>' +
        'computer systems. Accurately handling this situation<br>' +
        'requires creating a bridge table that assembles the sales rep<br>' +
        'combinations into groups. Figure 2-4 shows an example of<br>' +
        'the SalesRepGroup bridge table.<br>' +
        'The ETL process needs to look up the appropriate sales rep<br>' +
        'group key in the bridge table for the combination of sales<br>' +
        'reps in each incoming fact table record, and add a new<br>' +
        '126<br>' +
        'group if it doesn’t exist. Note that the bridge table in<br>' +
        'Figure 2-4 introduces a risk of double counting. If we sum<br>' +
        'dollar sales by sales rep, every sales rep will get credit for<br>' +
        'the total sale. For some analyses, this is the right answer,<br>' +
        'but for others you don’t want any double counting. It’s<br>' +
        'possible to handle this risk by adding a weighting factor<br>' +
        'column in the bridge table. The weighting factor is a<br>' +
        'fractional value that sums to one for each sales rep group.<br>' +
        'Multiply the weighting factor and the additive facts to<br>' +
        'allocate the facts according to the contribution of each<br>' +
        'individual in the group.<br>' +
        'Figure 2-4: An example SalesRepGroup bridge table<br>' +
        'Note that you might need to add a SalesRepGroupKey<br>' +
        'table between Orders and SalesRepGroup to support a<br>' +
        'true primary key - foreign key relationship. This turns this<br>' +
        '127<br>' +
        'fact-to-dimension instance into a dimension-to-dimension<br>' +
        'instance.<br>' +
        'Many-to-Many Between Dimensions<br>' +
        'The many-to-many relationship between dimensions is an<br>' +
        'important concept from an analytic point of view. Most<br>' +
        'dimensions are not entirely independent of one another.<br>' +
        'Dimension independence is more of a continuum than a<br>' +
        'binary state. At one end of the continuum, the store and<br>' +
        'product dimensions in a retail grocery chain are relatively<br>' +
        'independent, but not entirely. Some store formats don’t<br>' +
        'carry certain products. Other dimensions are much more<br>' +
        'closely related, but are difficult to combine into a single<br>' +
        'dimension because of their many-to-many relationship. In<br>' +
        'banking, for example, there is a direct relationship between<br>' +
        'account and customer, but it’s not one-to-one. Any given<br>' +
        'account can have one or more customers as signatories,<br>' +
        'and any given customer can have one or more accounts.<br>' +
        'Banks often view their data from an account perspective;<br>' +
        'the MonthAccountSnapshot is a common fact table in<br>' +
        'financial institutions. The account focus makes it difficult<br>' +
        'to view accounts by customer because of the<br>' +
        'many-to-many relationship. One approach would be to<br>' +
        'create a CustomerGroup bridge table that joins to the fact<br>' +
        'table, such as the SalesRepGroup table in the previous<br>' +
        'many-to-many example. A better approach takes<br>' +
        'advantage of the relationship between account and<br>' +
        'customer, as shown in Figure 2-5.<br>' +
        'Figure 2-5: An example many-to-many bridge table<br>' +
        'between dimensions<br>' +
        '128<br>' +
        'An AccountToCustomer bridge table between the<br>' +
        'account and customer dimensions can capture the<br>' +
        'many-to-many relationship with a couple of significant<br>' +
        'benefits. First, the relationship is already known in the<br>' +
        'source system, so creating the bridge table will be easier<br>' +
        'than the manual build process required for the<br>' +
        'SalesRepGroup table. Second, the account-customer<br>' +
        'relationship is interesting in its own right. The<br>' +
        'AccountToCustomer bridge table allows users to answer<br>' +
        'questions such as “What is the average number of accounts<br>' +
        'per customer?” without joining to any fact table.<br>' +
        'Bridge tables are often an indicator of an underlying<br>' +
        'business process. This is especially true if you must keep<br>' +
        'track of changes to bridge tables over time (that is, the<br>' +
        'relationship itself is type 2). For customers and accounts,<br>' +
        'the business process might be called account maintenance,<br>' +
        'and one of the transactions might be called “Add a<br>' +
        'signatory.” If three customers were associated with an<br>' +
        'account, there would be three Add transactions for that<br>' +
        'account in the source system. Usually these transactions<br>' +
        'and the business processes they represent are not important<br>' +
        'enough to track in the DW/BI system with their own fact<br>' +
        'tables. However, the relationships and changes they<br>' +
        'produce are important to analyzing the business. We<br>' +
        '129<br>' +
        'include them in the dimensional model as slowly changing<br>' +
        'dimensions, and in some cases as bridge tables.<br>' +
        'NOTE<br>' +
        'Analysis Services has functionality to<br>' +
        'support many-to-many dimensions.<br>' +
        'Analysis Services expects the same kind of<br>' +
        'structure that we described in this section.<br>' +
        'They call the bridge table an intermediate<br>' +
        'fact table, which is exactly what it is.<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about many-to-many<br>' +
        'relationships:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 262–265 for many-to-many<br>' +
        'between fact and dimension and pages<br>' +
        '205–206 for many-to-many between<br>' +
        'dimensions.<br>' +
        '• Books Online: Enter the search string<br>' +
        '“Defining a Many-to-Many Relationship”<br>' +
        'for a list of related Books Online topics.<br>' +
        'Hierarchies<br>' +
        'Hierarchies are meaningful, standard ways to group the<br>' +
        'data within a dimension so you can begin with the big<br>' +
        'picture and drill down to lower levels to investigate<br>' +
        '130<br>' +
        'anomalies. Hierarchies are the main paths for summarizing<br>' +
        'the data. Common hierarchies include organizational<br>' +
        'hierarchies, often starting from the individual person level;<br>' +
        'geographic hierarchies based on physical location, such as<br>' +
        'a customer address; product hierarchies that often<br>' +
        'correspond to merchandise rollups such as brand and<br>' +
        'category; and responsibility hierarchies such as sales<br>' +
        'territory that assign customers to sales reps (or vice versa).<br>' +
        'There are many industry-related hierarchies, such as the<br>' +
        'North American Industrial Classification System<br>' +
        '(replacement for the Standard Industrial Classification<br>' +
        '[SIC] code) or the World Health Organization’s<br>' +
        'International Classification of Diseases — Tenth<br>' +
        'Modification (ICD-10).<br>' +
        'Simple hierarchies involving a standard one-to-many<br>' +
        'rollup with only a few fixed levels should be denormalized<br>' +
        'right into the granular dimension. A four-level product<br>' +
        'hierarchy might start with product, which rolls up to brand,<br>' +
        'then to subcategory, and finally to category. Each of these<br>' +
        'levels would simply be columns in the product dimension<br>' +
        'table. In fact, this flattening of hierarchies is one of the<br>' +
        'main design tasks of creating a dimension table. Many<br>' +
        'organizations will have several different simple hierarchies<br>' +
        'in a given dimension to support different analytic<br>' +
        'requirements.<br>' +
        'Of course, not all hierarchies are simple. The challenge for<br>' +
        'the dimensional modeler is to determine how to balance<br>' +
        'the tradeoff between ease of use and flexibility in<br>' +
        'representing the more difficult hierarchies. There are at<br>' +
        'least two common hierarchy challenges: variable-depth (or<br>' +
        'ragged hierarchies) and frequently changing hierarchies.<br>' +
        '131<br>' +
        'Both of these problems require more complex solutions<br>' +
        'than simple denormalization. We briefly describe these<br>' +
        'solutions here, and refer you to more detailed information<br>' +
        'in the other Toolkit books if you should need it.<br>' +
        'Variable-Depth Hierarchies<br>' +
        'A good example of the variable-depth hierarchy is the<br>' +
        'manufacturing bill of materials that provides the<br>' +
        'information needed to build a particular product. In this<br>' +
        'case, parts can go into products or into intermediate layers,<br>' +
        'called sub-assemblies, which then go into products, also<br>' +
        'called top assemblies. This layering can go dozens of<br>' +
        'levels deep, or more, in a complex product (think about a<br>' +
        'Boeing 787).<br>' +
        'Those of you who were computer science majors may<br>' +
        'recall writing recursive subroutines and appreciate the<br>' +
        'efficiency of recursion for parsing a parent-child or<br>' +
        'self-referencing table. In SQL, this recursive structure is<br>' +
        'implemented by simply including a parent key field in the<br>' +
        'child record that points back to the parent record in the<br>' +
        'same table. For example, one of the fields in an Employee<br>' +
        'table could be the Parent Employee Key (or Manager<br>' +
        'Key).<br>' +
        'Recursive Capabilities<br>' +
        'SQL 99 introduced recursion into the “official”<br>' +
        'SQL language using the WITH Common Table<br>' +
        '132<br>' +
        'Expression syntax. All the major relational<br>' +
        'database products provide this functionality in<br>' +
        'some form, including SQL Server. Unfortunately,<br>' +
        'recursion isn’t a great solution in the relational<br>' +
        'environment because it requires more complex<br>' +
        'SQL than most front-end tools can handle. Even if<br>' +
        'the tool can recursively unpack the self-referencing<br>' +
        'dimension relationship, it then must be able to join<br>' +
        'the resulting dataset to the fact table. Very few of<br>' +
        'the query tools are able to generate the SQL<br>' +
        'required to navigate a parent-child relationship<br>' +
        'together with a dimension-fact join.<br>' +
        'On the other hand, this kind of recursive<br>' +
        'relationship is easy to build and manage in the<br>' +
        'Analysis Services dimensional database. Analysis<br>' +
        'Services uses different terminology: parent-child<br>' +
        'rather than variable-depth hierarchies.<br>' +
        'The Data Warehouse Toolkit, Second Edition describes a<br>' +
        'navigation bridge table in Chapter 6 that solves this<br>' +
        'problem in the relational world. But this solution is<br>' +
        'relatively unwieldy to manage and query. Fortunately, we<br>' +
        'have some powerful alternatives in SQL Server. Analysis<br>' +
        'Services has a built-in understanding of the parent-child<br>' +
        'data structure, and Reporting Services can navigate the<br>' +
        'parent-child hierarchy using some of the advanced<br>' +
        'properties of the tablix control. If you have variable-depth<br>' +
        'hierarchies and expect to use the relational database for<br>' +
        'reporting and analysis, it makes sense to include both the<br>' +
        '133<br>' +
        'parent-child fields and the navigation bridge table to meet<br>' +
        'the needs of various environments.<br>' +
        'Frequently Changing Hierarchies<br>' +
        'If you need to track changes in the variable-depth<br>' +
        'hierarchy over time, your problem becomes more complex,<br>' +
        'especially with the parent-child data structure. Tracking<br>' +
        'changes, as you recall, requires using a surrogate key. If<br>' +
        'someone is promoted, they will get a new row in the table<br>' +
        'with a new surrogate key. At that point, everyone who<br>' +
        'reports to that person will have to have their manager key<br>' +
        'updated. If manager key is a type 2 attribute, new rows<br>' +
        'with new surrogate keys will now need to be generated for<br>' +
        'these rows. If there are any folks who report to these<br>' +
        'people, the changes must ripple down until we reach the<br>' +
        'bottom of the org chart. In the worst case, a change to one<br>' +
        'of the CEO’s attributes, such as marital status, causes the<br>' +
        'CEO to get a new surrogate key. This means the people<br>' +
        'who report to the CEO will get new surrogate keys, and so<br>' +
        'on down the entire hierarchy.<br>' +
        'Ultimately, this problem is really about tracking the<br>' +
        'Human Resources business process. That is, if keeping<br>' +
        'track of all the changes that take place in your employee<br>' +
        'database is a high priority from an analytic point of view,<br>' +
        'you need to create a fact table, or a set of fact tables that<br>' +
        'track these events. Trying to cram all this event-based<br>' +
        'information into a type 2 dimension just doesn’t work very<br>' +
        'well.<br>' +
        '134<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about hierarchies:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 161–168.<br>' +
        '• The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, pages 268–270.<br>' +
        '• SQL Server Books Online: Enter the search<br>' +
        'string “Attributes and Attribute<br>' +
        'Hierarchies” as a starting point.<br>' +
        'Aggregate Dimensions<br>' +
        'You will often have data in the DW/BI system at different<br>' +
        'levels of granularity. Sometimes it comes to you that way,<br>' +
        'as with forecast data that is created at a level higher than<br>' +
        'the individual product. Other times you create it yourself<br>' +
        'to give the users better query performance. There are two<br>' +
        'ways to aggregate data in the warehouse, one by entirely<br>' +
        'removing a dimension, the other by rolling up in a<br>' +
        'dimension’s hierarchy. When you aggregate using a<br>' +
        'hierarchy rollup in the relational database, you need to<br>' +
        'provide a new, shrunken dimension at this aggregate level.<br>' +
        'Figure 2-6 shows how the Adventure Works Cycles<br>' +
        'Product table can be shrunken to the subcategory level to<br>' +
        'allow it to join to forecast data that is created at the<br>' +
        'subcategory level.<br>' +
        'Figure 2-6: A Subcategory table extracted from the<br>' +
        'Adventure Works Cycles Product dimension table<br>' +
        '135<br>' +
        'Each subcategory includes a mix of color, size, and weight<br>' +
        'attributes, for example. Therefore, most of the columns in<br>' +
        'the Product table do not make sense at the Subcategory<br>' +
        'level. This results in a much shorter dimension, hence the<br>' +
        'common term shrunken dimension.<br>' +
        'The keys for aggregate dimensions need to be generated in<br>' +
        'the ETL process and are not derived from the base table<br>' +
        'keys. Records can be added or subtracted from the base<br>' +
        'table over time and thus there is no guarantee that a key<br>' +
        '136<br>' +
        'from the base table can be used in the aggregate<br>' +
        'dimension.<br>' +
        'NOTE<br>' +
        'The Analysis Services OLAP engine<br>' +
        'automatically manages all this behind the<br>' +
        'scenes.<br>' +
        'You can easily hook in a fact table at any<br>' +
        'level of a dimension. Of course you may<br>' +
        'still need aggregate dimensions to support<br>' +
        'business process metrics (such as forecast),<br>' +
        'which live at the aggregate level only.<br>' +
        'RESOURCES<br>' +
        'The following resource offers additional<br>' +
        'information about aggregate dimensions:<br>' +
        '• Kimballgroup.com: Search for the string<br>' +
        '“Shrunken Dimensions” for an article on<br>' +
        'aggregate dimensions. This article also<br>' +
        'appears on page 539 of the Kimball Group<br>' +
        'Reader.<br>' +
        'Junk Dimensions<br>' +
        'Many business processes involve several flags or status<br>' +
        'indicators or types. In a pure dimensional model, these<br>' +
        'would each be placed in their own dimension table, which<br>' +
        '137<br>' +
        'often is just the surrogate key and the descriptive column,<br>' +
        'usually with only a few rows. For example, an order<br>' +
        'transaction might have a payment type dimension with<br>' +
        'only three allowed values: “Credit,” “Cash,” or “Account.”<br>' +
        'Another small order dimension could be transaction type<br>' +
        'with only two allowed values: “Order,” or “Refund.” In<br>' +
        'practice, using a single table for each small dimension can<br>' +
        'make the model confusing because there are too many<br>' +
        'tables. Worse, it makes the fact table much larger because<br>' +
        'each small dimension table adds a key column to the fact<br>' +
        'table.<br>' +
        'The design pattern we apply to this situation is called a<br>' +
        'junk dimension. A junk dimension is a combination of the<br>' +
        'columns of the separate small dimensions into a single<br>' +
        'table. Transaction type and payment type could be<br>' +
        'combined into a single table with two attribute columns:<br>' +
        'Transaction_Type and Payment_Type. The resulting<br>' +
        'TransactionInfo table would only need six rows to hold<br>' +
        'all possible combinations of payment type and transaction<br>' +
        'type. Each fact row would contain a single transaction info<br>' +
        'key that would join to the dimension row that holds the<br>' +
        'correct combination for that fact row.<br>' +
        'NOTE<br>' +
        'In Analysis Services, each of the different<br>' +
        'attributes included in the junk dimension<br>' +
        'becomes a separate attribute hierarchy. As<br>' +
        'in the relational data model, the disparate<br>' +
        '138<br>' +
        'attribute hierarchies would be grouped<br>' +
        'together under one dimension,<br>' +
        'TransactionInfo in our example. You’d<br>' +
        'hide the bottom level of the dimension that<br>' +
        'has the surrogate key representing the<br>' +
        'intersection of the codes.<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about junk dimensions:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 117–119.<br>' +
        '• The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, pages 263–265.<br>' +
        '• Kimballgroup.com: Search for the topic<br>' +
        '“Junk Dimensions” for a relevant article.<br>' +
        'The Three Fact Table Types<br>' +
        'There are three fundamental types of fact tables in the<br>' +
        'DW/BI system: transaction, periodic snapshot, and<br>' +
        'accumulating snapshot. Most of what we have described<br>' +
        'thus far falls into the transaction category. Transaction fact<br>' +
        'tables track each transaction as it occurs at a discrete point<br>' +
        'in time — when the transaction event occurred. The<br>' +
        'periodic snapshot fact table captures cumulative<br>' +
        'performance over specific time intervals. It aggregates<br>' +
        'many of the facts across the time period, providing users<br>' +
        'with a fast way to get totals. Where periodic snapshots are<br>' +
        '139<br>' +
        'taken at specific points in time, after the month-end close,<br>' +
        'for example, the accumulating snapshot is constantly<br>' +
        'updated over time. Accumulating snapshots is particularly<br>' +
        'valuable for combining data across several business<br>' +
        'processes in the value chain. Generally, the design of the<br>' +
        'accumulating snapshot includes several date fields to<br>' +
        'capture the dates when the item in question passes through<br>' +
        'each of the business processes or milestones in the value<br>' +
        'chain. For an orders accumulating snapshot that captures<br>' +
        'metrics about the complete life of an order, these dates<br>' +
        'might include the following:<br>' +
        '• Order date<br>' +
        '• Requested ship date<br>' +
        '• Manufactured date<br>' +
        '• Actual ship date<br>' +
        '• Arrival date<br>' +
        '• Invoice date<br>' +
        '• Payment received date<br>' +
        'The accumulating snapshot provides the status of open<br>' +
        'orders at any point in time and a history of completed<br>' +
        'orders just waiting to be scrutinized for interesting metrics.<br>' +
        'NOTE<br>' +
        'Transaction fact tables are clearly what<br>' +
        'Analysis Services was designed for. Your<br>' +
        'Analysis Services database can<br>' +
        'accommodate periodic and accumulating<br>' +
        'snapshots, but you do need to be careful.<br>' +
        '140<br>' +
        'The problem is not the model, but the<br>' +
        'process for updating the data. Snapshot fact<br>' +
        'tables — particularly accumulating<br>' +
        'snapshots — tend to be updated a lot in the<br>' +
        'ETL process. This is expensive but not<br>' +
        'intolerable in the relational database. It’s<br>' +
        'far more expensive for Analysis Services,<br>' +
        'which doesn’t really support fact table<br>' +
        'updates at all.<br>' +
        'For snapshot facts to work in Analysis<br>' +
        'Services for even moderate-sized data sets,<br>' +
        'you’ll need the Enterprise Edition feature<br>' +
        'that allows cube partitioning or you’ll need<br>' +
        'to reprocess the entire cube on every load.<br>' +
        'RESOURCES<br>' +
        'The following resources offer additional<br>' +
        'information about the three fact table types:<br>' +
        '• The Data Warehouse Toolkit, Second<br>' +
        'Edition, pages 128–130 and 132–135.<br>' +
        '• The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, pages 273–276.<br>' +
        '• Kimballgroup.com: Enter the search string<br>' +
        '“Snapshot Fact Table” for articles on the<br>' +
        'different fact table types.<br>' +
        'Aggregates<br>' +
        '141<br>' +
        'Aggregates are precalculated summary tables that serve the<br>' +
        'primary purpose of improving performance. If the database<br>' +
        'engine could instantly roll the data up to the highest level,<br>' +
        'you wouldn’t need aggregate tables. In fact, precalculating<br>' +
        'aggregates is one of the two reasons for the existence of<br>' +
        'OLAP engines such as Analysis Services (the other reason<br>' +
        'being more advanced analytic capabilities). SQL Server<br>' +
        'Analysis Services can create and manage aggregate tables<br>' +
        'in the relational platform (called relational OLAP or<br>' +
        'ROLAP) or in the OLAP engine. The decision to create<br>' +
        'aggregate tables in the relational versus OLAP engines is a<br>' +
        'tradeoff. If the aggregates are stored in Analysis Services’<br>' +
        'format, access to the data is limited to tools that generate<br>' +
        'MDX. If the aggregates are stored in the relational<br>' +
        'database, they can be accessed by tools that generate SQL.<br>' +
        'If your front-end tool is adept at generating MDX, using<br>' +
        'Analysis Services to manage aggregates has significant<br>' +
        'advantages. If you must support relational access,<br>' +
        'especially ad hoc access, you need to create and manage<br>' +
        'any aggregate tables needed for performance (using<br>' +
        'indexed views if possible), along with the associated<br>' +
        'aggregate dimensions described earlier.<br>' +
        'NOTE<br>' +
        'Kimball Group refers to the summary<br>' +
        'tables as aggregates and the summarization<br>' +
        'process as aggregation. SQL Server<br>' +
        'Analysis Services uses essentially the<br>' +
        'opposite terminology; in that environment,<br>' +
        '142<br>' +
        'an aggregation is the summary table, and<br>' +
        'aggregates are the rules that define how the<br>' +
        'data is rolled up.<br>' +
        'In this section, we have covered several of the common<br>' +
        'design challenges you will typically run up against in<br>' +
        'developing a dimensional model. This is not an exhaustive<br>' +
        'list, but it should be enough to help you understand the<br>' +
        'modeling process. If you are charged with the role of data<br>' +
        'modeler and this is your first dimensional modeling effort,<br>' +
        'we again strongly encourage you to continue your<br>' +
        'education by reading The Data Warehouse Toolkit, Second<br>' +
        'Edition.<br>' +
        'The Dimensional Modeling Process<br>' +
        'With a basic understanding of dimensional modeling and<br>' +
        'the core techniques under your belt, this section shifts<br>' +
        'focus to describe the process of building a dimensional<br>' +
        'model. Creating a dimensional model is a highly iterative<br>' +
        'and dynamic process. After a few preparation steps and<br>' +
        'some exploratory data profiling, the design process begins<br>' +
        'with an initial graphical model pulled from the bus matrix<br>' +
        'and presented at the entity level. This model is critically<br>' +
        'scrutinized in a high-level design session that also yields<br>' +
        'an initial list of attributes for each table and a list of issues<br>' +
        'requiring additional investigation. Once the high-level<br>' +
        'model is in place, the detailed modeling process takes the<br>' +
        'model table by table and drills down into the definitions,<br>' +
        'sources, relationships, data quality problems, and<br>' +
        'transformations required to populate the model. The last<br>' +
        '143<br>' +
        'phase of the modeling process involves reviewing and<br>' +
        'validating the model with several interested parties. The<br>' +
        'primary goals of this process are to create a model that<br>' +
        'meets the business requirements, provides the ETL team<br>' +
        'with a solid starting point and clear direction, and verifies<br>' +
        'that the data is available to fill out the model.<br>' +
        'Designing a dimensional model is a series of successive<br>' +
        'approximations, where you create more detailed and robust<br>' +
        'models based on your growing understanding of the source<br>' +
        'systems, the business needs, and the associated<br>' +
        'transformations. Often we’ve made changes that sounded<br>' +
        'clever at the time, but ended up changing them back in a<br>' +
        'later pass because they didn’t work, either from the user or<br>' +
        'technical perspective, or both! This series of iterations<br>' +
        'usually stops once the model clearly meets the business<br>' +
        'needs in a flexible and extensible way. This iterative<br>' +
        'process typically takes a few weeks for a single business<br>' +
        'process dimensional model, but can take longer depending<br>' +
        'on the complexity of the business process, availability of<br>' +
        'knowledgeable participants, existence of well-documented<br>' +
        'detailed business requirements, and the number of<br>' +
        'pre-existing reusable dimension tables.<br>' +
        'RESOURCES<br>' +
        'We dedicated an entire chapter to the<br>' +
        'dimensional modeling process in the<br>' +
        'Lifecycle Toolkit. This is required reading<br>' +
        'for the lead dimensional modeler: The Data<br>' +
        '144<br>' +
        'Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition, Chapter 7 (pages 287–325).<br>' +
        'NOTE<br>' +
        'Creating the dimensional model is where<br>' +
        'an expert can help. If your team is new to<br>' +
        'the dimensional modeling process, bringing<br>' +
        'in someone who has extensive experience<br>' +
        'creating dimensional models can save you<br>' +
        'weeks of time, pain, and suffering.<br>' +
        'However, do not let the consultant take<br>' +
        'over the process — make them lead the<br>' +
        'team and facilitate the effort so everyone<br>' +
        'can participate in the design process and<br>' +
        'learn why various design decisions were<br>' +
        'made. Don’t let the consultant disappear<br>' +
        'for a few days or a week or two and bring<br>' +
        'you back a completed model. The goal is to<br>' +
        'learn what goes into a dimensional model<br>' +
        'and why, so you know how to maintain and<br>' +
        'improve your model over time. More<br>' +
        'important, you need to know how to do the<br>' +
        'next one, so you don’t have to pay<br>' +
        'someone to do this for you forever.<br>' +
        'Preparation<br>' +
        '145<br>' +
        'It’s a good idea to do a little preparation before you pull<br>' +
        'everyone into a conference room to create the dimensional<br>' +
        'model. First, you need to figure out who is going to be<br>' +
        'involved and what they are supposed to do. Next, the core<br>' +
        'participants should revisit the business requirements and<br>' +
        'the data architecture strategy. Meanwhile, the lead modeler<br>' +
        'needs to get the modeling tools in place and prepare a set<br>' +
        'of naming conventions. Let’s discuss each of these<br>' +
        'preparation steps in a bit more detail.<br>' +
        'Identify Roles and Participants<br>' +
        'As Table 2-1 shows, several roles are involved in the<br>' +
        'modeling step, but a core modeling team of two or three<br>' +
        'people usually does most of the work. The core modeling<br>' +
        'team includes a data modeler with a strong technical<br>' +
        'background and solid experience with the source systems,<br>' +
        'and a business analyst who brings a solid understanding of<br>' +
        'how the data is used in the analysis process and how it<br>' +
        'could be made more useful or accessible. The core<br>' +
        'modeling team often includes someone from the ETL team<br>' +
        'with extensive source systems development experience<br>' +
        'and an interest in learning. The data modeler has overall<br>' +
        'responsibility for creating the dimensional model.<br>' +
        'Table 2-1: Major participants in creating the dimensional<br>' +
        'model<br>' +
        'Participant Purpose/Role in Modeling Process<br>' +
        'Data modeler Primary responsibility<br>' +
        'Business analyst Analysis and source expert, business definitions<br>' +
        'Data steward Drive agreement on enterprise names, definitions,<br>' +
        'rules, and data quality<br>' +
        '146<br>' +
        'Participant Purpose/Role in Modeling Process<br>' +
        'Business power user Describe and refine data sources and business rules<br>' +
        'from a user perspective<br>' +
        'Source system<br>' +
        'developer Source expert, business rules and data quality<br>' +
        'DBA Design guidance, early learning<br>' +
        'ETL designer Early learning<br>' +
        'ETL developer Early learning<br>' +
        'Steering/Governance<br>' +
        'Committee Naming, business definitions, model validation<br>' +
        'The core modeling team works closely with source system<br>' +
        'developers who can explain the contents, meaning,<br>' +
        'business rules, timing, and other intricacies of the<br>' +
        'particular source system involved in the dimensional<br>' +
        'model.<br>' +
        'We also suggest you include the DBA who will be<br>' +
        'implementing the physical database and the ETL designer<br>' +
        'and developer in the modeling process. These folks do so<br>' +
        'much better if they understand the business rationale for<br>' +
        'the model. This is especially true of the DBA who may<br>' +
        'have a transaction system background and does not<br>' +
        'understand the purpose of dimensional modeling.<br>' +
        'There are generally a few additional participants as well.<br>' +
        'Although you risk slowing down the design process a bit<br>' +
        'by including more people, the benefits of a more robust<br>' +
        'design and engaged partners are almost always worth the<br>' +
        'price.<br>' +
        'Revisit the Requirements<br>' +
        '147<br>' +
        'After the modeling team and the data strategy are in place,<br>' +
        'the team’s first step will be to pull out the detailed<br>' +
        'requirements documentation and carefully comb through<br>' +
        'it. If you skipped to this chapter with the idea that you<br>' +
        'could avoid all that business stuff, sorry, but you need to<br>' +
        'go back and do the work. The modeling team must<br>' +
        'understand the business problems the users are trying to<br>' +
        'solve and the kinds of analyses they perform to solve them.<br>' +
        'It’s the team’s job to translate those requirements into a<br>' +
        'flexible dimensional model that can support broad classes<br>' +
        'of analysis, not just re-create specific reports. This isn’t an<br>' +
        'easy job. The data modeler must be able to function at an<br>' +
        'advanced level in both the business and technical areas. In<br>' +
        'fact, much of the initial dimensional modeling effort<br>' +
        'actually begins as part of the requirements definition<br>' +
        'process.<br>' +
        'The detailed business requirements document discussed in<br>' +
        'Chapter 1 has several sections that describe the<br>' +
        'high-priority business processes in detail. That document<br>' +
        'identifies analytic requirements supported by each<br>' +
        'high-priority business process. It describes the broad<br>' +
        'classes of questions and problems that management and<br>' +
        'business analysts have been trying (or would like to try) to<br>' +
        'answer. The requirements document should include a list<br>' +
        'of data elements, example questions, and even a list of<br>' +
        'desired reports that would help answer the analytic<br>' +
        'questions. These all play a central role when it comes time<br>' +
        'to defining the dimensional model. Your dimensional<br>' +
        'model must not only be able to answer these specific<br>' +
        'questions easily when it is finished, but also allow business<br>' +
        'users to explore new opportunities.<br>' +
        '148<br>' +
        'Data-Driven Models versus Business<br>' +
        'Requirements–Driven Models<br>' +
        'An experienced dimensional modeler can build a<br>' +
        'reasonable dimensional model based on the source<br>' +
        'system data structures. However, this model will<br>' +
        'inevitably fall short of meeting the business needs<br>' +
        'in many small but substantive ways. These little<br>' +
        'shortcomings add up to a weak dimensional model.<br>' +
        'We recently worked with a client whose<br>' +
        'requirements gathering had revealed a need for a<br>' +
        'field called PricingCategory. It turns out there<br>' +
        'is no such thing as PricingCategory in the<br>' +
        'source system. However, there was a Rate_Code<br>' +
        'field that in combination with some other status<br>' +
        'flags was the basis of what the business users<br>' +
        'called PricingCategory. A dimensional model<br>' +
        'based literally on the source system would have<br>' +
        'included Rate_Code (maybe) and left the users to<br>' +
        're-create the business rules for determining<br>' +
        'PricingCategory every time they built a query<br>' +
        'that needed the field.<br>' +
        'In short, you must understand the business<br>' +
        'requirements in detail before you dive into the task<br>' +
        'of designing the dimensional model. Your designs<br>' +
        'will start with the source content but typically need<br>' +
        'to be augmented with additional fields defined by<br>' +
        'your organization’s business rules.<br>' +
        '149<br>' +
        'NOTE<br>' +
        'It is too easy for technical folks to build a<br>' +
        'data model that meets specific report<br>' +
        'requirements and only those requirements:<br>' +
        'to build a reporting system, not a DW/BI<br>' +
        'system. For that reason, we prefer to<br>' +
        'merely glance at the report specifications in<br>' +
        'the early phases of the design process.<br>' +
        'Once the data model is largely developed,<br>' +
        'look at the report specifications in depth to<br>' +
        'ensure the design can easily support those<br>' +
        'reports.<br>' +
        'Understand the Data Architecture Strategy<br>' +
        'One of the big DW/BI architecture discussions centers on<br>' +
        'the issue of how you structure and manage your data. The<br>' +
        'questions are: What data do you keep, where do you keep<br>' +
        'it within your technical architecture, and how is it<br>' +
        'structured. Our standard approach is to build a set of<br>' +
        'dimensional models in the relational database platform that<br>' +
        'hold the lowest level of detail. This atomic-level relational<br>' +
        'database enables consistent data definitions, business rules,<br>' +
        'and tracking of history. It supports the integration of data<br>' +
        'from multiple sources across the enterprise and from<br>' +
        'external sources such as vendors, customers, and<br>' +
        'third-party data providers. This atomic level also provides<br>' +
        'a means for including value-added data that is important to<br>' +
        'the analytic process but that currently exists only in<br>' +
        'spreadsheets on users’ desktops, or embedded deep within<br>' +
        '150<br>' +
        'a report definition. The atomic-level dimensional data<br>' +
        'warehouse is designed to be queried by users and built to<br>' +
        'meet their analytic needs and performance expectations.<br>' +
        'As we describe in Chapter 3, this is generally true even<br>' +
        'though we encourage you to build atomic-level Analysis<br>' +
        'Services databases from the atomic-level relational<br>' +
        'warehouse and use Analysis Services as the primary query<br>' +
        'platform.<br>' +
        'We believe both the relational and Analysis Services<br>' +
        'databases are key components of a successful enterprise<br>' +
        'Microsoft DW/BI system. The goal of this chapter, and of<br>' +
        'all the chapters that focus on the data track of the<br>' +
        'Lifecycle, is to make sure we build the system in a way<br>' +
        'that leverages the strengths of both tools. The good news is<br>' +
        'that a solid, well-designed dimensional model is the best<br>' +
        'foundation for both platforms.<br>' +
        'Set Up the Modeling Environment<br>' +
        'It helps to get a few tools in place before you dive into the<br>' +
        'modeling process. We often start the modeling process<br>' +
        'using a spreadsheet as our initial tool because it allows us<br>' +
        'to make changes easily. This spreadsheet captures the key<br>' +
        'elements of the logical model plus many of the physical<br>' +
        'attributes you’ll need later. It also gives you a place to<br>' +
        'begin capturing some of the ETL information, such as<br>' +
        'source system table and column(s), and a brief description<br>' +
        'of the extract and transformation rules. Finally, it includes<br>' +
        'the initial set of business metadata in the form of the<br>' +
        'names, descriptions, example values, data quality issues,<br>' +
        'and comments. In our ETL Toolkit book, we call this the<br>' +
        '“logical data map.” Regardless of the name, the central<br>' +
        '151<br>' +
        'idea is to describe each attribute in the final target and<br>' +
        'associate them all with their original sources and required<br>' +
        'transformations.<br>' +
        'DOWNLOADS<br>' +
        'You can get a copy of the modeling<br>' +
        'spreadsheet we use at the book’s web site.<br>' +
        'It includes a simple macro to generate the<br>' +
        'DDL to create the target tables in SQL<br>' +
        'Server. You can then use the<br>' +
        'reverse-engineering capabilities of your<br>' +
        'modeling tool to pull the model out of the<br>' +
        'database. Our spreadsheet also writes the<br>' +
        'metadata info, such as the column<br>' +
        'descriptions, transformations, and<br>' +
        'comments columns, into extended<br>' +
        'properties fields in the database system<br>' +
        'tables. Afterward, you can move this<br>' +
        'information into the metadata schema.<br>' +
        'Once the model gets fairly firm, typically after a few<br>' +
        'weeks, you can convert to your standard modeling tool.<br>' +
        'Most of the popular modeling tools (such as ERwin,<br>' +
        'PowerDesigner, and E/R Studio) allow you to lay out the<br>' +
        'logical model and capture physical and logical names,<br>' +
        'descriptions, and relationships.<br>' +
        'Once the design is considered complete (for the first<br>' +
        'round, anyway), the modeling tools can help the DBA<br>' +
        'forward engineer the model into the database, including<br>' +
        '152<br>' +
        'creating the tables, indexes, partitioning, views, and other<br>' +
        'physical elements of the database. Chapter 4 discusses<br>' +
        'these physical design issues. If you start using your<br>' +
        'modeling tool early on, things will be easier later.<br>' +
        'Establish Naming Conventions<br>' +
        'Naming conventions are the rules you use to consistently<br>' +
        'name the objects in your dimensional model, and<br>' +
        'ultimately in the physical database. Spending time<br>' +
        'determining your naming conventions is one of those<br>' +
        'irritating tasks that feels like make-work. But it is<br>' +
        'definitely worth it in the long run.<br>' +
        'Fortunately, it doesn’t have to become your life’s work.<br>' +
        'There are ways to abbreviate the process. First, don’t start<br>' +
        'from scratch. Use whatever naming conventions your<br>' +
        'organization has in place. Almost all large organizations<br>' +
        'have a group of data modelers and/or DBAs somewhere —<br>' +
        'they might be called data administration, or data<br>' +
        'management. Somewhere in that group is the holder of the<br>' +
        'organization’s official naming conventions. Given that you<br>' +
        'are reading the dimensional modeling chapter, you are<br>' +
        'likely either one of the folks in this group, or very close to<br>' +
        'them. Find the document and see if you can make the<br>' +
        'existing conventions work for the DW/BI system. Existing<br>' +
        'naming conventions don’t always work because sometimes<br>' +
        'they are not oriented toward user-friendly, descriptive<br>' +
        'names.<br>' +
        'As you develop the DW/BI naming conventions,<br>' +
        'remember that table and column names will be visible to<br>' +
        'the user community. They’re an important component of<br>' +
        '153<br>' +
        'the ad hoc user experience, and they show up in report<br>' +
        'titles and headers. Names must be descriptive, but not so<br>' +
        'long that users and report developers will be tempted to<br>' +
        'rename them every time they use the data element.<br>' +
        'DOWNLOADS<br>' +
        'If you’re in a smaller organization, don’t<br>' +
        'despair. The web site for this book has an<br>' +
        'example of naming conventions along with<br>' +
        'links to a few other examples on the<br>' +
        'internet. The point is, you don’t need to<br>' +
        'start from scratch.<br>' +
        'Data Profiling and Research<br>' +
        'Once you’ve done your prep work, you can get started on<br>' +
        'the model. Throughout the modeling process, the modeler<br>' +
        'needs to dig into the underlayers of the data to learn about<br>' +
        'its structure, content, relationships, and derivation rules.<br>' +
        'You need to verify that the data exists (or can be created),<br>' +
        'that it’s in a usable state, or at least its flaws are<br>' +
        'manageable, and that you understand what it will take to<br>' +
        'convert it into the dimensional model form. You don’t<br>' +
        'have to find every piece of bad data or completely<br>' +
        'document every transformation at this point. Leave a little<br>' +
        'bit for the ETL folks to do.<br>' +
        'This is not your first trip into data exploration. The<br>' +
        'requirements definition process in Chapter 1 included<br>' +
        'initial data profiling and data audit tasks to aid in the<br>' +
        '154<br>' +
        'assessment of feasibility. The findings from those tasks are<br>' +
        'the starting point for this one.<br>' +
        'Although data profiling is listed here as part of the<br>' +
        'preparation step, it’s an ongoing process. As you work<br>' +
        'through the model table by table, filling in the list of<br>' +
        'attributes, you’ll return to these data profiling and research<br>' +
        'tasks many times to resolve issues and clearly define each<br>' +
        'attribute.<br>' +
        'There are several useful sources for detailed information<br>' +
        'about an organization’s data, including the source system,<br>' +
        'data experts, and existing reporting systems.<br>' +
        'Data Profiling and Source System Exploration<br>' +
        'The data modeler usually has the benefit of both first-hand<br>' +
        'observation and documentation from the source system<br>' +
        'under investigation. Unfortunately, these two don’t always<br>' +
        'line up. Gather and carefully review whatever<br>' +
        'documentation is available for the source systems. This<br>' +
        'might include data models, file definitions, record layouts,<br>' +
        'written documentation, and source system programs. More<br>' +
        'advanced source systems may have their own metadata<br>' +
        'repositories with all this information already integrated for<br>' +
        'your convenience. Don’t count on this.<br>' +
        'Perusing the source system data itself usually provides a<br>' +
        'quick jolt of reality. First, what you see typically does not<br>' +
        'match the documentation you carefully gathered and<br>' +
        'reviewed. Second, it usually is more difficult to unravel<br>' +
        'than you would hope. The older the system, the more time<br>' +
        'it’s had to evolve. This evolution, usually driven by<br>' +
        '155<br>' +
        'short-term business needs, often takes the form of one or<br>' +
        'more of these standard fixes: substituted fields where data<br>' +
        'is stored in a field with a different name; overloaded fields<br>' +
        'where multiple values are stored in a single field; variable<br>' +
        'definition fields where the meaning of the content changes<br>' +
        'depending on its context; and freeform entry fields where<br>' +
        'there are no controls on the values being entered.<br>' +
        'This is only a short list — there are plenty of other<br>' +
        'problems to be discovered. You can discover many of the<br>' +
        'content, relationship, and quality problems firsthand<br>' +
        'through a process known as data profiling or data<br>' +
        'auditing. Data profiling is about using query and reporting<br>' +
        'tools to get a sense for the content of the system under<br>' +
        'investigation. Data profiling can be as simple as writing<br>' +
        'some SQL SELECT statements with COUNTs and<br>' +
        'DISTINCTs. An experienced modeler with a decent query<br>' +
        'tool and a source system data model can quickly develop a<br>' +
        'good understanding of the nature of the source system data<br>' +
        'required for a given business process dimensional model.<br>' +
        'There are also tools to help with the data profiling process.<br>' +
        'Integration Services has a data profiling task that can be<br>' +
        'configured to write out an XML file of profiling statistics<br>' +
        'including counts, length and value distributions, candidate<br>' +
        'keys, and column patterns. To use the data profiling task,<br>' +
        'you must create an SSIS package, include the task, and<br>' +
        'configure it to generate the statistics you would like to<br>' +
        'view against the tables you are profiling, and execute the<br>' +
        'task. You then open the Data Profile Viewer and navigate<br>' +
        'to the resulting XML output file.<br>' +
        '156<br>' +
        'If your source data is in SQL Server, we have created a set<br>' +
        'of reports that serve as a simple data profiling tool. Figure<br>' +
        '2-7 shows a data profile report for the<br>' +
        'Production.Product table from the Adventure Works<br>' +
        'Cycles transaction database. It gives a good sense for what<br>' +
        'the data in the Product table looks like. Starting at the top<br>' +
        'of the report, we see that there are 504 rows in the table.<br>' +
        'There are 504 distinct ProductIDs, Names, and<br>' +
        'ProductNumbers. It’s interesting to note the<br>' +
        'ProductNumber isn’t really a number at all. Moving<br>' +
        'down the list, only about half the products have a Color,<br>' +
        'the rest are NULL, and there are only nine distinct values<br>' +
        'for Color. A commercial tool would give you more<br>' +
        'information at the table level, and more sophisticated<br>' +
        'results at the detail level. But even this simple report gives<br>' +
        'you a good start on understanding the contents of the table.<br>' +
        'DOWNLOADS<br>' +
        'The Reporting Services project that was<br>' +
        'used to create the data profiling report<br>' +
        'shown in Figure 2-7 is available at the<br>' +
        'book’s web site. They work on SQL Server<br>' +
        'tables only, but they do give you an easy<br>' +
        'place to start.<br>' +
        'Your goal is to make sure the data exists to support the<br>' +
        'dimensional model and identify business rules and<br>' +
        'relationships that will have an impact on the model. Write<br>' +
        'down any interesting complexities you uncover so the ETL<br>' +
        'folks won’t have to re-discover them. You may not<br>' +
        '157<br>' +
        'understand the exact business rules and derivation<br>' +
        'formulas as part of the modeling phase, but the modeling<br>' +
        'team should all agree that what you are proposing is<br>' +
        'reasonable, or at least possible.<br>' +
        'Figure 2-7: A simple data profile report for the Adventure<br>' +
        'Works OLTP Product table<br>' +
        'Data profiling helps you understand the complexities of<br>' +
        'the source system data, but it should not be your only<br>' +
        'source of source information, so to speak. You need to<br>' +
        'develop good working relationships with the source system<br>' +
        'developers and DBAs. These folks often know the<br>' +
        'business rules and quality problems you could never find<br>' +
        'with a tool, such as the fact that a middle initial of $ means<br>' +
        'the customer is difficult to work with. Some of the expert<br>' +
        'business users will also be able to shed additional light on<br>' +
        'the data. They have often worked with it for years and<br>' +
        'know what transformations are needed to make it line up<br>' +
        '158<br>' +
        'with the official reports. The official reports themselves<br>' +
        'are also a good source of understanding. Look through the<br>' +
        'queries and report calculations to see what business rules<br>' +
        'they include. Even better, find the main developer of the<br>' +
        'existing reporting system and get him or her involved in<br>' +
        'the modeling process.<br>' +
        'Building Dimensional Models<br>' +
        'After a round of data exploration, the process of building<br>' +
        'dimensional models typically moves through three phases.<br>' +
        'The first is a high-level dimensional model design session<br>' +
        'that defines the boundaries of the business process<br>' +
        'dimensional model. The second phase is detailed model<br>' +
        'development that involves filling in the attributes table by<br>' +
        'table and resolving any issues or uncertainties. The third<br>' +
        'phase is a series of model review, redesign, and validation<br>' +
        'steps.<br>' +
        'High-Level Dimensional Model Design Session<br>' +
        'The first dimensional modeling design session is meant to<br>' +
        'put several major stakes in the ground in terms of the basic<br>' +
        'structure and content of the dimensional model. This<br>' +
        'session is facilitated by the lead data modeler, and involves<br>' +
        'the core modeling team and any interested participants<br>' +
        'from the source system group and the ETL group. It can<br>' +
        'take a day or more to work through the initial model, so set<br>' +
        'expectations accordingly.<br>' +
        '159<br>' +
        'NOTE<br>' +
        'It is extremely valuable to learn from the<br>' +
        'experience of those in your organization<br>' +
        'who developed earlier versions of reporting<br>' +
        'systems or data warehouses, but be very<br>' +
        'careful not to frighten or offend them. They<br>' +
        'may be threatened by the possibility that<br>' +
        'they will no longer be needed. They might<br>' +
        'be angry or offended that you, not they, are<br>' +
        'building the DW/BI system. This is where<br>' +
        'your interpersonal skills will save you.<br>' +
        'Work to include them as part of the team<br>' +
        'early on. Keep them informed as the<br>' +
        'project moves forward. Involve them in the<br>' +
        'design of the database. Teach them how to<br>' +
        'use the reporting tools and include them in<br>' +
        'the development of new reports and<br>' +
        'applications from the DW/BI system. Let<br>' +
        'them (and their boss) know how much you<br>' +
        'appreciate their help.<br>' +
        'The first part of this session involves creating the<br>' +
        'high-level dimensional model — a graphical<br>' +
        'representation of the dimension, fact, and utility tables<br>' +
        'involved in representing the business process. As we<br>' +
        'describe in the next section, you should follow a four-step<br>' +
        'process for creating this high-level dimensional model.<br>' +
        'The second part of the session is creating the initial list of<br>' +
        'attributes for each dimension. The three deliverables are:<br>' +
        '160<br>' +
        '1) the high-level graphical model; 2) the initial attribute<br>' +
        'list; and 3) the initial issues list.<br>' +
        'Creating the High-Level Dimensional Model: The<br>' +
        'Four-Step Modeling Process<br>' +
        'The initial task in the design session is to create a<br>' +
        'high-level dimensional model for the top priority business<br>' +
        'process. The high-level dimensional model is a data model<br>' +
        'at the entity level. Draft your starting point model straight<br>' +
        'from the bus matrix. You may also include any utility<br>' +
        'tables, such as lookup tables or user hierarchies, but<br>' +
        'usually these don’t surface until later in the process. The<br>' +
        'design process generally flows through four steps:<br>' +
        '• Step 1: Identify the business process. In other words, what<br>' +
        'row on the bus matrix should you start with? In most cases,<br>' +
        'you will already know the business process as the outcome<br>' +
        'of the prioritization process. It’s the row on the bus matrix<br>' +
        'associated with the top priority opportunity.<br>' +
        '• Step 2: Declare the grain. The grain is the level of detail<br>' +
        'captured in the fact table. Your goal is to describe the<br>' +
        'meaning of a single fact row. Filling in the statement, “The<br>' +
        'fact table has one row per X” where “X” is the business<br>' +
        'event, is a good way to get started. The answer might be one<br>' +
        'row per order line item, one row per customer call, or one<br>' +
        'row per employee status change. This is a subtle, but<br>' +
        'important step. If you aren’t clear on the grain, you may end<br>' +
        'up with a fact table that does not capture data at the atomic<br>' +
        'level, and is therefore less flexible. Make sure that the grain<br>' +
        'declaration is not a list of dimensions. That comes in the<br>' +
        'next step. Grain is a business event.<br>' +
        '• Step 3: Choose the dimensions. Now ask yourself the<br>' +
        'question, what objects participate in the target business<br>' +
        'process at the declared grain? Most of these will come<br>' +
        'directly from your understanding of the business process and<br>' +
        '161<br>' +
        'the bus matrix should give you a starting point. As you list<br>' +
        'dimensions, you will also begin listing all the individual<br>' +
        'attributes associated with each dimension. It helps to refer to<br>' +
        'user information requests and source system models to<br>' +
        'verify your choice of dimensions and their attributes.<br>' +
        '• Step 4: Choose the facts. The facts are the measures of the<br>' +
        'business process in question. There are usually a set of facts<br>' +
        'that are directly measured by the source system that supports<br>' +
        'the business process, and a set of facts that are derived from<br>' +
        'the base facts. Make sure the facts are true to the grain.<br>' +
        'The High-Level Graphical Model<br>' +
        'Graphically summarize the initial design session in a<br>' +
        'deliverable called the high-level graphical model (or the<br>' +
        'bubble chart, for short). The model shown in Figure 2-8 is<br>' +
        'an example of a starting point dimensional model for<br>' +
        'Adventure Works Cycles’ orders business process based<br>' +
        'on the bus matrix from Chapter 1. We improve on this<br>' +
        'model in the next section.<br>' +
        'Figure 2-8: Initial Adventure Works Cycles high-level<br>' +
        'orders dimensional model<br>' +
        '162<br>' +
        'Identifying Dimension Attributes and Fact Measures<br>' +
        'The second part of the initial design session involves<br>' +
        'filling in each table with a robust attribute list. List all the<br>' +
        'relevant attributes needed by the business, grouped<br>' +
        'according to the dimension or fact table to which they<br>' +
        'belong. If you’re using the dimensional modeling<br>' +
        'spreadsheet from the book’s web site to keep track of your<br>' +
        'attribute lists, create one worksheet per table in the model<br>' +
        'and fill in its attribute list.<br>' +
        '163<br>' +
        'The team will identify a large number of attributes for each<br>' +
        'dimension coming from a wide range of sources and a list<br>' +
        'of base level and computed measures for the fact tables.<br>' +
        'List them out brainstorm-style, grouping them by the<br>' +
        'dimension or fact table to which they belong. Don’t get<br>' +
        'caught up in naming or derivation yet; just pick a name<br>' +
        'and make a note of the alternative names and the<br>' +
        'controversy on the issues list, one of the deliverables from<br>' +
        'the high level modeling session. The issues list comes to<br>' +
        'life in the high level model design session, but it’s<br>' +
        'constantly updated throughout the detailed modeling<br>' +
        'process. It’s the best way we’ve found to remember all the<br>' +
        'little details about the problems we encountered and how<br>' +
        'we decided to resolve them.<br>' +
        'Assign someone the role of list keeper in every meeting.<br>' +
        'This person notes every data-related issue that comes up<br>' +
        'during the meeting and marks off previous issues that have<br>' +
        'been resolved. It helps to save time at the end of each<br>' +
        'meeting to review and validate the new entries and their<br>' +
        'assignments. The data modeler can be the keeper of the<br>' +
        'issues list, but we’ve often seen it fall into the hands of the<br>' +
        'project manager. This is in large part because keeping the<br>' +
        'list updated and encouraging progress on resolving issues<br>' +
        'are usually strengths of a good project manager.<br>' +
        'The results of this initial design session, the high level<br>' +
        'dimensional model, the attributes list, and the issues list,<br>' +
        'are the foundation for the logical and physical business<br>' +
        'process dimensional models. At this point, you’ve<br>' +
        'identified the dimension and fact tables, each with a list of<br>' +
        'its associated attributes or measures. This model, along<br>' +
        '164<br>' +
        'with the issues list, gives the data modeling team enough<br>' +
        'guidelines to carry the process into the next level of detail.<br>' +
        'Developing the Detailed Dimensional Model<br>' +
        'Once this high level dimensional model is completed, the<br>' +
        'hard work of filling in the dimension attributes and<br>' +
        'hierarchies, identifying and validating data sources, and<br>' +
        'defining names begins. At this point, the process gets<br>' +
        'specific to each individual organization. The last part of<br>' +
        'this chapter explores the process of identifying dimension<br>' +
        'attributes and facts, and validating the model in the context<br>' +
        'of the Adventure Works Cycles business.<br>' +
        'Detailed dimensional model development is primarily<br>' +
        'about filling in all the missing information in the<br>' +
        'dimensional model and testing it against the business<br>' +
        'requirements. This process is ultimately about defining the<br>' +
        'contents of the DW/BI system, table by table and column<br>' +
        'by column. Because the DW/BI system is an enterprise<br>' +
        'resource, these definitions must work for the entire<br>' +
        'enterprise. The data definition task is a business data<br>' +
        'governance task — the BI team and the data steward<br>' +
        'usually drive the process, but the business folks must<br>' +
        'determine and approve the standard names and definitions.<br>' +
        'This will take some time, but it’s an investment that will<br>' +
        'provide huge returns in terms of users’ ability to<br>' +
        'understand and willingness to accept the dimensional<br>' +
        'model. This is another one of those organizational<br>' +
        'processes that can be uncomfortable for the technically<br>' +
        'minded.<br>' +
        '165<br>' +
        'One of the most important decisions from this detailed<br>' +
        'modeling process is the assignment of the type 1 or type 2<br>' +
        'change tracking technique to each column in each<br>' +
        'dimension. This is a business decision and relates to the<br>' +
        'need for accurate historical analysis. If there is now, or<br>' +
        'may someday be, a need for predictive analytics using a<br>' +
        'given attribute, you must use type 2 change tracking. In a<br>' +
        'similar vein, if there is any need to report historical data as<br>' +
        'it was when it occurred, for example, sales by sales region<br>' +
        'as of December 31st last year, you must use type 2 change<br>' +
        'tracking.<br>' +
        'The team should meet on a regular basis, perhaps daily or<br>' +
        'every other day, to discuss the proposed alternatives and<br>' +
        'make decisions on open issues. Use these meetings to<br>' +
        'critically explore the progress, review any<br>' +
        'recommendations, and update the issues list. Focus these<br>' +
        'meetings on one or two tables at a time — too many and<br>' +
        'the meeting starts to get bogged down.<br>' +
        'Address all the data quality issues you find in a separate<br>' +
        'meeting (or two) with key business and source systems<br>' +
        'people. The quality experts tell us the only way to truly<br>' +
        'solve a quality problem is to fix it at its source. If you “fix”<br>' +
        'these data quality problems in the ETL system, you will<br>' +
        'reduce the incentive for the organization to fix them in the<br>' +
        'source systems. The reality is, the source systems team<br>' +
        'usually does not have resources or even the necessary<br>' +
        'information to fix all of the problems, or even just the most<br>' +
        'important ones. Therefore, you will end up taking on the<br>' +
        'task of fixing some of these problems in the ETL system<br>' +
        'until the business recognizes the importance of data quality<br>' +
        'and allocates resources to correct the source system. The<br>' +
        '166<br>' +
        'earlier you discuss data quality issues, the better, because<br>' +
        'many data quality problems end up being roadblocks, or at<br>' +
        'least add unexpected work to the ETL process.<br>' +
        'DOWNLOADS<br>' +
        'If you aren’t already using it, try the<br>' +
        'modeling spreadsheet mentioned in the<br>' +
        '“Preparation” section of this chapter. It<br>' +
        'provides a place to capture most of the<br>' +
        'important descriptive information about<br>' +
        'each table and attribute. It allows the<br>' +
        'modeler to quickly move or copy attributes<br>' +
        'around the model as needed. It is a<br>' +
        'spreadsheet, after all.<br>' +
        'The model will go through some major shifts during this<br>' +
        'phase. You’ll identify additional attributes, along with new<br>' +
        'dimensions and facts. If this is the primary focus of the<br>' +
        'dimensional modeling team, the model should begin to<br>' +
        'settle down with a week or two of intensive work.<br>' +
        'RESOURCES<br>' +
        'See the following resources for additional<br>' +
        'information about developing the detailed<br>' +
        'model:<br>' +
        '167<br>' +
        '• The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, Chapter 7, “Designing the<br>' +
        'Dimensional Model.”<br>' +
        '• Kimballgroup.com: Search for the topic<br>' +
        '“Naming Game” for an article on the<br>' +
        'process of driving organizational<br>' +
        'agreement on attribute names. It can also<br>' +
        'be found in the Kimball Reader on page<br>' +
        '220.<br>' +
        'Testing and Refining the Model<br>' +
        'Once the model is fairly stable, step back and test it against<br>' +
        'the business requirements. The requirements are an<br>' +
        'integral part of the model development process, but a<br>' +
        'separate test step helps you think at a more practical level.<br>' +
        'Approach the test by asking the question “How would I<br>' +
        'actually get this information out of the model?”<br>' +
        'The requirements document should have a bundle of test<br>' +
        'materials, including a candidate list of structured reports,<br>' +
        'example user reports, and ad hoc or future-oriented<br>' +
        'questions people would like to investigate. Pull these out<br>' +
        'and go through them one by one. For each request, decide<br>' +
        'how it could be answered (we often think up the SQL it<br>' +
        'would take) and assign it an effort score: low, medium, or<br>' +
        'high. A low effort query would be a simple SQL SELECT<br>' +
        'statement — no sub-selects or case statements or unions —<br>' +
        'that would be easy to construct in any desktop query tool.<br>' +
        'At the end of the test, most of the questions should fall into<br>' +
        'the low effort category — shoot for 75 percent.<br>' +
        'Invariably, this testing process leads to several refinements<br>' +
        'in the model. You may identify missing attributes or<br>' +
        '168<br>' +
        'hierarchies. Occasionally, you’ll make major structural<br>' +
        'changes to the model based on a deeper understanding than<br>' +
        'you had before you created the detailed model.<br>' +
        'Reviewing and Validating the Model<br>' +
        'Once you’re confident in the model’s stability, the process<br>' +
        'moves into the review and validation phase. This phase<br>' +
        'involves reviewing the model with successive audiences,<br>' +
        'each with different levels of technical expertise and<br>' +
        'business understanding. At a minimum, plan on talking to<br>' +
        'three groups. Start with the core DW/BI team and involved<br>' +
        'expert users. Next, review the model with key folks in the<br>' +
        'IT organization; the source system developers and DBAs<br>' +
        'can often spot errors in the model very quickly. (You may<br>' +
        'need to teach these folks about dimensional modeling<br>' +
        'before they start normalizing your dimensions.) Finally,<br>' +
        'get feedback from any core business users who were not<br>' +
        'directly involved in the model development process. After<br>' +
        'each meeting, incorporate the feedback into the<br>' +
        'dimensional model.<br>' +
        'You may want to end by reviewing the model with the<br>' +
        'broader user community. Do this more in the form of a<br>' +
        'presentation and tie the model back to the business<br>' +
        'requirements. A series of statements that show how a user<br>' +
        'might get answers to a range of questions pulled right from<br>' +
        'the requirements document can be very powerful.<br>' +
        'The modeling team will get valuable feedback from the<br>' +
        'review and validation process. The DW/BI team also gets<br>' +
        'value from these reviews in the form of a more informed<br>' +
        'and engaged user community.<br>' +
        '169<br>' +
        'Case Study: The Adventure Works Cycles Orders<br>' +
        'Dimensional Model<br>' +
        'This fourth major section of this chapter is meant to impart<br>' +
        'a sense of how business requirements drive the<br>' +
        'dimensional model. We draw our example from the<br>' +
        'Adventure Works Cycles business and source system data<br>' +
        'in the AdventureWorks transaction database. We use the<br>' +
        'initial orders dimensional model from Figure 2-8 as our<br>' +
        'starting point and discuss the major design changes that<br>' +
        'were driven by business requirements, including the facts<br>' +
        'and the employee, customer, and currency dimensions.<br>' +
        'The Orders Fact Table<br>' +
        'The grain of the fact table is at the order line item level.<br>' +
        'This is the atomic level, and the only point in the<br>' +
        'transaction where the product is specified. However, there<br>' +
        'are some facts, such as taxes and shipping, which appear at<br>' +
        'the order header level. Our goal would be to allocate those<br>' +
        'down to the individual line item level if at all possible.<br>' +
        'This would give us one atomic level fact table that can be<br>' +
        'rolled up to any level across any dimension for any<br>' +
        'order-related analytic purpose. This is how we create a<br>' +
        'flexible model with long-term viability.<br>' +
        'There could be some confusion about what goes into the<br>' +
        'Orders fact table at Adventure Works because there are<br>' +
        'two distribution channels: internet and resellers. Since the<br>' +
        'sales organization is generally only interested in sales for<br>' +
        'which they receive compensation, giving them a reseller<br>' +
        'sales fact table removes the risk of accidentally including<br>' +
        'internet sales in their analyses. However, separate fact<br>' +
        '170<br>' +
        'tables would require more complex cross-table queries to<br>' +
        'generate reports at the total company level. It makes more<br>' +
        'sense to combine all orders into a single fact table and use<br>' +
        'dimension constraints, (e.g.,<br>' +
        'Customer_Type = \'Reseller\'), to help the sales<br>' +
        'people get what they need.<br>' +
        'The Dimensions<br>' +
        'Using the dimensional model shown in Figure 2-8 as our<br>' +
        'starting point, we’ll summarize how the Adventure Works<br>' +
        'Cycles DW/BI team resolved major design issues with<br>' +
        'several dimensions. At the end of this section, we show the<br>' +
        'resulting updated high-level orders dimensional model.<br>' +
        'Employee<br>' +
        'Several business requirements had been voiced about<br>' +
        'tracking the impact of organizational changes, and about<br>' +
        'basic employee counts. (Note that these are questions<br>' +
        'about the Human Resources business process.) Without a<br>' +
        'type 2 dimension, it would be impossible to relate the<br>' +
        'employee attributes that were in effect at the time a<br>' +
        'transaction occurred with the actual transaction itself. Only<br>' +
        'current attribute values could be used to analyze history.<br>' +
        'Based on this discussion, the team decided to treat the<br>' +
        'employee dimension as a hybrid slowly changing<br>' +
        'dimension, with most of the attributes tracked as type 2<br>' +
        'attributes. This decision had a significant ripple effect on<br>' +
        'the model. First, it revealed a weakness in the sales<br>' +
        'territory dimension. Having a type 2 attribute called<br>' +
        'historical sales territory in the employee dimension tracks<br>' +
        '171<br>' +
        'the same information as the Sales Territory table, only<br>' +
        'better: It locks in both the sales territory and the sales rep<br>' +
        'that got credit at the time of the sale.<br>' +
        'As a result, the sales territory dimension can be removed<br>' +
        'from the model. Second, there is still a need to apply the<br>' +
        'currently assigned sales territory to all of history, which<br>' +
        'means keeping the current sales territory attribute in the<br>' +
        'employee dimension as a type 1 attribute. The ETL<br>' +
        'process will have to change all historical rows in the<br>' +
        'employee dimension for the current sales territory when<br>' +
        'the sales territory changes for a given employee.<br>' +
        'NOTE<br>' +
        'The decision to include type 2 attributes in<br>' +
        'the employee dimension really means<br>' +
        'expanding the scope to include a second<br>' +
        'business process: Human Resources<br>' +
        'transactions. In a large organization, the<br>' +
        'ETL process will essentially create an<br>' +
        'employee transaction fact table that can<br>' +
        'then be used to build the employee<br>' +
        'dimension. For Adventure Works Cycles,<br>' +
        'with fewer than 300 employees, this<br>' +
        'decision is probably not too onerous. For a<br>' +
        'larger organization with thousands of<br>' +
        'employees, this could be a lot of work.<br>' +
        'The team also noted that the issue of finding reliable<br>' +
        'historical information for employee changes will have to<br>' +
        '172<br>' +
        'be researched, but lack of historical data is a bad reason to<br>' +
        'avoid implementing type 2 tracking. The sooner you get<br>' +
        'started, the more history you will have.<br>' +
        'Customer and Reseller<br>' +
        'The initial design identified two customer-related<br>' +
        'dimensions, customer and reseller, based on the two major<br>' +
        'distribution channels. While these do share some<br>' +
        'attributes, like an address, the company has certain<br>' +
        'information about each that it doesn’t have about the other.<br>' +
        'The sense when the bus matrix was created was that the<br>' +
        'two customer types are different enough to be split into<br>' +
        'separate dimensions.<br>' +
        'Business requirements can help the team decide whether<br>' +
        'they need one or two versions of the customer dimension.<br>' +
        'First, there is a clear need to report total sales across both<br>' +
        'customer types in the same report, so the model must<br>' +
        'include an integrated master customer dimension of some<br>' +
        'kind. Second, there are fewer than 20,000 customers in<br>' +
        'total, and the distribution is highly imbalanced: Less than 4<br>' +
        'percent are resellers. Third, only 10 or so attributes are<br>' +
        'unique to reseller customers (items such as store annual<br>' +
        'sales and number of employees).<br>' +
        'Based on these requirements, the design team decided to<br>' +
        'combine all the attributes of the two customer types into a<br>' +
        'single, master customer dimension. This decision allows<br>' +
        'much greater reporting and analysis flexibility and<br>' +
        'simplifies the dimensional model. However, it means the<br>' +
        'users need to understand and be able to work with the idea<br>' +
        'that both customer types are in the same table. When they<br>' +
        '173<br>' +
        'want a count of internet customers, they will need to limit<br>' +
        'the customer type field to “Internet.” (It’s also possible to<br>' +
        'create views on the combined customer dimension that<br>' +
        'would look exactly like the separate dimensions, for those<br>' +
        'folks who can’t handle two customer types in one table!)<br>' +
        'Currency<br>' +
        'Every sale in the source system is captured in its original,<br>' +
        'local currency. The source system relies on a currency<br>' +
        'conversion table for translating currencies and reporting in<br>' +
        'U.S. dollars. This table tracks the conversion rate between<br>' +
        'the local currency and U.S. dollars both at the end of each<br>' +
        'day and as an average for each day. Getting standardized<br>' +
        'reports in U.S. dollars to compare across countries requires<br>' +
        'a fairly complicated query and has long been a sore point<br>' +
        'for most of the folks in headquarters. At the same time,<br>' +
        'sales people in the field want to create reports in local<br>' +
        'currency to show their customers. Finance, of course,<br>' +
        'wants both, along with the conversion table so they can<br>' +
        'assess the impact of exchange rates on budget variances.<br>' +
        'Based on these requirements, the design team decided to<br>' +
        'include both local currency and U.S. dollar fields in the<br>' +
        'fact table, with a currency dimension to indicate the<br>' +
        'currency of the local data. This means the ETL process<br>' +
        'would have to bring in the Exchange Rates table to<br>' +
        'convert non-U.S. sales into U.S. dollars.<br>' +
        'Although it was out of scope for the initial phase 1 project,<br>' +
        'the design team also decided to include the exchange rates<br>' +
        'as a separate business process dimensional model. The<br>' +
        'exchange rates model is essentially the Exchange Rates<br>' +
        '174<br>' +
        'fact table combined with the currency dimension and the<br>' +
        'date dimension. Making this available to the users was an<br>' +
        'easy political decision because it is incrementally very<br>' +
        'little work. The Exchange Rates table must already be<br>' +
        'brought into the ETL staging area in order to support the<br>' +
        'currency conversion in the Orders table. Besides, the<br>' +
        'Director of Finance is particularly interested in getting<br>' +
        'access to this data.<br>' +
        'NOTE<br>' +
        'This is already the second piece of scope<br>' +
        'creep in the design, the first being treating<br>' +
        'employee as a type 2 dimension. We don’t<br>' +
        'encourage this kind of scope creep in real<br>' +
        'life. In our experience, almost all DW/BI<br>' +
        'teams are overly ambitious in their first<br>' +
        'iteration. We’re constantly coaching our<br>' +
        'clients not to over-commit — your mantra<br>' +
        'should be under-promise and over-deliver.<br>' +
        'Those are the major changes that came out of the<br>' +
        'high-level modeling discussion. Before we show the<br>' +
        'updated model, let’s complete the second part of the<br>' +
        'design session because there still might be a few changes.<br>' +
        'Identifying Dimension Attributes and Facts for the Orders<br>' +
        'Business Process<br>' +
        'The second half of the initial design session involves<br>' +
        'creating an initial data element list. This is an attribute list<br>' +
        '175<br>' +
        'for each dimension and a list of fact-related data elements.<br>' +
        'The starting point for this list is the detailed requirements<br>' +
        'document — one of its appendices should be a list of key<br>' +
        'data elements (attributes) that people specifically identified<br>' +
        'as important.<br>' +
        'Figure 2-9 shows what a portion of the attributes list might<br>' +
        'look like for the Adventure Works Cycles orders business<br>' +
        'process dimensional model. The Sample Values column is<br>' +
        'helpful in identifying attributes.<br>' +
        'NOTE<br>' +
        'Creating a stand-alone attribute list can be<br>' +
        'helpful, but if you are using the modeling<br>' +
        'spreadsheet we described earlier in the<br>' +
        'chapter, you already have a place to keep<br>' +
        'your attribute lists.<br>' +
        'The process of creating the attribute list can trigger<br>' +
        'changes to the initial high-level dimensional model. The<br>' +
        'Adventure Works Cycles modeling team came across two<br>' +
        'attributes that did not have an obvious home: sales reason<br>' +
        'and channel. The business expert on the design team<br>' +
        'explained how sales reason comes from a list on the<br>' +
        'internet order form where customers could select one or<br>' +
        'more reasons for their purchase. This is an example of a<br>' +
        'many-to-many relationship, and a good candidate for a<br>' +
        'bridge table. However, discussions with the business users<br>' +
        'revealed that they were interested in the primary reason<br>' +
        'only, which can be identified in the ETL process. In an<br>' +
        '176<br>' +
        'effort to avoid additional scope creep, the team decided to<br>' +
        'include only the primary sales reason. (This will probably<br>' +
        'be a decision they regret in the long run.)<br>' +
        'Users also mentioned sales channel several times in the<br>' +
        'business requirements document — usually referring to<br>' +
        'resellers or the internet. If sales channel refers only to<br>' +
        'reseller and internet, the Customer Type field can handle<br>' +
        'this distinction. However, it was clear during the<br>' +
        'requirements interviews that there is a drive to open up<br>' +
        'new sales channels, including opening Adventure Works<br>' +
        'Cycles retail stores and providing private label bikes for<br>' +
        'large retailers.<br>' +
        'Figure 2-9: Promotion dimension portion of the<br>' +
        'Adventure Works Cycles initial Orders attribute list<br>' +
        'A quick query of the source system revealed there are only<br>' +
        'ten sales reasons and two sales channels so creating two<br>' +
        'separate dimensions seemed inefficient to the design team.<br>' +
        '177<br>' +
        'They opted to create a junk dimension called Order Info<br>' +
        'that would contain both concepts. The ETL process will<br>' +
        'have to manage the assignment of surrogate keys and<br>' +
        'watch for new entries in the source systems.<br>' +
        'Revisit the source system tables at this point, as a final<br>' +
        'check to make sure you haven’t left anything useful<br>' +
        'behind. This is meant to be a validation step, not a starting<br>' +
        'point.<br>' +
        'Not all of the data elements on the attribute list will<br>' +
        'necessarily be attributes of the final dimensional model.<br>' +
        'Some of them are not really attributes; rather they’re<br>' +
        'aggregates or constraints. Others are the same attribute<br>' +
        'masquerading under a different name. Still other attributes<br>' +
        'are missing altogether, either because they were so obvious<br>' +
        'people didn’t think to mention them, or they were so little<br>' +
        'used, people didn’t know to mention them. As you build<br>' +
        'the lists, keep an eye out for these kinds of redundancies<br>' +
        'and omissions. Start to boil down all this information to<br>' +
        'create the master attribute list for each table.<br>' +
        'The Final Draft of the Initial Orders Model<br>' +
        'At the end of the initial design session, the team has<br>' +
        'created a good high-level dimensional model for the orders<br>' +
        'business process. The high-level model shown in Figure<br>' +
        '2-10 is the result of merging the changes identified in the<br>' +
        'process of creating the attributes list (adding the order info<br>' +
        'dimension) along with the changes from the model design<br>' +
        'session itself (merging reseller and customer, removing the<br>' +
        'sales territory dimension, and adding the exchange rates<br>' +
        'business process). This dimensional model contains all the<br>' +
        '178<br>' +
        'elements needed to meet Adventure Works Cycles’ orders<br>' +
        'related business requirements in a simple, powerful,<br>' +
        'flexible form — at least as far as the team understands<br>' +
        'them at this point. This model will change, but it is a<br>' +
        'strong first pass.<br>' +
        'We encourage you to compare this model with the initial<br>' +
        'model in Figure 2-8 to see how it evolved during the initial<br>' +
        'design session. Review the business requirements<br>' +
        'described in Chapter 1 and available from the book’s web<br>' +
        'site to get a sense for how well the model will meet the<br>' +
        'needs.<br>' +
        'The last deliverable from the initial modeling session is the<br>' +
        'issues list. This list will evolve as you work through the<br>' +
        'details of each table, solving some issues and adding<br>' +
        'others.<br>' +
        'DOWNLOADS<br>' +
        'You can see an example of the issues list<br>' +
        'from the Adventure Works Cycles Orders<br>' +
        'business process design session on the<br>' +
        'book’s web site.<br>' +
        'Figure 2-10: The high-level dimensional model from the<br>' +
        'initial design session<br>' +
        '179<br>' +
        'Detailed Orders Dimensional Model Development<br>' +
        'Develop the detailed dimensional model one dimension at<br>' +
        'a time. Begin with the attribute information you captured<br>' +
        'in your modeling spreadsheet in the initial design session.<br>' +
        'Start with an easy dimension such as date, and fill in as<br>' +
        'much of the spreadsheet as possible based on current<br>' +
        'information. Source tables and columns are often fairly<br>' +
        'clear for most of the attributes. Transformations for most<br>' +
        'of the attributes are direct copies from the source. Target<br>' +
        'data types can be inferred based on the source system as<br>' +
        'well, although the data warehouse DBA will have the final<br>' +
        'say in determining the data types.<br>' +
        'Once the known information is filled in, the open issues<br>' +
        'are more obvious. At this point, it’s time to continue with<br>' +
        '180<br>' +
        'the data exploration/data profiling process described<br>' +
        'earlier in this chapter.<br>' +
        'Identifying SCD Change Types<br>' +
        'One of the columns in the modeling spreadsheet is the SCD<br>' +
        'Change Type. The data modeler will use this to identify<br>' +
        'how each attribute needs to be tracked over time and flag it<br>' +
        'appropriately. Remember, this is a business question. It’s<br>' +
        'okay to make a first pass and flag all the attributes whose<br>' +
        'changes obviously must be tracked over time, or whose<br>' +
        'changes have no impact on the business whatsoever. All of<br>' +
        'the less obvious attributes should be discussed with the<br>' +
        'modeling team. Review all of these change-tracking<br>' +
        'decisions with the core business users before you make a<br>' +
        'final decision.<br>' +
        'The ETL process must re-create the historical changes for<br>' +
        'every type 2 SCD attribute in each dimension, at least as<br>' +
        'far back in time as the oldest fact table rows the dimension<br>' +
        'will support. This is because the ETL process will need to<br>' +
        'load historical fact rows with the dimension surrogate keys<br>' +
        'that were in effect when the fact row occurred. Therefore<br>' +
        'the ETL developer has to go back into the transaction<br>' +
        'system to find all relevant historical changes that apply to<br>' +
        'the dimension in question. While this is not the data<br>' +
        'modeler’s task, it’s helpful to look for indicators as to<br>' +
        'whether or not re-creating historical dimension data will be<br>' +
        'difficult or even possible.<br>' +
        'Reviewing the Issues<br>' +
        '181<br>' +
        'Even though you may have filled in most of the<br>' +
        'spreadsheet, there will still be several issues that require a<br>' +
        'second or third opinion. In some cases, you can resolve<br>' +
        'these issues using the research tools described earlier in the<br>' +
        'chapter. In other cases, you need a sounding board to<br>' +
        'explore alternative solutions. Bring up open issues in the<br>' +
        'next data modeling team meeting and work them through.<br>' +
        'Then move on to the next dimension.<br>' +
        'Identifying the Facts<br>' +
        'Filling in the detailed fact table description is much like<br>' +
        'filling in the dimensions. Start by copying in the list of<br>' +
        'measures and filling in all the easy items. Then use the<br>' +
        'research tools to address as many of the open items as<br>' +
        'possible. Finally, work with the data modeling team to<br>' +
        'resolve the remaining issues.<br>' +
        'There are several issues that are specific to fact tables.<br>' +
        'These include:<br>' +
        '• Derived columns: Identify the formula and indicate whether<br>' +
        'the derivation is additive or semi-additive, as in a month-end<br>' +
        'account balance.<br>' +
        '• Allocations: In the case of the Adventure Works Cycles<br>' +
        'orders dimensional model, the grain is at the order line item<br>' +
        'level. The team must decide how to handle the handful of<br>' +
        'facts that are collected at the order level. Sales tax can easily<br>' +
        'be allocated to each line item. Other facts, such as shipping<br>' +
        'costs, might need to be allocated based on weight or size.<br>' +
        'NOTE<br>' +
        '182<br>' +
        'Don’t avoid the allocations! If you leave<br>' +
        'shipping costs in a fact table at the order<br>' +
        '(not line item) level, all your product<br>' +
        'related financial rollups will omit the<br>' +
        'shipping costs. Grit your teeth and allocate!<br>' +
        '• Conformed facts: The dollar sales field is a good example of<br>' +
        'creating a conformed fact. As the team discussed in the design<br>' +
        'session, there’s a need to have all transactions stated in a single<br>' +
        'currency (U.S. dollars), as well as the original local currency<br>' +
        'from the source system.<br>' +
        '• Degenerate dimensions: While no transformations need to be<br>' +
        'applied to any degenerate dimensions in the dimensional model,<br>' +
        'you do need to indicate which fields in the fact table are<br>' +
        'degenerate dimensions. In the orders dimensional model, there<br>' +
        'are several degenerate dimensions.<br>' +
        'Final Dimensional Model<br>' +
        'When all the design reviews are finished, the user<br>' +
        'meetings over, the source systems carefully scrutinized,<br>' +
        'and the requirements reviewed, it’s time to physically<br>' +
        'instantiate the dimensional model. This is a job for the<br>' +
        'DBAs, but we usually set up a test database and run the<br>' +
        'script from the spreadsheet. (You will probably already<br>' +
        'have done this several times by now so you could reverse<br>' +
        'engineer the model into your modeling tool to create a<br>' +
        'presentable data model.)<br>' +
        'DOWNLOADS<br>' +
        '183<br>' +
        'You can find the completed modeling<br>' +
        'spreadsheet we used to create the<br>' +
        'MDWT_AdventureWorks database at the<br>' +
        'book’s web site: kimballuniversity.com/html/<br>' +
        'booksMDWTtools.html.<br>' +
        'At this point, you’re ready to take on the real database<br>' +
        'physical design process and start thinking about designing<br>' +
        'the ETL system.<br>' +
        'Summary<br>' +
        'Designing dimensional models for business intelligence is<br>' +
        'no simple trick. The first part of this chapter concentrated<br>' +
        'on defining and describing the basic concepts of<br>' +
        'dimensional modeling: facts, dimensions, the bus matrix,<br>' +
        'and conformed dimensions. The next section expanded the<br>' +
        'description of dimensional modeling with key concepts<br>' +
        'such as surrogate keys and tracking changes with slowly<br>' +
        'changing dimensions. We described several techniques to<br>' +
        'model a broad range of common (and uncommon)<br>' +
        'business processes and relationships like many-to-many<br>' +
        'relationships, hierarchies, and junk dimensions.<br>' +
        'The third part of this chapter covered the process of<br>' +
        'dimensional modeling. Begin with a preparation step to<br>' +
        'identify the team, set up the modeling environment, and<br>' +
        'determine naming conventions. Begin the modeling<br>' +
        'process by using our four step approach to create a high<br>' +
        'level business dimensional model, along with attributes<br>' +
        'and issues lists. The next step is to develop the detailed<br>' +
        '184<br>' +
        'model, table by table and column by column, filling in all<br>' +
        'the needed information and addressing all the issues. The<br>' +
        'last step in the process of creating the dimensional model<br>' +
        'involves reviewing the proposed model with several<br>' +
        'interested parties, including other IT people and core<br>' +
        'business users.<br>' +
        'The last part of the chapter applied the dimensional<br>' +
        'modeling concepts and process to the Adventure Works<br>' +
        'Cycles case study, resulting in a dimensional model for the<br>' +
        'orders business process. This dimensional model will be<br>' +
        'the target for the physical database creation and the ETL<br>' +
        'system described over the next several chapters.<br>' +
        '185<br>';
    document.getElementById('chapter1').innerHTML = 'Part 1: Requirements, Realities, and Architecture<br>' +
        'Chapter 1: Defining Business Requirements<br>' +
        'Chapter 2: Designing the Business Process Dimensional<br>' +
        'Model<br>' +
        'Chapter 3: The Toolset<br>' +
        'Chapter 4: System Setup<br>' +
        'This first part of the lifecycle is where you lay the<br>' +
        'foundation for your success. Working with the business<br>' +
        'folks to understand and prioritize their requirements for<br>' +
        'analytics as we describe in Chapter 1 helps you set specific<br>' +
        'goals for your first pass through the lifecycle that are both<br>' +
        'valuable to the organization and achievable in a reasonable<br>' +
        'timeframe. Your understanding of the business<br>' +
        'requirements becomes the basis for designing a flexible,<br>' +
        'usable, high-performing dimensional model in Chapter 2.<br>' +
        'What you learn in the first two chapters helps you tackle<br>' +
        'the architecture and technology track at the top of the<br>' +
        'Lifecycle. Your business understanding helps you<br>' +
        'determine what architectural components are important for<br>' +
        'your DW/BI system. Once you know the problem, you can<br>' +
        'identify the specific functionality you need, and where that<br>' +
        'functionality will come from in the Microsoft SQL Server<br>' +
        'toolset. This, in turn, allows you to make decisions on the<br>' +
        'server configurations and disk subsystems that will form<br>' +
        'the basic infrastructure of your DW/BI system.<br>' +
        '48<br>' +
        'Part 1 is about getting the lay of the land before you decide<br>' +
        'what you are going to build and where you will build it.<br>' +
        'Your primary focus here is on identifying the most<br>' +
        'promising business opportunities and designing the data<br>' +
        'structures and system architectures needed to deliver them.<br>' +
        'By the end of this section, you should have all the pieces in<br>' +
        'place for you to dig into the development work of creating<br>' +
        'the DW/BI system database. Skip this section at your peril.<br>' +
        'The Kimball Lifecycle steps covered in Part 1<br>' +
        '49<br>' +
        'Chapter 1<br>' +
        'Defining Business Requirements<br>' +
        'Building the foundation.<br>' +
        'Business requirements are the bedrock of the successful<br>' +
        'data warehouse/business intelligence (DW/BI) system.<br>' +
        'Business requirements guide the development team in<br>' +
        'making the biggest strategic choices, such as prioritizing<br>' +
        'subject areas for implementation, and in making the<br>' +
        'smallest tactical design decisions, such as how to present<br>' +
        'key performance indicators on the users’ screens. In this<br>' +
        'chapter, we cover the process of gathering business<br>' +
        'requirements and converting them into a DW/BI system<br>' +
        'strategy. We describe the process of interviewing business<br>' +
        'and IT representatives and mapping their analytic<br>' +
        'requirements back to the core business processes (such as<br>' +
        'orders, page views, or account transactions) that generate<br>' +
        'the needed data. These business processes are the building<br>' +
        'blocks of the DW/BI system. After the requirements are<br>' +
        'documented, we offer a technique for working with senior<br>' +
        'management to prioritize the implementation of those<br>' +
        'business-process–based projects. We also illustrate these<br>' +
        'tasks with an example based on Microsoft’s sample<br>' +
        'database business, Adventure Works Cycles.<br>' +
        'As Figure 1-1 illustrates, the Business Requirements<br>' +
        'Definition step is the foundation of the Kimball Lifecycle<br>' +
        'methodology. Business requirements and their associated<br>' +
        'business value give you the guidance you need to make<br>' +
        '50<br>' +
        'decisions in all three downstream tracks. As you’ll see,<br>' +
        'they influence the project scope and plan, too.<br>' +
        'RESOURCES<br>' +
        'If you skipped the Introduction to this<br>' +
        'book, you should at least go back and read<br>' +
        'the overview of the Kimball Lifecycle<br>' +
        'because it is the organizing framework for<br>' +
        'this book and for implementing a<br>' +
        'successful DW/BI system.<br>' +
        'Figure 1-1: The Business Requirements Definition step of<br>' +
        'the Kimball Lifecycle<br>' +
        'This chapter is primarily about resisting temptation.<br>' +
        'Gathering business requirements is often outside a<br>' +
        'technical person’s comfort zone. The overall success of the<br>' +
        'project is largely determined by your understanding of the<br>' +
        'business requirements and your relationships with the<br>' +
        '51<br>' +
        'business people. Resist the temptation to just start loading<br>' +
        'data.<br>' +
        'In this chapter you learn the following:<br>' +
        '• The importance of understanding business requirements and<br>' +
        'securing solid business sponsorship<br>' +
        '• The steps used to define enterprise-level business<br>' +
        'requirements, including the interview process, synthesizing<br>' +
        'requirements into their underlying business processes,<br>' +
        'developing the enterprise analytic data framework called the<br>' +
        'data warehouse bus matrix, and prioritizing business<br>' +
        'processes with senior management<br>' +
        '• How to plan the initial business process dimensional model<br>' +
        'implementation and gather project-level business<br>' +
        'requirements<br>' +
        '• What goes into a typical requirements summary document<br>' +
        'and how it links to business requirements for analytics and<br>' +
        'business process implementations<br>' +
        'RESOURCES<br>' +
        'Throughout the book, we provide specific<br>' +
        'references to the various titles in the<br>' +
        'Kimball Toolkit library to help you find<br>' +
        'more details on the concept or technique<br>' +
        'described. Each book in the Toolkit series<br>' +
        'is described in the Introduction to this<br>' +
        'book.<br>' +
        'The Most Important Determinant of Long-Term Success<br>' +
        '52<br>' +
        'There is one common factor in successful business<br>' +
        'intelligence projects: delivering business value. Your DW/<br>' +
        'BI team must embrace the goal of enhancing business<br>' +
        'value as its primary purpose. This seems like an obvious<br>' +
        'statement, but most DW/BI folks are technologists at heart.<br>' +
        'We like the certainty of computers and programming and<br>' +
        'shy away from the vague uncertainties of the business side.<br>' +
        'You can’t deliver business value unless you work closely<br>' +
        'with business people. You need to understand their<br>' +
        'language and learn to see the world from their points of<br>' +
        'view. You’ll be working in a non-technical, highly<br>' +
        'ambiguous, politically sensitive environment. Are you<br>' +
        'feeling queasy yet? This unsettled environment is what the<br>' +
        'DW/BI system is all about. You must develop the business<br>' +
        'knowledge and people skills right along with your<br>' +
        'technical skills to meet the needs of your business users.<br>' +
        'We realize the entire team will not become smooth-talking<br>' +
        'MBAs. However, someone on the team must have strong<br>' +
        'business and communications skills, and everyone will be<br>' +
        'more effective if they learn more about the business.<br>' +
        'NOTE<br>' +
        'Perhaps your organization uses the “agile”<br>' +
        'development methodology. If so, then you<br>' +
        'have heard this story already! In the agile<br>' +
        'approach, projects are owned and driven by<br>' +
        'the business users. To learn more about this<br>' +
        'approach, see “agile software<br>' +
        '53<br>' +
        'development” on www.wikipedia.org. For<br>' +
        'guidance relating agile to DW/BI system<br>' +
        'development, see The Kimball Group<br>' +
        'Reader, pp. 109–112.<br>' +
        'So, while many DW/BI teams and consultants pay lip<br>' +
        'service to business value, the reality of their day-to-day<br>' +
        'behavior is that technology rules. Do not let this happen to<br>' +
        'you. Technology is important; business value is<br>' +
        'mandatory. We understand you bought this book to learn<br>' +
        'about the SQL Server DW/BI toolset, but SQL Server is<br>' +
        'just a tool. Your success in using that tool in your<br>' +
        'organization depends on your understanding of the<br>' +
        'organization’s unique requirements and priorities for<br>' +
        'business intelligence.<br>' +
        'As you read this book, you’ll encounter recommendations<br>' +
        'that may seem unnecessarily complicated or just plain<br>' +
        'unnecessary. Every time you’re tempted to dismiss the<br>' +
        'authors as overly fond of their design methodology or just<br>' +
        'overzealous, consider whether your reactions are driven by<br>' +
        'your technical convenience or by the business users’<br>' +
        'needs. Never lose sight of the business.<br>' +
        'Adventure Works Cycles Introduction<br>' +
        'It always helps to see new concepts in the context of a<br>' +
        'specific example. Since everyone’s organization is<br>' +
        'different, we’ll use some of the business requirements for<br>' +
        'Microsoft’s demo database company to illustrate the<br>' +
        '54<br>' +
        'process of defining business requirements described in this<br>' +
        'chapter.<br>' +
        'The current SQL Server sample business intelligence<br>' +
        'databases are based on a fictitious company called<br>' +
        'Adventure Works Cycles, a multinational manufacturer<br>' +
        'and seller of bicycles and accessories. The database and<br>' +
        'associated samples are not part of the software distribution<br>' +
        'set. Instead, you download them from the Microsoft<br>' +
        'code-sharing site called Codeplex (Search for “SQL Server<br>' +
        'Samples Database” at http://www.codeplex.com) or download<br>' +
        'the database from the Wiley web site at www.wiley.com/go/<br>' +
        'MsftDWToolkit2E. You will need to download and install the<br>' +
        'SQL Server 2008R2 version of the sample databases to<br>' +
        'follow the examples later in this book.<br>' +
        'DOWNLOADS<br>' +
        'You can find several detailed documents<br>' +
        'illustrating what the business<br>' +
        'requirements-gathering process might look<br>' +
        'like at a company such as Adventure<br>' +
        'Works Cycles on the book’s web site<br>' +
        '(http://kimballgroup.com/html/<br>' +
        'booksMDWTtools.html). These include<br>' +
        'interview summaries and additional<br>' +
        'background information.<br>' +
        'Uncovering Business Value<br>' +
        '55<br>' +
        'If you’re going to be driven by business value, you need to<br>' +
        'go out and identify, understand, and prioritize the needs of<br>' +
        'the business. This is easier said than done if your focus has<br>' +
        'historically been on technology. Fortunately, the Kimball<br>' +
        'Lifecycle provides the tools to work through an entire<br>' +
        'development iteration of a data warehouse, beginning with<br>' +
        'business requirements.<br>' +
        'Where do you start with your business intelligence<br>' +
        'system? What is the first step? Well, it depends on a host<br>' +
        'of factors, such as how your organization works, what you<br>' +
        'already know about the business, who is involved in the<br>' +
        'project at this point, what kinds of DW/BI efforts came<br>' +
        'before, and many other factors.<br>' +
        'Let’s talk about the most common scenario first, and then<br>' +
        'we’ll address a few exceptions. More often than not, the<br>' +
        'DW/BI system starts as a project hosted by the Information<br>' +
        'Technology (IT) department of the organization. The<br>' +
        'IT-driven DW/BI project gets cranked up because the CIO<br>' +
        'decides the company needs a data warehouse, so people<br>' +
        'and resources are assigned to build one. This is a<br>' +
        'dangerous situation. Please refer to the first point in this<br>' +
        'chapter: Focusing on business value is the most important<br>' +
        'determinant of long-term success. The problem with the<br>' +
        'IT-driven DW/BI system is that it almost always centers<br>' +
        'on technology. The team has been assigned the task of<br>' +
        'building a “warehouse,” so that’s exactly what they do.<br>' +
        'They get some hardware and some software and start<br>' +
        'extracting data.<br>' +
        'We know some of you are thinking, “Oops, I already<br>' +
        'bought the ETL server and the user reporting tools.” That’s<br>' +
        '56<br>' +
        'probably okay, but put those tools aside for the moment.<br>' +
        'Step away from the keyboard. If you get sucked into the<br>' +
        'technology, you’re missing the whole point. You can build<br>' +
        'a technically great DW/BI system that provides very little<br>' +
        'business value. As a result, your project will fail. You have<br>' +
        'to start with business value, and identifying business value<br>' +
        'involves several major steps:<br>' +
        '• Recruiting strong business sponsorship<br>' +
        '• Defining enterprise-level business requirements<br>' +
        '• Prioritizing business requirements<br>' +
        '• Planning the project<br>' +
        '• Defining project-level business requirements<br>' +
        'We’ll run through each of these steps in the following<br>' +
        'sections.<br>' +
        'Obtaining Sponsorship<br>' +
        'Developing solid business sponsorship is the best place to<br>' +
        'start the DW/BI project. Your business sponsors (it is<br>' +
        'generally good to have more than one) will take a lead role<br>' +
        'in determining the purpose, content, and priorities of the<br>' +
        'DW/BI system. You will call on them to secure resources<br>' +
        'and to evangelize the DW/BI system to the rest of the<br>' +
        'organization. This includes activities such as arranging for<br>' +
        'a planning meeting with senior staff, speaking to a room<br>' +
        'full of business users at the project kick-off, and getting<br>' +
        'spending approval for your new server. You need to find at<br>' +
        'least one person in the organization who scores well in<br>' +
        'each of the following areas:<br>' +
        '57<br>' +
        '• Visionary: Someone who has a sense for the value and<br>' +
        'potential of information and some clear, specific ideas on<br>' +
        'how to apply it.<br>' +
        '• Resourceful: Someone who is able to obtain the necessary<br>' +
        'resources and facilitate the organizational change the data<br>' +
        'warehouse will bring about.<br>' +
        '• Reasonable: Someone who can temper his or her enthusiasm<br>' +
        'with the understanding that it takes time and resources to<br>' +
        'build a major information system.<br>' +
        'If you’ve been with your company for a while, you already<br>' +
        'know who these people are. In this case, your task is to<br>' +
        'recruit them onto the project. However, if you’re new to<br>' +
        'the company, or you don’t get out of the IT group much,<br>' +
        'you’ll need to investigate and find your business sponsors.<br>' +
        'In either case, the best way to find and recruit these people<br>' +
        'is by conducting an enterprise business requirements<br>' +
        'gathering project. Obtaining business sponsorship is fairly<br>' +
        'easy and well worth the effort. Good business sponsorship<br>' +
        'can provide the resources and support you need to deliver<br>' +
        'real business value.<br>' +
        'RESOURCES<br>' +
        'Learn more about developing sponsorship<br>' +
        'in The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition, pages 16–21.<br>' +
        'Defining Enterprise-Level Business Requirements<br>' +
        'The successful DW/BI effort is an ongoing program<br>' +
        'guiding the multiple, iterative projects that build out the<br>' +
        '58<br>' +
        'DW/BI system. Before you concentrate your efforts on<br>' +
        'specific projects within the larger data warehouse<br>' +
        'endeavor, you need a broad enterprise perspective that will<br>' +
        'help you set priorities and make better, more flexible<br>' +
        'implementation decisions. One long-term goal of the DW/<br>' +
        'BI team is to build an enterprise information infrastructure.<br>' +
        'Clearly, you can’t do this unless you understand business<br>' +
        'requirements from an enterprise level.<br>' +
        'We almost always preface the first iteration of the<br>' +
        'Lifecycle with an enterprise business requirements<br>' +
        'definition project. This project is essentially a set of<br>' +
        'interviews, documentation, and a prioritization session<br>' +
        'with senior management. It provides a clear<br>' +
        'implementation plan for the DW/BI system, and it can be<br>' +
        'done in three to six weeks for most organizations.<br>' +
        'Every DW/BI program has to keep the enterprise<br>' +
        'requirements context in mind. Larger organizations need to<br>' +
        'begin by establishing this broad understanding because it<br>' +
        'is rare for the DW/BI team to have such an enterprise-level<br>' +
        'perspective. Even in a smaller company or a departmental<br>' +
        'effort, the enterprise perspective will help build in<br>' +
        'flexibility and resilience. It is also particularly important<br>' +
        'for organizations that are just starting their first DW/BI<br>' +
        'system (or starting over) because getting the enterprise<br>' +
        'perspective built into the initial project helps you avoid<br>' +
        'painful and costly redesign down the road. By the same<br>' +
        'token, DW/BI systems that are well into their<br>' +
        'implementation will gain by taking a little time to validate<br>' +
        'their understanding of the enterprise business<br>' +
        'requirements. This usually results in significant changes to<br>' +
        'the DW/BI system strategy. Better late than too late.<br>' +
        '59<br>' +
        'Given this need for an enterprise perspective, it’s best to<br>' +
        'preface your first Lifecycle iteration with a narrow-scoped<br>' +
        'enterprise requirements definition project as shown in<br>' +
        'Figure 1-2.<br>' +
        'In this subsection of the Lifecycle, defining the business<br>' +
        'requirements happens in several distinct steps. The rest of<br>' +
        'this section describes each of these steps in more detail.<br>' +
        'Figure 1-2: Prefacing the core Lifecycle with an<br>' +
        'Enterprise Requirements phase<br>' +
        'Business Process: The DW/BI System Unit of<br>' +
        'Work<br>' +
        'We use the term business process to mean an<br>' +
        'operational activity the organization engages in to<br>' +
        'accomplish its primary goals. You can think of<br>' +
        'business processes as the links in the organization’s<br>' +
        '60<br>' +
        'value chain. Each business process typically has its<br>' +
        'own operational system or module that enables it,<br>' +
        'such as the order entry system, or the call tracking<br>' +
        'system, or the inventory management system. The<br>' +
        'information generated by these business processes<br>' +
        'measures only the business process itself, but that<br>' +
        'information usually has value well beyond the<br>' +
        'boundaries of the individual business process.<br>' +
        'Information from a single business process, such as<br>' +
        'orders information, could be of great interest to<br>' +
        'sales, marketing, customer service, and other<br>' +
        'groups across the organization.<br>' +
        'Each business process is a unique, coherent<br>' +
        'measurement system implemented as an<br>' +
        'operational system. If you need data from a given<br>' +
        'business process, you need to extract that data in its<br>' +
        'business context. In other words, you need to pull<br>' +
        'the measures and all of the associated descriptors<br>' +
        'in a careful, systematic fashion. This makes the<br>' +
        'business process the fundamental unit of work for<br>' +
        'the DW/BI system. Unless you have unlimited<br>' +
        'resources, your DW/BI team will concentrate on<br>' +
        'designing and loading data from one business<br>' +
        'process at a time.<br>' +
        'Establishing Initial Enterprise Requirements Project Scope<br>' +
        'The initial scope usually covers only the enterprise-level<br>' +
        'requirements definition and requirements prioritization<br>' +
        'steps, leaving the detailed project implementation plan for<br>' +
        '61<br>' +
        'later when you have a much better idea of what the project<br>' +
        'needs to accomplish from a business perspective. The<br>' +
        'requirements and prioritization usually involve user<br>' +
        'interviews, interview write-ups, a few meetings, and the<br>' +
        'creation of the final requirements document. It typically<br>' +
        'takes three to six weeks (or more) depending on how many<br>' +
        'interviews you do.<br>' +
        'Combining Enterprise and Project<br>' +
        'Requirements Gathering<br>' +
        'Some organizations we’ve worked with have a<br>' +
        'clear understanding of which business process is<br>' +
        'their top priority right from the start. In these cases,<br>' +
        'we often combine the enterprise requirements<br>' +
        'definition step and the project requirements<br>' +
        'definition step into a single effort.<br>' +
        'This does not lessen the importance of<br>' +
        'understanding the full range of enterprise<br>' +
        'requirements for information. In fact, we almost<br>' +
        'always go through the enterprise prioritization<br>' +
        'process with senior management. However,<br>' +
        'because the top priority is clear early on, we make<br>' +
        'sure we gather enough detailed information about<br>' +
        'that business process and the data it generates in<br>' +
        'the same interview set so we can create the design<br>' +
        'for the first business process in one pass instead of<br>' +
        'two.<br>' +
        '62<br>' +
        'RESOURCES<br>' +
        'Learn more about DW/BI project planning<br>' +
        'and management in Chapter 2 of The Data<br>' +
        'Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition.<br>' +
        'Gathering and Documenting Enterprise-Level Business<br>' +
        'Requirements<br>' +
        'The enterprise requirements definition step is designed to<br>' +
        'gather a broad, horizontal view of the organization from a<br>' +
        'business point of view. The process flow chart in Figure<br>' +
        '1-3 breaks the Enterprise requirements definition box from<br>' +
        'Figure 1-2 down into its subtasks. As you see in Figure<br>' +
        '1-3, the bulk of the work involves gathering and<br>' +
        'documenting those requirements.<br>' +
        'While the four steps that are circled on the left side of the<br>' +
        'figure are shown as separate subtasks, we usually do them<br>' +
        'in a pipeline fashion, conducting and documenting an<br>' +
        'interview, and extracting its analytic requirements. As the<br>' +
        'interviews progress, we begin synthesizing what we’ve<br>' +
        'learned, identifying the business processes that support the<br>' +
        'users’ analytic needs. At the same time, it’s important to<br>' +
        'conduct initial data profiling to match analytic needs with<br>' +
        'data realities. At the end of the interview process, we build<br>' +
        'an initial bus matrix to summarize the business processes<br>' +
        'we’ve heard about during the interviews. We describe the<br>' +
        'bus matrix and each of the core subtasks in Figure 1-3 in<br>' +
        '63<br>' +
        'this section, leaving the senior management prioritization<br>' +
        'session for its own section.<br>' +
        'Figure 1-3: The enterprise requirements definition process<br>' +
        'flow chart<br>' +
        '64<br>' +
        '65<br>' +
        'Preparation<br>' +
        'Requirements definition is largely a process of<br>' +
        'interviewing business and technical people, but before you<br>' +
        'get started, you need to do a little preparation. Learn as<br>' +
        'much as you can about your business, your competitors,<br>' +
        'your industry, and your customers. Read your<br>' +
        'organization’s annual report; track down any internal<br>' +
        'strategy documents; go online and see what’s said about<br>' +
        'your organization, the competition, and the industry in the<br>' +
        'press. Find out what the big challenges are. Learn the<br>' +
        'terms and terminology the business people use to describe<br>' +
        'what they do. In short, do your homework.<br>' +
        'Part of the preparation process is figuring out whom you<br>' +
        'should actually interview. This usually involves carefully<br>' +
        'examining an org chart with your sponsor and other key<br>' +
        'supporters. For the enterprise requirements pass, start the<br>' +
        'interview list with the CEO and senior staff. Add the<br>' +
        'analysts and managers who are known as leaders in the<br>' +
        'business intelligence area — folks whom senior<br>' +
        'management and co-workers turn to when they need<br>' +
        'information. They are also usually the folks who bug IT<br>' +
        'the most. If you’ve been at your company more than 12<br>' +
        'months, you know who these people are. They have their<br>' +
        'own Access databases, they write SQL against the<br>' +
        'transaction system, and they create reports and charts with<br>' +
        'whatever tools they have available (mostly Excel and<br>' +
        'Access). Finally, add on a couple of the key IT folks who<br>' +
        'can educate you about the nature of the source systems and<br>' +
        'the quality of the data they collect.<br>' +
        '66<br>' +
        'NOTE<br>' +
        'You are just making the list of whom to<br>' +
        'interview at this point, not the interview<br>' +
        'schedule. When you do start the schedule,<br>' +
        'begin with a few people you know and trust<br>' +
        'before you turn to senior management. At<br>' +
        'the same time, make sure you get the<br>' +
        'elusive executives on the calendar as early<br>' +
        'as possible. Some of these folks can be<br>' +
        'tough to pin down.<br>' +
        'A major goal during the interviews is to build positive<br>' +
        'working relationships with the business folks. These<br>' +
        'relationships will hinge on your understanding of the<br>' +
        'business issues your organization faces. In short, be<br>' +
        'prepared. Fortunately, gathering this information is not as<br>' +
        'difficult as it used to be, thanks to the internet. However,<br>' +
        'you still have to read it.<br>' +
        'RESOURCES<br>' +
        'You can find additional information about<br>' +
        'preparing for interviews in The Data<br>' +
        'Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition, pages 68–80.<br>' +
        'Adventure Works Example: Preparation<br>' +
        '67<br>' +
        'Much of preparation is about knowing your business. Not<br>' +
        'being a real business, Adventure Works does not have<br>' +
        'much strategic or market information available. However,<br>' +
        'you can get a good sense for the nature of any business<br>' +
        'with SQL Management Studio and a basic knowledge of<br>' +
        'SQL, or even better, a query tool like Report Builder 3.0.<br>' +
        'You also need permission to query the source system (or a<br>' +
        'copy of it, so you don’t cause any transaction problems).<br>' +
        'A few simple SELECTs and SUMs can tell you what<br>' +
        'products are selling well, where they are selling, who is<br>' +
        'selling them, and how this has all changed over time. At<br>' +
        'Adventure Works, you could quickly find out that over 80<br>' +
        'percent of their business is bicycles, versus clothing or<br>' +
        'accessories, and almost half their business is overseas.<br>' +
        'From a time series perspective, the company has been<br>' +
        'growing rapidly and the internet sales channel is a major<br>' +
        'contributor to this growth. Figure 1-4 shows a pivot table<br>' +
        'of the results of a query against the<br>' +
        'AdventureWorks2008R2 transaction database.<br>' +
        'Figure 1-4: Adventure Works sales by category by year<br>' +
        '68<br>' +
        'If you have a couple of hours, you can dig into other parts<br>' +
        'of the business, such as customer support, manufacturing,<br>' +
        'and finance.<br>' +
        'DOWNLOADS<br>' +
        'See the web content for more examples of<br>' +
        'these kinds of queries and results for the<br>' +
        'Adventure Works business.<br>' +
        'Conduct Business and IT Interviews<br>' +
        'The key to success in requirements interviews is to<br>' +
        'remember your overall mission. You are designing a<br>' +
        'system to add significant long-term business value. The<br>' +
        'most common mistake in these interviews is to ask the<br>' +
        'business person what they want (or need). Asking this<br>' +
        'question is the equivalent of abdicating your design<br>' +
        'responsibility. You are saying, “Tell me what you want<br>' +
        'and I’ll build it.” At best, you will get a limited description<br>' +
        'of what the person wants to solve today’s problem. For<br>' +
        'example, you may get a request to have all the data<br>' +
        'provided for a given report in an Excel format. This may<br>' +
        'sound great to you because it is easy to understand and<br>' +
        'execute. So, you go off to extract, clean, and load that data<br>' +
        'into a data warehouse, then set up a distribution system to<br>' +
        'put it into Excel and email it to the analyst every night.<br>' +
        'Once you are finished, you show it to the analyst, and they<br>' +
        'say, “It’s nice, but what I really want is this other report in<br>' +
        'an Excel format.” This is when we hear statements like<br>' +
        '“The business people don’t know what they want!” Or,<br>' +
        '69<br>' +
        '“The business people don’t understand business<br>' +
        'intelligence!”<br>' +
        'This lack of understanding is not their fault, and it’s an<br>' +
        'easy mistake to avoid by changing the questions you ask in<br>' +
        'the interview. Remember, it’s your job to design the<br>' +
        'system. In order to do that, you have to understand the<br>' +
        'business; you need to know what your users do and how<br>' +
        'they use information to do it. Once you know this, the<br>' +
        'required system designs, data models, and BI applications<br>' +
        'all become clear.<br>' +
        'The easy way to find this out is to start out with the simple<br>' +
        'question: “What do you do? Tell me about your roles and<br>' +
        'responsibilities.” Explore each of the areas they describe in<br>' +
        'terms of the information they need, and the value they<br>' +
        'provide to the organization (or could provide with better,<br>' +
        'more accessible information). Be flexible and follow the<br>' +
        'leads provided by the interviewee. Avoid sticking to a<br>' +
        'predetermined script planned in too much detail.<br>' +
        'Interviewing is a very valuable skill.<br>' +
        'RESOURCES<br>' +
        'For more in-depth interviewing tips and<br>' +
        'techniques, see the articles in the Kimball<br>' +
        'Group Reader on pages 113 and 117.<br>' +
        'In this first pass at gathering requirements, you will<br>' +
        'interview more senior level folks across the different<br>' +
        'departments and get a comprehensive list of the major<br>' +
        '70<br>' +
        'challenges and opportunities your organization faces.<br>' +
        'These challenges and opportunities often (but not always)<br>' +
        'line up with the strategic goals and initiatives of the<br>' +
        'organization.<br>' +
        'You will also interview some of the source systems experts<br>' +
        'to understand the structure and content of the source<br>' +
        'systems and the nature of any data problems that might be<br>' +
        'lurking out there. You must also perform data profiling on<br>' +
        'all candidate data sources. (Data profiling is described in<br>' +
        'an upcoming section.)<br>' +
        'Debrief and Document Interviews<br>' +
        'At the end of each interview the interview team must take<br>' +
        'a few minutes to debrief. Review your notes, fill in the<br>' +
        'blanks, make sure you understand the terms you heard, and<br>' +
        'capture the key issues. The longer you wait to do this, the<br>' +
        'less you will remember. We’ve found ourselves staring at<br>' +
        'a sentence that reads, “The most important factor in our<br>' +
        'business is . . .” with no idea what came next. Debrief as<br>' +
        'soon as possible. As you go through your notes, highlight<br>' +
        'and add comments to fully describe the following items:<br>' +
        '• Common, repetitive business requirements themes<br>' +
        '• Business processes (data sources) needed<br>' +
        '• Business requirements for specific reports and analyses<br>' +
        '• Misunderstandings or incomplete notes (the lead interviewer<br>' +
        'should keep a list of open issues)<br>' +
        '• Data or other feasibility issues known to the team<br>' +
        '• Success criteria<br>' +
        'The individual interviews will yield a wealth of<br>' +
        'information including descriptions of the analytic<br>' +
        '71<br>' +
        'requirements and their associated business processes, a<br>' +
        'starting point for the organization’s overall information<br>' +
        'architecture, and a list of any feasibility issues such as poor<br>' +
        'data quality.<br>' +
        'Identifying the business requirements for analytics is the<br>' +
        'hardest part. Depending on who you are talking to, similar<br>' +
        'analytic opportunities may be described broadly or<br>' +
        'specifically. As the interviews progress, you’ll see<br>' +
        'common requirements repeated over time. For example,<br>' +
        'the marketing person responsible for internet promotions<br>' +
        'might describe an opportunity to improve promotion<br>' +
        'response and conversion rates by better targeting certain<br>' +
        'geographic and demographic subsets of the population.<br>' +
        'The person responsible for product promotions might<br>' +
        'describe an opportunity to improve conversions by<br>' +
        'offering product recommendations based on customer<br>' +
        'behaviors. Both of these opportunities could be grouped<br>' +
        'together under a broader heading called Improve Customer<br>' +
        'Acquisition.<br>' +
        'Each of these broader analytic themes should have brief<br>' +
        'descriptions of the kinds of reports or analyses you heard<br>' +
        'in the interview. It should also include some sense of the<br>' +
        'business value of meeting the requirements. In other<br>' +
        'words, how much is improving customer conversion rates,<br>' +
        'or negotiating better prices and terms worth? Look for<br>' +
        'action words to identify these opportunities. Words like<br>' +
        'improve, reduce, increase, and enhance all lead to a<br>' +
        'business requirement we’d like to know about.<br>' +
        'Do not put this review off. After a day of interviews, you<br>' +
        'will have a hard time remembering who you spoke with,<br>' +
        '72<br>' +
        'let alone the details of what they said. Be cautious about<br>' +
        'scheduling too many interviews in one day. Our rule of<br>' +
        'thumb is four interviews and four debriefing periods per<br>' +
        'day.<br>' +
        'It’s a good idea to write up a summary document of each<br>' +
        'interview based on the annotated set of interview notes as<br>' +
        'soon as you can. This is more work because you need to<br>' +
        'summarize the various analytic areas covered in the<br>' +
        'interview, but it is a good communication and<br>' +
        'relationship-building tool with the business folks. Share<br>' +
        'this summary with the interviewee and ask for feedback; it<br>' +
        'shows that you listened to what they had to say and have<br>' +
        'an interest in helping. It also gives them a chance to clarify<br>' +
        'any misunderstandings and add any relevant items they<br>' +
        'overlooked.<br>' +
        'RESOURCES<br>' +
        'You can find additional information about<br>' +
        'conducting the interviews and debriefing<br>' +
        'on pages 80–85 of The Data Warehouse<br>' +
        'Lifecycle Toolkit, Second Edition.<br>' +
        'Adventure Works Example: Interview Documentation<br>' +
        'Adventure Works has close to 300 employees. The CEO<br>' +
        'has seven direct reports, and the vast majority of<br>' +
        'employees are manufacturing workers. In most small- to<br>' +
        'medium-sized organizations like this, you could build a<br>' +
        'solid set of business requirements for analytics by<br>' +
        '73<br>' +
        'speaking to most of the senior staff and a subset of<br>' +
        'managers and analysts; maybe 15 people, plus or minus<br>' +
        'five.<br>' +
        'The primary content of each interview summary write up<br>' +
        'will be a list of business analysis requirements. Each<br>' +
        'analytic requirement should also include a list of the<br>' +
        'business processes that generate the data needed to support<br>' +
        'the analysis, and any associated issues or concerns, such as<br>' +
        'data quality or availability problems. Figure 1-5 shows a<br>' +
        'summary of the business requirements identified by the VP<br>' +
        'of Sales.<br>' +
        'DOWNLOADS<br>' +
        'You can find an Adventure Works<br>' +
        'organizational chart and an example<br>' +
        'interview summary from the Adventure<br>' +
        'Works VP of Sales in the web content<br>' +
        'downloads.<br>' +
        'It should come as no surprise that most of the VP of Sales’<br>' +
        'business requirements for analysis are based on data from<br>' +
        'the orders business process. However, there are several<br>' +
        'other business processes that inform decision making for<br>' +
        'the sales department. Note that the VP has described a<br>' +
        'requirement for a customer satisfaction dashboard. By<br>' +
        'decomposing it to its underlying data sources, it becomes<br>' +
        'clear that you will need data from three major source<br>' +
        'systems: orders, call tracking, and returns. This means<br>' +
        'three iterations of the Lifecycle with three sets of ETL<br>' +
        '74<br>' +
        'code, BI applications, testing, and deployment. Now is a<br>' +
        'good time to start educating and setting expectations.<br>' +
        'Data Auditing/Data Profiling<br>' +
        'At the same time you are interviewing people and creating<br>' +
        'the summaries, you will also need to do some queries<br>' +
        'against the source system data to get a firsthand<br>' +
        'understanding of the data issues. This kind of querying has<br>' +
        'come to be known as data profiling or data auditing, and<br>' +
        'there are several tools on the market designed to support it.<br>' +
        'There are three major points in the Lifecycle where data<br>' +
        'profiling is helpful. The first is here, during the<br>' +
        'requirements definition process where you should do a<br>' +
        'simple red light/green light assessment of your<br>' +
        'organization’s data assets. You aren’t looking for nuances<br>' +
        'at this point, but if a data source needs to be disqualified,<br>' +
        'now is the time. The second place to do data profiling is<br>' +
        'during the design of the dimensional model, and the third<br>' +
        'is during the design and implementation of the ETL<br>' +
        'process. You will want to do more in-depth data profiling<br>' +
        'once you select a specific business process and begin<br>' +
        'defining project-level business requirements. We describe<br>' +
        'data profiling in more detail when we discuss the<br>' +
        'dimensional modeling process in Chapter 2.<br>' +
        'Figure 1-5: Business requirements and supporting<br>' +
        'business processes from the interview summary<br>' +
        '75<br>' +
        'Creating the Program Requirements Findings Document<br>' +
        '76<br>' +
        'The overall findings document for the enterprise-level<br>' +
        'requirements includes the business process summaries, the<br>' +
        'bus matrix, and the prioritized results. You might want to<br>' +
        'include the interview summaries as an appendix for those<br>' +
        'readers who want all the detail.<br>' +
        'The bulk of the requirements document will be a list of the<br>' +
        'business processes and the business requirements they<br>' +
        'support. Each business process section should include<br>' +
        'some sense for the business value it would generate, the<br>' +
        'data quality and other feasibility issues associated with it,<br>' +
        'and the parts of the organization that will benefit from it.<br>' +
        'Some of the requirements on the list may represent new<br>' +
        'ways of doing business and will require new transaction<br>' +
        'systems, or at least significant changes to existing<br>' +
        'transaction systems. But even a quick review of the simple<br>' +
        'list provided begins to bring out ideas about how the DW/<br>' +
        'BI system can address some of these needs in the short<br>' +
        'term.<br>' +
        'Synthesize Around Business Processes<br>' +
        'The goal here is to tie the business requirements back to<br>' +
        'the underlying data needed to make them happen. This is<br>' +
        'the primary factor in determining the level of effort<br>' +
        'required to deliver a solution to a given business<br>' +
        'requirement.<br>' +
        'As you extract the business requirements from the<br>' +
        'interview summaries, you need to dig into each<br>' +
        'opportunity to identify the business process (or processes)<br>' +
        'that generate the data needed to perform the desired<br>' +
        'analyses. For example, the requirement to reduce<br>' +
        '77<br>' +
        'purchasing costs through better contract negotiations can<br>' +
        'be supported by historical data from the purchasing<br>' +
        'transaction system. You convert from requirements to<br>' +
        'business processes because business processes are the units<br>' +
        'of work in building the DW/BI system. Each business<br>' +
        'process is usually measured by a single source system<br>' +
        'module, which translates into a single pass through the<br>' +
        'Kimball Lifecycle process. (Refer back to the related<br>' +
        'sidebar, “Business Process: the DW/BI System Unit of<br>' +
        'Work,” for more information.)<br>' +
        'Although many business requirements need information<br>' +
        'only from a single business process, one challenge you<br>' +
        'will face is that some requirements need data from<br>' +
        'multiple business processes to meet the overall analytic<br>' +
        'needs. Customer and product profitability analyses are<br>' +
        'good examples. The “customer scorecard” may sound like<br>' +
        'a single analysis, but it actually requires data from many<br>' +
        'separate business processes. We call these consolidated<br>' +
        'requirements (or, occasionally, second level business<br>' +
        'processes) because they cannot be completely built until<br>' +
        'data from all prerequisite business processes have been<br>' +
        'loaded into the data warehouse.<br>' +
        'Tying business requirements to the underlying business<br>' +
        'processes helps determine the level of effort needed to<br>' +
        'support a given requirement. If a requirement must have<br>' +
        'data from more than one business process, it will take<br>' +
        'more than one iteration of the Lifecycle. While these<br>' +
        'passes can happen in parallel with additional resources,<br>' +
        'there is no way around doing the work.<br>' +
        '78<br>' +
        'Converting from business requirements to business<br>' +
        'processes involves thinking about which business<br>' +
        'processes are required to support each requirement. For<br>' +
        'example, you can get a start at improving promotion<br>' +
        'response rates with data from a single business process:<br>' +
        'orders. As long as the order data captures a promotion<br>' +
        'code, you can calculate response rates and purchase<br>' +
        'amounts associated with each promotion. Better analysis<br>' +
        'would need to include data from the promotions business<br>' +
        'process; specifically, what was the total population of<br>' +
        'prospects who received a promotion. Now you can see<br>' +
        'who responded from orders, and who didn’t from<br>' +
        'promotions. In the best case, you may want to bring in<br>' +
        'demographic data for prospects and use a data mining<br>' +
        'model to help identify the characteristics of prospects that<br>' +
        'are more likely to respond to a given promotion.<br>' +
        'Ultimately, complete promotions analysis would need data<br>' +
        'from at least three business processes (at least one,<br>' +
        'demographic data, comes from an external source). If<br>' +
        'improving promotion response rates is a top business<br>' +
        'priority, this decomposition process will help you show<br>' +
        'why it will take three iterations to complete the required<br>' +
        'data set. It will also show that basic analysis can begin<br>' +
        'once the first business process (orders) is loaded.<br>' +
        'The most demanding type of business requirement is often<br>' +
        'called a scorecard or executive dashboard. This<br>' +
        'deceptively simple application draws on data from almost<br>' +
        'all business processes in the organization. You can’t create<br>' +
        'the entire dashboard until you’ve built the whole data<br>' +
        'warehouse foundation. Or worse, you end up building the<br>' +
        'dashboard by hand every day, manually extracting,<br>' +
        'copying, and pasting data from all those sources to make it<br>' +
        '79<br>' +
        'work. It can be difficult to get business folks to understand<br>' +
        'the magnitude of the effort involved in creating this<br>' +
        '“simple” report.<br>' +
        'Adventure Works Example: Enterprise Requirements<br>' +
        'Documentation<br>' +
        'This section walks you through the steps to create a<br>' +
        'requirements findings document for the Adventure Works<br>' +
        'example. The easiest way to create the requirements<br>' +
        'summary is to start with the requirements from one<br>' +
        'interview, such as those shown in Figure 1-5, and add in<br>' +
        'requirements from subsequent interviews. Often<br>' +
        'requirements from different interviews will fall into<br>' +
        'existing business requirements categories.<br>' +
        'To understand what analyses will be enabled by each<br>' +
        'iteration of the Lifecycle, you need to re-sort the<br>' +
        'requirements by business process. Table 1-1 shows a<br>' +
        'subset of the analyses enabled by each individual business<br>' +
        'process. The Letter column is meant to serve as a<br>' +
        'shorthand reference you may use later in the prioritization<br>' +
        'process.<br>' +
        'This table includes analytic requirements from across the<br>' +
        'Adventure Works enterprise. The VP of Sales’<br>' +
        'requirements are underlined. You should keep track of<br>' +
        'where the individual requirement for analysis came from<br>' +
        'within the organization. This will help you track back to<br>' +
        'the originator later on, after you re-sort the requirements<br>' +
        'by underlying business process.<br>' +
        '80<br>' +
        'Table 1-1: A subset of business processes derived from<br>' +
        'the requirements interviews<br>' +
        'Letter Business<br>' +
        'Process<br>' +
        'Supported Business Analyses<br>' +
        'A Orders<br>' +
        'Orders reporting and analysis, orders forecasting,<br>' +
        'advertising effectiveness, customer satisfaction,<br>' +
        'production forecasting, product profitability, customer<br>' +
        'profitability<br>' +
        'B<br>' +
        'Orders<br>' +
        'forecast<br>' +
        'Sales performance, business planning, production<br>' +
        'forecasting<br>' +
        'C<br>' +
        'Call<br>' +
        'tracking<br>' +
        'Call center performance, customer satisfaction, product<br>' +
        'quality, call center resource planning, customer<br>' +
        'profitability, product profitability<br>' +
        'D Returns Customer satisfaction, product quality, customer<br>' +
        'profitability, product profitability, net sales<br>' +
        'Building the Initial Data Warehouse Bus Matrix<br>' +
        'As you identify the business processes needed to support<br>' +
        'each analytic requirement, you will also add those business<br>' +
        'processes to an enterprise data framework called the Data<br>' +
        'Warehouse Bus Matrix. This matrix maps your<br>' +
        'organizational business processes to the entities or objects<br>' +
        'that participate in those processes.<br>' +
        'Each row in the matrix is a business process. Figure 1-6<br>' +
        'shows a simplified example bus matrix for a retail<br>' +
        'company. Notice how the business processes down the left<br>' +
        'side of the matrix follow the organization’s value chain. In<br>' +
        'this case, the company buys goods from their vendors and<br>' +
        'stores them in distribution centers. Then, as goods are<br>' +
        'demanded by consumers, they are moved out to the retail<br>' +
        'stores where they’re held on shelves until the customer<br>' +
        'buys them and the goods leave the company’s value chain.<br>' +
        '81<br>' +
        'These business processes generally correspond to<br>' +
        'individual source systems or modules in the overall<br>' +
        'Enterprise Resource Planning (ERP) system.<br>' +
        'The columns in the bus matrix are the descriptive objects<br>' +
        'that participate in the various business processes, such as<br>' +
        'store, product, and date. They contrast with the<br>' +
        'measurement-driven business processes that label the rows<br>' +
        'of the matrix. We call these objects dimensions in the<br>' +
        'dimensional model. Each dimension participates in one or<br>' +
        'more business processes — we indicate this by placing an<br>' +
        'X in the intersecting cell in the matrix. For example, the<br>' +
        'Vendor dimension is involved in both the purchasing and<br>' +
        'delivery processes. The store sale business process, on the<br>' +
        'other hand, does not involve the vendor or distribution<br>' +
        'center.<br>' +
        'Figure 1-6: Example enterprise bus matrix for a retail<br>' +
        'company<br>' +
        'The bus matrix is essentially your enterprise dimensional<br>' +
        'data architecture. For each business process (row), you can<br>' +
        'see exactly which dimensions (columns) you need to<br>' +
        'implement. And for each dimension, you can see which<br>' +
        '82<br>' +
        'business processes it must support. This<br>' +
        'dimension-oriented view is the visual representation of<br>' +
        'conformed dimensions — a concept we define in the next<br>' +
        'chapter.<br>' +
        'The business processes in the bus matrix, and the analytic<br>' +
        'requirements they support (and the value those<br>' +
        'requirements represent) become the major inputs to the<br>' +
        'next step in the requirements definition process: a<br>' +
        'prioritization session with senior management.<br>' +
        'Adventure Works Example: Bus Matrix<br>' +
        'As you go through the interview process, you may be<br>' +
        'surprised to discover a many-to-many relationship between<br>' +
        'people and data. That is, people need access to data from<br>' +
        'multiple business processes, and many people often want<br>' +
        'to look at data from the same business process, but from<br>' +
        'their own business perspective. For instance, people in<br>' +
        'marketing might be interested in orders data by product<br>' +
        'over time, while folks in sales might be interested in the<br>' +
        'same orders data, only by sales rep and region. As a<br>' +
        'reminder, this means you would design the orders data<br>' +
        'model at the atomic level so the same data set could be<br>' +
        'used to support both sales and marketing. It does NOT<br>' +
        'mean you should have two separate data marts, one for<br>' +
        'each department.<br>' +
        'Figure 1-7 shows the start of a bus matrix for Adventure<br>' +
        'Works based on the interview with the VP of Sales that is<br>' +
        'included on the book’s web site.<br>' +
        '83<br>' +
        'Figure 1-7: Bus Matrix business processes referred to in<br>' +
        'the VP of Sales interview<br>' +
        'Here you see that one person, the VP of Sales, is interested<br>' +
        'in data from five different business processes.<br>' +
        'Once you’ve got the requirements documented, it will<br>' +
        'become clear that you can’t deliver them all at once. The<br>' +
        'prioritization process will help you and your organization<br>' +
        'figure out the appropriate order of events.<br>' +
        'Prioritizing the Business Requirements<br>' +
        'If you’re a technical person, it’s safe to say the<br>' +
        'prioritization process we describe here is one of the most<br>' +
        'powerful business tools you’ll ever use. This is a bold<br>' +
        'statement, but we have used this tool many times and have<br>' +
        'been repeatedly successful. We’ve conducted a few<br>' +
        'prioritization sessions where the client decided not to<br>' +
        'move forward with the DW/BI project right away. This<br>' +
        '84<br>' +
        'decision is usually reached because the prioritization<br>' +
        'process helped senior management better understand the<br>' +
        'nature of the commitment or the size of the data problems.<br>' +
        'This is also a success because it means they will work to<br>' +
        'fix the problems rather than try to build a DW/BI system<br>' +
        'on shaky ground.<br>' +
        'The prioritization process is a planning meeting involving<br>' +
        'the DW/BI team, the DW/BI project business sponsors,<br>' +
        'and other key senior managers from across the<br>' +
        'organization.<br>' +
        'In this meeting, you describe the business processes you<br>' +
        'identified in the enterprise requirements gathering process<br>' +
        'so everyone has an understanding of the full list of<br>' +
        'possibilities. Go into this session armed with a PowerPoint<br>' +
        'presentation that describes each business process, gives a<br>' +
        'few examples of the associated analyses it will support<br>' +
        'along with a feel for the business value of those analyses,<br>' +
        'and includes an initial sense of level of effort needed to<br>' +
        'implement the business process (its feasibility). Be as crisp<br>' +
        'and clear as possible. Try to keep this presentation under<br>' +
        '90 minutes. As you describe each business process, you<br>' +
        'also describe the relative effort involved in supplying the<br>' +
        'needed data. Once everyone has an understanding of the<br>' +
        'business processes and terminology, take a break.<br>' +
        'The second half of the session involves prioritizing the<br>' +
        'business processes. Lead the group in placing a sticky note<br>' +
        'for each business process onto a large version of a<br>' +
        'two-by-two grid like the Adventure Works example shown<br>' +
        'in Figure 1-8. This is an interesting exercise in negotiation<br>' +
        '85<br>' +
        'and education and can easily take another hour and a half<br>' +
        'or more.<br>' +
        'Figure 1-8: Example prioritization grid from Adventure<br>' +
        'Works Cycles<br>' +
        'The prioritization grid is deceptively simple: Study it<br>' +
        'carefully. The Y axis is about relative business value. The<br>' +
        'group needs to reach consensus on the relative impact of<br>' +
        'implementing each business process. The participants need<br>' +
        'to remember to take an organizational approach to<br>' +
        'assigning business value. There will always be someone<br>' +
        'who thinks any given business process is the absolute top<br>' +
        'priority. Gently remind them that there’s more to the<br>' +
        'business than their little slice.<br>' +
        'The X axis represents the level of effort each business<br>' +
        'process will take to implement. It is stated in terms of<br>' +
        'relative feasibility so the easier business processes go to<br>' +
        'the right (high feasibility) and the harder business<br>' +
        '86<br>' +
        'processes go to the left (low feasibility). The DW/BI team<br>' +
        'leads the assignment of feasibility because team members<br>' +
        'have a better sense about the technical difficulties involved<br>' +
        'in each business process (although feasibility is not just<br>' +
        'technical — there are often organizational and political<br>' +
        'difficulties as well).<br>' +
        'The true feasibility is not fully understood at this point. If<br>' +
        'you have someone on the team who’s been in the<br>' +
        'organization long enough, she should have a good sense<br>' +
        'for the level of effort required to implement each business<br>' +
        'process. One obvious factor is when business processes<br>' +
        'must be implemented together to support a high value,<br>' +
        'consolidated theme, such as customer or product<br>' +
        'profitability.<br>' +
        'The prioritization session is a good opportunity to educate<br>' +
        'the business folks about how bad things really are. You<br>' +
        'don’t want to sound negative, but it’s important to explain<br>' +
        'the level of effort it takes to gather the data and make it<br>' +
        'useful. For example, integrating customer IDs from two<br>' +
        'different source systems is a grind.<br>' +
        'When reviewing Figure 1-8, note that there are two items<br>' +
        'on the grid that are not actually business processes.<br>' +
        'Customer profitability and product profitability are<br>' +
        'consolidated themes that senior management has expressed<br>' +
        'significant interest in analyzing. These have been included<br>' +
        'on the grid to show their importance, but they are far over<br>' +
        'to the left to indicate the difficulty involved in building all<br>' +
        'the needed business processes. Given the number of<br>' +
        'analyses supported by data from the orders business<br>' +
        '87<br>' +
        'process, it should come as no surprise that orders is the top<br>' +
        'priority. The team should get to work on this right away!<br>' +
        'A Credibility Booster<br>' +
        'The prioritization process uses a common business<br>' +
        'school tool called the two-by-two matrix. This<br>' +
        'matrix was popularized in the early 1970s by the<br>' +
        'Boston Consulting Group. BCG used a<br>' +
        '“Growth-Share Matrix” to compare different<br>' +
        'business units in a portfolio by comparing relative<br>' +
        'market share with industry sales growth rates. A<br>' +
        'business unit with high market share in an industry<br>' +
        'with high growth rate was called a “Star.” By<br>' +
        'contrast, a business unit with low market share in a<br>' +
        'low-growth industry was a “Pet” (later referred to<br>' +
        'as a “Dog”).<br>' +
        'The great thing about the matrix is the positive<br>' +
        'impression the DW/BI team makes by cleverly<br>' +
        'adapting a classic MBA tool.<br>' +
        'Sources: The Boston Consulting Group,<br>' +
        'Perspectives on Experience, and The Product<br>' +
        'Portfolio (Boston, MA: The Boston Consulting<br>' +
        'Group, 1968).<br>' +
        'Once all the business processes have been placed and<br>' +
        'everyone agrees on their relative locations, convert the<br>' +
        'matrix to a prioritized list of projects. One way to do this is<br>' +
        'to start in the upper-right corner of the prioritization grid<br>' +
        '88<br>' +
        'and move to the lower-left corner, numbering the business<br>' +
        'processes as you encounter them. The two-dimensional<br>' +
        'nature of the matrix makes this a little difficult. Use the<br>' +
        'concept of concentric circles to establish a priority order,<br>' +
        'like ripples on a pond, centered in the upper-right corner.<br>' +
        'The output of the prioritization process is a list of business<br>' +
        'processes in priority order. This list is your DW/BI<br>' +
        'roadmap; it tells you which row on the matrix, and which<br>' +
        'dimensions, to implement first. Less tangible, but equally<br>' +
        'important outcomes of the prioritization process are senior<br>' +
        'management consensus around the DW/BI roadmap, and a<br>' +
        'general improvement in the relationships between IT and<br>' +
        'the business.<br>' +
        'In most cases, you will make only one pass at the<br>' +
        'enterprise requirements. Once the priorities are in place,<br>' +
        'the next pass and all subsequent passes will be at the level<br>' +
        'of the individual row on the bus matrix, the business<br>' +
        'process. Each row essentially becomes a project in the<br>' +
        'overall DW/BI program. From here on out, you will update<br>' +
        'enterprise business requirements and revisit priorities as<br>' +
        'the business changes, but most requirements definition<br>' +
        'efforts will be at the business process project level.<br>' +
        'RESOURCES<br>' +
        'Learn more about the requirements<br>' +
        'prioritization process on pages 91–93 of<br>' +
        '89<br>' +
        'The Data Warehouse Lifecycle Toolkit,<br>' +
        'Second Edition.<br>' +
        'Once you’ve completed the prioritization session you can<br>' +
        'finalize the overall requirements document by including<br>' +
        'the resulting list of prioritized business processes. At this<br>' +
        'point the conceptual foundation of the DW/BI system is in<br>' +
        'place. The rest of the Lifecycle depends on what you<br>' +
        'learned in these initial steps to make decisions and set<br>' +
        'priorities for all three tracks that follow, and on into the<br>' +
        'deployment, maintenance, and growth phases.<br>' +
        'RESOURCES<br>' +
        'You can find additional information about<br>' +
        'creating the requirements deliverables on<br>' +
        'pages 85–91 of The Data Warehouse<br>' +
        'Lifecycle Toolkit, Second Edition.<br>' +
        'Revisiting the Project Planning<br>' +
        'Now that you have a clear idea of your top priority<br>' +
        'business process, the data it generates, and the business<br>' +
        'requirements it supports, you can lay out a more detailed<br>' +
        'and precise project plan. This process is not much different<br>' +
        'from project planning for any major information<br>' +
        'technology project.<br>' +
        '90<br>' +
        'The plan will continue to evolve as you get more detail<br>' +
        'about the business requirements in the next step. There is a<br>' +
        'two-way arrow between the project planning and business<br>' +
        'requirements definition steps in Figure 1-1, but the<br>' +
        'backward flow is not as large because you gained<br>' +
        'significant understanding of the nature of the opportunity<br>' +
        'in the enterprise requirements gathering and narrowed your<br>' +
        'scope in the prioritization process.<br>' +
        'Gathering Project-Level Requirements<br>' +
        'Gathering project requirements follows the same basic<br>' +
        'process as the enterprise requirements gathering process<br>' +
        'described earlier. The difference is that now you have<br>' +
        'selected a particular business process on the bus matrix to<br>' +
        'implement. The enterprise requirements definition process<br>' +
        'provides a solid foundation for the project requirements.<br>' +
        'You now will deepen your understanding of the chosen<br>' +
        'business process.<br>' +
        'The project requirements gathering step is about pulling<br>' +
        'together the information you need to be successful in the<br>' +
        'three tracks that follow. Specifically, you need enough<br>' +
        'detail to create real, practical, flexible data models that will<br>' +
        'support a broad range of analytic needs. You need a solid<br>' +
        'understanding of the technical issues around data volumes,<br>' +
        'data cleaning, data movement, user access, and a host of<br>' +
        'other issues so you can create a capable, flexible technical<br>' +
        'architecture to support the warehouse now and in the<br>' +
        'future. Finally, you need a clear understanding of the<br>' +
        'business analysis requirements to build the initial set of<br>' +
        'business intelligence applications to demonstrate value<br>' +
        'from the very start.<br>' +
        '91<br>' +
        'The same three steps you followed in the enterprise<br>' +
        'requirements process apply to the project requirements<br>' +
        'process: preparation, interviews, and documentation.<br>' +
        'As we described in the enterprise requirements section,<br>' +
        'preparation is the critical first step. If you haven’t already,<br>' +
        'do your homework. Study the particular business process<br>' +
        'in detail. Figure out as much as you can about how it<br>' +
        'works before you begin the interviews. Learn the business<br>' +
        'terminology, the steps in the business process, and how it<br>' +
        'is measured.<br>' +
        'The goal with this round of interviews is to drill down on<br>' +
        'the selected business process in detail to understand the<br>' +
        'analyses, data models, and technologies required to make<br>' +
        'it work. This time you may take a more vertical slice of the<br>' +
        'organization, depending on the business process (some<br>' +
        'business processes have broader organizational appeal than<br>' +
        'others). Talk to the analysts, managers, report developers,<br>' +
        'and source systems people who can help you understand<br>' +
        'the intricacies of the business process in question. The<br>' +
        'actual interview process itself is generally the same as<br>' +
        'before.<br>' +
        'Applying this interview approach to the Adventure Works<br>' +
        'example, the team will need to hold an additional set of<br>' +
        'interviews to drill down on orders-related analyses before<br>' +
        'it can start working designing the Orders business process<br>' +
        'dimensional model. The team needs to understand several<br>' +
        'issues that were raised in the enterprise requirements<br>' +
        'process. We’ll look at the impact of a detailed<br>' +
        'understanding of questions like, “What is a customer?”<br>' +
        'and, “How do we determine the Sales Territory?” in the<br>' +
        '92<br>' +
        'next chapter, which is on dimensional modeling. The team<br>' +
        'should also get more specific about the kinds of new<br>' +
        'reports and analyses people want to see as input to the BI<br>' +
        'Application track.<br>' +
        'In fact, all of the information gathered in this second pass<br>' +
        'becomes the grist for the Adventure Works Cycles<br>' +
        'business dimensional modeling process case study in<br>' +
        'Chapter 2.<br>' +
        'Alternatives to Individual Interviews<br>' +
        'If interviews won’t work in your situation, we have<br>' +
        'had success with group requirements gathering<br>' +
        'sessions, but they are more risky. If you must do<br>' +
        'group sessions, here are a few tips:<br>' +
        '• Preparation is even more important. You have to<br>' +
        'know the business, and you also have to know what<br>' +
        'you want to accomplish and how you are going to go<br>' +
        'about it.<br>' +
        '• Have a clear agenda with times listed for each<br>' +
        'section, breaks, and food and drink. Reserve a good<br>' +
        'room with plenty of space and comfortable chairs.<br>' +
        'Make sure you have all the tools you need — flip<br>' +
        'charts, markers, white boards, computers, and a<br>' +
        'projector — whatever makes sense for your plan.<br>' +
        '• Get a strong, experienced design meeting leader to<br>' +
        'run the meetings. You have only a short time. If<br>' +
        'someone takes the meeting off course, you won’t get<br>' +
        'what you need.<br>' +
        'Depending on the business process selected, consider<br>' +
        'whether to interview your customers and suppliers. They<br>' +
        'are, or could be, business users of information in the DW/<br>' +
        '93<br>' +
        'BI system. In fact, the need to offer information outside<br>' +
        'the organization is common enough that many of the BI<br>' +
        'tool vendors include extranet access functionality as part<br>' +
        'of their product line. Listen carefully during the interviews<br>' +
        'to see if this is a likely source of significant business value<br>' +
        'for your organization.<br>' +
        'Interviews with key source system people and data<br>' +
        'profiling play a bigger role in the project requirements<br>' +
        'gathering process. Strive to learn as much as possible<br>' +
        'about both the business requirements and the data realities.<br>' +
        'The documentation process for the project requirements is<br>' +
        'similar to that of the enterprise definition process, except it<br>' +
        'is more detailed. Where the analytic requirements at the<br>' +
        'enterprise level ranged across all the business processes, at<br>' +
        'the project level, they should all be focused on the initial<br>' +
        'business process.<br>' +
        'Although the project requirements definition task sounds a<br>' +
        'bit abbreviated here, it is actually the definition task you<br>' +
        'will repeat over and over, every time you iterate through<br>' +
        'the Lifecycle to bring the next priority business process<br>' +
        'into the DW/BI system. Let’s hope you need to do the<br>' +
        'enterprise-level task only once, and then keep it updated.<br>' +
        'RESOURCES<br>' +
        'To learn more about defining project level<br>' +
        'requirements, see pages 93–101 of The<br>' +
        '94<br>' +
        'Data Warehouse Lifecycle Toolkit, Second<br>' +
        'Edition.<br>' +
        'Search kimballgroup.com for the topics “Business<br>' +
        'Requirements” and “Business Acceptance” for several<br>' +
        'related articles.<br>' +
        'Summary<br>' +
        'This chapter concentrated on the early tasks in the<br>' +
        'Lifecycle involving business requirements gathering,<br>' +
        'prioritization, and project planning. We gave special<br>' +
        'emphasis to the importance of understanding and<br>' +
        'documenting the business requirements.<br>' +
        'We described a process for gaining sponsorship, defining<br>' +
        'and documenting the enterprise-level business<br>' +
        'requirements, prioritizing the opportunities with senior<br>' +
        'business people, and gathering project requirements related<br>' +
        'to the top priority business process. This process also<br>' +
        'included the challenging task of tying the analytic<br>' +
        'requirements down to the business processes that provide<br>' +
        'the underlying information.<br>' +
        'The chapter also summarized some of the business<br>' +
        'requirements that might be found at a company like<br>' +
        'Adventure Works Cycles. The VP of Sales provided a set<br>' +
        'of analytic business requirements that tied to the business<br>' +
        'processes that fed into the bus matrix and the prioritization<br>' +
        'process.<br>' +
        '95<br>' +
        'These upfront business-related phases of each DW/BI<br>' +
        'project iteration are the most important. Unfortunately,<br>' +
        'they can be intimidating for technologists. Do not resist or<br>' +
        'avoid the requirements gathering phase of the project. The<br>' +
        'resulting understanding of the business issues, their<br>' +
        'priorities, and the data that supports their solution is<br>' +
        'priceless for the DW/BI team. The requirements document<br>' +
        'will be your reference point for all major decisions from<br>' +
        'here on out. You get huge value just from the content of<br>' +
        'the document alone.<br>' +
        'But wait, there’s more! The requirements gathering<br>' +
        'process also helps you build positive working relationships<br>' +
        'with the business people. As the business people<br>' +
        'participate in the requirements process, they see that<br>' +
        'you’ve done your homework. You understand them, you<br>' +
        'speak their language, you want to help solve the problem<br>' +
        '— in short, you get it.<br>' +
        'If that’s not enough to convince you, there are even more<br>' +
        'benefits to this process. Not only do you get documented<br>' +
        'requirements and better relationships, you gain active user<br>' +
        'support. As the business folks begin to understand your<br>' +
        'vision for an information solution, they see how your<br>' +
        'success ultimately leads to their success. They begin to see<br>' +
        'how their involvement will improve the chances of success<br>' +
        'for the DW/BI system and for the business itself.<br>' +
        '96<br>';
    document.getElementById('introduction').innerHTML = 'Introduction<br>' +
        'The goal of this book is to guide the reader down the best<br>' +
        'path toward designing and building a successful business<br>' +
        'intelligence system and its underlying data warehouse<br>' +
        'databases using the Microsoft SQL Server product set.<br>' +
        'The Data Warehouse and Business Intelligence System<br>' +
        'Data warehousing and business intelligence are techniques<br>' +
        'to provide business people with the information and tools<br>' +
        'they need to make both operational and strategic business<br>' +
        'decisions. We’ll break this down a bit so you can really<br>' +
        'understand the nature and magnitude of what you’re about<br>' +
        'to take on.<br>' +
        'First, your customers are the business people in the<br>' +
        'organization. Not all business people carry the same<br>' +
        'importance to you — you should be especially concerned<br>' +
        'with those who make strategic business decisions. One<br>' +
        'well-made business decision can translate to millions of<br>' +
        'dollars in many organizations. Your main customers are<br>' +
        'executives, managers, and analysts throughout the<br>' +
        'organization. The data warehouse and business intelligence<br>' +
        '(DW/BI) system is high impact and high profile.<br>' +
        'Strategic also means important. These are decisions that<br>' +
        'can make or break the organization. Therefore, the DW/BI<br>' +
        'system is a high-risk endeavor. When strategic decisions<br>' +
        'are made, someone often wins or loses. The DW/BI system<br>' +
        'is a highly political effort.<br>' +
        '21<br>' +
        'Increasingly, the DW/BI system also supports operational<br>' +
        'decisions, especially where the decision maker needs to<br>' +
        'see historical data or integrated data from multiple sources.<br>' +
        'Many analytic applications have this operational focus.<br>' +
        'Whether the decision making is strategic or operational,<br>' +
        'the DW/BI team needs to provide the information<br>' +
        'necessary to make decisions.<br>' +
        'Any given decision will likely require a unique subset of<br>' +
        'information that generally cannot be predetermined. You’ll<br>' +
        'need to build an information infrastructure that integrates<br>' +
        'data from across the organization, and potentially from<br>' +
        'outside the organization, and then cleans, aligns, and<br>' +
        'restructures the data to make it as flexible and usable as<br>' +
        'possible. Whereas most transaction system modules work<br>' +
        'with one type of information, such as billings, orders, or<br>' +
        'accounts receivable, the DW/BI system must eventually<br>' +
        'integrate them all. The DW/BI system requires technically<br>' +
        'sophisticated data gathering and management.<br>' +
        'Finally, you need to provide the business decision makers<br>' +
        'with the tools they need to make use of the data. In this<br>' +
        'context, “tools” means much more than just software. It<br>' +
        'means everything the business users need to understand<br>' +
        'what information is available, find the subsets they need,<br>' +
        'and structure the data to illuminate the underlying business<br>' +
        'dynamics. “Tools” includes training, documentation, and<br>' +
        'support, along with ad hoc query tools, reports, and<br>' +
        'analytic applications.<br>' +
        'Let’s review. The DW/BI system:<br>' +
        '• Is high profile and high impact<br>' +
        '22<br>' +
        '• Is high risk<br>' +
        '• Is highly political<br>' +
        '• Requires technically sophisticated and complex data<br>' +
        'gathering and management<br>' +
        '• Requires intensive user access, training, and support<br>' +
        'Creating and managing the DW/BI system is an extremely<br>' +
        'challenging task. We want you to take on this task with<br>' +
        'full knowledge of what you’re getting into. In our<br>' +
        'experience, it’s easier to deal with all of the challenges if<br>' +
        'you’re at least somewhat forewarned.<br>' +
        'We don’t mean this to discourage you, but rather to warn<br>' +
        'you before you jump in that the waters are swift and deep.<br>' +
        'All the reasons that make the data warehouse challenging<br>' +
        'are also what make it a fun and exciting project.<br>' +
        'The Kimball Group<br>' +
        'While it’s true that building and managing a successful<br>' +
        'DW/BI system is a challenge, it’s also true that there are<br>' +
        'ways to approach it that will increase your likelihood of<br>' +
        'success. That’s what the Kimball Group is all about.<br>' +
        'We’ve been working in the DW/BI area for more than 25<br>' +
        'years. The authors of this book, who are members of the<br>' +
        'Kimball Group, have spent their careers working on data<br>' +
        'warehousing and business intelligence systems as vendors,<br>' +
        'consultants, implementers, and users. Our motto is<br>' +
        '“Practical techniques — proven results.” We share a<br>' +
        'common drive to figure out the best way to build and<br>' +
        'manage a successful DW/BI system. We are also teachers<br>' +
        'at heart, with a strong desire to help you succeed and avoid<br>' +
        'the mistakes we and others have made.<br>' +
        '23<br>' +
        'Why We Wrote This Book<br>' +
        'Data warehousing and business intelligence have been<br>' +
        'around in much the same form since at least the 1970s, and<br>' +
        'continue to enjoy an incredibly long technology lifecycle.<br>' +
        'In 1995, when the primary authors formed our first<br>' +
        'consulting organization, one of us voiced the opinion that<br>' +
        'data warehousing was finished, that the wave had crested<br>' +
        'and we’d be lucky to get a few more projects before we<br>' +
        'had to go find real jobs again. Years later, data<br>' +
        'warehousing and business intelligence are still going<br>' +
        'strong.<br>' +
        'As the DW/BI industry has matured, it’s become<br>' +
        'dominated by single-source providers — a safe choice for<br>' +
        'risk-averse organizations. The DW/BI technology stack<br>' +
        'covers everything from esoteric source system knowledge<br>' +
        'to user interface design and best-practice BI applications.<br>' +
        'Database vendors are best positioned to provide end-to-end<br>' +
        'solutions. Since SQL Server 2000 and especially SQL<br>' +
        'Server 2005, Microsoft has been forcing the concept of a<br>' +
        'viable, single-source data warehouse system provider into<br>' +
        'reality, and at an attractive price.<br>' +
        'The book you’re currently holding is a substantial revision<br>' +
        'of The Microsoft Data Warehouse Toolkit with SQL Server<br>' +
        '2005. In addition to updating the content for new features<br>' +
        'and functionality such as PowerPivot and Master Data<br>' +
        'Services, the new version updates our previous<br>' +
        'recommendations with all that we’ve learned in recent<br>' +
        'years about building a DW/BI system with the Microsoft<br>' +
        'tools. The current book is based on the SQL Server 2008<br>' +
        'R2 release, but the vast majority of its recommendations<br>' +
        '24<br>' +
        'are valid for SQL Server 2008 as well. Any technology or<br>' +
        'recommendation that’s new for SQL Server 2008 R2 is<br>' +
        'clearly identified in the text.<br>' +
        'Who Should Read This Book<br>' +
        'This book covers the entire DW/BI system lifecycle. As a<br>' +
        'result, it offers useful guidance to every member of the<br>' +
        'DW/BI team, from the project manager to the business<br>' +
        'analyst, data modeler, ETL developer, DBA, BI<br>' +
        'application developer, and even to the business user. We<br>' +
        'believe the book will be valuable to anyone working on a<br>' +
        'Microsoft SQL Server DW/BI program.<br>' +
        'The primary audience for this book is the new DW/BI<br>' +
        'team that’s launching a project on the Microsoft SQL<br>' +
        'Server platform. We don’t assume you already have<br>' +
        'experience in building a DW/BI system. We do assume<br>' +
        'you have a basic familiarity with the Microsoft world:<br>' +
        'operating systems, infrastructure components, and<br>' +
        'resources. We also assume a basic understanding of<br>' +
        'relational databases (tables, columns, simple SQL) and<br>' +
        'some familiarity with the SQL Server relational database,<br>' +
        'although that’s not a requirement. Throughout the book we<br>' +
        'provide many references to other books and resources.<br>' +
        'A second audience is the experienced Kimball Method<br>' +
        'DW/BI practitioner who’s new to the Microsoft SQL<br>' +
        'Server toolset. We’ll point out which sections and chapters<br>' +
        'will be review for anyone who’s read our other Toolkit<br>' +
        'books and practiced our methodology. But we’ve found<br>' +
        'that it doesn’t hurt to read this material one more time!<br>' +
        '25<br>' +
        'Whatever your background, you’ll benefit most if you’re<br>' +
        'just starting on a new project. While we do provide<br>' +
        'suggestions on working with existing data warehouses, in<br>' +
        'the ideal case you won’t have to contend with any existing<br>' +
        'data warehouse or data marts — at least none that will<br>' +
        'remain in place after the new system is deployed.<br>' +
        'The Kimball Lifecycle<br>' +
        'We’ve all felt the empty pit of panic in our stomach when,<br>' +
        'deep into a project, we realize the scope and scale of the<br>' +
        'effort before us will take much more work than we<br>' +
        'imagined at the outset. Many DW/BI projects begin with<br>' +
        'the notion that you’ll just move some data to a new<br>' +
        'machine, clean it up a little, and develop some reports.<br>' +
        'That doesn’t sound so bad — six weeks of effort, two<br>' +
        'months at the most. You charge into the forest and soon<br>' +
        'realize it’s a lot darker and denser than you thought. In<br>' +
        'fact, you can’t even see the road out.<br>' +
        'The best way to avoid this sense of panic — and the<br>' +
        'resulting disaster — is to figure out where you’re going<br>' +
        'before you jump in. It helps to have a roadmap and<br>' +
        'directions to lead you safely through unfamiliar territory<br>' +
        '— one that will tell you the places you have to visit and<br>' +
        'point out the danger zones on the trip ahead. This book is<br>' +
        'that roadmap for the Microsoft SQL Server DW/BI system<br>' +
        'project. This book follows the basic flow of the Kimball<br>' +
        'Lifecycle described in the book The Data Warehouse<br>' +
        'Lifecycle Toolkit, Second Edition (Wiley, 2008). The steps,<br>' +
        'tasks, and dependencies of the Lifecycle were crafted<br>' +
        'based on our collective experience of what works. The<br>' +
        '26<br>' +
        'Lifecycle is an iterative approach based on four primary<br>' +
        'principles:<br>' +
        '• Focus on the business: Concentrate on identifying business<br>' +
        'requirements and their associated value. Use these efforts to<br>' +
        'develop solid relationships with the business side and<br>' +
        'sharpen your business sense and consultative skills.<br>' +
        '• Build an information infrastructure: Design a single,<br>' +
        'integrated, easy-to-use, high-performing information<br>' +
        'foundation that will meet the broad range of business<br>' +
        'requirements you’ve identified across the enterprise.<br>' +
        '• Deliver in meaningful increments: Build the data warehouse<br>' +
        'in increments that can be delivered in 6 to 12 month<br>' +
        'timeframes. Use clearly identified business value to<br>' +
        'determine the implementation order of the increments.<br>' +
        '• Deliver the entire solution: Provide all the elements<br>' +
        'necessary to deliver value to the business users. This means<br>' +
        'a solid, well designed, quality tested, accessible data<br>' +
        'warehouse database is only the start. You must also deliver<br>' +
        'ad hoc query tools, reporting applications and advanced<br>' +
        'analytics, training, support, website, and documentation.<br>' +
        'This book helps you follow these four principles by using<br>' +
        'the Kimball Lifecycle to build your DW/BI system. These<br>' +
        'four principles are woven into the fabric of the Lifecycle.<br>' +
        'The secret to understanding the Kimball Lifecycle is that<br>' +
        'it’s business-based, it takes a dimensional approach to<br>' +
        'designing data models for end user presentation, and it is a<br>' +
        'true lifecycle.<br>' +
        'Lifecycle Tracks and Task Areas<br>' +
        'The DW/BI system is a complex entity, and the<br>' +
        'methodology to build that system must help simplify that<br>' +
        'complexity. Figure 1 outlines the Kimball Lifecycle. The<br>' +
        '13 boxes show the major task areas involved in building a<br>' +
        '27<br>' +
        'successful data warehouse and the primary dependencies<br>' +
        'among those tasks.<br>' +
        'Figure 1: The Business Dimensional Lifecycle<br>' +
        'There are several observations to make about the Lifecycle<br>' +
        'at this level. First, notice the central role of the Business<br>' +
        'Requirements Definition box. Business requirements<br>' +
        'provide the foundation for the three tracks that follow.<br>' +
        'They also influence the project plan, hence the arrow<br>' +
        'pointing back to the Project Planning box. You usually end<br>' +
        'up modifying the plan based on a more detailed<br>' +
        'understanding of the business requirements and priorities.<br>' +
        'Second, the three tracks in the middle of the Lifecycle<br>' +
        'concentrate on three separate areas:<br>' +
        '• The top track is about technology. These tasks are primarily<br>' +
        'about determining what functionality you will need, and<br>' +
        '28<br>' +
        'planning which pieces of Microsoft technology you’ll use,<br>' +
        'and how you’ll install and configure them.<br>' +
        '• The middle track is about data. In the data track you’ll<br>' +
        'design and instantiate the dimensional model, and develop<br>' +
        'the Extract, Transformation, and Load (ETL) system to<br>' +
        'populate it. You could think of the data track as “building<br>' +
        'the data warehouse databases,” although your data<br>' +
        'warehouse will not succeed unless you surround it with the<br>' +
        'rest of the Lifecycle tasks.<br>' +
        '• The bottom track is about business intelligence applications.<br>' +
        'In these tasks you design and develop BI applications for the<br>' +
        'business users.<br>' +
        'The tracks combine when it’s time to deploy the system.<br>' +
        'This is a particularly delicate time because there’s only one<br>' +
        'chance to make a good first impression. Although we’ve<br>' +
        'placed maintenance after deployment in the diagram, you<br>' +
        'need to design your system with the ability and tools for<br>' +
        'maintaining it. The growth phase of the project links to the<br>' +
        'arrow heading back to the beginning. This simple arrow<br>' +
        'has major implications. The Lifecycle’s incremental<br>' +
        'approach is a fundamental element of delivering business<br>' +
        'value.<br>' +
        'Underlying the entire Lifecycle is the Project Management<br>' +
        'box. The most important thing to remember here is that<br>' +
        'you need a leader, and that person needs access to senior<br>' +
        'management. The team leader is ideally one of those<br>' +
        'difficult-to-find people who can communicate effectively<br>' +
        'with both technologists and business people, including the<br>' +
        'most senior executives in the company.<br>' +
        'Key Terminology and the Microsoft Toolset<br>' +
        '29<br>' +
        'The business intelligence industry is plagued with<br>' +
        'terminology that’s used imprecisely, or in contradictory<br>' +
        'ways. Some of the most long-standing debates in the<br>' +
        'industry derive as much from misunderstandings about<br>' +
        'what others mean by a term, as from true differences in<br>' +
        'philosophy. Keeping that in mind, we’ll try to be clear and<br>' +
        'consistent even if we don’t settle all the historical debates.<br>' +
        'We highlight some of the key terms here.<br>' +
        'As we define each term, we also relate it to the associated<br>' +
        'Microsoft technologies, most of which are components of<br>' +
        'SQL Server.<br>' +
        '• The data warehouse is the “platform for business<br>' +
        'intelligence.” In the Kimball Method, the data warehouse<br>' +
        'includes everything from the original data extracts to the<br>' +
        'software and applications that users see. We disagree with<br>' +
        'other authors who insist that the data warehouse is merely a<br>' +
        'centralized and highly normalized store of data in the back<br>' +
        'room, far from the end users. To reduce confusion, in this<br>' +
        'book we consistently use the phrase “data warehouse/<br>' +
        'business intelligence system” (DW/BI system) to mean the<br>' +
        'entire end-to-end system. When we’re talking specifically<br>' +
        'and exclusively about the atomic level user queryable data<br>' +
        'store, we call it the data warehouse database.<br>' +
        '• The business process dimensional model is a specific<br>' +
        'discipline for modeling data that is an alternative to<br>' +
        'normalized modeling. A dimensional model contains the<br>' +
        'same information as a normalized model but packages the<br>' +
        'data in a symmetrical format whose design goals are user<br>' +
        'understandability, business intelligence query performance,<br>' +
        'and resilience to change. Normalized models, sometimes<br>' +
        'called third normal form models, were designed to support<br>' +
        'the high-volume, single-row inserts and updates that define<br>' +
        'transaction systems, and generally fail at being<br>' +
        'understandable, fast, and resilient to change. We use the<br>' +
        'term “business process dimensional model” to refer both to<br>' +
        '30<br>' +
        'the logical dimensional model that supports a business<br>' +
        'process and the corresponding physical tables in the<br>' +
        'database. In other words, dimensional models are both<br>' +
        'logical and physical.<br>' +
        '• The relational database is a general purpose technology for<br>' +
        'storing, managing, and querying data. The SQL Server<br>' +
        'database engine is Microsoft’s relational database engine.<br>' +
        'The business process dimensional model can be stored in a<br>' +
        'relational database. Normalized data models that support<br>' +
        'transaction processing can also be stored in a relational<br>' +
        'database.<br>' +
        '• The online analytic processing (OLAP) database is a<br>' +
        'technology for storing, managing, and querying data<br>' +
        'specifically designed to support business intelligence uses.<br>' +
        'SQL Server Analysis Services is Microsoft’s OLAP<br>' +
        'database engine. The business process dimensional model<br>' +
        'can be stored in an OLAP database, but a transactional<br>' +
        'database cannot, unless it first undergoes transformation to<br>' +
        'cast it in an explicitly dimensional form.<br>' +
        '• An Extract, Transformation, and Load (ETL) system is a set<br>' +
        'of processes that clean, transform, combine, de-duplicate,<br>' +
        'household, archive, conform, and structure data for use in<br>' +
        'the data warehouse. These terms are described in this book.<br>' +
        'Early ETL systems were built using a combination of SQL<br>' +
        'and other scripts. While this is still true for some smaller<br>' +
        'ETL systems, larger and more serious systems use a<br>' +
        'specialized ETL tool. Moving forward, almost every DW/BI<br>' +
        'system will use an ETL tool such as SQL Server Integration<br>' +
        'Services because the benefits are significant and the<br>' +
        'incremental dollar cost is low or zero.<br>' +
        '• Business intelligence (BI) applications are predefined<br>' +
        'applications that query, analyze, and present information to<br>' +
        'support a business need. There is a spectrum of BI<br>' +
        'applications, ranging in complexity from a set of predefined<br>' +
        'static reports, all the way to an analytic application that<br>' +
        'directly affects transaction systems and the day-to-day<br>' +
        'operation of the organization. You can use SQL Server<br>' +
        'Reporting Services to build a reporting application, and a<br>' +
        '31<br>' +
        'wide range of Microsoft and third-party technologies to<br>' +
        'build complex analytic applications.<br>' +
        '• A data mining model is a statistical model, often used to<br>' +
        'predict future behavior based on data about past behavior or<br>' +
        'identify closely related subsets of a population called<br>' +
        'clusters. Data mining is a term for a loose (and<br>' +
        'ever-changing) collection of statistical techniques or<br>' +
        'algorithms that serve different purposes. The major<br>' +
        'categories are clustering, decision trees, neural networks,<br>' +
        'and prediction. Analysis Services Data Mining is an example<br>' +
        'of a data mining tool.<br>' +
        '• Ad hoc queries are formulated by the user on the spur of the<br>' +
        'moment. The dimensional modeling approach is widely<br>' +
        'recognized as the best technique to support ad hoc queries<br>' +
        'because the simple database structure is easy to understand.<br>' +
        'Microsoft Office, through Excel pivot tables and<br>' +
        'PowerPivot, is the most popular ad hoc query tool on the<br>' +
        'market. You can use Reporting Services Report Builder to<br>' +
        'perform ad hoc querying and report definition. Nonetheless,<br>' +
        'many systems supplement Excel and Report Builder with a<br>' +
        'third-party ad hoc query tool for their power users.<br>' +
        '• Once again, the data warehouse/business intelligence (DW/<br>' +
        'BI) system is the whole thing: source system extracts, ETL,<br>' +
        'dimensional database in both relational and OLAP, BI<br>' +
        'applications, and an ad hoc query tool. The DW/BI system<br>' +
        'also includes management tools and practices, user-oriented<br>' +
        'documentation and training, a security system, and all the<br>' +
        'other components that we discuss in this book.<br>' +
        'Roles and Responsibilities<br>' +
        'The DW/BI system requires a number of different roles<br>' +
        'and skills, from both the business and technical<br>' +
        'communities, during its lifecycle. In this section, we<br>' +
        'review the major roles involved in creating a DW/BI<br>' +
        'system. There is seldom a one-to-one relationship between<br>' +
        'roles and people. We’ve worked with teams as small as<br>' +
        '32<br>' +
        'one person, and as large as forty (and know of much larger<br>' +
        'teams). The vast majority of DW/BI teams fall between<br>' +
        'three and ten full-time members, with access to others as<br>' +
        'required.<br>' +
        'It’s common for a single DW/BI team to take on both<br>' +
        'development and operational duties. This is different from<br>' +
        'most technology project teams, and is related to the highly<br>' +
        'iterative nature of the DW/BI project development cycle.<br>' +
        'The following roles are associated with design and<br>' +
        'development activities:<br>' +
        '• The DW/BI manager is responsible for overall leadership<br>' +
        'and direction of the project. The DW/BI manager must be<br>' +
        'able to communicate effectively with both senior business<br>' +
        'and IT management. The manager must also be able to work<br>' +
        'with the team to formulate the overall architecture of the<br>' +
        'DW/BI system.<br>' +
        '• The project manager is responsible for day-to-day<br>' +
        'management of project tasks and activities during system<br>' +
        'development.<br>' +
        '• The business project lead is a member of the business<br>' +
        'community and works closely with the project manager.<br>' +
        '• The business systems analyst (or business analyst) is<br>' +
        'responsible for leading the business requirements definition<br>' +
        'activities, and often participates in the development of the<br>' +
        'business process dimensional model. The business systems<br>' +
        'analyst needs to be able to bridge the gap between business<br>' +
        'and technology.<br>' +
        '• The data modeler is responsible for performing detailed data<br>' +
        'analysis including data profiling, and developing the detailed<br>' +
        'dimensional model.<br>' +
        '• The system architect(s) design the various components of the<br>' +
        'DW/BI system. These include the ETL system, security<br>' +
        'system, auditing system, and maintenance systems.<br>' +
        '33<br>' +
        '• The development database administrator (DBA) creates the<br>' +
        'relational data warehouse database(s) and is responsible for<br>' +
        'the overall physical design including disk layout,<br>' +
        'partitioning, and initial indexing plan.<br>' +
        '• The OLAP database designer creates the OLAP databases.<br>' +
        '• The ETL system developer creates Integration Services<br>' +
        'packages, scripts, and other elements to move data from the<br>' +
        'source databases into the data warehouse.<br>' +
        '• The test lead sets up the test environment, writes scripts to<br>' +
        'automate test execution, develops and distributes reports on<br>' +
        'the test log database, reaches out to the business user<br>' +
        'community to get user input into data quality tests, manages<br>' +
        'the ongoing process of automated data quality testing once<br>' +
        'the system is in production, and publishes data quality<br>' +
        'reports to the user community.<br>' +
        '• The DW/BI management tools developer writes any custom<br>' +
        'tools that are necessary for the ongoing management of the<br>' +
        'DW/BI system. Examples of such tools include a simple UI<br>' +
        'for entering metadata, scripts or Integration Services<br>' +
        'packages to perform system backups and restores, and a<br>' +
        'simple UI for maintaining dimension hierarchies.<br>' +
        '• The BI application developer is responsible for building the<br>' +
        'BI applications, including the standard reports and any<br>' +
        'advanced analytic applications required by the business. This<br>' +
        'role is also responsible for developing any custom<br>' +
        'components in the BI portal and integrating data mining<br>' +
        'models into business operations.<br>' +
        'Most of the rest of the roles play a part in the latter stages<br>' +
        'of the DW/BI project development cycle, as the team<br>' +
        'moves toward deploying and operating the system. A few<br>' +
        'of the roles are strictly operational.<br>' +
        '• The data steward is responsible for ensuring the data in the<br>' +
        'data warehouse is accurate. The data stewardship role is<br>' +
        'often best filled by someone in the business user community,<br>' +
        'who has a deep understanding of the data and can well judge<br>' +
        'its accuracy.<br>' +
        '34<br>' +
        '• The security manager specifies new user access roles that<br>' +
        'the business users need, and adds users to existing roles. The<br>' +
        'security manager also determines the security procedures in<br>' +
        'the ETL “back room” of the DW/BI system.<br>' +
        '• The relational database administrator (DBA) is responsible<br>' +
        'for managing the performance and operations of the<br>' +
        'relational data warehouse database.<br>' +
        '• The OLAP DBA is responsible for managing the<br>' +
        'performance and operations of the OLAP data warehouse<br>' +
        'database.<br>' +
        '• The compliance manager ensures that the DW/BI policies<br>' +
        'and operations comply with corporate and regulatory<br>' +
        'directives such as privacy policies, HIPAA, and<br>' +
        'Sarbanes-Oxley. The compliance manager works closely<br>' +
        'with the security manager and internal audit.<br>' +
        '• The metadata manager has the final word on what metadata<br>' +
        'is collected, where it is kept, and how it’s published to the<br>' +
        'business community. As we discuss in Chapter 15, metadata<br>' +
        'tends not to be managed unless there’s a person identified to<br>' +
        'lead the charge.<br>' +
        '• The data mining analyst is deeply familiar with the business<br>' +
        'and usually has some background in statistics. The data<br>' +
        'mining analyst develops data mining models and works with<br>' +
        'the BI application developers to design operational<br>' +
        'applications that use the data mining models.<br>' +
        '• The BI portal content manager manages the BI portal. She<br>' +
        'determines the content that’s on the portal and how it’s laid<br>' +
        'out, and keeps it fresh.<br>' +
        '• The DW/BI educator creates and delivers the training<br>' +
        'materials for the DW/BI system.<br>' +
        '• User support personnel within the DW/BI team must be<br>' +
        'available to help business users, especially with ad hoc<br>' +
        'access. Corporate-wide help desks tend not to have the<br>' +
        'specialized expertise necessary to do more than assist with<br>' +
        'minor connectivity issues.<br>' +
        'How This Book Is Organized<br>' +
        '35<br>' +
        'We’ve divided the book into four parts:<br>' +
        '1. Requirements, Realities, and Architecture<br>' +
        '2. Building and Populating the Databases<br>' +
        '3. Developing the BI Applications<br>' +
        '4. Deploying and Managing the DW/BI System<br>' +
        'Part 1: Requirements, Realities, and Architecture<br>' +
        'Part 1 sets the stage for the rest of the book. Most of you<br>' +
        'are eager to get your hands on the Microsoft toolset. That’s<br>' +
        'fine while you’re experimenting and learning about the<br>' +
        'technology, but it’s the kiss of death for a project. Stop,<br>' +
        'back away from the keyboard, and think about what you’re<br>' +
        'setting out to do.<br>' +
        'Chapter 1: Defining Business Requirements<br>' +
        'Part 1 begins with a brief summary of the Kimball<br>' +
        'Lifecycle. We drill down on the most important step,<br>' +
        'gathering the business requirements, and briefly present<br>' +
        'the business requirements for the Adventure Works Cycles<br>' +
        'case study used throughout the book. Chapter 1 refers to<br>' +
        'the Business Requirements Definition box in Figure 1.<br>' +
        'Readers who are very familiar with the Kimball Method<br>' +
        'can skip the first part of the chapter but should read the<br>' +
        'case study.<br>' +
        '36<br>' +
        'Chapter 2: Designing the Business Process Dimensional<br>' +
        'Model<br>' +
        'We present a brief primer on how to develop a<br>' +
        'dimensional model. This chapter presents terminology and<br>' +
        'concepts used throughout the book, so it’s vital that you<br>' +
        'understand this material. This chapter refers to the<br>' +
        'Dimensional Modeling box in Figure 1.<br>' +
        'Readers who are very familiar with the Kimball Method<br>' +
        'can skim most of this material and review the Adventure<br>' +
        'Works overview at the end of the chapter.<br>' +
        'Chapter 3: The Toolset<br>' +
        'The Architecture and Product Selection tasks are<br>' +
        'straightforward for a Microsoft DW/BI system. In this<br>' +
        'short chapter we talk in more detail about how and where<br>' +
        'to use the various components of SQL Server, other<br>' +
        'Microsoft products, and even where you’re most likely to<br>' +
        'use third-party software in your system. This chapter<br>' +
        'provides a brief overview of the Technical Architecture<br>' +
        'Design and Product Selection & Installation boxes in<br>' +
        'Figure 1.<br>' +
        'Even readers who are very familiar with SQL Server 2005<br>' +
        'should review this chapter, as it contains information about<br>' +
        'the new features of SQL Server 2008 R2, some of which<br>' +
        'are significantly different.<br>' +
        'Chapter 4: System Setup<br>' +
        '37<br>' +
        'Chapter 4 is focused on the Product Selection &<br>' +
        'Installation box in Figure 1, and describes how to install<br>' +
        'and configure the various components of SQL Server 2008<br>' +
        'R2. We talk about system sizing and configuration, and<br>' +
        'how — and why — you might choose to distribute your<br>' +
        'DW/BI system across multiple servers.<br>' +
        'Part 2: Building and Populating the Databases<br>' +
        'The second part of the book presents the steps required to<br>' +
        'effectively build and populate the data warehouse<br>' +
        'databases. Most Microsoft DW/BI systems will implement<br>' +
        'the dimensional data warehouse in both the relational<br>' +
        'database and the Analysis Services database.<br>' +
        'Chapter 5: Creating the Relational Data Warehouse<br>' +
        'Chapter 5 talks about creating the database structures for<br>' +
        'the relational data warehouse. We’re not moving data yet,<br>' +
        'but we’re getting closer. We begin by talking about the<br>' +
        'minor differences between the Kimball logical design and<br>' +
        'the physical data models, including issues such as the<br>' +
        'initial indexing plan, key structures, and storage decisions.<br>' +
        'One of your key decisions for the relational data<br>' +
        'warehouse is whether or not to partition the fact data. As<br>' +
        'we discuss, partitioning provides many advantages, and is<br>' +
        'a necessity for large data warehouses.<br>' +
        'Chapter 6: Master Data Management<br>' +
        'Master data is reference data that is managed centrally for<br>' +
        'an organization. New with SQL Server 2008 R2, Master<br>' +
        '38<br>' +
        'Data Services provides a toolset for building a master data<br>' +
        'management system. Chapter 6 describes master data<br>' +
        'management and the Master Data Services tools, then<br>' +
        'discusses some quick and easy uses of this new technology<br>' +
        'to improve the data warehouse. Over time, some<br>' +
        'organizations may shift the management of their<br>' +
        'dimensions from a classic ETL system implemented in<br>' +
        'SQL Server Integration Services, toward more active data<br>' +
        'stewardship via Master Data Services.<br>' +
        'Chapter 7: Designing and Developing the ETL System<br>' +
        'Finally it’s time to start moving data. This chapter talks<br>' +
        'about the basic design for your ETL system. We begin by<br>' +
        'introducing SQL Server Integration Services (SSIS), then<br>' +
        'walk through how to use SSIS to build the 34 subsystems<br>' +
        'of any ETL system. The 34 subsystems have four major<br>' +
        'groups: data extraction, data cleaning and conforming, data<br>' +
        'presentation, and system management. Each of these areas<br>' +
        'is discussed within the context of SSIS.<br>' +
        'Chapter 8: The Core Analysis Services OLAP Database<br>' +
        'We recommend that your Microsoft DW/BI system use<br>' +
        'Analysis Services OLAP as the main database for users to<br>' +
        'query. The more closely the relational database and ETL<br>' +
        'process are designed to meet your business requirements,<br>' +
        'the easier it is to design the Analysis Services database.<br>' +
        'The Analysis Services wizards are easy to use, and with a<br>' +
        'small system, you don’t need to worry very much about<br>' +
        'advanced settings. However, if you have large data<br>' +
        'volumes or a lot of users, you need to develop a deep<br>' +
        'understanding of the OLAP engine. Much of this chapter is<br>' +
        '39<br>' +
        'focused on helping you learn enough to implement<br>' +
        'Analysis Services across your enterprise.<br>' +
        'Analysis Services contains three major pieces of<br>' +
        'functionality: the core OLAP database, the data mining<br>' +
        'platform, and the PowerPivot user-driven Excel analytics.<br>' +
        'Data mining is addressed in Chapter 13, and PowerPivot in<br>' +
        'Chapter 11.<br>' +
        'Chapter 8 revisits the Physical Design and ETL boxes of<br>' +
        'Figure 1, this time from the perspective of the OLAP<br>' +
        'database engine.<br>' +
        'Chapter 9: Design Requirements for Real-Time BI<br>' +
        'Chapter 9 takes on the topic of real-time business<br>' +
        'intelligence, discussing how to bring real-time data —<br>' +
        'loosely defined as data refreshed more frequently than<br>' +
        'daily — into the DW/BI system. SQL Server contains<br>' +
        'many features to enable real-time business intelligence.<br>' +
        'We talk about how to use these features, and the inevitable<br>' +
        'tradeoffs you face when implementing real-time BI.<br>' +
        'Part 3: Developing the BI Applications<br>' +
        'The third part of the book presents the steps required to<br>' +
        'present the data to the business users. BI applications are a<br>' +
        'key component of the complete DW/BI system. To most<br>' +
        'business users, BI applications are synonymous with the<br>' +
        'data warehouse. BI applications range from simple static<br>' +
        'reports to complex data mining applications, user-driven<br>' +
        'ad hoc analyses using PowerPivot and Excel, and the BI<br>' +
        '40<br>' +
        'portal that provides a single point of entry into the business<br>' +
        'intelligence system.<br>' +
        'Chapter 10: Building BI Applications in Reporting<br>' +
        'Services<br>' +
        'Chapter 10 provides the basic information you need to<br>' +
        'understand the range of BI applications available to you.<br>' +
        'We start with an introduction to BI applications in general.<br>' +
        'We then offer a BI applications development process in the<br>' +
        'context of the Kimball Lifecycle. The rest of the chapter<br>' +
        'drills into Reporting Services as a platform for creating<br>' +
        'and distributing standard reports.<br>' +
        'Chapter 11: PowerPivot and Excel<br>' +
        'The core component of PowerPivot is an in-memory<br>' +
        'database add-in for Excel 2010. It allows Excel users to<br>' +
        'work with millions of rows of data at memory speeds.<br>' +
        'Business users can join data from multiple, disparate<br>' +
        'sources in the PowerPivot database, and create complex<br>' +
        'calculations and measures.<br>' +
        'Chapter 11 begins with a look at Excel as an analysis and<br>' +
        'reporting tool. The rest of the chapter is dedicated to<br>' +
        'PowerPivot, starting with a brief description of PowerPivot<br>' +
        'and its product architecture. The bulk of the chapter is<br>' +
        'dedicated to working through an example. We’ll finish up<br>' +
        'with a brief discussion of PowerPivot in the SharePoint<br>' +
        'environment and its role in the overall DW/BI system.<br>' +
        'Chapter 12: The BI Portal and SharePoint<br>' +
        '41<br>' +
        'The BI portal is the primary starting point in the<br>' +
        'information quest for a large part of the business<br>' +
        'community. It needs to be structured in a way that allows<br>' +
        'people to find what they are looking for within an ever<br>' +
        'increasing number of reports and analyses. SharePoint is<br>' +
        'Microsoft’s offering in the portal platform category.<br>' +
        'The first part of Chapter 12 is a discussion of the BI portal<br>' +
        'including design guidelines and a simple example. In the<br>' +
        'second part, we take a high level look at SharePoint as a BI<br>' +
        'portal platform and discuss the process of getting<br>' +
        'SharePoint going with a set of BI-related functionality<br>' +
        'including Reporting Services and PowerPivot for<br>' +
        'SharePoint.<br>' +
        'Chapter 13: Incorporating Data Mining<br>' +
        'Data mining is perhaps the most powerful — and certainly<br>' +
        'the least understood — technology in the BI toolkit. This<br>' +
        'chapter defines data mining, and provides examples of<br>' +
        'how it can be used. We talk about Microsoft’s data mining<br>' +
        'technology, including the algorithms that are included with<br>' +
        'SQL Server Analysis Services. We provide practical<br>' +
        'guidance on how to build a data mining model and how to<br>' +
        'incorporate the results of data mining into your systems.<br>' +
        'To make this theoretical discussion more concrete, we<br>' +
        'work through two case studies.<br>' +
        'Part 4: Deploying and Managing the DW/BI System<br>' +
        'The final section of the book includes information about<br>' +
        'how to deploy and operate your DW/BI system. It is one of<br>' +
        'the most exciting sections of the entire book.<br>' +
        '42<br>' +
        'Chapter 14: Designing and Implementing Security<br>' +
        'We start our discussion of the DW/BI system’s security by<br>' +
        'encouraging you to develop an open access policy for<br>' +
        'information. Sensitive data must of course be protected,<br>' +
        'but we think most contents of the data warehouse should<br>' +
        'be available to most authenticated users.<br>' +
        'Even with an open security policy, some data must be<br>' +
        'protected. We describe how to control access in the<br>' +
        'various components of SQL Server: Reporting Services,<br>' +
        'the relational database, and Analysis Services. We also<br>' +
        'discuss the separate issues of security in the back room<br>' +
        'development area of the data warehouse.<br>' +
        'The discussion of security is most closely related to the<br>' +
        'Deployment and Maintenance boxes of Figure 1.<br>' +
        'Chapter 15: Metadata Plan<br>' +
        'Lots of people talk about metadata, but we’ve seen few<br>' +
        'examples of it being implemented thoroughly and<br>' +
        'successfully. We’d like to have seen an integrated<br>' +
        'metadata service in SQL Server, which we could simply<br>' +
        'describe to you, but that’s not the case. Instead, we spend<br>' +
        'most of this chapter detailing the metadata that we think is<br>' +
        'most important, and describing the steps to maintain and<br>' +
        'publish that information.<br>' +
        'Metadata is related to the Deployment and Maintenance<br>' +
        'boxes of Figure 1.<br>' +
        'Chapter 16: Deployment<br>' +
        '43<br>' +
        'Deploying the DW/BI system consists of two major sets of<br>' +
        'tasks. First, you need to deploy the system. This effort<br>' +
        'consists primarily of testing: testing of data, processes,<br>' +
        'performance, and the deployment scripts themselves. The<br>' +
        'deployment scripts should include a playbook, with<br>' +
        'step-by-step instructions for how to deploy the system<br>' +
        'changes.<br>' +
        'The other major set of deployment activities is focused<br>' +
        'more on the business users than on the technology. You<br>' +
        'need to develop and deliver training and documentation<br>' +
        'materials. You need to pull together the BI portal that we<br>' +
        'describe in Chapter 12. And you need to develop a plan for<br>' +
        'supporting the business users, who will inevitably have<br>' +
        'questions.<br>' +
        'Chapter 17: Operations and Maintenance<br>' +
        'As business people begin to use the warehouse to answer<br>' +
        'their questions on a regular basis, they’ll come to rely on<br>' +
        'it. If users don’t believe the warehouse is reliable, they’ll<br>' +
        'go back to their old ways of getting information. This<br>' +
        'reliance is a kind of trust, and you must do everything you<br>' +
        'can to build and keep that trust. You need to monitor usage<br>' +
        'and performance — both for data loads and user queries.<br>' +
        'Track system resources and make sure you don’t run out of<br>' +
        'disk space. In short, maintain the warehouse as the<br>' +
        'production system it now is. You must be meticulous in<br>' +
        'your attention to the quality of the data that’s loaded into<br>' +
        'the data warehouse. Once a business user loses trust in the<br>' +
        'accuracy of the data, that trust is nearly impossible to<br>' +
        'regain.<br>' +
        '44<br>' +
        'Chapter 18: Present Imperatives and Future Outlook<br>' +
        'Chapter 18 reviews the major phases of the DW/BI project<br>' +
        'and highlights where the most significant risks are to the<br>' +
        'overall success of the project. We finish the book with a<br>' +
        'wish list of features and functionality that we hope to see<br>' +
        'in the Microsoft BI toolset in the years to come.<br>' +
        'Additional Information<br>' +
        'This book includes most of the information you need to<br>' +
        'successfully build and deploy a basic DW/BI system using<br>' +
        'SQL Server 2008 Release 2. In an effort to keep the book<br>' +
        'small enough to fit into a large backpack, we chose not to<br>' +
        'replicate tool instructions that could be easily found in<br>' +
        'SQL Server Books Online. Where appropriate, we provide<br>' +
        'search topics to assist in finding related materials. In<br>' +
        'several places, we recommend that you work through the<br>' +
        'tutorials provided by the SQL Server team before you can<br>' +
        'expect to fully understand some technical material.<br>' +
        'This book doesn’t attempt to re-teach the fundamentals of<br>' +
        'data warehousing. We summarize many of the concepts<br>' +
        'and techniques found in the other volumes in the Kimball<br>' +
        'Data Warehouse Toolkit series rather than including all the<br>' +
        'details found in those books. We also provide references to<br>' +
        'key sections of those books as needed. Table 1 shows the<br>' +
        'core books in the Toolkit series, their major focus, and<br>' +
        'their primary audiences.<br>' +
        'These books encapsulate the collective wisdom of the<br>' +
        'Kimball Group about data warehousing and business<br>' +
        '45<br>' +
        'intelligence. We recommend that you add these books to<br>' +
        'your team’s library.<br>' +
        'We’ve laced this book with tips, key concepts, sidebars,<br>' +
        'and chapter pointers to make it more usable and easily<br>' +
        'referenced. We draw attention to some of these with the<br>' +
        'following formats:<br>' +
        'REFERENCE<br>' +
        'Look for reference pointers to find other<br>' +
        'materials you can use to supplement your<br>' +
        'DW/BI library. This includes references to<br>' +
        'SQL Server Books Online, other books and<br>' +
        'articles, and online reference materials.<br>' +
        'RESOURCES<br>' +
        'Resources provide references to other<br>' +
        'Kimball Group material, such as the<br>' +
        'Toolkit books, articles, or design tips.<br>' +
        'note Notes provide some extra information on the topic<br>' +
        'under discussion, adding explanation or details to clarify<br>' +
        'the material.<br>' +
        'warning Warnings help you avoid potential dangers that<br>' +
        'might cost you time, data, or sanity.<br>' +
        '46<br>' +
        'DOWNLOADS<br>' +
        'Resources that you can download.<br>' +
        'Table 1: The core Kimball Data Warehouse Toolkit titles<br>' +
        'Title Subject Primary Audience<br>' +
        'The Data<br>' +
        'Warehouse<br>' +
        'Lifecycle Toolkit,<br>' +
        'Second Edition<br>' +
        'Implementation<br>' +
        'guide<br>' +
        'Good overview for all project<br>' +
        'participants; key tool for project<br>' +
        'managers, business analysts, and data<br>' +
        'modelers<br>' +
        'The Data<br>' +
        'Warehouse<br>' +
        'Toolkit, Second<br>' +
        'Edition<br>' +
        'Dimensional<br>' +
        'data modeling<br>' +
        'Data modelers, business analysts,<br>' +
        'DBAs, ETL developers<br>' +
        'The Data<br>' +
        'Warehouse ETL<br>' +
        'Toolkit<br>' +
        'ETL system<br>' +
        'architecture ETL architects and developers<br>' +
        'The Kimball<br>' +
        'Group Reader<br>' +
        'DW/BI system<br>' +
        'design and<br>' +
        'development<br>' +
        'A topical reference book for all project<br>' +
        'participants<br>' +
        'On the Website<br>' +
        'We’ve collected most of the listings and examples and<br>' +
        'made them available on the book’s website along with<br>' +
        'additional links and references: http://www.kimballgroup.com/<br>' +
        'html/booksMDWT.html<br>' +
        '47<br>';
    document.getElementById('head').innerHTML = '<br>' +
        'The Microsoft® Data Warehouse Toolkit: With SQL<br>' +
        'Server 2008 R2 and the Microsoft® Business<br>' +
        'Intelligence Toolset, Second Edition<br>' +
        'Published by<br>' +
        'Wiley Publishing, Inc.<br>' +
        '10475 Crosspoint Boulevard<br>' +
        'Indianapolis, IN 46256<br>' +
        'www.wiley.com<br>' +
        'Copyright © 2011 by Joy Mundy and Warren<br>' +
        'Thornthwaite with Ralph Kimball<br>' +
        'Published by Wiley Publishing, Inc., Indianapolis, Indiana<br>' +
        'Published simultaneously in Canada<br>' +
        'ISBN: 978-0-470-64038-8ISBN: 978-1-118-06793-2<br>' +
        '(ebk)ISBN: 978-1-118-06795-6 (ebk)ISBN:<br>' +
        '978-1-118-06794-9 (ebk)<br>' +
        'Manufactured in the United States of America<br>' +
        '10 9 8 7 6 5 4 3 2 1<br>' +
        'No part of this publication may be reproduced, stored in a<br>' +
        'retrieval system or transmitted in any form or by any<br>' +
        'means, electronic, mechanical, photocopying, recording,<br>' +
        'scanning or otherwise, except as permitted under Sections<br>' +
        '107 or 108 of the 1976 United States Copyright Act,<br>' +
        'without either the prior written permission of the<br>' +
        'Publisher, or authorization through payment of the<br>' +
        '11<br>' +
        'appropriate per-copy fee to the Copyright Clearance<br>' +
        'Center, 222 Rosewood Drive, Danvers, MA 01923, (978)<br>' +
        '750-8400, fax (978) 646-8600. Requests to the Publisher<br>' +
        'for permission should be addressed to the Permissions<br>' +
        'Department, John Wiley & Sons, Inc., 111 River Street,<br>' +
        'Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008,<br>' +
        'or online at http://www.wiley.com/go/permissions.<br>' +
        'Limit of Liability/Disclaimer of Warranty: The<br>' +
        'publisher and the author make no representations or<br>' +
        'warranties with respect to the accuracy or completeness of<br>' +
        'the contents of this work and specifically disclaim all<br>' +
        'warranties, including without limitation warranties of<br>' +
        'fitness for a particular purpose. No warranty may be<br>' +
        'created or extended by sales or promotional materials. The<br>' +
        'advice and strategies contained herein may not be suitable<br>' +
        'for every situation. This work is sold with the<br>' +
        'understanding that the publisher is not engaged in<br>' +
        'rendering legal, accounting, or other professional services.<br>' +
        'If professional assistance is required, the services of a<br>' +
        'competent professional person should be sought. Neither<br>' +
        'the publisher nor the author shall be liable for damages<br>' +
        'arising herefrom. The fact that an organization or Web site<br>' +
        'is referred to in this work as a citation and/or a potential<br>' +
        'source of further information does not mean that the author<br>' +
        'or the publisher endorses the information the organization<br>' +
        'or website may provide or recommendations it may make.<br>' +
        'Further, readers should be aware that Internet websites<br>' +
        'listed in this work may have changed or disappeared<br>' +
        'between when this work was written and when it is read.<br>' +
        'For general information on our other products and services<br>' +
        'please contact our Customer Care Department within the<br>' +
        '12<br>' +
        'United States at (877) 762-2974, outside the United States<br>' +
        'at (317) 572-3993 or fax (317) 572-4002.<br>' +
        'Wiley also publishes its books in a variety of electronic<br>' +
        'formats. Some content that appears in print may not be<br>' +
        'available in electronic books.<br>' +
        'Library of Congress Control Number: 2011920894<br>' +
        'Trademarks: Wiley and the Wiley logo are trademarks or<br>' +
        'registered trademarks of John Wiley & Sons, Inc. and/or<br>' +
        'its affiliates, in the United States and other countries, and<br>' +
        'may not be used without written permission. Microsoft is a<br>' +
        'registered trademark of Microsoft Corporation. All other<br>' +
        'trademarks are the property of their respective owners.<br>' +
        'Wiley Publishing, Inc. is not associated with any product<br>' +
        'or vendor mentioned in this book.<br>' +
        '13<br>' +
        'About the Authors<br>' +
        'Joy Mundy has focused on DW/BI systems since 1992<br>' +
        'with stints at Stanford, WebTV, and Microsoft’s SQL<br>' +
        'Server product development organization. Joy graduated<br>' +
        'from Tufts University with a BA in Economics, and from<br>' +
        'Stanford University with an MS in Engineering Economic<br>' +
        'Systems.<br>' +
        'Warren Thornthwaite began his DW/BI career in 1980.<br>' +
        'After managing Metaphor’s consulting organization, he<br>' +
        'worked for Stanford University and WebTV. Warren holds<br>' +
        'a BA in Communications Studies from the University of<br>' +
        'Michigan and an MBA from the University of<br>' +
        'Pennsylvania’s Wharton School.<br>' +
        'Ralph Kimball founded the Kimball Group. Since the mid<br>' +
        '1980s, he has been the DW/BI industry’s thought leader on<br>' +
        'the dimensional approach and has trained more than<br>' +
        '10,000 IT professionals. Prior to working at Metaphor and<br>' +
        'founding Red Brick Systems, Ralph co-invented the Star<br>' +
        'workstation at Xerox’s Palo Alto Research Center<br>' +
        '(PARC). Ralph has a Ph.D. in Electrical Engineering from<br>' +
        'Stanford University.<br>' +
        '14<br>' +
        'Credits<br>' +
        'Executive Editor<br>' +
        'Robert Elliott<br>' +
        'Project Editors<br>' +
        'Sara Shlaer<br>' +
        'Ginny Munroe<br>' +
        'Technical Editor<br>' +
        'Ralph Kimball<br>' +
        'Senior Production Editor<br>' +
        'Debra Banninger<br>' +
        'Copy Editor<br>' +
        'Kim Cofer<br>' +
        'Editorial Director<br>' +
        'Robyn B. Siesky<br>' +
        'Editorial Manager<br>' +
        'Mary Beth Wakefield<br>' +
        'Freelancer Editorial Manager<br>' +
        '15<br>' +
        'Rosemarie Graham<br>' +
        'Marketing Manager<br>' +
        'Ashley Zurcher<br>' +
        'Production Manager<br>' +
        'Tim Tate<br>' +
        'Vice President and Executive Group Publisher<br>' +
        'Richard Swadley<br>' +
        'Vice President and Executive Publisher<br>' +
        'Barry Pruett<br>' +
        'Associate Publisher<br>' +
        'Jim Minatel<br>' +
        'Project Coordinator, Cover<br>' +
        'Katie Crocker<br>' +
        'Compositor<br>' +
        'Craig Johnson, Happenstance Type-O-Rama<br>' +
        'Proofreader<br>' +
        'Jen Larsen, Word One<br>' +
        '16<br>' +
        'Indexer<br>' +
        'Robert Swanson<br>' +
        'Cover Image<br>' +
        '© Getty Images<br>' +
        'Cover Designer<br>' +
        'Ryan Sneed<br>' +
        '17<br>' +
        'Acknowledgments<br>' +
        'First, we want to thank the thousands of you who have<br>' +
        'read the Kimball Group’s Toolkit books, attended our<br>' +
        'courses, and engaged us in consulting projects. We always<br>' +
        'learn from you, and you’ve had a profound impact on our<br>' +
        'thinking and the business intelligence industry.<br>' +
        'This book would not have been written without the<br>' +
        'assistance of many people on the SQL Server product<br>' +
        'development team. Dave Wickert reviewed the PowerPivot<br>' +
        'and SharePoint chapters and provided many excellent<br>' +
        'suggestions for improving the content. Bryan Smith was<br>' +
        'kind enough to read the chapters on Integration Services<br>' +
        'and Analysis Services, and those chapters are the better for<br>' +
        'his assistance. Carolyn Chau reviewed the Reporting<br>' +
        'Services chapter, Eric Hanson read the relational database<br>' +
        'chapter, Pej Javaheri commented on the SharePoint<br>' +
        'chapter, and Raman Iyer read the data mining chapter. Our<br>' +
        'sincere gratitude to all of them.<br>' +
        'Other members of the SQL Server team provided<br>' +
        'significant assistance reviewing the SQL Server 2005<br>' +
        'version of this book, and we were too embarrassed to<br>' +
        'impose on them a second time. These include Bill Baker,<br>' +
        'Stuart Ozer, Grant Dickinson, Donald Farmer, Siva<br>' +
        'Harinath, Jamie MacLennan, John Miller, Ashvini Sharma,<br>' +
        'Stephen Quinn, and Rob Zare.<br>' +
        'Our colleagues at the Kimball Group were invaluable.<br>' +
        'Their encouragement kept us going while we were writing<br>' +
        'the book, and their reviews helped us polish and prune<br>' +
        '18<br>' +
        'material. Ralph Kimball, of course, had a huge impact on<br>' +
        'the book, not just from his writing and thinking in the<br>' +
        'business intelligence arena but more directly by helping us<br>' +
        'improve the book’s overall structure and flow.<br>' +
        'Sara Shlaer, Ginny Munroe, and Bob Elliott, our editors at<br>' +
        'Wiley, have been very helpful and encouraging. It’s been a<br>' +
        'pleasure to work with them.<br>' +
        'To our life partners, thanks for being there when we<br>' +
        'needed you, for giving us the time we needed, and for<br>' +
        'occasionally reminding us that it was time to take a break.<br>' +
        'Tony Navarrete and Elizabeth Wright, the book wouldn’t<br>' +
        'exist without you.<br>' +
        '19<br>' +
        'Foreword<br>' +
        'In the five years since the first edition was published,<br>' +
        'Microsoft has made impressive progress in building out its<br>' +
        'data warehousing and business intelligence tools suite. It is<br>' +
        'gratifying to those of us who work in this space to see the<br>' +
        'steady commitment that Microsoft has made to provide<br>' +
        'usable, professional quality tools. During these five years,<br>' +
        'Warren and Joy have consulted with dozens of clients,<br>' +
        'taught scores of classes, answered hundreds if not<br>' +
        'thousands of questions, had many “schema lunches” where<br>' +
        'the schema diagrams competed with the food, and have<br>' +
        'pounded on every module in Microsoft’s DW/BI toolset.<br>' +
        'This current edition remains a unique reference, combining<br>' +
        'overall perspectives on what the tools do with accurate<br>' +
        'assessments of how well they do it. This book teaches<br>' +
        'judgment, not button clicks!<br>' +
        '—Ralph Kimball<br>' +
        '20<br>';

    document.getElementById('tableOfContents').innerHTML = 'Table of Contents<br>' +
        'Cover<br>' +
        'Title Page<br>' +
        'Copyright<br>' +
        'About the Authors<br>' +
        'Credits<br>' +
        'Acknowledgments<br>' +
        'Foreword<br>' +
        'Introduction<br>' +
        'The Data Warehouse and Business Intelligence System<br>' +
        'The Kimball Lifecycle<br>' +
        'How This Book Is Organized<br>' +
        'Additional Information<br>' +
        'On the Website<br>' +
        'Part 1: Requirements, Realities, and Architecture<br>' +
        'Chapter 1: Defining Business Requirements<br>' +
        'The Most Important Determinant of Long-Term Success<br>' +
        '2<br>' +
        'Adventure Works Cycles Introduction<br>' +
        'Uncovering Business Value<br>' +
        'Prioritizing the Business Requirements<br>' +
        'Revisiting the Project Planning<br>' +
        'Gathering Project-Level Requirements<br>' +
        'Summary<br>' +
        'Chapter 2: Designing the Business Process Dimensional<br>' +
        'Model<br>' +
        'Dimensional Modeling Concepts and Terminology<br>' +
        'Additional Design Concepts and Techniques<br>' +
        'The Dimensional Modeling Process<br>' +
        'Case Study: The Adventure Works Cycles Orders<br>' +
        'Dimensional Model<br>' +
        'Summary<br>' +
        'Chapter 3: The Toolset<br>' +
        'The Microsoft DW/BI Toolset<br>' +
        'Why Use the Microsoft Toolset?<br>' +
        'Architecture of a Microsoft DW/BI System<br>' +
        '3<br>' +
        'Overview of the Microsoft Tools<br>' +
        'Summary<br>' +
        'Chapter 4: System Setup<br>' +
        'System Sizing Considerations<br>' +
        'System Configuration Considerations<br>' +
        'Software Installation and Configuration<br>' +
        'Summary<br>' +
        'Part 2: Building and Populating the Databases<br>' +
        'Chapter 5: Creating the Relational Data Warehouse<br>' +
        'Getting Started<br>' +
        'Complete the Physical Design<br>' +
        'Define Storage and Create Constraints and Supporting<br>' +
        'Objects<br>' +
        'Partitioned Tables<br>' +
        'Finishing Up<br>' +
        'Summary<br>' +
        'Chapter 6: Master Data Management<br>' +
        'Managing Master Reference Data<br>' +
        '4<br>' +
        'Introducing SQL Server Master Data Services<br>' +
        'Creating a Simple Application<br>' +
        'Summary<br>' +
        'Chapter 7: Designing and Developing the ETL System<br>' +
        'Round Up the Requirements<br>' +
        'Develop the ETL Plan<br>' +
        'Introducing SQL Server Integration Services<br>' +
        'The Major Subsystems of ETL<br>' +
        'Extracting Data<br>' +
        'Cleaning and Conforming Data<br>' +
        'Delivering Data for Presentation<br>' +
        'Managing the ETL Environment<br>' +
        'Summary<br>' +
        'Chapter 8: The Core Analysis Services OLAP Database<br>' +
        'Overview of Analysis Services OLAP<br>' +
        'Designing the OLAP Structure<br>' +
        'Physical Design Considerations<br>' +
        '5<br>' +
        'Summary<br>' +
        'Chapter 9: Design Requirements for Real-Time BI<br>' +
        'Real-Time Triage<br>' +
        'Scenarios and Solutions<br>' +
        'Summary<br>' +
        'Part 3: Developing the BI Applications<br>' +
        'Chapter 10: Building BI Applications in Reporting Services<br>' +
        'A Brief Overview of BI Applications<br>' +
        'A High-Level Architecture for Reporting<br>' +
        'The Reporting System Design and Development Process<br>' +
        'Building and Delivering Reports<br>' +
        'Ad Hoc Reporting Options<br>' +
        'Summary<br>' +
        'Chapter 11: PowerPivot and Excel<br>' +
        'Using Excel for Analysis and Reporting<br>' +
        'The PowerPivot Architecture: Excel on Steroids<br>' +
        'Creating and Using PowerPivot Databases<br>' +
        '6<br>' +
        'PowerPivot for SharePoint<br>' +
        'PowerPivot’s Role in a Managed DW/BI Environment<br>' +
        'Summary<br>' +
        'Chapter 12: The BI Portal and SharePoint<br>' +
        'The BI Portal<br>' +
        'Planning the BI Portal<br>' +
        'Using SharePoint as the BI Portal<br>' +
        'Summary<br>' +
        'Chapter 13: Incorporating Data Mining<br>' +
        'Defining Data Mining<br>' +
        'SQL Server Data Mining Architecture Overview<br>' +
        'Microsoft Data Mining Algorithms<br>' +
        'The Data Mining Process<br>' +
        'Data Mining Examples<br>' +
        'Summary<br>' +
        'Part 4: Deploying and Managing the DW/BI System<br>' +
        'Chapter 14: Designing and Implementing Security<br>' +
        '7<br>' +
        'Identifying the Security Manager<br>' +
        'Securing the Hardware and Operating System<br>' +
        'Securing the Development Environment<br>' +
        'Securing the Data<br>' +
        'Securing the Components of the DW/BI System<br>' +
        'Usage Monitoring<br>' +
        'Summary<br>' +
        'Chapter 15: Metadata Plan<br>' +
        'Metadata Basics<br>' +
        'Metadata Standards<br>' +
        'SQL Server 2008 R2 Metadata<br>' +
        'A Practical Metadata Approach<br>' +
        'Summary<br>' +
        'Chapter 16: Deployment<br>' +
        'Setting Up the Environments<br>' +
        'Testing<br>' +
        'Deploying to Production<br>' +
        '8<br>' +
        'Data Warehouse and BI Documentation<br>' +
        'User Training<br>' +
        'User Support<br>' +
        'Desktop Readiness and Configuration<br>' +
        'Summary<br>' +
        'Chapter 17: Operations and Maintenance<br>' +
        'Providing User Support<br>' +
        'System Management<br>' +
        'Summary<br>' +
        'Chapter 18: Present Imperatives and Future Outlook<br>' +
        'Growing the DW/BI System<br>' +
        'Lifecycle Review with Common Problems<br>' +
        'What We Like in the Microsoft BI Toolset<br>' +
        'Future Directions: Room for Improvement<br>' +
        'Conclusion<br>' +
        'Index<br>' +
        'Advertisements<br>' +
        '9<br>' +
        '10<br>';
    </script>
</body>

</html>